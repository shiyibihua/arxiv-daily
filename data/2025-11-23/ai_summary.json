{
    "papers": [
        {
            "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
            "authors": [
                "Jun Cen",
                "Siteng Huang",
                "Yuqian Yuan",
                "Hangjie Yuan",
                "Chaohui Yu",
                "Yuming Jiang",
                "Jiayan Guo",
                "Kehan Li",
                "Hao Luo",
                "Fan Wang",
                "Xin Li",
                "Deli Zhao",
                "Hao Chen"
            ],
            "arxiv_id": "2511.17502v1",
            "summary": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.",
            "headline_zh": "提出统一视觉-语言-动作与世界模型，以联合学习环境动态和动作规划。",
            "intro_zh": [
                "核心问题：如何统一视觉-语言-动作模型与世界模型，以增强环境理解和动作生成。",
                "方法要点：世界模型预测未来图像状态，VLA模型生成动作，两者相互增强。",
                "实验或效果：在LIBERO仿真中达97.4%成功率，真实世界任务成功率提升50%。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "世界模型",
                "机器人任务",
                "联合学习",
                "环境动态预测"
            ],
            "_index": 0
        },
        {
            "title": "Native 3D Editing with Full Attention",
            "authors": [
                "Weiwei Cai",
                "Shuangkang Fang",
                "Weicai Ye",
                "Xin Dong",
                "Yunhan Yang",
                "Xuanyang Zhang",
                "Wei Cheng",
                "Yanpei Cao",
                "Gang Yu",
                "Tao Chen"
            ],
            "arxiv_id": "2511.17501v1",
            "summary": "Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.",
            "headline_zh": "提出原生3D编辑框架以解决现有方法速度慢和几何不一致问题",
            "intro_zh": [
                "现有3D编辑方法优化慢或2D提升导致几何不一致和质量下降",
                "构建大规模多模态数据集，探索交叉注意力和3D令牌拼接策略",
                "实验显示令牌拼接更高效，在质量、一致性和指令忠实度上领先"
            ],
            "tags_zh": [
                "3D编辑",
                "指令引导",
                "多模态数据集",
                "令牌拼接",
                "几何一致性"
            ],
            "_index": 1
        },
        {
            "title": "HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation",
            "authors": [
                "Yuezhan Tao",
                "Dexter Ong",
                "Fernando Cladera",
                "Jason Hughes",
                "Camillo J. Taylor",
                "Pratik Chaudhari",
                "Vijay Kumar"
            ],
            "arxiv_id": "2511.17497v1",
            "summary": "We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.",
            "headline_zh": "提出HALO系统，实现高海拔单目视觉语言引导的空中探索与导航",
            "intro_zh": [
                "核心问题：高海拔单目视觉实时3D重建与大规模室外环境语义探索",
                "方法要点：结合GPS/IMU，实时生成度量-语义地图，支持自然语言任务规划",
                "实验或效果：仿真中减少探索时间，真实世界验证在40米高覆盖2.46万平方米"
            ],
            "tags_zh": [
                "单目视觉导航",
                "度量-语义映射",
                "语言条件控制",
                "无人机自主探索",
                "实时3D重建"
            ],
            "_index": 2
        },
        {
            "title": "MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments",
            "authors": [
                "Zhiyu Huang",
                "Zewei Zhou",
                "Tianhui Cai",
                "Yun Zhang",
                "Jiaqi Ma"
            ],
            "arxiv_id": "2511.17496v1",
            "summary": "Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.",
            "headline_zh": "提出MDG框架以解决交通环境中多智能体行为建模的效率与通用性问题",
            "intro_zh": [
                "现有扩散和自回归方法受限于迭代采样或任务特定设计，影响效率与复用",
                "MDG通过独立噪声时空张量重构，实现局部去噪和可控轨迹生成",
                "在Waymo和nuPlan基准上实现竞争性闭环性能，并提供高效开环生成"
            ],
            "tags_zh": [
                "多智能体行为建模",
                "掩码去噪生成",
                "交通环境模拟",
                "轨迹生成",
                "自动驾驶仿真"
            ],
            "_index": 3
        },
        {
            "title": "EvDiff: High Quality Video with an Event Camera",
            "authors": [
                "Weilun Li",
                "Lei Sun",
                "Ruixi Gao",
                "Qi Jiang",
                "Yuqin Ma",
                "Kaiwei Wang",
                "Ming-Hsuan Yang",
                "Luc Van Gool",
                "Danda Pani Paudel"
            ],
            "arxiv_id": "2511.17492v1",
            "summary": "As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.",
            "headline_zh": "提出EvDiff扩散模型以从事件相机生成高质量彩色视频",
            "intro_zh": [
                "事件相机重建强度图像存在绝对亮度模糊的严重不适定问题",
                "采用代理训练框架和单步扩散模型，结合时间一致编码器降低计算成本",
                "在真实数据集上，像素级和感知指标优于现有方法，平衡保真度与真实感"
            ],
            "tags_zh": [
                "事件相机",
                "扩散模型",
                "视频生成",
                "代理训练",
                "时间一致性"
            ],
            "_index": 4
        },
        {
            "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
            "authors": [
                "Yolo Yunlong Tang",
                "Daiki Shimada",
                "Hang Hua",
                "Chao Huang",
                "Jing Bi",
                "Rogerio Feris",
                "Chenliang Xu"
            ],
            "arxiv_id": "2511.17490v1",
            "summary": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.",
            "headline_zh": "提出Video-R4模型，通过视觉反刍解决文本丰富视频中的细粒度推理问题",
            "intro_zh": [
                "核心问题：文本丰富视频中细小、瞬态文本线索易被忽略，导致幻觉和推理失败",
                "方法要点：引入视觉反刍，迭代选择帧、放大区域、重新编码像素以更新推理状态",
                "实验或效果：在M4-ViteVQA上达到SOTA，并泛化到文档和通用视频QA任务"
            ],
            "tags_zh": [
                "视频推理",
                "视觉反刍",
                "文本丰富视频",
                "强化学习",
                "多模态大模型",
                "细粒度证据"
            ],
            "_index": 5
        },
        {
            "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
            "authors": [
                "Mark Endo",
                "Serena Yeung-Levy"
            ],
            "arxiv_id": "2511.17487v1",
            "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
            "headline_zh": "提出Extract+Think方法以解决小多模态模型中感知与推理瓶颈问题",
            "intro_zh": [
                "核心问题：LLM缩小化导致视觉能力下降，可能影响感知而非仅推理",
                "方法要点：引入视觉提取调优，训练模型提取指令相关视觉细节",
                "实验或效果：结合逐步推理，提升小模型效率与性能，设定新标准"
            ],
            "tags_zh": [
                "小多模态模型",
                "视觉提取调优",
                "逐步推理",
                "感知瓶颈",
                "LLM缩小化"
            ],
            "_index": 6
        },
        {
            "title": "An Artificial Intelligence Framework for Measuring Human Spine Aging Using MRI",
            "authors": [
                "Roozbeh Bazargani",
                "Saqib Abdullah Basar",
                "Daniel Daly-Grafstein",
                "Rodrigo Solis Pompa",
                "Soojin Lee",
                "Saurabh Garg",
                "Yuntong Ma",
                "John A. Carrino",
                "Siavash Khallaghi",
                "Sam Hashemi"
            ],
            "arxiv_id": "2511.17485v1",
            "summary": "The human spine is a complex structure composed of 33 vertebrae. It holds the body and is important for leading a healthy life. The spine is vulnerable to age-related degenerations that can be identified through magnetic resonance imaging (MRI). In this paper we propose a novel computer-vison-based deep learning method to estimate spine age using images from over 18,000 MRI series. Data are restricted to subjects with only age-related spine degeneration. Eligibility criteria are created by identifying common age-based clusters of degenerative spine conditions using uniform manifold approximation and projection (UMAP) and hierarchical density-based spatial clustering of applications with noise (HDBSCAN). Model selection is determined using a detailed ablation study on data size, loss, and the effect of different spine regions. We evaluate the clinical utility of our model by calculating the difference between actual spine age and model-predicted age, the spine age gap (SAG), and examining the association between these differences and spine degenerative conditions and lifestyle factors. We find that SAG is associated with conditions including disc bulges, disc osteophytes, spinal stenosis, and fractures, as well as lifestyle factors like smoking and physically demanding work, and thus may be a useful biomarker for measuring overall spine health.",
            "headline_zh": "提出基于深度学习的MRI脊柱年龄估计框架以评估脊柱健康",
            "intro_zh": [
                "核心问题：脊柱随年龄退化影响健康，需从MRI图像中量化评估。",
                "方法要点：使用UMAP和HDBSCAN聚类筛选数据，并通过消融研究优化模型。",
                "实验效果：脊柱年龄差与退行性疾病及生活方式因素显著相关。"
            ],
            "tags_zh": [
                "脊柱年龄估计",
                "深度学习",
                "MRI分析",
                "退行性疾病",
                "生物标志物"
            ],
            "_index": 7
        },
        {
            "title": "Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions",
            "authors": [
                "Neel Sortur",
                "Justin Goodwin",
                "Purvik Patel",
                "Luis Enrique Martinez",
                "Tzofi Klinghoffer",
                "Rajmonda S. Caceres",
                "Robin Walters"
            ],
            "arxiv_id": "2511.17484v1",
            "summary": "Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.",
            "headline_zh": "提出Radar2Shape扩散模型，从部分观测高频雷达信号重建任意3D形状",
            "intro_zh": [
                "核心问题：高频雷达信号形状重建复杂，现有方法难处理部分观测和任意形状",
                "方法要点：使用去噪扩散模型，关联雷达频率与多分辨率形状特征",
                "实验或效果：在模拟和真实数据上验证，能泛化并重建任意形状"
            ],
            "tags_zh": [
                "3D形状重建",
                "高频雷达",
                "去噪扩散模型",
                "多分辨率特征",
                "部分观测信号"
            ],
            "_index": 8
        },
        {
            "title": "Counterfactual World Models via Digital Twin-conditioned Video Diffusion",
            "authors": [
                "Yiqing Shen",
                "Aiza Maksutova",
                "Chenjia Li",
                "Mathias Unberath"
            ],
            "arxiv_id": "2511.17481v1",
            "summary": "World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as \"what would happen if this object was removed?\", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.",
            "headline_zh": "提出CWMDT框架，通过数字孪生和视频扩散模型实现反事实世界建模",
            "intro_zh": [
                "核心问题：传统世界模型无法处理反事实查询，如对象移除等假设性干预。",
                "方法要点：构建数字孪生编码场景结构，利用大语言模型推理干预传播，并条件视频扩散生成序列。",
                "实验或效果：在两个基准测试中达到最先进性能，验证数字孪生作为控制信号的有效性。"
            ],
            "tags_zh": [
                "反事实世界模型",
                "数字孪生",
                "视频扩散模型",
                "大语言模型",
                "场景干预",
                "视频生成"
            ],
            "_index": 9
        },
        {
            "title": "GPR-OdomNet: Difference and Similarity-Driven Odometry Estimation Network for Ground Penetrating Radar-Based Localization",
            "authors": [
                "Huaichao Wang",
                "Xuanxin Fan",
                "Ji Liu",
                "Haifeng Li",
                "Dezhen Song"
            ],
            "arxiv_id": "2511.17457v1",
            "summary": "When performing robot/vehicle localization using ground penetrating radar (GPR) to handle adverse weather and environmental conditions, existing techniques often struggle to accurately estimate distances when processing B-scan images with minor distinctions. This study introduces a new neural network-based odometry method that leverages the similarity and difference features of GPR B-scan images for precise estimation of the Euclidean distances traveled between the B-scan images. The new custom neural network extracts multi-scale features from B-scan images taken at consecutive moments and then determines the Euclidean distance traveled by analyzing the similarities and differences between these features. To evaluate our method, an ablation study and comparison experiments have been conducted using the publicly available CMU-GPR dataset. The experimental results show that our method consistently outperforms state-of-the-art counterparts in all tests. Specifically, our method achieves a root mean square error (RMSE), and achieves an overall weighted RMSE of 0.449 m across all data sets, which is a 10.2\\% reduction in RMSE when compared to the best state-of-the-art method.",
            "headline_zh": "提出GPR-OdomNet，利用B-scan图像相似与差异特征，提升探地雷达定位精度。",
            "intro_zh": [
                "核心问题：探地雷达B-scan图像差异小，现有方法距离估计不准确。",
                "方法要点：神经网络提取多尺度特征，分析相似与差异以估计欧氏距离。",
                "实验效果：在CMU-GPR数据集上，RMSE降低10.2%，优于现有方法。"
            ],
            "tags_zh": [
                "探地雷达定位",
                "里程计估计",
                "B-scan图像处理",
                "神经网络",
                "多尺度特征提取",
                "相似性分析"
            ],
            "_index": 10
        },
        {
            "title": "Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift",
            "authors": [
                "Björn Michele",
                "Alexandre Boulch",
                "Gilles Puy",
                "Tuan-Hung Vu",
                "Renaud Marlet",
                "Nicolas Courty"
            ],
            "arxiv_id": "2511.17455v1",
            "summary": "Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.",
            "headline_zh": "改进多模态蒸馏以提升激光雷达语义分割在域偏移下的性能",
            "intro_zh": [
                "核心问题：激光雷达语义分割模型在域偏移下泛化能力差，需跨域适应。",
                "方法要点：利用视觉基础模型特征，通过无监督图像到激光雷达知识蒸馏优化。",
                "实验或效果：在四个挑战性设置中达到最先进结果，代码将开源。"
            ],
            "tags_zh": [
                "语义分割",
                "域适应",
                "知识蒸馏",
                "激光雷达点云",
                "视觉基础模型"
            ],
            "_index": 11
        },
        {
            "title": "Illustrator's Depth: Monocular Layer Index Prediction for Image Decomposition",
            "authors": [
                "Nissim Maruani",
                "Peiying Zhang",
                "Siddhartha Chaudhuri",
                "Matthew Fisher",
                "Nanxuan Zhao",
                "Vladimir G. Kim",
                "Pierre Alliez",
                "Mathieu Desbrun",
                "Wang Yifan"
            ],
            "arxiv_id": "2511.17454v1",
            "summary": "We introduce Illustrator's Depth, a novel definition of depth that addresses a key challenge in digital content creation: decomposing flat images into editable, ordered layers. Inspired by an artist's compositional process, illustrator's depth infers a layer index to each pixel, forming an interpretable image decomposition through a discrete, globally consistent ordering of elements optimized for editability. We also propose and train a neural network using a curated dataset of layered vector graphics to predict layering directly from raster inputs. Our layer index inference unlocks a range of powerful downstream applications. In particular, it significantly outperforms state-of-the-art baselines for image vectorization while also enabling high-fidelity text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive depth-aware editing. By reframing depth from a physical quantity to a creative abstraction, illustrator's depth prediction offers a new foundation for editable image decomposition.",
            "headline_zh": "提出Illustrator's Depth以解决图像分解为可编辑有序层的挑战",
            "intro_zh": [
                "核心问题：如何从平面图像分解出可编辑、有序的图层，以支持数字内容创作。",
                "方法要点：定义Illustrator's Depth，通过神经网络预测像素层索引，实现全局一致的离散排序。",
                "实验或效果：在图像矢量化、文本到矢量图形生成等应用中优于基线，支持深度感知编辑。"
            ],
            "tags_zh": [
                "图像分解",
                "层索引预测",
                "神经网络训练",
                "图像矢量化",
                "深度感知编辑"
            ],
            "_index": 12
        },
        {
            "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
            "authors": [
                "Yidong Huang",
                "Zun Wang",
                "Han Lin",
                "Dong-Ki Kim",
                "Shayegan Omidshafiei",
                "Jaehong Yoon",
                "Yue Zhang",
                "Mohit Bansal"
            ],
            "arxiv_id": "2511.17450v1",
            "summary": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
            "headline_zh": "提出SketchVerify框架，通过草图验证规划提升物理感知视频生成的动态一致性。",
            "intro_zh": [
                "核心问题：现有视频生成方法依赖单次或迭代规划，导致运动简单或计算成本高。",
                "方法要点：训练免费，基于草图验证循环预测、排序候选运动计划，确保语义对齐与物理合理性。",
                "实验或效果：在基准测试中显著提升运动质量、物理真实性和长期一致性，且效率更高。"
            ],
            "tags_zh": [
                "视频生成",
                "运动规划",
                "物理感知",
                "草图验证",
                "训练免费方法",
                "动态一致性"
            ],
            "_index": 13
        },
        {
            "title": "MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models",
            "authors": [
                "Yuqi Li",
                "Junhao Dong",
                "Chuanguang Yang",
                "Shiping Wen",
                "Piotr Koniusz",
                "Tingwen Huang",
                "Yingli Tian",
                "Yew-Soon Ong"
            ],
            "arxiv_id": "2511.17448v1",
            "summary": "Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.",
            "headline_zh": "提出多模态多教师对抗蒸馏框架以增强视觉语言模型的对抗鲁棒性",
            "intro_zh": [
                "视觉语言模型在安全关键应用中对抗鲁棒性不足，传统单教师方法知识多样性有限",
                "采用双教师知识融合架构，动态权重分配和自适应加权函数平衡模态间知识转移",
                "在ImageNet和零样本基准上，ViT-B-32模型鲁棒准确率提升4.32%，训练效率提高2.3倍"
            ],
            "tags_zh": [
                "视觉语言模型",
                "对抗鲁棒性",
                "知识蒸馏",
                "多教师学习",
                "模态融合",
                "动态权重分配"
            ],
            "_index": 14
        },
        {
            "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
            "authors": [
                "Binger Chen",
                "Tacettin Emre Bök",
                "Behnood Rasti",
                "Volker Markl",
                "Begüm Demir"
            ],
            "arxiv_id": "2511.17442v1",
            "summary": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
            "headline_zh": "提出REMSA LLM代理以解决遥感基础模型选择难题",
            "intro_zh": [
                "遥感基础模型选择困难，文档分散、格式异构、部署约束多样",
                "构建RS-FMD数据库，开发REMSA代理，通过自然语言查询自动选择模型",
                "在专家验证基准上，REMSA优于基线方法，提供透明解释"
            ],
            "tags_zh": [
                "遥感基础模型",
                "模型选择代理",
                "LLM应用",
                "多模态数据",
                "自然语言查询",
                "基准评估"
            ],
            "_index": 15
        },
        {
            "title": "RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation",
            "authors": [
                "Shihan Wu",
                "Xuecheng Liu",
                "Shaoxuan Xie",
                "Pengwei Wang",
                "Xinghang Li",
                "Bowen Yang",
                "Zhe Li",
                "Kai Zhu",
                "Hongyu Wu",
                "Yiheng Liu",
                "Zhaoye Long",
                "Yue Wang",
                "Chong Liu",
                "Dihan Wang",
                "Ziqiang Ni",
                "Xiang Yang",
                "You Liu",
                "Ruoxuan Feng",
                "Runtian Xu",
                "Lei Zhang",
                "Denghang Huang",
                "Chenghao Jin",
                "Anlan Yin",
                "Xinlong Wang",
                "Zhenguo Sun",
                "Junkai Zhao",
                "Mengfei Du",
                "Mingyu Cao",
                "Xiansheng Chen",
                "Hongyang Cheng",
                "Xiaojie Zhang",
                "Yankai Fu",
                "Ning Chen",
                "Cheng Chi",
                "Sixiang Chen",
                "Huaihai Lyu",
                "Xiaoshuai Hao",
                "Yankai Fu",
                "Yequan Wang",
                "Bo Lei",
                "Dong Liu",
                "Xi Yang",
                "Yance Jiao",
                "Tengfei Pan",
                "Yunyan Zhang",
                "Songjing Wang",
                "Ziqian Zhang",
                "Xu Liu",
                "Ji Zhang",
                "Caowei Meng",
                "Zhizheng Zhang",
                "Jiyang Gao",
                "Song Wang",
                "Xiaokun Leng",
                "Zhiqiang Xie",
                "Zhenzhen Zhou",
                "Peng Huang",
                "Wu Yang",
                "Yandong Guo",
                "Yichao Zhu",
                "Suibing Zheng",
                "Hao Cheng",
                "Xinmin Ding",
                "Yang Yue",
                "Huanqian Wang",
                "Chi Chen",
                "Jingrui Pang",
                "YuXi Qian",
                "Haoran Geng",
                "Lianli Gao",
                "Haiyuan Li",
                "Bin Fang",
                "Gao Huang",
                "Yaodong Yang",
                "Hao Dong",
                "He Wang",
                "Hang Zhao",
                "Yadong Mu",
                "Di Hu",
                "Hao Zhao",
                "Tiejun Huang",
                "Shanghang Zhang",
                "Yonghua Lin",
                "Zhongyuan Wang",
                "Guocai Yao"
            ],
            "arxiv_id": "2511.17441v1",
            "summary": "Bimanual manipulation is essential for achieving human-like dexterity in robots, but the large-scale and diverse bimanual robot datasets remain scarce due to hardware heterogeneity across robotic platforms. To address the challenge, we present RoboCOIN, a comprehensive multi-embodiment bimanual manipulation dataset with over 180,000 demonstrations collected from 15 distinct robotic platforms. The dataset covers 16 scenarios, including residential, commercial, and working environments, with 421 tasks systematically organized by bimanual coordination patterns and object properties. Our key innovation is a hierarchical capability pyramid that provides multi-level annotations, spanning trajectory-level concepts, segment-level subtasks, and frame-level kinematics. We further develop CoRobot, a comprehensive processing framework featuring Robot Trajectory Markup Language (RTML) for quality assessment, automated annotation generation, and unified multi-embodiment management. Extensive experiments demonstrate the reliability and effectiveness of RoboCOIN in multi-embodiment bimanual learning, with significant performance improvements across various model architectures and robotic platforms. The complete dataset and framework are open-sourced and publicly available for further research purposes. Project website: https://FlagOpen.github.io/RoboCOIN/.",
            "headline_zh": "提出RoboCOIN数据集以解决多平台双臂操作数据稀缺问题",
            "intro_zh": [
                "核心问题：双臂操作数据因机器人硬件异构而稀缺，限制机器人灵巧性发展。",
                "方法要点：构建大规模多平台数据集，采用分层能力金字塔进行多级标注。",
                "实验或效果：实验验证数据集在多平台双臂学习中可靠有效，提升模型性能。"
            ],
            "tags_zh": [
                "双臂操作",
                "多平台数据集",
                "分层标注",
                "机器人轨迹标记语言",
                "多体现学习",
                "机器人学习"
            ],
            "_index": 16
        },
        {
            "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation",
            "authors": [
                "Shrikant Kendre",
                "Austin Xu",
                "Honglu Zhou",
                "Michael Ryoo",
                "Shafiq Joty",
                "Juan Carlos Niebles"
            ],
            "arxiv_id": "2511.17432v1",
            "summary": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.",
            "headline_zh": "提出SMILE复合指标，结合词法和语义评估以改进问答系统评价。",
            "intro_zh": [
                "传统问答评估指标依赖n-gram，忽略深层语义理解，导致评估不准确。",
                "SMILE融合句子级和关键词级语义，并保留词法匹配，平衡精确性与相关性。",
                "在文本、图像和视频QA任务中，SMILE与人类判断高度相关，且计算轻量。"
            ],
            "tags_zh": [
                "问答系统评估",
                "语义相似度",
                "词法匹配",
                "复合指标",
                "轻量计算"
            ],
            "_index": 17
        },
        {
            "title": "Self-Supervised Learning by Curvature Alignment",
            "authors": [
                "Benyamin Ghojogh",
                "M. Hadi Sepanj",
                "Paul Fieguth"
            ],
            "arxiv_id": "2511.17426v1",
            "summary": "Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.",
            "headline_zh": "提出曲率对齐自监督学习框架以增强数据局部几何建模",
            "intro_zh": [
                "自监督学习忽略数据流形局部几何，仅关注统计特征",
                "引入曲率正则化，基于近邻余弦交互计算曲率并跨视图对齐",
                "在MNIST和CIFAR-10上实验，性能优于Barlow Twins和VICReg"
            ],
            "tags_zh": [
                "自监督学习",
                "曲率正则化",
                "数据流形",
                "冗余减少",
                "局部几何",
                "图像分类"
            ],
            "_index": 18
        },
        {
            "title": "Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers",
            "authors": [
                "Christopher Boland",
                "Sotirios Tsaftaris",
                "Sonia Dahdouh"
            ],
            "arxiv_id": "2511.17421v1",
            "summary": "Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.",
            "headline_zh": "提出中间层知识蒸馏框架以解决医学图像分析中的捷径学习问题",
            "intro_zh": [
                "核心问题：深度学习模型易学习训练数据中虚假相关特征，导致医学图像预测缺乏稳健性。",
                "方法要点：利用任务相关数据微调的教师网络，指导学生网络中间层，缓解捷径学习。",
                "实验或效果：在多个数据集上优于传统方法，接近无偏数据基线，提升泛化能力。"
            ],
            "tags_zh": [
                "医学图像分析",
                "捷径学习",
                "知识蒸馏",
                "中间层学习",
                "稳健性提升",
                "深度学习"
            ],
            "_index": 19
        },
        {
            "title": "SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding",
            "authors": [
                "Nikolay Nikolov",
                "Giuliano Albanese",
                "Sombit Dey",
                "Aleksandar Yanev",
                "Luc Van Gool",
                "Jan-Nico Zaech",
                "Danda Pani Paudel"
            ],
            "arxiv_id": "2511.17411v1",
            "summary": "Robotic Foundation Models (RFMs) hold great promise as generalist, end-to-end systems for robot control. Yet their ability to generalize across new environments, tasks, and embodiments remains limited. We argue that a major bottleneck lies in their foundations: most RFMs are built by fine-tuning internet-pretrained Vision-Language Models (VLMs). However, these VLMs are trained on 2D image-language tasks and lack the 3D spatial reasoning inherently required for embodied control in the 3D world. Bridging this gap directly with large-scale robotic data is costly and difficult to scale. Instead, we propose to enrich easy-to-collect non-robotic image data with 3D annotations and enhance a pretrained VLM with 3D understanding capabilities. Following this strategy, we train SPEAR-VLM, a 3D-aware VLM that infers object coordinates in 3D space from a single 2D image. Building on SPEAR-VLM, we introduce our main contribution, $~\\textbf{SPEAR-1}$: a robotic foundation model that integrates grounded 3D perception with language-instructed embodied control. Trained on $\\sim$45M frames from 24 Open X-Embodiment datasets, SPEAR-1 outperforms or matches state-of-the-art models such as $π_0$-FAST and $π_{0.5}$, while it uses 20$\\times$ fewer robot demonstrations. This carefully-engineered training strategy unlocks new VLM capabilities and as a consequence boosts the reliability of embodied control beyond what is achievable with only robotic data. We make our model weights and 3D-annotated datasets publicly available.",
            "headline_zh": "提出SPEAR-1机器人基础模型，通过增强3D感知解决机器人控制泛化问题",
            "intro_zh": [
                "核心问题：机器人基础模型泛化能力受限，因依赖缺乏3D空间推理的2D视觉语言模型",
                "方法要点：训练SPEAR-VLM从单张2D图像推断3D坐标，并集成到语言指令控制中",
                "实验或效果：在24个数据集上训练，性能优于或匹配先进模型，且机器人演示数据减少20倍"
            ],
            "tags_zh": [
                "机器人基础模型",
                "3D空间推理",
                "视觉语言模型",
                "单图像3D坐标推断",
                "语言指令控制",
                "数据高效训练"
            ],
            "_index": 20
        },
        {
            "title": "Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment",
            "authors": [
                "Xiaoshan Zhou",
                "Carol C. Menassa",
                "Vineet R. Kamat"
            ],
            "arxiv_id": "2511.17401v1",
            "summary": "Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.",
            "headline_zh": "提出基于体现动力学的贝叶斯学习框架，实现脑机接口辅助移动机器人的连续追踪运动控制。",
            "intro_zh": [
                "核心问题：现有脑机接口运动控制系统多限于离散命令，缺乏实时连续速度与方向调整能力。",
                "方法要点：利用体现动力学解码加速度级运动表征，结合贝叶斯推理和在线学习优化控制。",
                "实验或效果：在公开数据集上，相比基线方法，归一化均方误差降低72%，验证了方法的有效性。"
            ],
            "tags_zh": [
                "脑机接口",
                "连续运动控制",
                "贝叶斯推理",
                "体现动力学",
                "在线学习",
                "辅助机器人"
            ],
            "_index": 21
        },
        {
            "title": "Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?",
            "authors": [
                "Sukwon Yun",
                "Heming Yao",
                "Burkhard Hoeckendorf",
                "David Richmond",
                "Aviv Regev",
                "Russell Littman"
            ],
            "arxiv_id": "2511.17400v1",
            "summary": "Vision Transformers ($\\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: \"Is it necessary to model all channel interactions?\". Inspired by the philosophy of Sparse Mixture-of-Experts ($\\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.",
            "headline_zh": "提出MoE-ViT以解决多通道图像中注意力计算效率低的问题",
            "intro_zh": [
                "多通道图像中通道交互建模导致注意力计算二次增长，效率低下",
                "采用稀疏混合专家架构，将通道视为专家，轻量路由器选择相关专家",
                "在JUMP-CP和So2Sat数据集上实现效率提升，性能未损失或增强"
            ],
            "tags_zh": [
                "稀疏混合专家",
                "多通道图像",
                "视觉Transformer",
                "注意力效率",
                "计算优化"
            ],
            "_index": 22
        },
        {
            "title": "MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment",
            "authors": [
                "Huangbiao Xu",
                "Huanqi Wu",
                "Xiao Ke",
                "Junyi Wu",
                "Rui Xu",
                "Jinglin Xu"
            ],
            "arxiv_id": "2511.17397v1",
            "summary": "Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.",
            "headline_zh": "提出MCMoE框架，通过专家混合完成缺失模态，解决不完整多模态动作质量评估问题。",
            "intro_zh": [
                "核心问题：推理时模态缺失导致多模态模型失效和性能下降。",
                "方法要点：自适应门控模态生成器动态融合可用信息重构缺失模态。",
                "实验或效果：在三个公共AQA基准上实现完整和不完整多模态学习的最优结果。"
            ],
            "tags_zh": [
                "多模态动作质量评估",
                "模态缺失完成",
                "专家混合模型",
                "自适应门控生成",
                "跨模态表示学习",
                "单阶段训练"
            ],
            "_index": 23
        },
        {
            "title": "Designing and Generating Diverse, Equitable Face Image Datasets for Face Verification Tasks",
            "authors": [
                "Georgia Baltsou",
                "Ioannis Sarridis",
                "Christos Koutlis",
                "Symeon Papadopoulos"
            ],
            "arxiv_id": "2511.17393v1",
            "summary": "Face verification is a significant component of identity authentication in various applications including online banking and secure access to personal devices. The majority of the existing face image datasets often suffer from notable biases related to race, gender, and other demographic characteristics, limiting the effectiveness and fairness of face verification systems. In response to these challenges, we propose a comprehensive methodology that integrates advanced generative models to create varied and diverse high-quality synthetic face images. This methodology emphasizes the representation of a diverse range of facial traits, ensuring adherence to characteristics permissible in identity card photographs. Furthermore, we introduce the Diverse and Inclusive Faces for Verification (DIF-V) dataset, comprising 27,780 images of 926 unique identities, designed as a benchmark for future research in face verification. Our analysis reveals that existing verification models exhibit biases toward certain genders and races, and notably, applying identity style modifications negatively impacts model performance. By tackling the inherent inequities in existing datasets, this work not only enriches the discussion on diversity and ethics in artificial intelligence but also lays the foundation for developing more inclusive and reliable face verification technologies",
            "headline_zh": "提出生成多样化合成人脸图像方法以解决人脸验证数据集偏见问题",
            "intro_zh": [
                "现有数据集存在种族、性别等偏见，影响人脸验证系统公平性",
                "使用生成模型创建高质量合成图像，强调多样性和身份证照片特征",
                "构建DIF-V数据集，分析显示模型对特定群体有偏见，风格修改损害性能"
            ],
            "tags_zh": [
                "人脸验证",
                "数据集偏见",
                "生成模型",
                "合成图像",
                "公平性评估"
            ],
            "_index": 24
        },
        {
            "title": "MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration",
            "authors": [
                "Runxun Zhang",
                "Yizhou Liu",
                "Li Dongrui",
                "Bo XU",
                "Jingwei Wei"
            ],
            "arxiv_id": "2511.17392v1",
            "summary": "Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.",
            "headline_zh": "提出MorphSeek以解决医学图像配准中高维变形空间优化问题",
            "intro_zh": [
                "核心问题：可变形图像配准面临高维变形空间和体素级监督稀缺的挑战",
                "方法要点：在潜在特征空间使用高斯策略头进行细粒度优化，支持粗到精细化",
                "实验或效果：在多个3D基准测试中Dice指标提升，标签效率高且延迟低"
            ],
            "tags_zh": [
                "可变形图像配准",
                "潜在表示优化",
                "强化学习",
                "医学图像分析",
                "高斯策略",
                "弱监督学习"
            ],
            "_index": 25
        },
        {
            "title": "Human Imitated Bipedal Locomotion with Frequency Based Gait Generator Network",
            "authors": [
                "Yusuf Baran Ates",
                "Omer Morgul"
            ],
            "arxiv_id": "2511.17387v1",
            "summary": "Learning human-like, robust bipedal walking remains difficult due to hybrid dynamics and terrain variability. We propose a lightweight framework that combines a gait generator network learned from human motion with Proximal Policy Optimization (PPO) controller for torque control. Despite being trained only on flat or mildly sloped ground, the learned policies generalize to steeper ramps and rough surfaces. Results suggest that pairing spectral motion priors with Deep Reinforcement Learning (DRL) offers a practical path toward natural and robust bipedal locomotion with modest training cost.",
            "headline_zh": "提出基于频率的步态生成器网络与PPO控制器，实现仿人双足行走",
            "intro_zh": [
                "核心问题：混合动力学和地形变化使仿人双足行走学习困难",
                "方法要点：结合人类运动学习的步态生成器与PPO进行扭矩控制",
                "实验或效果：在平坦或缓坡训练，泛化到陡坡和粗糙地面"
            ],
            "tags_zh": [
                "双足行走",
                "步态生成",
                "深度强化学习",
                "PPO控制器",
                "运动模仿"
            ],
            "_index": 26
        },
        {
            "title": "IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation",
            "authors": [
                "Yifan Li",
                "Lichi Li",
                "Anh Dao",
                "Xinyu Zhou",
                "Yicheng Qiao",
                "Zheda Mai",
                "Daeun Lee",
                "Zichen Chen",
                "Zhen Tan",
                "Mohit Bansal",
                "Yu Kong"
            ],
            "arxiv_id": "2511.17384v1",
            "summary": "While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the \"collision rate\" and \"warning rate\" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.",
            "headline_zh": "提出IndustryNav基准以评估具身代理在动态工业导航中的空间推理能力",
            "intro_zh": [
                "现有具身基准在动态真实环境中空间推理评估不足，聚焦静态家庭场景",
                "基于12个高保真Unity仓库场景，结合自我中心视觉与全局里程计评估规划",
                "引入碰撞率和警告率指标，发现主流VLLMs在路径规划和避障方面存在缺陷"
            ],
            "tags_zh": [
                "具身智能",
                "空间推理",
                "动态导航",
                "工业环境",
                "基准评估",
                "碰撞避免"
            ],
            "_index": 27
        },
        {
            "title": "Non-Parametric Probabilistic Robustness: A Conservative Metric with Optimized Perturbation Distributions",
            "authors": [
                "Zheng Wang",
                "Yi Zhang",
                "Siddartha Khastgir",
                "Carsten Maple",
                "Xingyu Zhao"
            ],
            "arxiv_id": "2511.17380v1",
            "summary": "Deep learning (DL) models, despite their remarkable success, remain vulnerable to small input perturbations that can cause erroneous outputs, motivating the recent proposal of probabilistic robustness (PR) as a complementary alternative to adversarial robustness (AR). However, existing PR formulations assume a fixed and known perturbation distribution, an unrealistic expectation in practice. To address this limitation, we propose non-parametric probabilistic robustness (NPPR), a more practical PR metric that does not rely on any predefined perturbation distribution. Following the non-parametric paradigm in statistical modeling, NPPR learns an optimized perturbation distribution directly from data, enabling conservative PR evaluation under distributional uncertainty. We further develop an NPPR estimator based on a Gaussian Mixture Model (GMM) with Multilayer Perceptron (MLP) heads and bicubic up-sampling, covering various input-dependent and input-independent perturbation scenarios. Theoretical analyses establish the relationships among AR, PR, and NPPR. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet across ResNet18/50, WideResNet50 and VGG16 validate NPPR as a more practical robustness metric, showing up to 40\\% more conservative (lower) PR estimates compared to assuming those common perturbation distributions used in state-of-the-arts.",
            "headline_zh": "提出非参数概率鲁棒性以解决扰动分布未知的深度学习模型鲁棒性评估问题",
            "intro_zh": [
                "现有概率鲁棒性假设固定扰动分布，不切实际",
                "NPPR从数据学习优化扰动分布，提供保守鲁棒性评估",
                "实验在多个数据集和模型上验证NPPR更保守实用"
            ],
            "tags_zh": [
                "概率鲁棒性",
                "非参数学习",
                "深度学习安全",
                "扰动分布优化",
                "鲁棒性评估"
            ],
            "_index": 28
        },
        {
            "title": "Vector Cost Behavioral Planning for Autonomous Robotic Systems with Contemporary Validation Strategies",
            "authors": [
                "Benjamin R. Toaz",
                "Quentin Goss",
                "John Thompson",
                "Seta Boğosyan",
                "Shaunak D. Bopardikar",
                "Mustafa İlhan Akbaş",
                "Metin Gökaşan"
            ],
            "arxiv_id": "2511.17375v1",
            "summary": "The vector cost bimatrix game is a method for multi-objective decision making that enables autonomous robotic systems to optimize for multiple goals at once while avoiding worst-case scenarios in neglected objectives. We expand this approach to arbitrary numbers of objectives and compare its performance to scalar weighted sum methods during competitive motion planning. Explainable Artificial Intelligence (XAI) software is used to aid in the analysis of high dimensional decision-making data. State-space Exploration of Multidimensional Boundaries using Adherence Strategies (SEMBAS) is applied to explore performance modes in the parameter space as a sensitivity study for the baseline and proposed frameworks. While some works have explored aspects of game theoretic planning and intelligent systems validation separately, we combine each of these into a novel and comprehensive simulation pipeline. This integration demonstrates a dramatic improvement of the vector cost method over scalarization and offers an interpretable and generalizable framework for robotic behavioral planning. Code available at https://github.com/toazbenj/race_simulation. The video companion to this work is available at https://tinyurl.com/vectorcostvideo.",
            "headline_zh": "提出向量成本双矩阵游戏方法，用于自主机器人多目标行为规划，优于标量化方法。",
            "intro_zh": [
                "核心问题：自主机器人需同时优化多目标并避免最坏情况，现有标量加权和方法不足。",
                "方法要点：扩展向量成本双矩阵游戏至任意目标数，结合XAI和SEMBAS进行高维数据分析。",
                "实验或效果：仿真显示向量成本方法性能显著优于标量化，提供可解释通用框架。"
            ],
            "tags_zh": [
                "多目标决策",
                "向量成本游戏",
                "机器人行为规划",
                "可解释AI",
                "参数空间探索"
            ],
            "_index": 29
        },
        {
            "title": "Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data",
            "authors": [
                "Yixuan Pan",
                "Ruoyi Qiao",
                "Li Chen",
                "Kashyap Chitta",
                "Liang Pan",
                "Haoguang Mai",
                "Qingwen Bu",
                "Hao Zhao",
                "Cunyuan Zheng",
                "Ping Luo",
                "Hongyang Li"
            ],
            "arxiv_id": "2511.17373v1",
            "summary": "Humanoid robots are envisioned to perform a wide range of tasks in human-centered environments, requiring controllers that combine agility with robust balance. Recent advances in locomotion and whole-body tracking have enabled impressive progress in either agile dynamic skills or stability-critical behaviors, but existing methods remain specialized, focusing on one capability while compromising the other. In this work, we introduce AMS (Agility Meets Stability), the first framework that unifies both dynamic motion tracking and extreme balance maintenance in a single policy. Our key insight is to leverage heterogeneous data sources: human motion capture datasets that provide rich, agile behaviors, and physically constrained synthetic balance motions that capture stability configurations. To reconcile the divergent optimization goals of agility and stability, we design a hybrid reward scheme that applies general tracking objectives across all data while injecting balance-specific priors only into synthetic motions. Further, an adaptive learning strategy with performance-driven sampling and motion-specific reward shaping enables efficient training across diverse motion distributions. We validate AMS extensively in simulation and on a real Unitree G1 humanoid. Experiments demonstrate that a single policy can execute agile skills such as dancing and running, while also performing zero-shot extreme balance motions like Ip Man's Squat, highlighting AMS as a versatile control paradigm for future humanoid applications.",
            "headline_zh": "提出AMS框架以统一人形机器人的动态运动跟踪与极端平衡控制",
            "intro_zh": [
                "核心问题：现有方法在敏捷性与稳定性间难以兼顾，导致控制器功能单一。",
                "方法要点：利用异构数据源和混合奖励方案，结合自适应学习策略训练单一策略。",
                "实验或效果：在仿真和真实机器人上验证，能执行舞蹈、奔跑及零样本平衡动作。"
            ],
            "tags_zh": [
                "人形机器人控制",
                "异构数据学习",
                "混合奖励设计",
                "自适应训练策略",
                "动态运动跟踪",
                "平衡维护"
            ],
            "_index": 30
        },
        {
            "title": "METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model",
            "authors": [
                "Yankai Fu",
                "Ning Chen",
                "Junkai Zhao",
                "Shaozhe Shan",
                "Guocai Yao",
                "Pengwei Wang",
                "Zhongyuan Wang",
                "Shanghang Zhang"
            ],
            "arxiv_id": "2511.17366v1",
            "summary": "Building a generalist robot that can perceive, reason, and act across diverse tasks remains an open challenge, especially for dexterous manipulation. A major bottleneck lies in the scarcity of large-scale, action-annotated data for dexterous skills, as teleoperation is difficult and costly. Human data, with its vast scale and diverse manipulation behaviors, provides rich priors for learning robotic actions. While prior works have explored leveraging human demonstrations, they are often constrained by limited scenarios and a large visual gap between human and robots. To eliminate these limitations, we propose METIS, a vision-language-action (VLA) model for dexterous manipulation pretrained on multi-source egocentric datasets. We first construct EgoAtlas, which integrates large-scale human and robotic data from multiple sources, all unified under a consistent action space. We further extract motion-aware dynamics, a compact and discretized motion representation, which provides efficient and expressive supervision for VLA training. Built upon them, METIS integrates reasoning and acting into a unified framework, enabling effective deployment to downstream dexterous manipulation tasks. Our method demonstrates exceptional dexterous manipulation capabilities, achieving highest average success rate in six real-world tasks. Experimental results also highlight the superior generalization and robustness to out-of-distribution scenarios. These findings emphasize METIS as a promising step toward a generalist model for dexterous manipulation.",
            "headline_zh": "提出METIS模型，通过多源自我中心数据训练解决灵巧操作的数据稀缺问题",
            "intro_zh": [
                "核心问题：灵巧操作缺乏大规模动作标注数据，人类与机器人视觉差异大",
                "方法要点：构建EgoAtlas数据集，统一动作空间并提取运动感知动态表示",
                "实验或效果：在六项真实任务中达到最高平均成功率，泛化性强"
            ],
            "tags_zh": [
                "灵巧操作",
                "视觉语言动作模型",
                "多源数据集成",
                "运动表示",
                "自我中心视觉"
            ],
            "_index": 31
        },
        {
            "title": "SVRecon: Sparse Voxel Rasterization for Surface Reconstruction",
            "authors": [
                "Seunghun Oh",
                "Jaesung Choe",
                "Dongjae Lee",
                "Daeun Lee",
                "Seunghoon Jeong",
                "Yu-Chiang Frank Wang",
                "Jaesik Park"
            ],
            "arxiv_id": "2511.17364v1",
            "summary": "We extend the recently proposed sparse voxel rasterization paradigm to the task of high-fidelity surface reconstruction by integrating Signed Distance Function (SDF), named SVRecon. Unlike 3D Gaussians, sparse voxels are spatially disentangled from their neighbors and have sharp boundaries, which makes them prone to local minima during optimization. Although SDF values provide a naturally smooth and continuous geometric field, preserving this smoothness across independently parameterized sparse voxels is nontrivial. To address this challenge, we promote coherent and smooth voxel-wise structure through (1) robust geometric initialization using a visual geometry model and (2) a spatial smoothness loss that enforces coherent relationships across parent-child and sibling voxel groups. Extensive experiments across various benchmarks show that our method achieves strong reconstruction accuracy while having consistently speedy convergence. The code will be made public.",
            "headline_zh": "提出SVRecon方法，通过稀疏体素光栅化实现高保真表面重建",
            "intro_zh": [
                "核心问题：稀疏体素在优化中易陷入局部极小，且难以保持跨体素的平滑性。",
                "方法要点：结合SDF，采用几何初始化和空间平滑损失促进体素间一致性。",
                "实验或效果：在多个基准测试中实现高重建精度和快速收敛。"
            ],
            "tags_zh": [
                "表面重建",
                "稀疏体素光栅化",
                "符号距离函数",
                "几何优化",
                "体素平滑性"
            ],
            "_index": 32
        },
        {
            "title": "ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP",
            "authors": [
                "Linxiang Su",
                "András Balogh"
            ],
            "arxiv_id": "2511.17362v1",
            "summary": "Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.",
            "headline_zh": "提出ATAC方法以增强CLIP在测试时对抗扰动的鲁棒性",
            "intro_zh": [
                "CLIP在零样本图像-文本匹配中易受图像对抗扰动攻击",
                "ATAC在嵌入空间计算增强诱导漂移向量，基于角度一致性校正嵌入",
                "实验显示ATAC鲁棒性显著提升，平均超越先前方法近50%，计算开销低"
            ],
            "tags_zh": [
                "测试时防御",
                "对抗鲁棒性",
                "CLIP模型",
                "嵌入空间校正",
                "图像增强"
            ],
            "_index": 33
        },
        {
            "title": "SuperQuadricOcc: Multi-Layer Gaussian Approximation of Superquadrics for Real-Time Self-Supervised Occupancy Estimation",
            "authors": [
                "Seamie Hayes",
                "Reenu Mohandas",
                "Tim Brophy",
                "Alexandre Boulch",
                "Ganesh Sistu",
                "Ciaran Eising"
            ],
            "arxiv_id": "2511.17361v1",
            "summary": "Semantic occupancy estimation enables comprehensive scene understanding for automated driving, providing dense spatial and semantic information essential for perception and planning. While Gaussian representations have been widely adopted in self-supervised occupancy estimation, the deployment of a large number of Gaussian primitives drastically increases memory requirements and is not suitable for real-time inference. In contrast, superquadrics permit reduced primitive count and lower memory requirements due to their diverse shape set. However, implementation into a self-supervised occupancy model is nontrivial due to the absence of a superquadric rasterizer to enable model supervision. Our proposed method, SuperQuadricOcc, employs a superquadric-based scene representation. By leveraging a multi-layer icosphere-tessellated Gaussian approximation of superquadrics, we enable Gaussian rasterization for supervision during training. On the Occ3D dataset, SuperQuadricOcc achieves a 75\\% reduction in memory footprint, 124\\% faster inference, and a 5.9\\% improvement in mIoU compared to previous Gaussian-based methods, without the use of temporal labels. To our knowledge, this is the first occupancy model to enable real-time inference while maintaining competitive performance. The use of superquadrics reduces the number of primitives required for scene modeling by 84\\% relative to Gaussian-based approaches. Finally, evaluation against prior methods is facilitated by our fast superquadric voxelization module. The code will be released as open source.",
            "headline_zh": "提出SuperQuadricOcc以解决自动驾驶中实时语义占据估计的内存和速度问题",
            "intro_zh": [
                "高斯表示在自监督占据估计中内存需求高，不适合实时推理",
                "使用超二次曲面减少基元数量，并通过多层高斯近似实现监督训练",
                "在Occ3D数据集上内存减少75%，推理加速124%，mIoU提升5.9%"
            ],
            "tags_zh": [
                "语义占据估计",
                "超二次曲面",
                "高斯近似",
                "自监督学习",
                "实时推理",
                "自动驾驶"
            ],
            "_index": 34
        },
        {
            "title": "UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification",
            "authors": [
                "Taixi Chen",
                "Jingyun Chen",
                "Nancy Guo"
            ],
            "arxiv_id": "2511.17355v1",
            "summary": "Cell-level radiomics features provide fine-grained insights into tumor phenotypes and have the potential to significantly enhance diagnostic accuracy on hematoxylin and eosin (H&E) images. By capturing micro-level morphological and intensity patterns, these features support more precise tumor identification and improve AI interpretability by highlighting diagnostically relevant cells for pathologist review. However, most existing studies focus on slide-level or patch-level tumor classification, leaving cell-level radiomics analysis largely unexplored. Moreover, there is currently no dedicated backbone specifically designed for radiomics data. Inspired by the recent success of the Mamba architecture in vision and language domains, we introduce a Unified Attention-Mamba (UAM) backbone for cell-level classification using radiomics features. Unlike previous hybrid approaches that integrate Attention and Mamba modules in fixed proportions, our unified design flexibly combines their capabilities within a single cohesive architecture, eliminating the need for manual ratio tuning and improving encode capability. We develop two UAM variants to comprehensively evaluate the benefits of this unified structure. Building on this backbone, we further propose a multimodal UAM framework that jointly performs cell-level classification and image segmentation. Experimental results demonstrate that UAM achieves state-of-the-art performance across both tasks on public benchmarks, surpassing leading image-based foundation models. It improves cell classification accuracy from 74% to 78% ($n$=349,882 cells), and tumor segmentation precision from 75% to 80% ($n$=406 patches). These findings highlight the effectiveness and promise of UAM as a unified and extensible multimodal foundation for radiomics-driven cancer diagnosis.",
            "headline_zh": "提出统一注意力-Mamba骨干网络，用于肿瘤细胞分类的多模态框架",
            "intro_zh": [
                "现有研究多关注切片或斑块级肿瘤分类，细胞级放射组学分析未被充分探索",
                "统一设计灵活结合注意力和Mamba模块，无需手动调整比例，提升编码能力",
                "实验显示细胞分类准确率从74%提升至78%，肿瘤分割精度从75%提升至80%"
            ],
            "tags_zh": [
                "细胞级放射组学",
                "注意力机制",
                "Mamba架构",
                "多模态学习",
                "肿瘤分类",
                "图像分割"
            ],
            "_index": 35
        },
        {
            "title": "DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture",
            "authors": [
                "Xiangteng He",
                "Shunsuke Sakai",
                "Kun Yuan",
                "Nicolas Padoy",
                "Tatsuhito Hasegawa",
                "Leonid Sigal"
            ],
            "arxiv_id": "2511.17354v1",
            "summary": "Image-based Joint-Embedding Predictive Architecture (I-JEPA) learns visual representations by predicting latent embeddings of masked regions from visible context. However, it treats all regions uniformly and independently, lacking an explicit notion of where or in what order predictions should be made. Inspired by human visual perception, which deploys attention selectively and sequentially from the most informative to secondary regions, we propose DSeq-JEPA, a Discriminative Sequential Joint-Embedding Predictive Architecture that bridges predictive and autoregressive self-supervised learning, integrating JEPA-style latent prediction with GPT-style sequential reasoning. Specifically, DSeq-JEPA (i) first identifies primary discriminative regions based on a transformer-derived saliency map, emphasizing the distribution of visual importance, and then (ii) predicts subsequent regions in this discriminative order, progressively forming a curriculum-like semantic progression from primary to secondary cues -- a form of GPT-style pre-training. Extensive experiments across diverse tasks, including image classification (ImageNet), fine-grained visual categorization (iNaturalist21, CUB-200-2011, Stanford-Cars), detection and segmentation (MS-COCO, ADE20K), and low-level reasoning tasks (Clevr/Count, Clevr/Dist), demonstrate that DSeq-JEPA consistently focuses on more discriminative and generalizable representations than I-JEPA variants. Project page: https://github.com/SkyShunsuke/DSeq-JEPA.",
            "headline_zh": "提出DSeq-JEPA以改进图像表示学习，通过顺序预测增强判别性。",
            "intro_zh": [
                "I-JEPA在预测掩码区域时缺乏顺序和判别性，导致表示学习不充分。",
                "DSeq-JEPA结合JEPA潜在预测与GPT顺序推理，按显著性顺序预测区域。",
                "实验显示在分类、检测等任务中，DSeq-JEPA比I-JEPA变体更有效。"
            ],
            "tags_zh": [
                "自监督学习",
                "图像表示学习",
                "顺序预测",
                "判别性区域",
                "联合嵌入预测"
            ],
            "_index": 36
        },
        {
            "title": "Learning Latent Transmission and Glare Maps for Lens Veiling Glare Removal",
            "authors": [
                "Xiaolong Qian",
                "Qi Jiang",
                "Lei Sun",
                "Zongxi Yu",
                "Kailun Yang",
                "Peixuan Wu",
                "Jiacheng Zhou",
                "Yao Gao",
                "Yaoguang Ma",
                "Ming-Hsuan Yang",
                "Kaiwei Wang"
            ],
            "arxiv_id": "2511.17353v1",
            "summary": "Beyond the commonly recognized optical aberrations, the imaging performance of compact optical systems-including single-lens and metalens designs-is often further degraded by veiling glare caused by stray-light scattering from non-ideal optical surfaces and coatings, particularly in complex real-world environments. This compound degradation undermines traditional lens aberration correction yet remains underexplored. A major challenge is that conventional scattering models (e.g., for dehazing) fail to fit veiling glare due to its spatial-varying and depth-independent nature. Consequently, paired high-quality data are difficult to prepare via simulation, hindering application of data-driven veiling glare removal models. To this end, we propose VeilGen, a generative model that learns to simulate veiling glare by estimating its underlying optical transmission and glare maps in an unsupervised manner from target images, regularized by Stable Diffusion (SD)-based priors. VeilGen enables paired dataset generation with realistic compound degradation of optical aberrations and veiling glare, while also providing the estimated latent optical transmission and glare maps to guide the veiling glare removal process. We further introduce DeVeiler, a restoration network trained with a reversibility constraint, which utilizes the predicted latent maps to guide an inverse process of the learned scattering model. Extensive experiments on challenging compact optical systems demonstrate that our approach delivers superior restoration quality and physical fidelity compared with existing methods. These suggest that VeilGen reliably synthesizes realistic veiling glare, and its learned latent maps effectively guide the restoration process in DeVeiler. All code and datasets will be publicly released at https://github.com/XiaolongQian/DeVeiler.",
            "headline_zh": "提出VeilGen和DeVeiler以解决紧凑光学系统中的杂散光去除问题",
            "intro_zh": [
                "紧凑光学系统因杂散光导致图像退化，传统散射模型难以拟合",
                "VeilGen无监督学习潜在传输和眩光图，结合SD先验生成配对数据",
                "DeVeiler利用潜在图指导恢复，实验显示优于现有方法的性能"
            ],
            "tags_zh": [
                "杂散光去除",
                "生成模型",
                "无监督学习",
                "光学系统恢复",
                "潜在图估计"
            ],
            "_index": 37
        },
        {
            "title": "Label-Efficient Skeleton-based Recognition with Stable-Invertible Graph Convolutional Networks",
            "authors": [
                "Hichem Sahbi"
            ],
            "arxiv_id": "2511.17345v1",
            "summary": "Skeleton-based action recognition is a hotspot in image processing. A key challenge of this task lies in its dependence on large, manually labeled datasets whose acquisition is costly and time-consuming. This paper devises a novel, label-efficient method for skeleton-based action recognition using graph convolutional networks (GCNs). The contribution of the proposed method resides in learning a novel acquisition function -- scoring the most informative subsets for labeling -- as the optimum of an objective function mixing data representativity, diversity and uncertainty. We also extend this approach by learning the most informative subsets using an invertible GCN which allows mapping data from ambient to latent spaces where the inherent distribution of the data is more easily captured. Extensive experiments, conducted on two challenging skeleton-based recognition datasets, show the effectiveness and the outperformance of our label-frugal GCNs against the related work.",
            "headline_zh": "提出稳定可逆图卷积网络以解决骨架动作识别中的标签效率问题",
            "intro_zh": [
                "核心问题：骨架动作识别依赖大量手动标注数据，获取成本高且耗时。",
                "方法要点：学习新颖采集函数，优化数据代表性、多样性和不确定性，并使用可逆GCN映射数据。",
                "实验或效果：在两个挑战性数据集上验证，方法优于相关工作，实现标签高效识别。"
            ],
            "tags_zh": [
                "骨架动作识别",
                "图卷积网络",
                "标签效率",
                "可逆网络",
                "数据采集函数"
            ],
            "_index": 38
        },
        {
            "title": "Loomis Painter: Reconstructing the Painting Process",
            "authors": [
                "Markus Pobitzer",
                "Chang Liu",
                "Chenyi Zhuang",
                "Teng Long",
                "Bin Ren",
                "Nicu Sebe"
            ],
            "arxiv_id": "2511.17344v1",
            "summary": "Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.",
            "headline_zh": "提出统一框架以解决多媒介绘画过程生成中的一致性问题",
            "intro_zh": [
                "现有绘画教程视频缺乏交互性，生成模型存在跨媒介和时序不一致问题",
                "采用语义驱动风格控制、跨媒介风格增强和反向绘画训练策略",
                "构建大规模数据集，评估跨媒介一致性、时序连贯性和图像保真度"
            ],
            "tags_zh": [
                "绘画过程生成",
                "扩散模型",
                "风格控制",
                "跨媒介一致性",
                "时序连贯性",
                "数据集构建"
            ],
            "_index": 39
        },
        {
            "title": "Refracting Reality: Generating Images with Realistic Transparent Objects",
            "authors": [
                "Yue Yin",
                "Enze Tao",
                "Dylan Campbell"
            ],
            "arxiv_id": "2511.17340v1",
            "summary": "Generative image models can produce convincingly real images, with plausible shapes, textures, layouts and lighting. However, one domain in which they perform notably poorly is in the synthesis of transparent objects, which exhibit refraction, reflection, absorption and scattering. Refraction is a particular challenge, because refracted pixel rays often intersect with surfaces observed in other parts of the image, providing a constraint on the color. It is clear from inspection that generative models have not distilled the laws of optics sufficiently well to accurately render refractive objects. In this work, we consider the problem of generating images with accurate refraction, given a text prompt. We synchronize the pixels within the object's boundary with those outside by warping and merging the pixels using Snell's Law of Refraction, at each step of the generation trajectory. For those surfaces that are not directly observed in the image, but are visible via refraction or reflection, we recover their appearance by synchronizing the image with a second generated image -- a panorama centered at the object -- using the same warping and merging procedure. We demonstrate that our approach generates much more optically-plausible images that respect the physical constraints.",
            "headline_zh": "提出基于折射定律的图像生成方法，以解决透明物体渲染不准确的问题。",
            "intro_zh": [
                "核心问题：生成模型在透明物体渲染中折射效果差，未充分学习光学规律。",
                "方法要点：使用斯涅尔折射定律同步像素，结合全景图像恢复不可见表面。",
                "实验或效果：生成图像在光学合理性上显著提升，符合物理约束。"
            ],
            "tags_zh": [
                "图像生成",
                "透明物体渲染",
                "折射模拟",
                "光学约束",
                "像素同步"
            ],
            "_index": 40
        },
        {
            "title": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM",
            "authors": [
                "Chiori Hori",
                "Yoshiki Masuyama",
                "Siddarth Jain",
                "Radu Corcodel",
                "Devesh Jha",
                "Diego Romeres",
                "Jonathan Le Roux"
            ],
            "arxiv_id": "2511.17335v1",
            "summary": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.",
            "headline_zh": "提出长上下文Q-Former与多模态LLM集成，以改进人机交互中的动作确认与规划。",
            "intro_zh": [
                "核心问题：现有方法依赖片段级处理，未利用长视频上下文信息。",
                "方法要点：引入长上下文Q-Former，整合左右上下文依赖和文本嵌入。",
                "实验或效果：在YouCook2语料库中，长上下文Q-Former提升确认生成和动作规划性能。"
            ],
            "tags_zh": [
                "长上下文理解",
                "多模态LLM",
                "人机交互",
                "动作确认生成",
                "视频理解"
            ],
            "_index": 41
        },
        {
            "title": "NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior",
            "authors": [
                "Dongbo Shi",
                "Shen Cao",
                "Bojian Wu",
                "Jinhui Guo",
                "Lubin Fan",
                "Renjie Chen",
                "Ligang Liu",
                "Jieping Ye"
            ],
            "arxiv_id": "2511.17322v1",
            "summary": "In this paper, we introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors. Existing methods, particularly NoPe-NeRF, which focus solely on the local relationships within images, often struggle to recover accurate camera poses in complex scenarios. To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation. This method significantly improves the quality of initial poses. Additionally, we introduce global optimization phase that incorporates geometric consistency constraints through bundle adjustment, which integrates feature trajectories to further refine poses and collectively boost the quality of NeRF. Notably, our method is the first work that seamlessly combines the local and global cues with NeRF, and outperforms state-of-the-art methods in both pose estimation accuracy and novel view synthesis. Extensive evaluations on benchmark datasets demonstrate our superior performance and robustness, even in challenging scenes, thus validating our design choices.",
            "headline_zh": "提出NoPe-NeRF++，通过局部到全局优化在无姿态先验下训练NeRF。",
            "intro_zh": [
                "核心问题：现有方法在复杂场景中难以恢复准确相机姿态。",
                "方法要点：结合局部联合优化与全局束调整，提升姿态估计。",
                "实验或效果：在基准数据集上优于先进方法，验证鲁棒性。"
            ],
            "tags_zh": [
                "神经辐射场",
                "相机姿态估计",
                "局部优化",
                "全局优化",
                "束调整",
                "新视角合成"
            ],
            "_index": 42
        },
        {
            "title": "FORWARD: Dataset of a forwarder operating in rough terrain",
            "authors": [
                "Mikael Lundbäck",
                "Erik Wallin",
                "Carola Häggström",
                "Mattias Nyström",
                "Andreas Grönlund",
                "Mats Richardson",
                "Petrus Jönsson",
                "William Arnvik",
                "Lucas Hedström",
                "Arvid Fälldin",
                "Martin Servin"
            ],
            "arxiv_id": "2511.17318v1",
            "summary": "We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.",
            "headline_zh": "提出FORWARD数据集以支持森林机械在崎岖地形中的可通行性、感知和自主控制研究",
            "intro_zh": [
                "核心问题：森林机械在崎岖地形中作业时，如何提升可通行性、感知和自主控制能力。",
                "方法要点：提供高分辨率多模态数据，包括传感器记录、视频和地形数据，覆盖18小时作业。",
                "实验或效果：数据集用于开发AI模型和算法，支持模拟器校准和自动化场景测试。"
            ],
            "tags_zh": [
                "多模态数据集",
                "森林机械",
                "可通行性研究",
                "自主控制",
                "传感器融合",
                "地形感知"
            ],
            "_index": 43
        },
        {
            "title": "MuM: Multi-View Masked Image Modeling for 3D Vision",
            "authors": [
                "David Nordström",
                "Johan Edstedt",
                "Fredrik Kahl",
                "Georg Bökman"
            ],
            "arxiv_id": "2511.17309v1",
            "summary": "Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.",
            "headline_zh": "提出多视图掩码图像建模方法以增强3D视觉特征学习",
            "intro_zh": [
                "核心问题：现有自监督学习模型如DINOv3侧重语义理解，缺乏几何推理能力。",
                "方法要点：扩展MAE至多视图，统一掩码并使用轻量解码器与帧间注意力。",
                "实验或效果：在重建、匹配和姿态估计任务中优于DINOv3和CroCo v2。"
            ],
            "tags_zh": [
                "多视图学习",
                "掩码图像建模",
                "3D视觉",
                "自监督学习",
                "几何推理"
            ],
            "_index": 44
        },
        {
            "title": "SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion",
            "authors": [
                "Jiajie Guo",
                "Qingpeng Zhu",
                "Jin Zeng",
                "Xiaolong Wu",
                "Changyong He",
                "Weida Wang"
            ],
            "arxiv_id": "2511.17308v1",
            "summary": "Multimodal large language models (MLLMs) have achieved significant progress in image and language tasks due to the strong reasoning capability of large language models (LLMs). Nevertheless, most MLLMs suffer from limited spatial reasoning ability to interpret and infer spatial arrangements in three-dimensional space. In this work, we propose a novel vision encoder based on hierarchical fusion of geometry and semantics features, generating spatial-aware visual embedding and boosting the spatial grounding capability of MLLMs. Specifically, we first unveil that the spatial ambiguity shortcoming stems from the lossy embedding of the vision encoder utilized in most existing MLLMs (e.g., CLIP), restricted to instance-level semantic features. This motivates us to complement CLIP with the geometry features from vision-only self-supervised learning via a hierarchical adapter, enhancing the spatial awareness in the proposed SpatialGeo. The network is efficiently trained using pretrained LLaVA model and optimized with random feature dropping to avoid trivial solutions relying solely on the CLIP encoder. Experimental results show that SpatialGeo improves the accuracy in spatial reasoning tasks, enhancing state-of-the-art models by at least 8.0% in SpatialRGPT-Bench with approximately 50% less memory cost during inference. The source code is available via https://ricky-plus.github.io/SpatialGeoPages/.",
            "headline_zh": "提出SpatialGeo通过几何-语义融合增强多模态大语言模型的空间推理能力",
            "intro_zh": [
                "核心问题：现有MLLMs空间推理能力弱，源于视觉编码器嵌入损失和空间模糊性。",
                "方法要点：基于CLIP补充几何特征，使用分层适配器融合几何与语义特征。",
                "实验或效果：在SpatialRGPT-Bench上准确率提升至少8.0%，推理内存成本降低约50%。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "空间推理",
                "几何-语义融合",
                "视觉编码器",
                "分层适配器",
                "内存优化"
            ],
            "_index": 45
        },
        {
            "title": "BiFingerPose: Bimodal Finger Pose Estimation for Touch Devices",
            "authors": [
                "Xiongjun Guan",
                "Zhiyu Pan",
                "Jianjiang Feng",
                "Jie Zhou"
            ],
            "arxiv_id": "2511.17306v1",
            "summary": "Finger pose offers promising opportunities to expand human computer interaction capability of touchscreen devices. Existing finger pose estimation algorithms that can be implemented in portable devices predominantly rely on capacitive images, which are currently limited to estimating pitch and yaw angles and exhibit reduced accuracy when processing large-angle inputs (especially when it is greater than 45 degrees). In this paper, we propose BiFingerPose, a novel bimodal based finger pose estimation algorithm capable of simultaneously and accurately predicting comprehensive finger pose information. A bimodal input is explored, including a capacitive image and a fingerprint patch obtained from the touchscreen with an under-screen fingerprint sensor. Our approach leads to reliable estimation of roll angle, which is not achievable using only a single modality. In addition, the prediction performance of other pose parameters has also been greatly improved. The evaluation of a 12-person user study on continuous and discrete interaction tasks further validated the advantages of our approach. Specifically, BiFingerPose outperforms previous SOTA methods with over 21% improvement in prediction performance, 2.5 times higher task completion efficiency, and 23% better user operation accuracy, demonstrating its practical superiority. Finally, we delineate the application space of finger pose with respect to enhancing authentication security and improving interactive experiences, and develop corresponding prototypes to showcase the interaction potential. Our code will be available at https://github.com/XiongjunGuan/DualFingerPose.",
            "headline_zh": "提出BiFingerPose以解决触摸设备上手指姿态估计的精度和角度限制问题",
            "intro_zh": [
                "现有电容图像方法估计手指姿态时，限于俯仰和偏航角，大角度输入精度下降",
                "采用双模态输入，结合电容图像和指纹补丁，可靠估计滚转角并提升其他参数性能",
                "用户研究显示，预测性能提升超21%，任务完成效率提高2.5倍，操作准确率提升23%"
            ],
            "tags_zh": [
                "手指姿态估计",
                "双模态融合",
                "触摸设备交互",
                "电容图像",
                "指纹传感器",
                "人机交互"
            ],
            "_index": 46
        },
        {
            "title": "MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning",
            "authors": [
                "Wenrui Zhang",
                "Xinggang Wang",
                "Bin Feng",
                "Wenyu Liu"
            ],
            "arxiv_id": "2511.17300v1",
            "summary": "Optical Chemical Structure Recognition (OCSR) plays a pivotal role in modern chemical informatics, enabling the automated conversion of chemical structure images from scientific literature, patents, and educational materials into machine-readable molecular representations. This capability is essential for large-scale chemical data mining, drug discovery pipelines, and Large Language Model (LLM) applications in related domains. However, existing OCSR systems face significant challenges in accurately recognizing stereochemical information due to the subtle visual cues that distinguish stereoisomers, such as wedge and dash bonds, ring conformations, and spatial arrangements. To address these challenges, we propose MolSight, a comprehensive learning framework for OCSR that employs a three-stage training paradigm. In the first stage, we conduct pre-training on large-scale but noisy datasets to endow the model with fundamental perception capabilities for chemical structure images. In the second stage, we perform multi-granularity fine-tuning using datasets with richer supervisory signals, systematically exploring how auxiliary tasks-specifically chemical bond classification and atom localization-contribute to molecular formula recognition. Finally, we employ reinforcement learning for post-training optimization and introduce a novel stereochemical structure dataset. Remarkably, we find that even with MolSight's relatively compact parameter size, the Group Relative Policy Optimization (GRPO) algorithm can further enhance the model's performance on stereomolecular. Through extensive experiments across diverse datasets, our results demonstrate that MolSight achieves state-of-the-art performance in (stereo)chemical optical structure recognition.",
            "headline_zh": "提出MolSight框架以解决光学化学结构识别中立体化学信息准确识别难题",
            "intro_zh": [
                "核心问题：现有系统难以准确识别立体化学信息，如楔形键和环构象",
                "方法要点：采用三阶段训练，包括SMILES预训练、多粒度学习和强化学习优化",
                "实验或效果：在多样化数据集上实现最先进性能，尤其在立体分子识别中表现突出"
            ],
            "tags_zh": [
                "光学化学结构识别",
                "SMILES预训练",
                "多粒度学习",
                "强化学习优化",
                "立体化学识别",
                "分子表示转换"
            ],
            "_index": 47
        },
        {
            "title": "MonoSpheres: Large-Scale Monocular SLAM-Based UAV Exploration through Perception-Coupled Mapping and Planning",
            "authors": [
                "Tomáš Musil",
                "Matěj Petrlík",
                "Martin Saska"
            ],
            "arxiv_id": "2511.17299v1",
            "summary": "Autonomous exploration of unknown environments is a key capability for mobile robots, but it is largely unsolved for robots equipped with only a single monocular camera and no dense range sensors. In this paper, we present a novel approach to monocular vision-based exploration that can safely cover large-scale unstructured indoor and outdoor 3D environments by explicitly accounting for the properties of a sparse monocular SLAM frontend in both mapping and planning. The mapping module solves the problems of sparse depth data, free-space gaps, and large depth uncertainty by oversampling free space in texture-sparse areas and keeping track of obstacle position uncertainty. The planning module handles the added free-space uncertainty through rapid replanning and perception-aware heading control. We further show that frontier-based exploration is possible with sparse monocular depth data when parallax requirements and the possibility of textureless surfaces are taken into account. We evaluate our approach extensively in diverse real-world and simulated environments, including ablation studies. To the best of the authors' knowledge, the proposed method is the first to achieve 3D monocular exploration in real-world unstructured outdoor environments. We open-source our implementation to support future research.",
            "headline_zh": "提出MonoSpheres方法，通过感知耦合的建图与规划实现单目SLAM无人机大规模探索",
            "intro_zh": [
                "核心问题：单目相机在未知环境中自主探索时，深度数据稀疏、自由空间间隙和深度不确定性大。",
                "方法要点：建图模块过采样纹理稀疏区域，规划模块快速重规划并感知感知航向控制。",
                "实验或效果：在真实和模拟环境中广泛评估，首次实现真实世界非结构化户外3D单目探索。"
            ],
            "tags_zh": [
                "单目SLAM",
                "无人机探索",
                "感知耦合规划",
                "稀疏深度建图",
                "前沿探索",
                "不确定性处理"
            ],
            "_index": 48
        },
        {
            "title": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation",
            "authors": [
                "Chuancheng Shi",
                "Shangze Li",
                "Shiming Guo",
                "Simiao Xie",
                "Wenhua Wu",
                "Jingtong Dou",
                "Chao Wu",
                "Canran Xiao",
                "Cong Wang",
                "Zifeng Cheng",
                "Fei Shen",
                "Tat-Seng Chua"
            ],
            "arxiv_id": "2511.17282v1",
            "summary": "Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.",
            "headline_zh": "提出神经元激活与层增强方法以解决多语言文本到图像生成中的文化偏差问题",
            "intro_zh": [
                "核心问题：多语言T2I模型输出存在文化中性或英语偏见，源于文化相关表征激活不足",
                "方法要点：定位文化敏感神经元，采用推理时激活和层目标增强策略提升文化一致性",
                "实验或效果：在CultureBench上验证，文化一致性提升，同时保持保真度和多样性"
            ],
            "tags_zh": [
                "文本到图像生成",
                "文化一致性",
                "神经元激活",
                "层目标增强",
                "多语言模型",
                "文化偏差"
            ],
            "_index": 49
        },
        {
            "title": "Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data",
            "authors": [
                "Julien Merand",
                "Boris Meden",
                "Mathieu Grossard"
            ],
            "arxiv_id": "2511.17276v1",
            "summary": "This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.",
            "headline_zh": "提出基于CVAE的方法从点云数据估计多指抓手的关节配置",
            "intro_zh": [
                "核心问题：从多指抓手的点云数据中高效确定关节配置，避免传统逆运动学方法的复杂决策或近似求解。",
                "方法要点：使用条件变分自编码器，以关键结构点云为输入，重构对应的关节配置。",
                "实验或效果：在MultiDex数据集上验证，运行时间0.05毫秒，精度与先进方法相当。"
            ],
            "tags_zh": [
                "关节配置估计",
                "条件变分自编码器",
                "点云数据处理",
                "多指抓手",
                "抓取规划",
                "逆运动学"
            ],
            "_index": 50
        },
        {
            "title": "Range-Edit: Semantic Mask Guided Outdoor LiDAR Scene Editing",
            "authors": [
                "Suchetan G. Uppur",
                "Hemant Kumar",
                "Vaibhav Kumar"
            ],
            "arxiv_id": "2511.17269v1",
            "summary": "Training autonomous driving and navigation systems requires large and diverse point cloud datasets that capture complex edge case scenarios from various dynamic urban settings. Acquiring such diverse scenarios from real-world point cloud data, especially for critical edge cases, is challenging, which restricts system generalization and robustness. Current methods rely on simulating point cloud data within handcrafted 3D virtual environments, which is time-consuming, computationally expensive, and often fails to fully capture the complexity of real-world scenes. To address some of these issues, this research proposes a novel approach that addresses the problem discussed by editing real-world LiDAR scans using semantic mask-based guidance to generate novel synthetic LiDAR point clouds. We incorporate range image projection and semantic mask conditioning to achieve diffusion-based generation. Point clouds are transformed to 2D range view images, which are used as an intermediate representation to enable semantic editing using convex hull-based semantic masks. These masks guide the generation process by providing information on the dimensions, orientations, and locations of objects in the real environment, ensuring geometric consistency and realism. This approach demonstrates high-quality LiDAR point cloud generation, capable of producing complex edge cases and dynamic scenes, as validated on the KITTI-360 dataset. This offers a cost-effective and scalable solution for generating diverse LiDAR data, a step toward improving the robustness of autonomous driving systems.",
            "headline_zh": "提出基于语义掩码的LiDAR场景编辑方法，以生成多样合成点云数据。",
            "intro_zh": [
                "核心问题：真实LiDAR数据难以获取复杂边缘案例，限制自动驾驶系统泛化。",
                "方法要点：使用范围图像投影和语义掩码引导扩散模型，实现几何一致编辑。",
                "实验或效果：在KITTI-360数据集验证，能生成高质量动态场景和边缘案例。"
            ],
            "tags_zh": [
                "LiDAR点云编辑",
                "语义掩码引导",
                "扩散模型生成",
                "自动驾驶数据增强",
                "范围图像投影"
            ],
            "_index": 51
        },
        {
            "title": "Simulation of Active Soft Nets for Capture of Space Debris",
            "authors": [
                "Leone Costi",
                "Dario Izzo"
            ],
            "arxiv_id": "2511.17266v1",
            "summary": "In this work, we propose a simulator, based on the open-source physics engine MuJoCo, for the design and control of soft robotic nets for the autonomous removal of space debris. The proposed simulator includes net dynamics, contact between the net and the debris, self-contact of the net, orbital mechanics, and a controller that can actuate thrusters on the four satellites at the corners of the net. It showcases the case of capturing Envisat, a large ESA satellite that remains in orbit as space debris following the end of its mission. This work investigates different mechanical models, which can be used to simulate the net dynamics, simulating various degrees of compliance, and different control strategies to achieve the capture of the debris, depending on the relative position of the net and the target. Unlike previous works on this topic, we do not assume that the net has been previously ballistically thrown toward the target, and we start from a relatively static configuration. The results show that a more compliant net achieves higher performance when attempting the capture of Envisat. Moreover, when paired with a sliding mode controller, soft nets are able to achieve successful capture in 100% of the tested cases, whilst also showcasing a higher effective area at contact and a higher number of contact points between net and Envisat.",
            "headline_zh": "提出基于MuJoCo的软网模拟器，用于空间碎片自主捕获",
            "intro_zh": [
                "核心问题：空间碎片自主捕获，需模拟软网动态与轨道力学",
                "方法要点：集成软网动态、接触模型和控制器，支持不同柔顺度",
                "实验或效果：软网与滑模控制器结合，捕获成功率100%，接触点更多"
            ],
            "tags_zh": [
                "软机器人",
                "空间碎片捕获",
                "物理模拟",
                "滑模控制",
                "轨道力学"
            ],
            "_index": 52
        },
        {
            "title": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback",
            "authors": [
                "Bulat Khaertdinov",
                "Mirela Popa",
                "Nava Tintarev"
            ],
            "arxiv_id": "2511.17255v1",
            "summary": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.",
            "headline_zh": "提出基于相关性反馈的机制，以提升视觉语言模型的文本到图像检索性能",
            "intro_zh": [
                "核心问题：视觉语言模型检索性能提升常需微调或更大模型，缺乏推理时优化方法",
                "方法要点：引入四种反馈策略，包括伪相关反馈、生成相关反馈和注意力反馈摘要器",
                "实验或效果：在Flickr30k和COCO数据集上，反馈策略使MRR@5提升1-5%，增强鲁棒性"
            ],
            "tags_zh": [
                "视觉语言模型",
                "文本到图像检索",
                "相关性反馈",
                "伪相关反馈",
                "生成相关反馈",
                "注意力反馈摘要器"
            ],
            "_index": 53
        },
        {
            "title": "Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats",
            "authors": [
                "Jiaye Qian",
                "Ge Zheng",
                "Yuchen Zhu",
                "Sibei Yang"
            ],
            "arxiv_id": "2511.17254v1",
            "summary": "Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.",
            "headline_zh": "提出统一干预框架以缓解多模态大模型在不同对齐格式下的幻觉问题",
            "intro_zh": [
                "核心问题：多模态大模型在图像-文本任务中易产生幻觉，源于多个因果路径的交互作用",
                "方法要点：基于Transformer因果架构，识别并干预关键幻觉头，适应判别式和生成式对齐格式",
                "实验或效果：在多个基准测试中，该方法一致减少不同对齐类型的幻觉"
            ],
            "tags_zh": [
                "多模态大模型",
                "幻觉缓解",
                "因果路径干预",
                "对齐格式适应",
                "Transformer架构"
            ],
            "_index": 54
        },
        {
            "title": "Blind Deconvolution for Color Images Using Normalized Quaternion Kernels",
            "authors": [
                "Yuming Yang",
                "Michael K. Ng",
                "Zhigang Jia",
                "Wei Wang"
            ],
            "arxiv_id": "2511.17253v1",
            "summary": "In this work, we address the challenging problem of blind deconvolution for color images. Existing methods often convert color images to grayscale or process each color channel separately, which overlooking the relationships between color channels. To handle this issue, we formulate a novel quaternion fidelity term designed specifically for color image blind deconvolution. This fidelity term leverages the properties of quaternion convolution kernel, which consists of four kernels: one that functions similarly to a non-negative convolution kernel to capture the overall blur, and three additional convolution kernels without constraints corresponding to red, green and blue channels respectively model their unknown interdependencies. In order to preserve image intensity, we propose to use the normalized quaternion kernel in the blind deconvolution process. Extensive experiments on real datasets of blurred color images show that the proposed method effectively removes artifacts and significantly improves deblurring effect, demonstrating its potential as a powerful tool for color image deconvolution.",
            "headline_zh": "提出归一化四元数核方法以解决彩色图像盲去卷积问题",
            "intro_zh": [
                "核心问题：彩色图像盲去卷积中忽略颜色通道间关系，导致去模糊效果不佳。",
                "方法要点：设计四元数保真项，利用四元数卷积核建模颜色通道间未知依赖关系。",
                "实验或效果：在真实模糊彩色图像数据集上验证，有效去除伪影并显著提升去模糊效果。"
            ],
            "tags_zh": [
                "彩色图像盲去卷积",
                "四元数核",
                "归一化卷积",
                "颜色通道关系",
                "图像去模糊"
            ],
            "_index": 55
        },
        {
            "title": "Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning",
            "authors": [
                "Mohammed Alnemari"
            ],
            "arxiv_id": "2511.17242v1",
            "summary": "This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.\n  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.\n  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks.",
            "headline_zh": "提出等变感知结构化剪枝框架，用于资源受限边缘部署，结合自适应微调。",
            "intro_zh": [
                "核心问题：资源受限环境中，保持几何变换不变性的模型压缩需求。",
                "方法要点：结合G-CNNs与等变感知结构化剪枝，保留等变属性并减少参数。",
                "实验效果：在EuroSAT等数据集上，参数减少29.3%，精度恢复显著。"
            ],
            "tags_zh": [
                "等变卷积网络",
                "结构化剪枝",
                "自适应微调",
                "模型压缩",
                "几何变换鲁棒性",
                "边缘部署"
            ],
            "_index": 56
        },
        {
            "title": "Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables",
            "authors": [
                "Anshul Singh",
                "Rohan Chaudhary",
                "Gagneet Singh",
                "Abhay Kumary"
            ],
            "arxiv_id": "2511.17238v1",
            "summary": "The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \\textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.",
            "headline_zh": "提出MirageTVQA基准以评估视觉语言模型在真实世界多语言和噪声表格中的表现",
            "intro_zh": [
                "现有表格问答数据集多为英语且格式完美，与现实场景存在差距",
                "构建多语言含噪声表格基准，包含近6万QA对和24种语言",
                "评估显示模型在视觉噪声下性能下降超35%，存在英语优先偏见"
            ],
            "tags_zh": [
                "视觉语言模型",
                "表格问答",
                "多语言基准",
                "视觉噪声",
                "性能评估",
                "真实世界场景"
            ],
            "_index": 57
        },
        {
            "title": "A ROS2 Interface for Universal Robots Collaborative Manipulators Based on ur_rtde",
            "authors": [
                "Alessio Saccuti",
                "Riccardo Monica",
                "Jacopo Aleotti"
            ],
            "arxiv_id": "2511.17237v1",
            "summary": "In this paper a novel ROS2 driver for UR robot manipulators is presented, based on the ur_rtde C++ library. The proposed driver aims to be a flexible solution, adaptable to a wide range of applications. The driver exposes the high-level commands of Universal Robots URScripts, and custom commands can be added using a plugin system. Several commands have been implemented, including motion execution along a waypoint-based path. The driver is published as open source.",
            "headline_zh": "提出基于ur_rtde的ROS2驱动，为UR协作机械臂提供灵活解决方案。",
            "intro_zh": [
                "核心问题：缺乏通用ROS2驱动支持UR机械臂的多样化应用。",
                "方法要点：基于ur_rtde库，暴露URScripts高级命令并支持插件自定义。",
                "实验或效果：实现基于路径点的运动执行，并开源发布。"
            ],
            "tags_zh": [
                "ROS2驱动",
                "UR协作机械臂",
                "ur_rtde库",
                "路径规划",
                "开源软件"
            ],
            "_index": 58
        },
        {
            "title": "TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making",
            "authors": [
                "Shanshan Li",
                "Da Huang",
                "Yu He",
                "Yanwei Fu",
                "Yu-Gang Jiang",
                "Xiangyang Xue"
            ],
            "arxiv_id": "2511.17225v1",
            "summary": "In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.",
            "headline_zh": "提出TP-MDDN基准和AWMSystem以解决多需求导航中的自主决策问题",
            "intro_zh": [
                "核心问题：传统需求驱动导航无法处理多需求和任务偏好，限制了真实世界应用。",
                "方法要点：AWMSystem集成BreakLLM、LocateLLM和StatusMLLM模块，结合MASMap空间记忆和双节奏动作生成。",
                "实验或效果：在感知精度和导航鲁棒性上优于现有基线方法。"
            ],
            "tags_zh": [
                "多需求导航",
                "自主决策系统",
                "空间记忆映射",
                "长视野任务",
                "强化学习控制",
                "语义理解"
            ],
            "_index": 59
        },
        {
            "title": "QueryOcc: Query-based Self-Supervision for 3D Semantic Occupancy",
            "authors": [
                "Adam Lilja",
                "Ji Lan",
                "Junsheng Fu",
                "Lars Hammarstrand"
            ],
            "arxiv_id": "2511.17221v1",
            "summary": "Learning 3D scene geometry and semantics from images is a core challenge in computer vision and a key capability for autonomous driving. Since large-scale 3D annotation is prohibitively expensive, recent work explores self-supervised learning directly from sensor data without manual labels. Existing approaches either rely on 2D rendering consistency, where 3D structure emerges only implicitly, or on discretized voxel grids from accumulated lidar point clouds, limiting spatial precision and scalability. We introduce QueryOcc, a query-based self-supervised framework that learns continuous 3D semantic occupancy directly through independent 4D spatio-temporal queries sampled across adjacent frames. The framework supports supervision from either pseudo-point clouds derived from vision foundation models or raw lidar data. To enable long-range supervision and reasoning under constant memory, we introduce a contractive scene representation that preserves near-field detail while smoothly compressing distant regions. QueryOcc surpasses previous camera-based methods by 26% in semantic RayIoU on the self-supervised Occ3D-nuScenes benchmark while running at 11.6 FPS, demonstrating that direct 4D query supervision enables strong self-supervised occupancy learning. https://research.zenseact.com/publications/queryocc/",
            "headline_zh": "提出QueryOcc框架，通过4D查询实现自监督3D语义占据学习，用于自动驾驶场景。",
            "intro_zh": [
                "核心问题：从图像学习3D场景几何与语义，但大规模3D标注成本高，现有方法空间精度和可扩展性有限。",
                "方法要点：使用独立4D时空查询直接学习连续3D语义占据，支持伪点云或原始激光雷达监督。",
                "实验效果：在自监督Occ3D-nuScenes基准上，语义RayIoU提升26%，运行速度11.6 FPS。"
            ],
            "tags_zh": [
                "3D语义占据",
                "自监督学习",
                "查询机制",
                "自动驾驶",
                "4D时空建模",
                "压缩场景表示"
            ],
            "_index": 60
        },
        {
            "title": "Dual-domain Adaptation Networks for Realistic Image Super-resolution",
            "authors": [
                "Chaowei Fang",
                "Bolin Fu",
                "De Cheng",
                "Lechao Cheng",
                "Guanbin Li"
            ],
            "arxiv_id": "2511.17217v1",
            "summary": "Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.",
            "headline_zh": "提出双域适应网络以解决真实图像超分辨率中数据不足问题",
            "intro_zh": [
                "核心问题：真实图像超分辨率面临真实LR-HR数据稀缺，影响模型学习基本特征",
                "方法要点：结合空间域参数选择更新与低秩适应，并添加频域分支增强高频恢复",
                "实验或效果：在RealSR等基准测试中优于现有方法，代码已开源"
            ],
            "tags_zh": [
                "图像超分辨率",
                "域适应",
                "频域分析",
                "低秩适应",
                "真实图像处理"
            ],
            "_index": 61
        },
        {
            "title": "FisheyeGaussianLift: BEV Feature Lifting for Surround-View Fisheye Camera Perception",
            "authors": [
                "Shubham Sonarghare",
                "Prasad Deshpande",
                "Ciaran Hogan",
                "Deepika-Rani Kaliappan-Mahalingam",
                "Ganesh Sistu"
            ],
            "arxiv_id": "2511.17210v1",
            "summary": "Accurate BEV semantic segmentation from fisheye imagery remains challenging due to extreme non-linear distortion, occlusion, and depth ambiguity inherent to wide-angle projections. We present a distortion-aware BEV segmentation framework that directly processes multi-camera high-resolution fisheye images,utilizing calibrated geometric unprojection and per-pixel depth distribution estimation. Each image pixel is lifted into 3D space via Gaussian parameterization, predicting spatial means and anisotropic covariances to explicitly model geometric uncertainty. The projected 3D Gaussians are fused into a BEV representation via differentiable splatting, producing continuous, uncertainty-aware semantic maps without requiring undistortion or perspective rectification. Extensive experiments demonstrate strong segmentation performance on complex parking and urban driving scenarios, achieving IoU scores of 87.75% for drivable regions and 57.26% for vehicles under severe fisheye distortion and diverse environmental conditions.",
            "headline_zh": "提出FisheyeGaussianLift框架，通过高斯参数化提升BEV特征以解决鱼眼相机感知中的失真和深度模糊问题。",
            "intro_zh": [
                "核心问题：鱼眼图像因非线性失真、遮挡和深度模糊，导致BEV语义分割困难。",
                "方法要点：利用几何反投影和深度分布估计，将像素提升为3D高斯以建模不确定性。",
                "实验或效果：在停车和城市驾驶场景中，实现高IoU分数，如可行驶区域87.75%。"
            ],
            "tags_zh": [
                "鱼眼相机感知",
                "BEV语义分割",
                "高斯参数化",
                "失真建模",
                "多相机融合",
                "不确定性估计"
            ],
            "_index": 62
        },
        {
            "title": "Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers",
            "authors": [
                "Cris Claessens",
                "Christiaan Viviers",
                "Giacomo D'Amicantonio",
                "Egor Bondarev",
                "Fons van der Sommen"
            ],
            "arxiv_id": "2511.17209v1",
            "summary": "We introduce SPECTRE, a fully transformer-based foundation model for volumetric computed tomography (CT). Our Self-Supervised & Cross-Modal Pretraining for CT Representation Extraction (SPECTRE) approach utilizes scalable 3D Vision Transformer architectures and modern self-supervised and vision-language pretraining strategies to learn general-purpose CT representations. Volumetric CT poses unique challenges, such as extreme token scaling, geometric anisotropy, and weak or noisy clinical supervision, that make standard transformer and contrastive learning recipes ineffective out of the box. The framework jointly optimizes a local transformer for high-resolution volumetric feature extraction and a global transformer for whole-scan context modeling, making large-scale 3D attention computationally tractable. Notably, SPECTRE is trained exclusively on openly available CT datasets, demonstrating that high-performing, generalizable representations can be achieved without relying on private data. Pretraining combines DINO-style self-distillation with SigLIP-based vision-language alignment using paired radiology reports, yielding features that are both geometrically consistent and clinically meaningful. Across multiple CT benchmarks, SPECTRE consistently outperforms prior CT foundation models in both zero-shot and fine-tuned settings, establishing SPECTRE as a scalable, open, and fully transformer-based foundation model for 3D medical imaging.",
            "headline_zh": "提出SPECTRE基础模型，通过自监督与跨模态预训练解决体积CT表示学习挑战",
            "intro_zh": [
                "核心问题：体积CT存在令牌扩展、几何各向异性及弱监督，标准Transformer方法不适用",
                "方法要点：联合优化局部和全局Transformer，结合DINO自蒸馏与SigLIP跨模态对齐",
                "实验或效果：在多个CT基准测试中，零样本和微调设置下均优于先前模型"
            ],
            "tags_zh": [
                "体积CT表示学习",
                "自监督预训练",
                "跨模态对齐",
                "3D视觉Transformer",
                "医学影像基础模型"
            ],
            "_index": 63
        },
        {
            "title": "SING3R-SLAM: Submap-based Indoor Monocular Gaussian SLAM with 3D Reconstruction Priors",
            "authors": [
                "Kunyi Li",
                "Michael Niemeyer",
                "Sen Wang",
                "Stefano Gasperini",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "arxiv_id": "2511.17207v1",
            "summary": "Recent advances in dense 3D reconstruction enable the accurate capture of local geometry; however, integrating them into SLAM is challenging due to drift and redundant point maps, which limit efficiency and downstream tasks, such as novel view synthesis. To address these issues, we propose SING3R-SLAM, a globally consistent and compact Gaussian-based dense RGB SLAM framework. The key idea is to combine locally consistent 3D reconstructions with a unified global Gaussian representation that jointly refines scene geometry and camera poses, enabling efficient and versatile 3D mapping for multiple downstream applications. SING3R-SLAM first builds locally consistent submaps through our lightweight tracking and reconstruction module, and then progressively aligns and fuses them into a global Gaussian map that enforces cross-view geometric consistency. This global map, in turn, provides feedback to correct local drift and enhance the robustness of tracking. Extensive experiments demonstrate that SING3R-SLAM achieves state-of-the-art tracking, 3D reconstruction, and novel view rendering, resulting in over 12% improvement in tracking and producing finer, more detailed geometry, all while maintaining a compact and memory-efficient global representation on real-world datasets.",
            "headline_zh": "提出SING3R-SLAM以解决单目SLAM中的漂移和冗余问题，实现高效3D重建。",
            "intro_zh": [
                "核心问题：单目SLAM中漂移和冗余点云限制效率和下游任务应用。",
                "方法要点：结合局部一致子图与全局高斯表示，联合优化几何和相机位姿。",
                "实验或效果：在真实数据集上实现SOTA跟踪、重建和渲染，跟踪精度提升超12%。"
            ],
            "tags_zh": [
                "单目SLAM",
                "3D重建",
                "高斯表示",
                "子图融合",
                "漂移校正",
                "新颖视图渲染"
            ],
            "_index": 64
        },
        {
            "title": "Continual Alignment for SAM: Rethinking Foundation Models for Medical Image Segmentation in Continual Learning",
            "authors": [
                "Jiayi Wang",
                "Wei Dai",
                "Haoyu Wang",
                "Sihan Yang",
                "Haixia Bi",
                "Jian Sun"
            ],
            "arxiv_id": "2511.17201v1",
            "summary": "In medical image segmentation, heterogeneous privacy policies across institutions often make joint training on pooled datasets infeasible, motivating continual image segmentation-learning from data streams without catastrophic forgetting. While the Segment Anything Model (SAM) offers strong zero-shot priors and has been widely fine-tuned across downstream tasks, its large parameter count and computational overhead challenge practical deployment. This paper demonstrates that the SAM paradigm is highly promising once its computational efficiency and performance can be balanced. To this end, we introduce the Alignment Layer, a lightweight, plug-and-play module which aligns encoder-decoder feature distributions to efficiently adapt SAM to specific medical images, improving accuracy while reducing computation. Building on SAM and the Alignment Layer, we then propose Continual Alignment for SAM (CA-SAM), a continual learning strategy that automatically adapts the appropriate Alignment Layer to mitigate catastrophic forgetting, while leveraging SAM's zero-shot priors to preserve strong performance on unseen medical datasets. Experimented across nine medical segmentation datasets under continual-learning scenario, CA-SAM achieves state-of-the-art performance. Our code, models and datasets will be released on \\mbox{https://github.com/azzzzyo/Continual-Alignment-for-SAM.}",
            "headline_zh": "提出CA-SAM方法以解决医学图像分割中持续学习的灾难性遗忘问题",
            "intro_zh": [
                "核心问题：医学图像分割中，异构隐私政策阻碍联合训练，导致持续学习时灾难性遗忘",
                "方法要点：引入轻量对齐层，调整SAM特征分布，提升效率与准确性",
                "实验或效果：在九个数据集上测试，CA-SAM实现最先进性能，代码开源"
            ],
            "tags_zh": [
                "医学图像分割",
                "持续学习",
                "灾难性遗忘",
                "对齐层",
                "SAM模型",
                "轻量模块"
            ],
            "_index": 65
        },
        {
            "title": "VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation",
            "authors": [
                "Hanyu Zhou",
                "Chuanhao Ma",
                "Gim Hee Lee"
            ],
            "arxiv_id": "2511.17199v1",
            "summary": "Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation. Our model is guided by two key designs: 1) 4D-aware visual representation. We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism. 2) Spatiotemporal action representation. We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning, and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.",
            "headline_zh": "提出VLA-4D模型，通过4D感知实现机器人操作的时空一致性",
            "intro_zh": [
                "现有VLA模型在时空一致性操作中表现不佳，难以实现时间连贯控制",
                "方法包括4D感知视觉表示和时空动作表示，融合时间与空间信息",
                "实验验证模型在多种机器人操作任务中优于现有方法"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "4D感知",
                "时空一致性",
                "机器人操作",
                "多模态融合"
            ],
            "_index": 66
        },
        {
            "title": "Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism",
            "authors": [
                "Kaiyu Li",
                "Jiayu Wang",
                "Zhi Wang",
                "Hui Qiao",
                "Weizhan Zhang",
                "Deyu Meng",
                "Xiangyong Cao"
            ],
            "arxiv_id": "2511.17198v1",
            "summary": "LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.",
            "headline_zh": "提出分层任务抽象机制以解决专业领域多步任务规划问题",
            "intro_zh": [
                "核心问题：通用LLM代理在需要严格工作流的专业领域表现不佳",
                "方法要点：基于任务依赖图构建分层多代理系统，确保程序正确性",
                "实验或效果：在GeoPlan-bench上，EarthAgent显著优于现有单/多代理系统"
            ],
            "tags_zh": [
                "分层任务抽象",
                "多代理系统",
                "地理空间分析",
                "任务规划",
                "专业领域代理"
            ],
            "_index": 67
        },
        {
            "title": "Real Noise Decoupling for Hyperspectral Image Denoising",
            "authors": [
                "Yingkai Zhang",
                "Tao Zhang",
                "Jing Nie",
                "Ying Fu"
            ],
            "arxiv_id": "2511.17196v1",
            "summary": "Hyperspectral image (HSI) denoising is a crucial step in enhancing the quality of HSIs. Noise modeling methods can fit noise distributions to generate synthetic HSIs to train denoising networks. However, the noise in captured HSIs is usually complex and difficult to model accurately, which significantly limits the effectiveness of these approaches. In this paper, we propose a multi-stage noise-decoupling framework that decomposes complex noise into explicitly modeled and implicitly modeled components. This decoupling reduces the complexity of noise and enhances the learnability of HSI denoising methods when applied to real paired data. Specifically, for explicitly modeled noise, we utilize an existing noise model to generate paired data for pre-training a denoising network, equipping it with prior knowledge to handle the explicitly modeled noise effectively. For implicitly modeled noise, we introduce a high-frequency wavelet guided network. Leveraging the prior knowledge from the pre-trained module, this network adaptively extracts high-frequency features to target and remove the implicitly modeled noise from real paired HSIs. Furthermore, to effectively eliminate all noise components and mitigate error accumulation across stages, a multi-stage learning strategy, comprising separate pre-training and joint fine-tuning, is employed to optimize the entire framework. Extensive experiments on public and our captured datasets demonstrate that our proposed framework outperforms state-of-the-art methods, effectively handling complex real-world noise and significantly enhancing HSI quality.",
            "headline_zh": "提出多阶段噪声解耦框架以解决高光谱图像去噪中复杂噪声建模难题",
            "intro_zh": [
                "核心问题：真实高光谱图像噪声复杂，难以准确建模，限制去噪方法效果。",
                "方法要点：将噪声分解为显式和隐式分量，利用预训练网络和高频小波引导网络分别处理。",
                "实验或效果：在公开和自采集数据集上优于现有方法，显著提升图像质量。"
            ],
            "tags_zh": [
                "高光谱图像去噪",
                "噪声解耦",
                "多阶段学习",
                "小波引导网络",
                "预训练策略"
            ],
            "_index": 68
        },
        {
            "title": "PostCam: Camera-Controllable Novel-View Video Generation with Query-Shared Cross-Attention",
            "authors": [
                "Yipeng Chen",
                "Zhichao Ye",
                "Zhenzhou Fang",
                "Xinyu Chen",
                "Xiaoyu Zhang",
                "Jialing Liu",
                "Nan Wang",
                "Haomin Liu",
                "Guofeng Zhang"
            ],
            "arxiv_id": "2511.17185v1",
            "summary": "We propose PostCam, a framework for novel-view video generation that enables post-capture editing of camera trajectories in dynamic scenes. We find that existing video recapture methods suffer from suboptimal camera motion injection strategies; such suboptimal designs not only limit camera control precision but also result in generated videos that fail to preserve fine visual details from the source video. To achieve more accurate and flexible motion manipulation, PostCam introduces a query-shared cross-attention module. It integrates two distinct forms of control signals: the 6-DoF camera poses and the 2D rendered video frames. By fusing them into a unified representation within a shared feature space, our model can extract underlying motion cues, which enhances both control precision and generation quality. Furthermore, we adopt a two-stage training strategy: the model first learns coarse camera control from pose inputs, and then incorporates visual information to refine motion accuracy and enhance visual fidelity. Experiments on both real-world and synthetic datasets demonstrate that PostCam outperforms state-of-the-art methods by over 20% in camera control precision and view consistency, while achieving the highest video generation quality. Our project webpage is publicly available at: https://cccqaq.github.io/PostCam.github.io/",
            "headline_zh": "提出PostCam框架，通过查询共享交叉注意力实现动态场景中相机轨迹的编辑。",
            "intro_zh": [
                "现有视频重捕获方法相机运动注入策略不佳，影响控制精度和视觉细节保留。",
                "引入查询共享交叉注意力模块，融合6-DoF相机位姿和2D渲染帧以提升控制精度。",
                "实验显示相机控制精度和视图一致性提升超20%，生成质量优于现有方法。"
            ],
            "tags_zh": [
                "新视角视频生成",
                "相机控制",
                "交叉注意力",
                "动态场景编辑",
                "两阶段训练"
            ],
            "_index": 69
        },
        {
            "title": "Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition",
            "authors": [
                "Aditya Mishra",
                "Akshay Agarwal",
                "Haroon Lone"
            ],
            "arxiv_id": "2511.17183v1",
            "summary": "Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.",
            "headline_zh": "提出LENS-Net和INTSD数据集以解决夜间交通标志识别挑战",
            "intro_zh": [
                "核心问题：夜间交通标志识别因视觉噪声和数据集稀缺而困难，现有方法在低光照下不鲁棒。",
                "方法要点：LENS-Net集成自适应图像增强检测器和多模态CLIP-GCNN分类器，提升识别鲁棒性。",
                "实验或效果：在INTSD数据集上评估，LENS-Net超越现有框架，消融研究验证组件有效性。"
            ],
            "tags_zh": [
                "夜间交通标志识别",
                "多模态学习",
                "图像增强",
                "数据集构建",
                "图卷积网络",
                "跨模态注意力"
            ],
            "_index": 70
        },
        {
            "title": "Investigating self-supervised representations for audio-visual deepfake detection",
            "authors": [
                "Dragos-Alexandru Boldisor",
                "Stefan Smeu",
                "Dan Oneata",
                "Elisabeta Oneata"
            ],
            "arxiv_id": "2511.17181v1",
            "summary": "Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.",
            "headline_zh": "系统评估自监督表征在音视频深度伪造检测中的有效性、可解释性和互补性",
            "intro_zh": [
                "核心问题：自监督表征在音视频深度伪造检测中的应用潜力未被充分探索，且跨数据集泛化能力不足。",
                "方法要点：系统评估自监督特征在音频、视频和多模态中的表现，关注语义区域而非虚假伪影。",
                "实验或效果：发现特征捕获互补的伪造相关信息，但泛化失败可能源于数据集特性。"
            ],
            "tags_zh": [
                "自监督表征",
                "音视频深度伪造检测",
                "多模态评估",
                "可解释性分析",
                "跨数据集泛化"
            ],
            "_index": 71
        },
        {
            "title": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models",
            "authors": [
                "Kento Kawaharazuka",
                "Yoshiki Obinata",
                "Naoaki Kanazawa",
                "Haoyu Jia",
                "Kei Okada"
            ],
            "arxiv_id": "2511.17178v1",
            "summary": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.",
            "headline_zh": "提出结合大语言模型与黑盒优化的方法以提升机器人设计效率",
            "intro_zh": [
                "核心问题：黑盒优化采样效率低，难以处理复杂结构或离散值。",
                "方法要点：并行使用黑盒优化和大语言模型采样，提供问题设置与反馈。",
                "实验或效果：该方法能更高效探索设计解，但存在未知局限性。"
            ],
            "tags_zh": [
                "机器人设计优化",
                "黑盒优化",
                "大语言模型",
                "多目标优化",
                "采样效率"
            ],
            "_index": 72
        },
        {
            "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle",
            "authors": [
                "Mario Markov",
                "Stefan Maria Ailuro",
                "Luc Van Gool",
                "Konrad Schindler",
                "Danda Pani Paudel"
            ],
            "arxiv_id": "2511.17171v1",
            "summary": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",
            "headline_zh": "提出FireScope框架以解决跨大陆野火风险预测问题",
            "intro_zh": [
                "核心问题：现有方法缺乏因果推理和多模态理解，难以可靠泛化野火风险预测。",
                "方法要点：基于VLM的推理生成框架，结合强化学习和视觉监督预测风险栅格与推理轨迹。",
                "实验或效果：在跨大陆测试中性能显著提升，推理轨迹被验证为忠实且语义有意义。"
            ],
            "tags_zh": [
                "野火风险预测",
                "视觉语言模型",
                "跨大陆泛化",
                "推理轨迹",
                "多模态数据",
                "栅格生成"
            ],
            "_index": 73
        },
        {
            "title": "Reflection-Based Relative Localization for Cooperative UAV Teams Using Active Markers",
            "authors": [
                "Tim Lakemann",
                "Daniel Bonilla Licea",
                "Viktor Walter",
                "Martin Saska"
            ],
            "arxiv_id": "2511.17166v1",
            "summary": "Reflections of active markers in the environment are a common source of ambiguity in onboard visual relative localization. This work presents a novel approach for onboard relative localization in multi-robot teams that exploits these typically unwanted reflections of active markers in the environment. It operates without prior knowledge of robot size or predefined marker configurations and remains independent of surface properties, an essential feature for heterogeneous micro-aerial swarms cooperating in unknown environments. It explicitly accounts for uncertainties caused by non-flat surfaces, with a particular focus on dynamic water surfaces, which are especially relevant for marine deployments. We validated the approach in both indoor and outdoor experiments, demonstrating that the proposed reflection-based localization system operates reliably without prior knowledge of team member size and achieves greater effective range (above 30 m) and accuracy than state-of-the-art methods. The video and source code of this work will be made publicly available after publication.",
            "headline_zh": "提出反射式相对定位方法，利用主动标记反射解决无人机编队在未知环境中的定位模糊问题。",
            "intro_zh": [
                "核心问题：主动标记反射在视觉相对定位中常导致模糊，影响多机器人团队协作。",
                "方法要点：无需机器人尺寸或标记配置先验，独立于表面属性，特别处理非平坦表面如动态水面。",
                "实验或效果：室内外实验验证，有效范围超30米，精度优于现有方法，适用于异构微空中群。"
            ],
            "tags_zh": [
                "相对定位",
                "主动标记反射",
                "无人机编队",
                "未知环境",
                "非平坦表面",
                "视觉定位"
            ],
            "_index": 74
        },
        {
            "title": "Exploring the added value of pretherapeutic MR descriptors in predicting breast cancer pathologic complete response to neoadjuvant chemotherapy",
            "authors": [
                "Caroline Malhaire",
                "Fatine Selhane",
                "Marie-Judith Saint-Martin",
                "Vincent Cockenpot",
                "Pia Akl",
                "Enora Laas",
                "Audrey Bellesoeur",
                "Catherine Ala Eddine",
                "Melodie Bereby-Kahane",
                "Julie Manceau",
                "Delphine Sebbag-Sfez",
                "Jean-Yves Pierga",
                "Fabien Reyal",
                "Anne Vincent-Salomon",
                "Herve Brisse",
                "Frederique Frouin"
            ],
            "arxiv_id": "2511.17158v1",
            "summary": "Objectives: To evaluate the association between pretreatment MRI descriptors and breast cancer (BC) pathological complete response (pCR) to neoadjuvant chemotherapy (NAC). Materials \\& Methods: Patients with BC treated by NAC with a breast MRI between 2016 and 2020 were included in this retrospective observational single-center study. MR studies were described using the standardized BI-RADS and breast edema score on T2-weighted MRI. Univariable and multivariable logistic regression analyses were performed to assess variables association with pCR according to residual cancer burden. Random forest classifiers were trained to predict pCR on a random split including 70% of the database and were validated on the remaining cases. Results: Among 129 BC, 59 (46%) achieved pCR after NAC (luminal (n=7/37, 19%), triple negative (TN) (n=30/55, 55%), HER2+ (n=22/37, 59%). Clinical and biological items associated with pCR were BC subtype (p<0.001), T stage 0/I/II (p=0.008), higher Ki67 (p=0.005) and higher tumor-infiltrating lymphocytes levels (p=0.016). Univariate analysis showed that the following MRI features, oval or round shape (p=0.047), unifocality (p=0.026), non-spiculated margins (p=0.018), no associated non-mass enhancement (NME) (p = 0.024) and a lower MRI size (p = 0.031) were significantly associated with pCR. Unifocality and non-spiculated margins remained independently associated with pCR at multivariable analysis. Adding significant MRI features to clinicobiological variables in random forest classifiers significantly increased sensitivity (0.67 versus 0.62), specificity (0.69 versus 0.67) and precision (0.71 versus 0.67) for pCR prediction. Conclusion: Non-spiculated margins and unifocality are independently associated with pCR and can increase models performance to predict BC response to NAC. Clinical Relevance Statement: A multimodal approach integrating pretreatment MRI features with clinicobiological predictors, including TILs, could be employed to develop machine learning models for identifying patients at risk of non-response. This may enable consideration of alternative therapeutic strategies to optimize treatment outcomes",
            "headline_zh": "评估MRI特征以预测乳腺癌新辅助化疗病理完全缓解",
            "intro_zh": [
                "核心问题：探索治疗前MRI特征与乳腺癌病理完全缓解的关联。",
                "方法要点：使用BI-RADS标准和逻辑回归分析MRI特征。",
                "实验效果：添加MRI特征提升随机森林模型预测性能。"
            ],
            "tags_zh": [
                "乳腺癌预测",
                "MRI特征分析",
                "病理完全缓解",
                "随机森林模型",
                "新辅助化疗"
            ],
            "_index": 75
        },
        {
            "title": "UI-Styler: Ultrasound Image Style Transfer with Class-Aware Prompts for Cross-Device Diagnosis Using a Frozen Black-Box Inference Network",
            "authors": [
                "Nhat-Tuong Do-Tran",
                "Ngoc-Hoang-Lam Le",
                "Ching-Chun Huang"
            ],
            "arxiv_id": "2511.17155v1",
            "summary": "The appearance of ultrasound images varies across acquisition devices, causing domain shifts that degrade the performance of fixed black-box downstream inference models when reused. To mitigate this issue, it is practical to develop unpaired image translation (UIT) methods that effectively align the statistical distributions between source and target domains, particularly under the constraint of a reused inference-blackbox setting. However, existing UIT approaches often overlook class-specific semantic alignment during domain adaptation, resulting in misaligned content-class mappings that can impair diagnostic accuracy. To address this limitation, we propose UI-Styler, a novel ultrasound-specific, class-aware image style transfer framework. UI-Styler leverages a pattern-matching mechanism to transfer texture patterns embedded in the target images onto source images while preserving the source structural content. In addition, we introduce a class-aware prompting strategy guided by pseudo labels of the target domain, which enforces accurate semantic alignment with diagnostic categories. Extensive experiments on ultrasound cross-device tasks demonstrate that UI-Styler consistently outperforms existing UIT methods, achieving state-of-the-art performance in distribution distance and downstream tasks, such as classification and segmentation.",
            "headline_zh": "提出UI-Styler框架，通过类感知提示解决超声图像跨设备诊断中的域偏移问题。",
            "intro_zh": [
                "超声图像因设备差异导致域偏移，降低黑盒推理模型性能。",
                "采用模式匹配和类感知提示策略，实现纹理转移和语义对齐。",
                "实验显示在分类和分割任务中优于现有方法，提升分布距离和下游性能。"
            ],
            "tags_zh": [
                "超声图像风格迁移",
                "跨设备诊断",
                "类感知提示",
                "域适应",
                "黑盒推理网络"
            ],
            "_index": 76
        },
        {
            "title": "DiffRefiner: Coarse to Fine Trajectory Planning via Diffusion Refinement with Semantic Interaction for End to End Autonomous Driving",
            "authors": [
                "Liuhan Yin",
                "Runkun Ju",
                "Guodong Guo",
                "Erkang Cheng"
            ],
            "arxiv_id": "2511.17150v1",
            "summary": "Unlike discriminative approaches in autonomous driving that predict a fixed set of candidate trajectories of the ego vehicle, generative methods, such as diffusion models, learn the underlying distribution of future motion, enabling more flexible trajectory prediction. However, since these methods typically rely on denoising human-crafted trajectory anchors or random noise, there remains significant room for improvement. In this paper, we propose DiffRefiner, a novel two-stage trajectory prediction framework. The first stage uses a transformer-based Proposal Decoder to generate coarse trajectory predictions by regressing from sensor inputs using predefined trajectory anchors. The second stage applies a Diffusion Refiner that iteratively denoises and refines these initial predictions. In this way, we enhance the performance of diffusion-based planning by incorporating a discriminative trajectory proposal module, which provides strong guidance for the generative refinement process. Furthermore, we design a fine-grained denoising decoder to enhance scene compliance, enabling more accurate trajectory prediction through enhanced alignment with the surrounding environment. Experimental results demonstrate that DiffRefiner achieves state-of-the-art performance, attaining 87.4 EPDMS on NAVSIM v2, and 87.1 DS along with 71.4 SR on Bench2Drive, thereby setting new records on both public benchmarks. The effectiveness of each component is validated via ablation studies as well.",
            "headline_zh": "提出DiffRefiner框架，通过扩散精炼提升端到端自动驾驶轨迹规划性能",
            "intro_zh": [
                "生成式轨迹预测方法依赖噪声或锚点，存在改进空间",
                "采用两阶段方法：Transformer生成粗轨迹，扩散模型迭代精炼",
                "在NAVSIM v2和Bench2Drive基准上实现SOTA，验证组件有效性"
            ],
            "tags_zh": [
                "自动驾驶轨迹规划",
                "扩散模型",
                "Transformer解码器",
                "两阶段预测",
                "场景对齐"
            ],
            "_index": 77
        },
        {
            "title": "A lightweight detector for real-time detection of remote sensing images",
            "authors": [
                "Qianyi Wang",
                "Guoqiang Ren"
            ],
            "arxiv_id": "2511.17147v1",
            "summary": "Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.",
            "headline_zh": "提出DMG-YOLO轻量检测器以解决遥感图像小目标实时检测问题",
            "intro_zh": [
                "遥感图像小目标检测与实时性平衡是核心挑战",
                "采用双分支特征提取与全局局部融合模块提升检测性能",
                "在VisDrone2019等数据集上实现高mAP与轻量模型"
            ],
            "tags_zh": [
                "遥感图像检测",
                "轻量检测器",
                "小目标检测",
                "实时检测",
                "特征融合"
            ],
            "_index": 78
        },
        {
            "title": "Learning to Look Closer: A New Instance-Wise Loss for Small Cerebral Lesion Segmentation",
            "authors": [
                "Luc Bouteille",
                "Alexander Jaus",
                "Jens Kleesiek",
                "Rainer Stiefelhagen",
                "Lukas Heine"
            ],
            "arxiv_id": "2511.17146v1",
            "summary": "Traditional loss functions in medical image segmentation, such as Dice, often under-segment small lesions because their small relative volume contributes negligibly to the overall loss. To address this, instance-wise loss functions and metrics have been proposed to evaluate segmentation quality on a per-lesion basis. We introduce CC-DiceCE, a loss function based on the CC-Metrics framework, and compare it with the existing blob loss. Both are benchmarked against a DiceCE baseline within the nnU-Net framework, which provides a robust and standardized setup. We find that CC-DiceCE loss increases detection (recall) with minimal to no degradation in segmentation performance, albeit at the cost of slightly more false positives. Furthermore, our multi-dataset study shows that CC-DiceCE generally outperforms blob loss.",
            "headline_zh": "提出CC-DiceCE损失函数以解决小脑病变分割中的欠分割问题",
            "intro_zh": [
                "传统损失函数如Dice对小病变分割效果差，因其体积小对整体损失贡献低",
                "CC-DiceCE基于CC-Metrics框架，在nnU-Net中与blob loss和DiceCE基准对比",
                "实验显示CC-DiceCE提高检测召回率，分割性能稳定，但假阳性略有增加"
            ],
            "tags_zh": [
                "医学图像分割",
                "损失函数",
                "小病变检测",
                "nnU-Net框架",
                "实例级评估"
            ],
            "_index": 79
        },
        {
            "title": "One-Step Diffusion Transformer for Controllable Real-World Image Super-Resolution",
            "authors": [
                "Yushun Fang",
                "Yuxiang Chen",
                "Shibo Yin",
                "Qiang Hu",
                "Jiangchao Yao",
                "Ya Zhang",
                "Xiaoyun Zhang",
                "Yanfeng Wang"
            ],
            "arxiv_id": "2511.17138v1",
            "summary": "Recent advances in diffusion-based real-world image super-resolution (Real-ISR) have demonstrated remarkable perceptual quality, yet the balance between fidelity and controllability remains a problem: multi-step diffusion-based methods suffer from generative diversity and randomness, resulting in low fidelity, while one-step methods lose control flexibility due to fidelity-specific finetuning. In this paper, we present ODTSR, a one-step diffusion transformer based on Qwen-Image that performs Real-ISR considering fidelity and controllability simultaneously: a newly introduced visual stream receives low-quality images (LQ) with adjustable noise (Control Noise), and the original visual stream receives LQs with consistent noise (Prior Noise), forming the Noise-hybrid Visual Stream (NVS) design. ODTSR further employs Fidelity-aware Adversarial Training (FAA) to enhance controllability and achieve one-step inference. Extensive experiments demonstrate that ODTSR not only achieves state-of-the-art (SOTA) performance on generic Real-ISR, but also enables prompt controllability on challenging scenarios such as real-world scene text image super-resolution (STISR) of Chinese characters without training on specific datasets.",
            "headline_zh": "提出ODTSR以解决真实世界图像超分辨率中保真度与可控性的平衡问题",
            "intro_zh": [
                "核心问题：多步扩散方法生成多样但保真度低，一步方法保真度高但可控性差",
                "方法要点：引入噪声混合视觉流设计，结合可调噪声和一致噪声，实现一步推理",
                "实验或效果：在通用Real-ISR和中文场景文本超分辨率上达到SOTA，无需特定数据集训练"
            ],
            "tags_zh": [
                "图像超分辨率",
                "扩散模型",
                "可控生成",
                "一步推理",
                "噪声混合流",
                "保真度训练"
            ],
            "_index": 80
        },
        {
            "title": "A Multi-Stage Optimization Framework for Deploying Learned Image Compression on FPGAs",
            "authors": [
                "Jiaxun Fang",
                "Li Chen"
            ],
            "arxiv_id": "2511.17135v1",
            "summary": "Deep learning-based image compression (LIC) has achieved state-of-the-art rate-distortion (RD) performance, yet deploying these models on resource-constrained FPGAs remains a major challenge. This work presents a complete, multi-stage optimization framework to bridge the gap between high-performance floating-point models and efficient, hardware-friendly integer-based implementations. First, we address the fundamental problem of quantization-induced performance degradation. We propose a Dynamic Range-Aware Quantization (DRAQ) method that uses statistically-calibrated activation clipping and a novel weight regularization scheme to counteract the effects of extreme data outliers and large dynamic ranges, successfully creating a high-fidelity 8-bit integer model. Second, building on this robust foundation, we introduce two hardware-aware optimization techniques tailored for FPGAs. A progressive mixed-precision search algorithm exploits FPGA flexibility to assign optimal, non-uniform bit-widths to each layer, minimizing complexity while preserving performance. Concurrently, a channel pruning method, adapted to work with the Generalized Divisive Normalization (GDN) layers common in LIC, removes model redundancy by eliminating inactive channels. Our comprehensive experiments show that the foundational DRAQ method reduces the BD-rate overhead of a GDN-based model from $30\\%$ to $6.3\\%$. The subsequent hardware-aware optimizations further reduce computational complexity by over $20\\%$ with negligible impact on RD performance, yielding a final model that is both state-of-the-art in efficiency and superior in quality to existing FPGA-based LIC implementations.",
            "headline_zh": "提出多阶段优化框架以在FPGA上高效部署学习型图像压缩模型",
            "intro_zh": [
                "核心问题：学习型图像压缩模型在FPGA部署时面临量化性能下降和资源限制挑战。",
                "方法要点：采用动态范围感知量化、混合精度搜索和通道剪枝优化硬件实现。",
                "实验或效果：量化方法将BD-rate开销从30%降至6.3%，优化后计算复杂度降低超20%。"
            ],
            "tags_zh": [
                "学习型图像压缩",
                "FPGA部署",
                "量化优化",
                "混合精度搜索",
                "通道剪枝",
                "硬件感知优化"
            ],
            "_index": 81
        },
        {
            "title": "Off the Planckian Locus: Using 2D Chromaticity to Improve In-Camera Color",
            "authors": [
                "SaiKiran Tedla",
                "Joshua E. Little",
                "Hakki Can Karaimer",
                "Michael S. Brown"
            ],
            "arxiv_id": "2511.17133v1",
            "summary": "Traditional in-camera colorimetric mapping relies on correlated color temperature (CCT)-based interpolation between pre-calibrated transforms optimized for Planckian illuminants such as CIE A and D65. However, modern lighting technologies such as LEDs can deviate substantially from the Planckian locus, exposing the limitations of relying on conventional one-dimensional CCT for illumination characterization. This paper demonstrates that transitioning from 1D CCT (on the Planckian locus) to a 2D chromaticity space (off the Planckian locus) improves colorimetric accuracy across various mapping approaches. In addition, we replace conventional CCT interpolation with a lightweight multi-layer perceptron (MLP) that leverages 2D chromaticity features for robust colorimetric mapping under non-Planckian illuminants. A lightbox-based calibration procedure incorporating representative LED sources is used to train our MLP. Validated across diverse LED lighting, our method reduces angular reproduction error by 22% on average in LED-lit scenes, maintains backward compatibility with traditional illuminants, accommodates multi-illuminant scenes, and supports real-time in-camera deployment with negligible additional computational cost.",
            "headline_zh": "提出基于2D色度与MLP的色度映射方法，提升非普朗克光源下的色彩准确性",
            "intro_zh": [
                "传统相机色彩映射依赖CCT插值，但LED光源偏离普朗克轨迹导致精度不足",
                "使用2D色度空间和轻量MLP替代CCT插值，增强非普朗克光源下的鲁棒性",
                "实验显示LED场景下角度再现误差平均降低22%，保持实时部署与向后兼容"
            ],
            "tags_zh": [
                "相机色彩映射",
                "2D色度空间",
                "多层感知机",
                "非普朗克光源",
                "LED照明",
                "实时部署"
            ],
            "_index": 82
        },
        {
            "title": "OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation",
            "authors": [
                "Qi Jiang",
                "Xiaolong Qian",
                "Yao Gao",
                "Lei Sun",
                "Kailun Yang",
                "Zhonghua Yi",
                "Wenyong Li",
                "Ming-Hsuan Yang",
                "Luc Van Gool",
                "Kaiwei Wang"
            ],
            "arxiv_id": "2511.17126v1",
            "summary": "Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.",
            "headline_zh": "提出OmniLens++框架以解决盲镜头像差校正的泛化问题",
            "intro_zh": [
                "核心问题：现有方法在数据扩展和光学退化先验利用上泛化能力不足",
                "方法要点：扩展镜头库数据多样性并引入潜在PSF表示作为先验指导",
                "实验或效果：在真实和合成镜头库上展示领先的盲像差校正性能"
            ],
            "tags_zh": [
                "盲镜头像差校正",
                "镜头库预训练",
                "潜在PSF表示",
                "光学退化建模",
                "泛化能力提升"
            ],
            "_index": 83
        },
        {
            "title": "PEGS: Physics-Event Enhanced Large Spatiotemporal Motion Reconstruction via 3D Gaussian Splatting",
            "authors": [
                "Yijun Xu",
                "Jingrui Zhang",
                "Hongyi Liu",
                "Yuhan Chen",
                "Yuanyang Wang",
                "Qingyao Guo",
                "Dingwen Wang",
                "Lei Yu",
                "Chu He"
            ],
            "arxiv_id": "2511.17116v1",
            "summary": "Reconstruction of rigid motion over large spatiotemporal scales remains a challenging task due to limitations in modeling paradigms, severe motion blur, and insufficient physical consistency. In this work, we propose PEGS, a framework that integrates Physical priors with Event stream enhancement within a 3D Gaussian Splatting pipeline to perform deblurred target-focused modeling and motion recovery. We introduce a cohesive triple-level supervision scheme that enforces physical plausibility via an acceleration constraint, leverages event streams for high-temporal resolution guidance, and employs a Kalman regularizer to fuse multi-source observations. Furthermore, we design a motion-aware simulated annealing strategy that adaptively schedules the training process based on real-time kinematic states. We also contribute the first RGB-Event paired dataset targeting natural, fast rigid motion across diverse scenarios. Experiments show PEGS's superior performance in reconstructing motion over large spatiotemporal scales compared to mainstream dynamic methods.",
            "headline_zh": "提出PEGS框架，结合物理先验与事件流增强，解决大时空尺度刚体运动重建问题",
            "intro_zh": [
                "核心问题：大时空尺度刚体运动重建受限于建模范式、严重运动模糊和物理一致性不足",
                "方法要点：集成物理先验与事件流，采用三重监督和运动感知模拟退火策略",
                "实验或效果：在多样化场景中优于主流动态方法，重建性能优越"
            ],
            "tags_zh": [
                "大时空运动重建",
                "3D高斯泼溅",
                "事件流增强",
                "物理先验",
                "运动去模糊"
            ],
            "_index": 84
        },
        {
            "title": "ChainV: Atomic Visual Hints Make Multimodal Reasoning Shorter and Better",
            "authors": [
                "Yuan Zhang",
                "Ming Lu",
                "Junwen Pan",
                "Tao Huang",
                "Kuan Cheng",
                "Qi She",
                "Shanghang Zhang"
            ],
            "arxiv_id": "2511.17106v1",
            "summary": "Recent advances in multimodal reasoning models have demonstrated impressive capabilities across text and vision. However, even leading models exhibit redundant self-reflection when generating lengthy reasoning chains. While training-free CoT compression methods have emerged in the LLMs domain, they rely on static visual references and thus provide limited gains for multimodal reasoning. Therefore, we propose ChainV, a framework that dynamically integrates visual hints into the reasoning process, thereby making multimodal reasoning shorter and better. Specifically, ChainV first performs a coarse visual patch selection based on the previous reasoning step, then refines it by identifying the most representative atomic visual hint according to the averaged attention intensity. Additionally, ChainV introduces a consistency-based evaluation mechanism to assess the reliability of the chosen hint, guiding the model to adaptively adjust its level of self-reflection. Eventually, the pixel coordinates of the selected visual hint and its reliability are incorporated into thinking with a Bernoulli stochastic process. Experiments indicate that our method significantly improves reasoning accuracy and efficiency, especially on math-intensive benchmarks where visual hints are crucial for multi-step symbolic reasoning. For example, ChainV achieves $2.3\\%$ improvement on the MathVista within MIMO-VL-RL, while reducing inference latency by $51.4\\%$ and shortening output token length by $24.5\\%$.",
            "headline_zh": "提出ChainV框架，通过动态视觉提示优化多模态推理的准确性与效率",
            "intro_zh": [
                "核心问题：多模态推理模型存在冗余自反思，导致推理链过长且效率低下",
                "方法要点：动态选择原子视觉提示，基于注意力强度与一致性评估优化推理过程",
                "实验或效果：在MathVista等基准上提升准确率2.3%，降低延迟51.4%和输出长度24.5%"
            ],
            "tags_zh": [
                "多模态推理",
                "视觉提示选择",
                "推理效率优化",
                "注意力机制",
                "数学推理基准"
            ],
            "_index": 85
        },
        {
            "title": "Bridging Visual Affective Gap: Borrowing Textual Knowledge by Learning from Noisy Image-Text Pairs",
            "authors": [
                "Daiqing Wu",
                "Dongbao Yang",
                "Yu Zhou",
                "Can Ma"
            ],
            "arxiv_id": "2511.17103v1",
            "summary": "Visual emotion recognition (VER) is a longstanding field that has garnered increasing attention with the advancement of deep neural networks. Although recent studies have achieved notable improvements by leveraging the knowledge embedded within pre-trained visual models, the lack of direct association between factual-level features and emotional categories, called the \"affective gap\", limits the applicability of pre-training knowledge for VER tasks. On the contrary, the explicit emotional expression and high information density in textual modality eliminate the \"affective gap\". Therefore, we propose borrowing the knowledge from the pre-trained textual model to enhance the emotional perception of pre-trained visual models. We focus on the factual and emotional connections between images and texts in noisy social media data, and propose Partitioned Adaptive Contrastive Learning (PACL) to leverage these connections. Specifically, we manage to separate different types of samples and devise distinct contrastive learning strategies for each type. By dynamically constructing negative and positive pairs, we fully exploit the potential of noisy samples. Through comprehensive experiments, we demonstrate that bridging the \"affective gap\" significantly improves the performance of various pre-trained visual models in downstream emotion-related tasks. Our code is released on https://github.com/wdqqdw/PACL.",
            "headline_zh": "提出分区自适应对比学习以解决视觉情感识别中的情感鸿沟问题",
            "intro_zh": [
                "核心问题：视觉情感识别中预训练模型存在情感鸿沟，即事实特征与情感类别缺乏直接关联",
                "方法要点：利用噪声图像-文本对，通过分区自适应对比学习动态构建正负样本对",
                "实验或效果：在情感相关下游任务中显著提升多种预训练视觉模型的性能"
            ],
            "tags_zh": [
                "视觉情感识别",
                "情感鸿沟",
                "对比学习",
                "图像-文本对",
                "预训练模型",
                "噪声数据"
            ],
            "_index": 86
        },
        {
            "title": "Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation",
            "authors": [
                "Shuo Wang",
                "Yucheng Wang",
                "Guoxin Lian",
                "Yongcai Wang",
                "Maiyue Chen",
                "Kaihui Wang",
                "Bo Zhang",
                "Zhizhong Su",
                "Yutian Zhou",
                "Wanting Li",
                "Deying Li",
                "Zhaoxin Fan"
            ],
            "arxiv_id": "2511.17097v1",
            "summary": "Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.",
            "headline_zh": "提出语义进展推理方法以提升视觉语言导航的连贯性",
            "intro_zh": [
                "核心问题：现有方法忽视观察与指令序列的单调共进特性，导致导航不一致。",
                "方法要点：通过三阶段框架实现语义进展预测，无需昂贵标注。",
                "实验效果：在R2R-CE和RxR-CE数据集上实现最先进的成功率和效率。"
            ],
            "tags_zh": [
                "视觉语言导航",
                "语义进展推理",
                "自对齐预训练",
                "强化学习优化",
                "多步指令理解"
            ],
            "_index": 87
        },
        {
            "title": "Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models",
            "authors": [
                "He Huang",
                "Zixuan Hu",
                "Dongxiao Li",
                "Yao Xiao",
                "Ling-Yu Duan"
            ],
            "arxiv_id": "2511.17094v1",
            "summary": "Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\\% and 16.04\\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.",
            "headline_zh": "提出ReCoVAD框架以稀疏推理实现高效视频异常检测",
            "intro_zh": [
                "核心问题：密集帧推理在视频异常检测中计算成本高，是否必要未知",
                "方法要点：仿生双通路设计，反射通路快速响应，意识通路精炼更新",
                "实验效果：在UCF-Crime和XD-Violence数据集上处理帧数减少，性能领先"
            ],
            "tags_zh": [
                "视频异常检测",
                "稀疏推理",
                "预训练模型",
                "仿生框架",
                "训练免费方法"
            ],
            "_index": 88
        },
        {
            "title": "SPAGS: Sparse-View Articulated Object Reconstruction from Single State via Planar Gaussian Splatting",
            "authors": [
                "Di Wu",
                "Liu Liu",
                "Xueyu Yuan",
                "Qiaoyu Jun",
                "Wenxiao Chen",
                "Ruilong Yan",
                "Yiming Tang",
                "Liangtu Song"
            ],
            "arxiv_id": "2511.17092v1",
            "summary": "Articulated objects are ubiquitous in daily environments, and their 3D reconstruction holds great significance across various fields. However, existing articulated object reconstruction methods typically require costly inputs such as multi-stage and multi-view observations. To address the limitations, we propose a category-agnostic articulated object reconstruction framework via planar Gaussian Splatting, which only uses sparse-view RGB images from a single state. Specifically, we first introduce a Gaussian information field to perceive the optimal sparse viewpoints from candidate camera poses. Then we compress 3D Gaussians into planar Gaussians to facilitate accurate estimation of normal and depth. The planar Gaussians are optimized in a coarse-to-fine manner through depth smooth regularization and few-shot diffusion. Moreover, we introduce a part segmentation probability for each Gaussian primitive and update them by back-projecting part segmentation masks of renderings. Extensive experimental results demonstrate that our method achieves higher-fidelity part-level surface reconstruction on both synthetic and real-world data than existing methods. Codes will be made publicly available.",
            "headline_zh": "提出基于平面高斯溅射的稀疏视图铰接物体重建框架，仅需单状态图像输入",
            "intro_zh": [
                "核心问题：现有铰接物体重建方法需多阶段多视图输入，成本高昂",
                "方法要点：引入高斯信息场感知最优视图，压缩3D高斯为平面高斯优化",
                "实验效果：在合成和真实数据上实现更高保真度的部件级表面重建"
            ],
            "tags_zh": [
                "铰接物体重建",
                "平面高斯溅射",
                "稀疏视图",
                "部件分割",
                "深度平滑正则化",
                "少样本扩散"
            ],
            "_index": 89
        },
        {
            "title": "Spanning Tree Autoregressive Visual Generation",
            "authors": [
                "Sangkyu Lee",
                "Changho Lee",
                "Janghoon Han",
                "Hosung Song",
                "Tackgeun You",
                "Hwasup Lim",
                "Stanley Jungkyu Choi",
                "Honglak Lee",
                "Youngjae Yu"
            ],
            "arxiv_id": "2511.17089v1",
            "summary": "We present Spanning Tree Autoregressive (STAR) modeling, which can incorporate prior knowledge of images, such as center bias and locality, to maintain sampling performance while also providing sufficiently flexible sequence orders to accommodate image editing at inference. Approaches that expose randomly permuted sequence orders to conventional autoregressive (AR) models in visual generation for bidirectional context either suffer from a decline in performance or compromise the flexibility in sequence order choice at inference. Instead, STAR utilizes traversal orders of uniform spanning trees sampled in a lattice defined by the positions of image patches. Traversal orders are obtained through breadth-first search, allowing us to efficiently construct a spanning tree whose traversal order ensures that the connected partial observation of the image appears as a prefix in the sequence through rejection sampling. Through the tailored yet structured randomized strategy compared to random permutation, STAR preserves the capability of postfix completion while maintaining sampling performance without any significant changes to the model architecture widely adopted in the language AR modeling.",
            "headline_zh": "提出Spanning Tree Autoregressive建模，以在视觉生成中结合先验知识，保持采样性能并支持灵活序列编辑。",
            "intro_zh": [
                "核心问题：传统自回归模型在视觉生成中，随机序列顺序导致性能下降或推理灵活性受限。",
                "方法要点：利用均匀生成树的遍历顺序，通过广度优先搜索和拒绝采样构建序列，确保部分观察作为前缀。",
                "实验或效果：在保持采样性能的同时，提供灵活序列顺序，无需显著改变模型架构。"
            ],
            "tags_zh": [
                "视觉生成",
                "自回归建模",
                "序列顺序优化",
                "图像编辑",
                "生成树遍历"
            ],
            "_index": 90
        },
        {
            "title": "H-GAR: A Hierarchical Interaction Framework via Goal-Driven Observation-Action Refinement for Robotic Manipulation",
            "authors": [
                "Yijie Zhu",
                "Rui Shao",
                "Ziyang Liu",
                "Jie He",
                "Jizhihui Liu",
                "Jiuru Wang",
                "Zitong Yu"
            ],
            "arxiv_id": "2511.17079v1",
            "summary": "Unified video and action prediction models hold great potential for robotic manipulation, as future observations offer contextual cues for planning, while actions reveal how interactions shape the environment. However, most existing approaches treat observation and action generation in a monolithic and goal-agnostic manner, often leading to semantically misaligned predictions and incoherent behaviors. To this end, we propose H-GAR, a Hierarchical interaction framework via Goal-driven observation-Action Refinement.To anchor prediction to the task objective, H-GAR first produces a goal observation and a coarse action sketch that outline a high-level route toward the goal. To enable explicit interaction between observation and action under the guidance of the goal observation for more coherent decision-making, we devise two synergistic modules. (1) Goal-Conditioned Observation Synthesizer (GOS) synthesizes intermediate observations based on the coarse-grained actions and the predicted goal observation. (2) Interaction-Aware Action Refiner (IAAR) refines coarse actions into fine-grained, goal-consistent actions by leveraging feedback from the intermediate observations and a Historical Action Memory Bank that encodes prior actions to ensure temporal consistency. By integrating goal grounding with explicit action-observation interaction in a coarse-to-fine manner, H-GAR enables more accurate manipulation. Extensive experiments on both simulation and real-world robotic manipulation tasks demonstrate that H-GAR achieves state-of-the-art performance.",
            "headline_zh": "提出H-GAR分层框架，通过目标驱动优化观察与动作，提升机器人操作性能。",
            "intro_zh": [
                "现有方法在机器人操作中，常以整体方式生成观察与动作，导致预测语义错位和行为不连贯。",
                "H-GAR采用分层方法，先生成目标观察和粗略动作，再通过协同模块细化动作和合成中间观察。",
                "在仿真和真实机器人任务中，H-GAR实现了最先进的性能，验证了其有效性。"
            ],
            "tags_zh": [
                "机器人操作",
                "分层交互框架",
                "目标驱动预测",
                "观察-动作交互",
                "动作细化"
            ],
            "_index": 91
        },
        {
            "title": "A segment anchoring-based balancing algorithm for agricultural multi-robot task allocation with energy constraints",
            "authors": [
                "Peng Chen",
                "Jing Liang",
                "Kang-Jia Qiao",
                "Hui Song",
                "Tian-lei Ma",
                "Kun-Jie Yu",
                "Cai-Tong Yue",
                "Ponnuthurai Nagaratnam Suganthan",
                "Witold Pedryc"
            ],
            "arxiv_id": "2511.17076v1",
            "summary": "Multi-robot systems have emerged as a key technology for addressing the efficiency and cost challenges in labor-intensive industries. In the representative scenario of smart farming, planning efficient harvesting schedules for a fleet of electric robots presents a highly challenging frontier problem. The complexity arises not only from the need to find Pareto-optimal solutions for the conflicting objectives of makespan and transportation cost, but also from the necessity to simultaneously manage payload constraints and finite battery capacity. When robot loads are dynamically updated during planned multi-trip operations, a mandatory recharge triggered by energy constraints introduces an unscheduled load reset. This interaction creates a complex cascading effect that disrupts the entire schedule and renders traditional optimization methods ineffective. To address this challenge, this paper proposes the segment anchoring-based balancing algorithm (SABA). The core of SABA lies in the organic combination of two synergistic mechanisms: the sequential anchoring and balancing mechanism, which leverages charging decisions as `anchors' to systematically reconstruct disrupted routes, while the proportional splitting-based rebalancing mechanism is responsible for the fine-grained balancing and tuning of the final solutions' makespans. Extensive comparative experiments, conducted on a real-world case study and a suite of benchmark instances, demonstrate that SABA comprehensively outperforms 6 state-of-the-art algorithms in terms of both solution convergence and diversity. This research provides a novel theoretical perspective and an effective solution for the multi-robot task allocation problem under energy constraints.",
            "headline_zh": "提出分段锚定平衡算法以解决农业多机器人能量约束任务分配问题",
            "intro_zh": [
                "核心问题：能量约束下多机器人任务分配中，充电导致负载重置和调度中断。",
                "方法要点：结合顺序锚定和比例分割机制，重构路径并优化完工时间。",
                "实验或效果：在真实案例和基准测试中，优于6种先进算法收敛性和多样性。"
            ],
            "tags_zh": [
                "多机器人系统",
                "任务分配",
                "能量约束",
                "优化算法",
                "智能农业",
                "调度问题"
            ],
            "_index": 92
        },
        {
            "title": "Diversity Has Always Been There in Your Visual Autoregressive Models",
            "authors": [
                "Tong Wang",
                "Guanyu Yang",
                "Nian Liu",
                "Kai Wang",
                "Yaxing Wang",
                "Abdelrahman M Shaker",
                "Salman Khan",
                "Fahad Shahbaz Khan",
                "Senmao Li"
            ],
            "arxiv_id": "2511.17074v1",
            "summary": "Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models. However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models. In this paper, we introduce DiverseVAR, a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis. Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/DiverseVAR.",
            "headline_zh": "提出DiverseVAR以解决视觉自回归模型的多样性崩溃问题",
            "intro_zh": [
                "VAR模型存在多样性崩溃，输出变异性降低，类似少步蒸馏扩散模型",
                "通过抑制输入和放大输出的关键特征图组件，无需额外训练恢复多样性",
                "实验显示显著增强生成多样性，对性能影响可忽略，保持高保真合成"
            ],
            "tags_zh": [
                "视觉自回归模型",
                "生成多样性",
                "特征图组件",
                "无训练优化",
                "高保真合成"
            ],
            "_index": 93
        },
        {
            "title": "ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion",
            "authors": [
                "Junming Liu",
                "Yifei Sun",
                "Weihua Cheng",
                "Yujin Kang",
                "Yirong Chen",
                "Ding Wang",
                "Guosun Zeng"
            ],
            "arxiv_id": "2511.17068v1",
            "summary": "Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.",
            "headline_zh": "提出ReBrain框架，通过检索增强扩散从稀疏CT重建脑MRI",
            "intro_zh": [
                "核心问题：稀疏CT体积导致脑MRI重建困难，影响疾病诊断。",
                "方法要点：使用BBDM合成MRI，结合检索CT通过ControlNet引导生成。",
                "实验或效果：在SynthRAD2023和BraTS数据集上实现先进性能。"
            ],
            "tags_zh": [
                "脑MRI重建",
                "跨模态合成",
                "扩散模型",
                "检索增强",
                "稀疏CT",
                "ControlNet"
            ],
            "_index": 94
        },
        {
            "title": "REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting",
            "authors": [
                "Di Wu",
                "Liu Liu",
                "Anran Huang",
                "Yuyan Liu",
                "Qiaoyu Jun",
                "Shaofan Liu",
                "Liangtu Song",
                "Cewu Lu"
            ],
            "arxiv_id": "2511.17059v1",
            "summary": "Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS~\\cite{wu2025reartgs} introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.",
            "headline_zh": "提出REArtGS++以解决铰接物体重建中螺丝关节和多部件挑战，引入时间几何约束与平面高斯溅射",
            "intro_zh": [
                "核心问题：铰接物体如抽屉的重建在螺丝关节和多部件时困难，且缺乏未见状态的几何约束。",
                "方法要点：建模解耦螺丝运动，通过部件运动混合优化高斯与关节参数，并施加平面和时间一致正则化。",
                "实验或效果：在合成和真实铰接物体上优于现有方法，实现泛化部件级表面重建和关节参数估计。"
            ],
            "tags_zh": [
                "铰接物体重建",
                "高斯溅射",
                "时间几何约束",
                "关节参数估计",
                "部件级表面重建"
            ],
            "_index": 95
        },
        {
            "title": "RL-AD-Net: Reinforcement Learning Guided Adaptive Displacement in Latent Space for Refined Point Cloud Completion",
            "authors": [
                "Bhanu Pratap Paregi",
                "Vaibhav Kumar"
            ],
            "arxiv_id": "2511.17054v1",
            "summary": "Recent point cloud completion models, including transformer-based, denoising-based, and other state-of-the-art approaches, generate globally plausible shapes from partial inputs but often leave local geometric inconsistencies. We propose RL-AD-Net, a reinforcement learning (RL) refinement framework that operates in the latent space of a pretrained point autoencoder. The autoencoder encodes completions into compact global feature vectors (GFVs), which are selectively adjusted by an RL agent to improve geometric fidelity. To ensure robustness, a lightweight non-parametric PointNN selector evaluates the geometric consistency of both the original completion and the RL-refined output, retaining the better reconstruction. When ground truth is available, both Chamfer Distance and geometric consistency metrics guide refinement. Training is performed separately per category, since the unsupervised and dynamic nature of RL makes convergence across highly diverse categories challenging. Nevertheless, the framework can be extended to multi-category refinement in future work. Experiments on ShapeNetCore-2048 demonstrate that while baseline completion networks perform reasonable under their training-style cropping, they struggle in random cropping scenarios. In contrast, RL-AD-Net consistently delivers improvements across both settings, highlighting the effectiveness of RL-guided ensemble refinement. The approach is lightweight, modular, and model-agnostic, making it applicable to a wide range of completion networks without requiring retraining.",
            "headline_zh": "提出RL-AD-Net，通过强化学习在潜在空间调整点云补全，提升几何一致性。",
            "intro_zh": [
                "点云补全模型常产生局部几何不一致问题。",
                "使用强化学习在预训练自编码器潜在空间微调全局特征向量。",
                "实验显示在随机裁剪场景下，RL-AD-Net优于基线模型。"
            ],
            "tags_zh": [
                "点云补全",
                "强化学习",
                "潜在空间优化",
                "几何一致性",
                "模型无关框架"
            ],
            "_index": 96
        },
        {
            "title": "OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding",
            "authors": [
                "Teng Fu",
                "Mengyang Zhao",
                "Ke Niu",
                "Kaixin Peng",
                "Bin Li"
            ],
            "arxiv_id": "2511.17053v1",
            "summary": "LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.",
            "headline_zh": "提出OmniPT框架，利用大视觉语言模型统一行人跟踪与语义理解任务",
            "intro_zh": [
                "核心问题：大视觉语言模型在实例级任务如行人跟踪中性能不足，需结合高级语义理解",
                "方法要点：采用RL-中期训练-SFT-RL训练流程，使模型输出格式化边界框并提升跟踪能力",
                "实验或效果：在跟踪基准测试中表现优于先前方法，验证了框架有效性"
            ],
            "tags_zh": [
                "行人跟踪",
                "大视觉语言模型",
                "强化学习训练",
                "语义理解",
                "统一框架"
            ],
            "_index": 97
        },
        {
            "title": "PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning",
            "authors": [
                "Jingyun Chen",
                "Linghan Cai",
                "Zhikang Wang",
                "Yi Huang",
                "Songhan Jiang",
                "Shenjin Huang",
                "Hongpeng Wang",
                "Yongbing Zhang"
            ],
            "arxiv_id": "2511.17052v1",
            "summary": "Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.",
            "headline_zh": "提出PathAgent框架，通过大语言模型代理推理实现全切片病理图像的可解释分析",
            "intro_zh": [
                "核心问题：现有全切片图像分析缺乏显式推理轨迹，导致预测不透明且不可解释。",
                "方法要点：使用无训练LLM代理框架，模拟专家逐步分析，包括导航、感知和执行模块。",
                "实验或效果：在五个数据集上零样本泛化强，超越基线，并获人类病理学家协作评估认可。"
            ],
            "tags_zh": [
                "全切片图像分析",
                "大语言模型代理",
                "可解释推理",
                "零样本泛化",
                "病理诊断辅助"
            ],
            "_index": 98
        },
        {
            "title": "RoomPlanner: Explicit Layout Planner for Easier LLM-Driven 3D Room Generation",
            "authors": [
                "Wenzhuo Sun",
                "Mingjian Liang",
                "Wenxuan Song",
                "Xuelian Cheng",
                "Zongyuan Ge"
            ],
            "arxiv_id": "2511.17048v1",
            "summary": "In this paper, we propose RoomPlanner, the first fully automatic 3D room generation framework for painlessly creating realistic indoor scenes with only short text as input. Without any manual layout design or panoramic image guidance, our framework can generate explicit layout criteria for rational spatial placement. We begin by introducing a hierarchical structure of language-driven agent planners that can automatically parse short and ambiguous prompts into detailed scene descriptions. These descriptions include raw spatial and semantic attributes for each object and the background, which are then used to initialize 3D point clouds. To position objects within bounded environments, we implement two arrangement constraints that iteratively optimize spatial arrangements, ensuring a collision-free and accessible layout solution. In the final rendering stage, we propose a novel AnyReach Sampling strategy for camera trajectory, along with the Interval Timestep Flow Sampling (ITFS) strategy, to efficiently optimize the coarse 3D Gaussian scene representation. These approaches help reduce the total generation time to under 30 minutes. Extensive experiments demonstrate that our method can produce geometrically rational 3D indoor scenes, surpassing prior approaches in both rendering speed and visual quality while preserving editability. The code will be available soon.",
            "headline_zh": "提出RoomPlanner框架，通过显式布局规划实现基于LLM的3D房间自动生成",
            "intro_zh": [
                "核心问题：短文本输入下自动生成几何合理的3D室内场景，无需手动布局或全景图像指导",
                "方法要点：使用分层语言代理解析提示，结合布局约束优化空间排列，并采用高效采样策略渲染",
                "实验或效果：生成时间低于30分钟，在渲染速度和视觉质量上超越先前方法，保持可编辑性"
            ],
            "tags_zh": [
                "3D房间生成",
                "显式布局规划",
                "语言驱动代理",
                "空间优化约束",
                "高效渲染采样"
            ],
            "_index": 99
        },
        {
            "title": "RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis",
            "authors": [
                "Linfeng Dong",
                "Yuchen Yang",
                "Hao Wu",
                "Wei Wang",
                "Yuenan HouZhihang Zhong",
                "Xiao Sun"
            ],
            "arxiv_id": "2511.17045v1",
            "summary": "We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision",
            "headline_zh": "提出RacketVision数据集与基准，以统一分析球拍类运动中的球和球拍。",
            "intro_zh": [
                "核心问题：球拍类运动中球跟踪、球拍姿态估计和轨迹预测的复杂交互问题。",
                "方法要点：引入大规模细粒度球拍姿态标注，采用CrossAttention机制融合多模态特征。",
                "实验或效果：CrossAttention提升轨迹预测性能，优于单模态基线。"
            ],
            "tags_zh": [
                "球拍姿态估计",
                "球轨迹预测",
                "多模态融合",
                "运动分析数据集",
                "计算机视觉基准"
            ],
            "_index": 100
        },
        {
            "title": "MedImageInsight for Thoracic Cavity Health Classification from Chest X-rays",
            "authors": [
                "Rama Krishna Boya",
                "Mohan Kireeti Magalanadu",
                "Azaruddin Palavalli",
                "Rupa Ganesh Tekuri",
                "Amrit Pattanayak",
                "Prasanthi Enuga",
                "Vignesh Esakki Muthu",
                "Vivek Aditya Boya"
            ],
            "arxiv_id": "2511.17043v1",
            "summary": "Chest radiography remains one of the most widely used imaging modalities for thoracic diagnosis, yet increasing imaging volumes and radiologist workload continue to challenge timely interpretation. In this work, we investigate the use of MedImageInsight, a medical imaging foundational model, for automated binary classification of chest X-rays into Normal and Abnormal categories. Two approaches were evaluated: (1) fine-tuning MedImageInsight for end-to-end classification, and (2) employing the model as a feature extractor for a transfer learning pipeline using traditional machine learning classifiers. Experiments were conducted using a combination of the ChestX-ray14 dataset and real-world clinical data sourced from partner hospitals. The fine-tuned classifier achieved the highest performance, with an ROC-AUC of 0.888 and superior calibration compared to the transfer learning models, demonstrating performance comparable to established architectures such as CheXNet. These results highlight the effectiveness of foundational medical imaging models in reducing task-specific training requirements while maintaining diagnostic reliability. The system is designed for integration into web-based and hospital PACS workflows to support triage and reduce radiologist burden. Future work will extend the model to multi-label pathology classification to provide preliminary diagnostic interpretation in clinical environments.",
            "headline_zh": "提出MedImageInsight模型用于胸部X光片正常与异常二元分类，以减轻放射科医生负担。",
            "intro_zh": [
                "核心问题：胸部X光片解读工作量大，需自动化分类以支持及时诊断。",
                "方法要点：评估微调MedImageInsight和特征提取结合传统分类器两种方法。",
                "实验或效果：微调模型ROC-AUC达0.888，性能与CheXNet相当，适用于临床集成。"
            ],
            "tags_zh": [
                "胸部X光分类",
                "医学影像基础模型",
                "二元分类",
                "ROC-AUC评估",
                "临床工作流集成"
            ],
            "_index": 101
        },
        {
            "title": "Do Vision-Language Models Understand Visual Persuasiveness?",
            "authors": [
                "Gyuwon Park"
            ],
            "arxiv_id": "2511.17036v1",
            "summary": "Recent advances in vision-language models (VLMs) have enabled impressive multi-modal reasoning and understanding. Yet, whether these models truly grasp visual persuasion-how visual cues shape human attitudes and decisions-remains unclear. To probe this question, we construct a high-consensus dataset for binary persuasiveness judgment and introduce the taxonomy of Visual Persuasive Factors (VPFs), encompassing low-level perceptual, mid-level compositional, and high-level semantic cues. We also explore cognitive steering and knowledge injection strategies for persuasion-relevant reasoning. Empirical analysis across VLMs reveals a recall-oriented bias-models over-predict high persuasiveness-and weak discriminative power for low/mid-level features. In contrast, high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment. Among intervention strategies, simple instruction or unguided reasoning scaffolds yield marginal or negative effects, whereas concise, object-grounded rationales significantly improve precision and F1 scores. These results indicate that VLMs core limitation lies not in recognizing persuasive objects but in linking them to communicative intent.",
            "headline_zh": "提出视觉说服因素分类与干预策略，评估视觉语言模型对视觉说服力的理解。",
            "intro_zh": [
                "核心问题：视觉语言模型是否理解视觉说服力，即视觉线索如何影响人类态度和决策。",
                "方法要点：构建高共识数据集，引入视觉说服因素分类，探索认知引导和知识注入策略。",
                "实验或效果：模型存在召回偏向，高级语义对齐预测力强，对象基础理性显著提升性能。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "视觉说服力",
                "多模态推理",
                "数据集构建",
                "干预策略",
                "语义对齐"
            ],
            "_index": 102
        },
        {
            "title": "Energy Scaling Laws for Diffusion Models: Quantifying Compute and Carbon Emissions in Image Generation",
            "authors": [
                "Aniketh Iyengar",
                "Jiaqi Han",
                "Boris Ruf",
                "Vincent Grari",
                "Marcin Detyniecki",
                "Stefano Ermon"
            ],
            "arxiv_id": "2511.17031v1",
            "summary": "The rapidly growing computational demands of diffusion models for image generation have raised significant concerns about energy consumption and environmental impact. While existing approaches to energy optimization focus on architectural improvements or hardware acceleration, there is a lack of principled methods to predict energy consumption across different model configurations and hardware setups. We propose an adaptation of Kaplan scaling laws to predict GPU energy consumption for diffusion models based on computational complexity (FLOPs). Our approach decomposes diffusion model inference into text encoding, iterative denoising, and decoding components, with the hypothesis that denoising operations dominate energy consumption due to their repeated execution across multiple inference steps. We conduct comprehensive experiments across four state-of-the-art diffusion models (Stable Diffusion 2, Stable Diffusion 3.5, Flux, and Qwen) on three GPU architectures (NVIDIA A100, A4000, A6000), spanning various inference configurations including resolution (256x256 to 1024x1024), precision (fp16/fp32), step counts (10-50), and classifier-free guidance settings. Our energy scaling law achieves high predictive accuracy within individual architectures (R-squared > 0.9) and exhibits strong cross-architecture generalization, maintaining high rank correlations across models and enabling reliable energy estimation for unseen model-hardware combinations. These results validate the compute-bound nature of diffusion inference and provide a foundation for sustainable AI deployment planning and carbon footprint estimation.",
            "headline_zh": "提出基于计算复杂度的扩散模型能耗预测方法，以支持可持续AI部署。",
            "intro_zh": [
                "扩散模型图像生成能耗高，缺乏跨模型和硬件的预测方法。",
                "采用Kaplan缩放定律，分解推理过程，假设去噪操作主导能耗。",
                "多模型和GPU实验验证预测准确性高，支持跨架构泛化。"
            ],
            "tags_zh": [
                "扩散模型",
                "能耗预测",
                "缩放定律",
                "GPU能耗",
                "可持续AI"
            ],
            "_index": 103
        },
        {
            "title": "Parameter-Free Neural Lens Blur Rendering for High-Fidelity Composites",
            "authors": [
                "Lingyan Ruan",
                "Bin Chen",
                "Taehyun Rhee"
            ],
            "arxiv_id": "2511.17014v1",
            "summary": "Consistent and natural camera lens blur is important for seamlessly blending 3D virtual objects into photographed real-scenes. Since lens blur typically varies with scene depth, the placement of virtual objects and their corresponding blur levels significantly affect the visual fidelity of mixed reality compositions. Existing pipelines often rely on camera parameters (e.g., focal length, focus distance, aperture size) and scene depth to compute the circle of confusion (CoC) for realistic lens blur rendering. However, such information is often unavailable to ordinary users, limiting the accessibility and generalizability of these methods. In this work, we propose a novel compositing approach that directly estimates the CoC map from RGB images, bypassing the need for scene depth or camera metadata. The CoC values for virtual objects are inferred through a linear relationship between its signed CoC map and depth, and realistic lens blur is rendered using a neural reblurring network. Our method provides flexible and practical solution for real-world applications. Experimental results demonstrate that our method achieves high-fidelity compositing with realistic defocus effects, outperforming state-of-the-art techniques in both qualitative and quantitative evaluations.",
            "headline_zh": "提出参数自由神经镜头模糊渲染方法，用于高保真混合现实合成",
            "intro_zh": [
                "核心问题：混合现实中虚拟对象与真实场景融合时，镜头模糊不一致影响视觉保真度，且传统方法依赖相机参数和场景深度，普通用户难以获取。",
                "方法要点：直接从RGB图像估计弥散圆图，通过线性关系推断虚拟对象模糊，使用神经重模糊网络渲染真实镜头模糊。",
                "实验或效果：实验显示方法在定性和定量评估中优于现有技术，实现高保真合成与真实散焦效果。"
            ],
            "tags_zh": [
                "镜头模糊渲染",
                "混合现实合成",
                "弥散圆估计",
                "神经重模糊网络",
                "参数自由方法"
            ],
            "_index": 104
        },
        {
            "title": "MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints",
            "authors": [
                "Yiwen Ying",
                "Hanjing Ye",
                "Senzi Luo",
                "Luyao Liu",
                "Yu Zhan",
                "Li He",
                "Hong Zhang"
            ],
            "arxiv_id": "2511.17013v1",
            "summary": "Obstacle avoidance in complex and dynamic environments is a critical challenge for real-time robot navigation. Model-based and learning-based methods often fail in highly dynamic scenarios because traditional methods assume a static environment and cannot adapt to real-time changes, while learning-based methods rely on single-frame observations for motion constraint estimation, limiting their adaptability. To overcome these limitations, this paper proposes a novel framework that leverages multi-frame point constraints, including current and future frames predicted by a dedicated module, to enable proactive end-to-end navigation. By incorporating a prediction module that forecasts the future path of moving obstacles based on multi-frame observations, our method allows the robot to proactively anticipate and avoid potential dangers. This proactive planning capability significantly enhances navigation robustness and efficiency in unknown dynamic environments. Simulations and real-world experiments validate the effectiveness of our approach.",
            "headline_zh": "提出多帧点约束框架以解决动态环境中机器人导航的实时避障问题",
            "intro_zh": [
                "核心问题：传统方法假设静态环境，学习型方法依赖单帧观测，难以适应高度动态场景。",
                "方法要点：利用多帧点约束，包括预测模块生成未来帧，实现主动端到端导航。",
                "实验或效果：仿真和真实实验验证了在未知动态环境中导航的鲁棒性和效率提升。"
            ],
            "tags_zh": [
                "机器人导航",
                "动态环境避障",
                "多帧预测",
                "端到端学习",
                "主动规划"
            ],
            "_index": 105
        },
        {
            "title": "FLUID: Training-Free Face De-identification via Latent Identity Substitution",
            "authors": [
                "Jinhyeong Park",
                "Shaheryar Muhammad",
                "Seangmin Lee",
                "Jong Taek Lee",
                "Soon Ki Jung"
            ],
            "arxiv_id": "2511.17005v1",
            "summary": "We present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a training-free framework that directly substitutes identity in the latent space of pretrained diffusion models. Inspired by substitution mechanisms in chemistry, we reinterpret identity editing as semantic displacement in the latent h-space of a pretrained unconditional diffusion model. Our framework discovers identity-editing directions through optimization guided by novel reagent losses, which supervise for attribute preservation and identity suppression. We further propose both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experimental results on CelebA-HQ and FFHQ demonstrate that FLUID achieves a superior trade-off between identity suppression and attribute preservation, outperforming state-of-the-art de-identification methods in both qualitative and quantitative metrics.",
            "headline_zh": "提出FLUID框架，通过潜在身份替换实现免训练人脸去识别。",
            "intro_zh": [
                "核心问题：人脸去识别需平衡身份抑制与属性保留。",
                "方法要点：在预训练扩散模型潜在空间优化身份编辑方向。",
                "实验效果：在CelebA-HQ和FFHQ上优于现有方法。"
            ],
            "tags_zh": [
                "人脸去识别",
                "潜在空间编辑",
                "扩散模型",
                "身份抑制",
                "属性保留"
            ],
            "_index": 106
        },
        {
            "title": "Vision Language Models are Confused Tourists",
            "authors": [
                "Patrick Amadeus Irawan",
                "Ikhlasul Akmal Hanif",
                "Muhammad Dehan Al Kautsar",
                "Genta Indra Winata",
                "Fajri Koto",
                "Alham Fikri Aji"
            ],
            "arxiv_id": "2511.17004v1",
            "summary": "Although the cultural dimension has been one of the key aspects in evaluating Vision-Language Models (VLMs), their ability to remain stable across diverse cultural inputs remains largely untested, despite being crucial to support diversity and multicultural societies. Existing evaluations often rely on benchmarks featuring only a singular cultural concept per image, overlooking scenarios where multiple, potentially unrelated cultural cues coexist. To address this gap, we introduce ConfusedTourist, a novel cultural adversarial robustness suite designed to assess VLMs' stability against perturbed geographical cues. Our experiments reveal a critical vulnerability, where accuracy drops heavily under simple image-stacking perturbations and even worsens with its image-generation-based variant. Interpretability analyses further show that these failures stem from systematic attention shifts toward distracting cues, diverting the model from its intended focus. These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding.",
            "headline_zh": "提出ConfusedTourist套件以评估视觉语言模型在混合文化线索下的稳定性",
            "intro_zh": [
                "现有评估忽视多文化线索共存场景，模型文化稳定性未充分测试",
                "引入文化对抗鲁棒性套件，通过图像堆叠和生成扰动评估模型",
                "实验显示模型准确率大幅下降，注意力被干扰线索分散"
            ],
            "tags_zh": [
                "视觉语言模型",
                "文化对抗鲁棒性",
                "多文化线索",
                "注意力机制",
                "模型稳定性"
            ],
            "_index": 107
        },
        {
            "title": "Stable Offline Hand-Eye Calibration for any Robot with Just One Mark",
            "authors": [
                "Sicheng Xie",
                "Lingchen Meng",
                "Zhiying Du",
                "Shuyuan Tu",
                "Haidong Cao",
                "Jiaqi Leng",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "arxiv_id": "2511.17001v1",
            "summary": "Imitation learning has achieved remarkable success in a variety of robotic tasks by learning a mapping function from camera-space observations to robot-space actions. Recent work indicates that the use of robot-to-camera transformation information ({\\ie}, camera extrinsics) benefits the learning process and produces better results. However, camera extrinsics are oftentimes unavailable and estimation methods usually suffer from local minima and poor generalizations. In this paper, we present CalibAll, a simple yet effective method that \\textbf{requires only a single mark} and performs training-free, stable, and accurate camera extrinsic estimation across diverse robots and datasets through a coarse-to-fine calibration pipeline. In particular, we annotate a single mark on an end-effector (EEF), and leverage the correspondence ability emerged from vision foundation models (VFM) to automatically localize the corresponding mark across robots in diverse datasets. Using this mark, together with point tracking and the 3D EEF trajectory, we obtain a coarse camera extrinsic via temporal Perspective-n-Point (PnP). This estimate is further refined through a rendering-based optimization that aligns rendered and ground-true masks, yielding accurate and stable camera extrinsic. Experimental results demonstrate that our method outperforms state-of-the-art approaches, showing strong robustness and general effectiveness across three robot platforms. It also produces useful auxiliary annotations such as depth maps, link-wise masks, and end-effector 2D trajectories, which can further support downstream tasks.",
            "headline_zh": "提出CalibAll方法，仅需单一标记实现稳定离线手眼标定",
            "intro_zh": [
                "核心问题：机器人手眼标定中相机外参估计常不可用，现有方法易陷局部最优且泛化差",
                "方法要点：利用视觉基础模型定位标记，结合点跟踪与3D轨迹，通过粗到精流程优化外参",
                "实验或效果：在三个机器人平台上超越先进方法，展现强鲁棒性和通用性"
            ],
            "tags_zh": [
                "手眼标定",
                "相机外参估计",
                "视觉基础模型",
                "离线校准",
                "机器人视觉"
            ],
            "_index": 108
        },
        {
            "title": "VLM-Augmented Degradation Modeling for Image Restoration Under Adverse Weather Conditions",
            "authors": [
                "Qianyi Shao",
                "Yuanfan Zhang",
                "Renxiang Xiao",
                "Liang Hu"
            ],
            "arxiv_id": "2511.16998v1",
            "summary": "Reliable visual perception under adverse weather conditions, such as rain, haze, snow, or a mixture of them, is desirable yet challenging for autonomous driving and outdoor robots. In this paper, we propose a unified Memory-Enhanced Visual-Language Recovery (MVLR) model that restores images from different degradation levels under various weather conditions. MVLR couples a lightweight encoder-decoder backbone with a Visual-Language Model (VLM) and an Implicit Memory Bank (IMB). The VLM performs chain-of-thought inference to encode weather degradation priors and the IMB stores continuous latent representations of degradation patterns. The VLM-generated priors query the IMB to retrieve fine-grained degradation prototypes. These prototypes are then adaptively fused with multi-scale visual features via dynamic cross-attention mechanisms, enhancing restoration accuracy while maintaining computational efficiency. Extensive experiments on four severe-weather benchmarks show that MVLR surpasses single-branch and Mixture-of-Experts baselines in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). These results indicate that MVLR offers a practical balance between model compactness and expressiveness for real-time deployment in diverse outdoor conditions.",
            "headline_zh": "提出MVLR模型以解决恶劣天气下图像恢复问题",
            "intro_zh": [
                "核心问题：恶劣天气导致图像退化，影响自动驾驶和户外机器人视觉感知。",
                "方法要点：结合视觉语言模型和隐式记忆库，通过链式推理和动态交叉注意力增强恢复。",
                "实验或效果：在多个基准测试中PSNR和SSIM指标优于基线，平衡紧凑性和表达力。"
            ],
            "tags_zh": [
                "图像恢复",
                "视觉语言模型",
                "隐式记忆库",
                "恶劣天气",
                "动态交叉注意力",
                "实时部署"
            ],
            "_index": 109
        },
        {
            "title": "DepthFocus: Controllable Depth Estimation for See-Through Scenes",
            "authors": [
                "Junhong Min",
                "Jimin Kim",
                "Cheol-Hui Min",
                "Minwook Kim",
                "Youngpil Jeon",
                "Minyong Choi"
            ],
            "arxiv_id": "2511.16993v1",
            "summary": "Depth in the real world is rarely singular. Transmissive materials create layered ambiguities that confound conventional perception systems. Existing models remain passive, attempting to estimate static depth maps anchored to the nearest surface, while humans actively shift focus to perceive a desired depth. We introduce DepthFocus, a steerable Vision Transformer that redefines stereo depth estimation as intent-driven control. Conditioned on a scalar depth preference, the model dynamically adapts its computation to focus on the intended depth, enabling selective perception within complex scenes. The training primarily leverages our newly constructed 500k multi-layered synthetic dataset, designed to capture diverse see-through effects. DepthFocus not only achieves state-of-the-art performance on conventional single-depth benchmarks like BOOSTER, a dataset notably rich in transparent and reflective objects, but also quantitatively demonstrates intent-aligned estimation on our newly proposed real and synthetic multi-depth datasets. Moreover, it exhibits strong generalization capabilities on unseen see-through scenes, underscoring its robustness as a significant step toward active and human-like 3D perception.",
            "headline_zh": "提出DepthFocus可控深度估计模型，以解决透视场景中的多层深度模糊问题。",
            "intro_zh": [
                "核心问题：真实世界深度多层面，透射材料导致传统系统难以估计动态焦点深度。",
                "方法要点：基于深度偏好标量，使用可引导Vision Transformer实现意图驱动的深度估计。",
                "实验或效果：在BOOSTER等基准上表现优异，并在新多深度数据集上验证意图对齐估计。"
            ],
            "tags_zh": [
                "深度估计",
                "可控视觉",
                "透视场景",
                "Vision Transformer",
                "合成数据集"
            ],
            "_index": 110
        },
        {
            "title": "DReX: Pure Vision Fusion of Self-Supervised and Convolutional Representations for Image Complexity Prediction",
            "authors": [
                "Jonathan Skaza",
                "Parsa Madinei",
                "Ziqi Wen",
                "Miguel Eckstein"
            ],
            "arxiv_id": "2511.16991v1",
            "summary": "Visual complexity prediction is a fundamental problem in computer vision with applications in image compression, retrieval, and classification. Understanding what makes humans perceive an image as complex is also a long-standing question in cognitive science. Recent approaches have leveraged multimodal models that combine visual and linguistic representations, but it remains unclear whether language information is necessary for this task. We propose DReX (DINO-ResNet Fusion), a vision-only model that fuses self-supervised and convolutional representations through a learnable attention mechanism to predict image complexity. Our architecture integrates multi-scale hierarchical features from ResNet-50 with semantically rich representations from DINOv3 ViT-S/16, enabling the model to capture both low-level texture patterns and high-level semantic structure. DReX achieves state-of-the-art performance on the IC9600 benchmark (Pearson r = 0.9581), surpassing previous methods--including those trained on multimodal image-text data--while using approximately 21.5x fewer learnable parameters. Furthermore, DReX generalizes robustly across multiple datasets and metrics, achieving superior results on Pearson and Spearman correlation, Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). Ablation and attention analyses confirm that DReX leverages complementary cues from both backbones, with the DINOv3 [CLS] token enhancing sensitivity to visual complexity. Our findings suggest that visual features alone can be sufficient for human-aligned complexity prediction and that, when properly fused, self-supervised transformers and supervised deep convolutional neural networks offer complementary and synergistic benefits for this task.",
            "headline_zh": "提出DReX融合自监督与卷积表示，用于图像复杂度预测。",
            "intro_zh": [
                "核心问题：视觉复杂度预测，应用于图像压缩和认知科学。",
                "方法要点：融合ResNet-50和DINOv3特征，使用注意力机制。",
                "实验效果：在IC9600基准上达到SOTA，参数减少21.5倍。"
            ],
            "tags_zh": [
                "图像复杂度预测",
                "特征融合",
                "自监督学习",
                "卷积神经网络",
                "注意力机制"
            ],
            "_index": 111
        },
        {
            "title": "RadioKMoE: Knowledge-Guided Radiomap Estimation with Kolmogorov-Arnold Networks and Mixture-of-Experts",
            "authors": [
                "Fupei Guo",
                "Kerry Pan",
                "Songyang Zhang",
                "Yue Wang",
                "Zhi Ding"
            ],
            "arxiv_id": "2511.16986v1",
            "summary": "Radiomap serves as a vital tool for wireless network management and deployment by providing powerful spatial knowledge of signal propagation and coverage. However, increasingly complex radio propagation behavior and surrounding environments pose strong challenges for radiomap estimation (RME). In this work, we propose a knowledge-guided RME framework that integrates Kolmogorov-Arnold Networks (KAN) with Mixture-of-Experts (MoE), namely RadioKMoE. Specifically, we design a KAN module to predict an initial coarse coverage map, leveraging KAN's strength in approximating physics models and global radio propagation patterns. The initial coarse map, together with environmental information, drives our MoE network for precise radiomap estimation. Unlike conventional deep learning models, the MoE module comprises expert networks specializing in distinct radiomap patterns to improve local details while preserving global consistency. Experimental results in both multi- and single-band RME demonstrate the enhanced accuracy and robustness of the proposed RadioKMoE in radiomap estimation.",
            "headline_zh": "提出RadioKMoE框架以解决复杂环境下的无线信号覆盖图估计问题",
            "intro_zh": [
                "核心问题：复杂无线电传播行为和环境挑战无线信号覆盖图估计的准确性",
                "方法要点：结合KAN预测粗覆盖图和MoE网络精修，提升局部细节与全局一致性",
                "实验或效果：多频段和单频段实验显示，RadioKMoE在估计精度和鲁棒性上优于传统方法"
            ],
            "tags_zh": [
                "无线信号覆盖图估计",
                "Kolmogorov-Arnold网络",
                "混合专家模型",
                "无线电传播建模",
                "深度学习应用"
            ],
            "_index": 112
        },
        {
            "title": "A Diversity-optimized Deep Ensemble Approach for Accurate Plant Leaf Disease Detection",
            "authors": [
                "Sai Nath Chowdary Medikonduru",
                "Hongpeng Jin",
                "Yanzhao Wu"
            ],
            "arxiv_id": "2511.16982v1",
            "summary": "Plant diseases pose a significant threat to global agriculture, causing over $220 billion in annual economic losses and jeopardizing food security. The timely and accurate detection of these diseases from plant leaf images is critical to mitigating their adverse effects. Deep neural network Ensembles (Deep Ensembles) have emerged as a powerful approach to enhancing prediction accuracy by leveraging the strengths of diverse Deep Neural Networks (DNNs). However, selecting high-performing ensemble member models is challenging due to the inherent difficulty in measuring ensemble diversity. In this paper, we introduce the Synergistic Diversity (SQ) framework to enhance plant disease detection accuracy. First, we conduct a comprehensive analysis of the limitations of existing ensemble diversity metrics (denoted as Q metrics), which often fail to identify optimal ensemble teams. Second, we present the SQ metric, a novel measure that captures the synergy between ensemble members and consistently aligns with ensemble accuracy. Third, we validate our SQ approach through extensive experiments on a plant leaf image dataset, which demonstrates that our SQ metric substantially improves ensemble selection and enhances detection accuracy. Our findings pave the way for a more reliable and efficient image-based plant disease detection.",
            "headline_zh": "提出协同多样性框架以提升植物叶片病害检测精度",
            "intro_zh": [
                "现有集成多样性指标难以识别最优集成团队，影响检测准确性。",
                "引入协同多样性指标，捕捉成员间协同效应，与集成精度一致。",
                "在植物叶片图像数据集上验证，SQ指标显著改进集成选择和检测精度。"
            ],
            "tags_zh": [
                "植物病害检测",
                "深度集成",
                "多样性指标",
                "图像分析",
                "集成选择"
            ],
            "_index": 113
        },
        {
            "title": "Gradient-Driven Natural Selection for Compact 3D Gaussian Splatting",
            "authors": [
                "Xiaobin Deng",
                "Qiuli Yu",
                "Changyu Diao",
                "Min Li",
                "Duanqing Xu"
            ],
            "arxiv_id": "2511.16980v1",
            "summary": "3DGS employs a large number of Gaussian primitives to fit scenes, resulting in substantial storage and computational overhead. Existing pruning methods rely on manually designed criteria or introduce additional learnable parameters, yielding suboptimal results. To address this, we propose an natural selection inspired pruning framework that models survival pressure as a regularization gradient field applied to opacity, allowing the optimization gradients--driven by the goal of maximizing rendering quality--to autonomously determine which Gaussians to retain or prune. This process is fully learnable and requires no human intervention. We further introduce an opacity decay technique with a finite opacity prior, which accelerates the selection process without compromising pruning effectiveness. Compared to 3DGS, our method achieves over 0.6 dB PSNR gain under 15\\% budgets, establishing state-of-the-art performance for compact 3DGS. Project page https://xiaobin2001.github.io/GNS-web.",
            "headline_zh": "提出基于自然选择的剪枝框架以压缩3D高斯溅射存储与计算开销",
            "intro_zh": [
                "3D高斯溅射使用大量高斯基元导致高存储与计算成本",
                "利用优化梯度驱动自然选择剪枝，无需人工干预或额外参数",
                "在15%预算下PSNR增益超0.6dB，实现紧凑3D高斯溅射最优性能"
            ],
            "tags_zh": [
                "3D高斯溅射",
                "剪枝优化",
                "自然选择",
                "渲染质量",
                "紧凑表示",
                "梯度驱动"
            ],
            "_index": 114
        },
        {
            "title": "The Finer the Better: Towards Granular-aware Open-set Domain Generalization",
            "authors": [
                "Yunyun Wang",
                "Zheng Duan",
                "Xinyue Liao",
                "Ke-Jia Chen",
                "Songcan Chen"
            ],
            "arxiv_id": "2511.16979v1",
            "summary": "Open-Set Domain Generalization (OSDG) tackles the realistic scenario where deployed models encounter both domain shifts and novel object categories. Despite impressive progress with vision-language models like CLIP, existing methods still fall into the dilemma between structural risk of known-classes and open-space risk from unknown-classes, and easily suffers from over-confidence, especially when distinguishing ``hard unknowns\" that share fine-grained visual similarities with known classes. To this end, we propose a Semantic-enhanced CLIP (SeeCLIP) framework that explicitly addresses this dilemma through fine-grained semantic enhancement. In SeeCLIP, we propose a semantic-aware prompt enhancement module to decompose images into discriminative semantic tokens, enabling nuanced vision-language alignment beyond coarse category labels. To position unknown prompts effectively, we introduce duplex contrastive learning with complementary objectives, that is, repulsion to maintain separability from known classes, and cohesion to preserve semantic proximity. Further, our semantic-guided diffusion module synthesizes pseudo-unknowns by perturbing extracted semantic tokens, generating challenging samples that are visually similar to known classes yet exhibit key local differences. These hard negatives force the model to learn finer decision boundaries. Extensive experiments across five benchmarks demonstrate consistent improvements of 3% accuracy and 5% H-score over state-of-the-art methods.",
            "headline_zh": "提出SeeCLIP框架以解决开放集域泛化中细粒度未知类识别难题",
            "intro_zh": [
                "核心问题：开放集域泛化中模型易对与已知类视觉相似的硬未知类过度自信",
                "方法要点：通过语义增强提示和双工对比学习提升细粒度视觉语言对齐",
                "实验或效果：在五个基准测试中准确率提升3%，H-score提升5%"
            ],
            "tags_zh": [
                "开放集域泛化",
                "视觉语言模型",
                "细粒度语义",
                "对比学习",
                "伪未知生成"
            ],
            "_index": 115
        },
        {
            "title": "Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring on Edge Devices",
            "authors": [
                "Jigyasa Gupta",
                "Soumya Goyal",
                "Anil Kumar",
                "Ishan Jindal"
            ],
            "arxiv_id": "2511.16965v1",
            "summary": "Synthesizing realistic cooked food images from raw inputs on edge devices is a challenging generative task, requiring models to capture complex changes in texture, color and structure during cooking. Existing image-to-image generation methods often produce unrealistic results or are too resource-intensive for edge deployment. We introduce the first oven-based cooking-progression dataset with chef-annotated doneness levels and propose an edge-efficient recipe and cooking state guided generator that synthesizes realistic food images conditioned on raw food image. This formulation enables user-preferred visual targets rather than fixed presets. To ensure temporal consistency and culinary plausibility, we introduce a domain-specific \\textit{Culinary Image Similarity (CIS)} metric, which serves both as a training loss and a progress-monitoring signal. Our model outperforms existing baselines with significant reductions in FID scores (30\\% improvement on our dataset; 60\\% on public datasets)",
            "headline_zh": "提出边缘设备上基于烹饪状态引导的生成器以合成真实熟食图像并监控烹饪进度",
            "intro_zh": [
                "核心问题：边缘设备上合成真实熟食图像困难，现有方法不真实或资源消耗大",
                "方法要点：引入烹饪状态引导生成器，结合食谱和烹饪状态条件生成图像",
                "实验或效果：在数据集上FID分数显著降低，改进30%至60%"
            ],
            "tags_zh": [
                "图像合成",
                "边缘计算",
                "烹饪进度监控",
                "生成对抗网络",
                "领域特定度量"
            ],
            "_index": 116
        },
        {
            "title": "Two Heads Better than One: Dual Degradation Representation for Blind Super-Resolution",
            "authors": [
                "Hsuan Yuan",
                "Shao-Yu Weng",
                "I-Hsuan Lo",
                "Wei-Chen Chiu",
                "Yu-Syuan Xu",
                "Hao-Chien Hsueh",
                "Jen-Hui Chuang",
                "Ching-Chun Huang"
            ],
            "arxiv_id": "2511.16963v1",
            "summary": "Previous methods have demonstrated remarkable performance in single image super-resolution (SISR) tasks with known and fixed degradation (e.g., bicubic downsampling). However, when the actual degradation deviates from these assumptions, these methods may experience significant declines in performance. In this paper, we propose a Dual Branch Degradation Extractor Network to address the blind SR problem. While some blind SR methods assume noise-free degradation and others do not explicitly consider the presence of noise in the degradation model, our approach predicts two unsupervised degradation embeddings that represent blurry and noisy information. The SR network can then be adapted to blur embedding and noise embedding in distinct ways. Furthermore, we treat the degradation extractor as a regularizer to capitalize on differences between SR and HR images. Extensive experiments on several benchmarks demonstrate our method achieves SOTA performance in the blind SR problem.",
            "headline_zh": "提出双分支退化提取网络以解决盲超分辨率问题",
            "intro_zh": [
                "盲超分辨率中实际退化与假设不符导致性能下降",
                "预测无监督模糊和噪声嵌入，SR网络分别适应",
                "在多个基准测试中实现最先进性能"
            ],
            "tags_zh": [
                "盲超分辨率",
                "退化提取",
                "双分支网络",
                "无监督嵌入",
                "图像超分"
            ],
            "_index": 117
        },
        {
            "title": "MatPedia: A Universal Generative Foundation for High-Fidelity Material Synthesis",
            "authors": [
                "Di Luo",
                "Shuhui Yang",
                "Mingxin Yang",
                "Jiawei Lu",
                "Yixuan Tang",
                "Xintong Han",
                "Zhuo Chen",
                "Beibei Wang",
                "Chunchao Guo"
            ],
            "arxiv_id": "2511.16957v1",
            "summary": "Physically-based rendering (PBR) materials are fundamental to photorealistic graphics, yet their creation remains labor-intensive and requires specialized expertise. While generative models have advanced material synthesis, existing methods lack a unified representation bridging natural image appearance and PBR properties, leading to fragmented task-specific pipelines and inability to leverage large-scale RGB image data. We present MatPedia, a foundation model built upon a novel joint RGB-PBR representation that compactly encodes materials into two interdependent latents: one for RGB appearance and one for the four PBR maps encoding complementary physical properties. By formulating them as a 5-frame sequence and employing video diffusion architectures, MatPedia naturally captures their correlations while transferring visual priors from RGB generation models. This joint representation enables a unified framework handling multiple material tasks--text-to-material generation, image-to-material generation, and intrinsic decomposition--within a single architecture. Trained on MatHybrid-410K, a mixed corpus combining PBR datasets with large-scale RGB images, MatPedia achieves native $1024\\times1024$ synthesis that substantially surpasses existing approaches in both quality and diversity.",
            "headline_zh": "提出MatPedia统一生成基础模型，以解决高保真材料合成中的表示分裂问题。",
            "intro_zh": [
                "核心问题：现有材料生成方法缺乏统一表示，无法桥接自然图像外观与PBR属性。",
                "方法要点：采用联合RGB-PBR表示，将材料编码为RGB和PBR潜在序列，使用视频扩散架构。",
                "实验或效果：在MatHybrid-410K数据集上训练，实现1024×1024合成，质量和多样性超越现有方法。"
            ],
            "tags_zh": [
                "材料合成",
                "联合表示",
                "视频扩散",
                "PBR材料",
                "生成模型",
                "高保真渲染"
            ],
            "_index": 118
        },
        {
            "title": "Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models",
            "authors": [
                "Dailan He",
                "Guanlin Feng",
                "Xingtong Ge",
                "Yazhe Niu",
                "Yi Zhang",
                "Bingqi Ma",
                "Guanglu Song",
                "Yu Liu",
                "Hongsheng Li"
            ],
            "arxiv_id": "2511.16955v1",
            "summary": "Group Relative Policy Optimization (GRPO) has shown promise in aligning image and video generative models with human preferences. However, applying it to modern flow matching models is challenging because of its deterministic sampling paradigm. Current methods address this issue by converting Ordinary Differential Equations (ODEs) to Stochastic Differential Equations (SDEs), which introduce stochasticity. However, this SDE-based GRPO suffers from issues of inefficient credit assignment and incompatibility with high-order solvers for fewer-step sampling. In this paper, we first reinterpret existing SDE-based GRPO methods from a distance optimization perspective, revealing their underlying mechanism as a form of contrastive learning. Based on this insight, we propose Neighbor GRPO, a novel alignment algorithm that completely bypasses the need for SDEs. Neighbor GRPO generates a diverse set of candidate trajectories by perturbing the initial noise conditions of the ODE and optimizes the model using a softmax distance-based surrogate leaping policy. We establish a theoretical connection between this distance-based objective and policy gradient optimization, rigorously integrating our approach into the GRPO framework. Our method fully preserves the advantages of deterministic ODE sampling, including efficiency and compatibility with high-order solvers. We further introduce symmetric anchor sampling for computational efficiency and group-wise quasi-norm reweighting to address reward flattening. Extensive experiments demonstrate that Neighbor GRPO significantly outperforms SDE-based counterparts in terms of training cost, convergence speed, and generation quality.",
            "headline_zh": "提出Neighbor GRPO以解决流匹配模型对齐中的采样效率问题",
            "intro_zh": [
                "核心问题：SDE-based GRPO在流匹配模型中存在信用分配低效和与高阶求解器不兼容",
                "方法要点：通过扰动ODE初始噪声生成多样轨迹，使用基于距离的代理策略优化模型",
                "实验或效果：在训练成本、收敛速度和生成质量上显著优于SDE-based方法"
            ],
            "tags_zh": [
                "流匹配模型",
                "策略优化",
                "对比学习",
                "ODE采样",
                "对齐算法"
            ],
            "_index": 119
        },
        {
            "title": "Point-Supervised Facial Expression Spotting with Gaussian-Based Instance-Adaptive Intensity Modeling",
            "authors": [
                "Yicheng Deng",
                "Hideaki Hayashi",
                "Hajime Nagahara"
            ],
            "arxiv_id": "2511.16952v1",
            "summary": "Automatic facial expression spotting, which aims to identify facial expression instances in untrimmed videos, is crucial for facial expression analysis. Existing methods primarily focus on fully-supervised learning and rely on costly, time-consuming temporal boundary annotations. In this paper, we investigate point-supervised facial expression spotting (P-FES), where only a single timestamp annotation per instance is required for training. We propose a unique two-branch framework for P-FES. First, to mitigate the limitation of hard pseudo-labeling, which often confuses neutral and expression frames with various intensities, we propose a Gaussian-based instance-adaptive intensity modeling (GIM) module to model instance-level expression intensity distribution for soft pseudo-labeling. By detecting the pseudo-apex frame around each point label, estimating the duration, and constructing an instance-level Gaussian distribution, GIM assigns soft pseudo-labels to expression frames for more reliable intensity supervision. The GIM module is incorporated into our framework to optimize the class-agnostic expression intensity branch. Second, we design a class-aware apex classification branch that distinguishes macro- and micro-expressions solely based on their pseudo-apex frames. During inference, the two branches work independently: the class-agnostic expression intensity branch generates expression proposals, while the class-aware apex-classification branch is responsible for macro- and micro-expression classification.Furthermore, we introduce an intensity-aware contrastive loss to enhance discriminative feature learning and suppress neutral noise by contrasting neutral frames with expression frames with various intensities. Extensive experiments on the SAMM-LV, CAS(ME)$^2$, and CAS(ME)$^3$ datasets demonstrate the effectiveness of our proposed framework.",
            "headline_zh": "提出基于高斯实例自适应强度建模的点监督框架以解决面部表情定位问题",
            "intro_zh": [
                "核心问题：点监督面部表情定位依赖单时间戳标注，避免昂贵边界标注。",
                "方法要点：使用高斯建模软伪标签优化强度分支，结合分类分支区分宏微表情。",
                "实验或效果：在SAMM-LV等数据集验证有效性，提升定位和分类性能。"
            ],
            "tags_zh": [
                "面部表情定位",
                "点监督学习",
                "高斯强度建模",
                "软伪标签",
                "宏微表情分类"
            ],
            "_index": 120
        },
        {
            "title": "FingerCap: Fine-grained Finger-level Hand Motion Captioning",
            "authors": [
                "Xin Shen",
                "Rui Zhu",
                "Lei Shen",
                "Xinyu Wang",
                "Kaihao Zhang",
                "Tianqing Zhu",
                "Shuchen Wu",
                "Chenxi Miao",
                "Weikang Li",
                "Yang Li",
                "Deguo Xia",
                "Jizhou Huang",
                "Xin Yu"
            ],
            "arxiv_id": "2511.16951v1",
            "summary": "Understanding fine-grained human hand motion is fundamental to visual perception, embodied intelligence, and multimodal communication. In this work, we propose Fine-grained Finger-level Hand Motion Captioning (FingerCap), which aims to generate textual descriptions that capture detailed finger-level semantics of hand actions. To support this task, we curate FingerCap-40K, a large-scale corpus of 40K paired hand-motion videos and captions spanning two complementary sources: concise instruction-style finger motions and diverse, naturalistic hand-object interactions. To enable effective evaluation, we employ HandJudge, a LLM-based rubric that measures finger-level correctness and motion completeness. Temporal sparsity remains a fundamental bottleneck for current Video-MLLMs, since sparse RGB sampling is insufficient to capture the subtle, high-frequency dynamics underlying fine finger motions. As a simple and compute-friendly remedy, we introduce FiGOP (Finger Group-of-Pictures), which pairs each RGB keyframe with subsequent hand keypoints until the next keyframe. A lightweight temporal encoder converts the keypoints into motion embeddings and integrates them with RGB features. FiGOP adapts the classic GOP concept to finger motion, recovering fine temporal cues without increasing RGB density. Experiments on FingerCap-40K show that strong open- and closed-source Video-MLLMs still struggle with finger-level reasoning, while our FiGOP-augmented model yield consistent gains under HandJudge and human studies.",
            "headline_zh": "提出FiGOP方法以解决视频多模态大模型在细粒度手指运动理解中的时间稀疏性问题",
            "intro_zh": [
                "核心问题：视频多模态大模型因RGB采样稀疏，难以捕捉手指细微高频动态。",
                "方法要点：引入FiGOP，将RGB关键帧与后续手部关键点配对，编码运动嵌入。",
                "实验或效果：在FingerCap-40K数据集上，FiGOP增强模型在HandJudge评估中表现更优。"
            ],
            "tags_zh": [
                "手指运动描述",
                "视频多模态大模型",
                "时间稀疏性",
                "手部关键点",
                "运动嵌入",
                "细粒度理解"
            ],
            "_index": 121
        },
        {
            "title": "MobileOcc: A Human-Aware Semantic Occupancy Dataset for Mobile Robots",
            "authors": [
                "Junseo Kim",
                "Guido Dumont",
                "Xinyu Gao",
                "Gang Chen",
                "Holger Caesar",
                "Javier Alonso-Mora"
            ],
            "arxiv_id": "2511.16949v1",
            "summary": "Dense 3D semantic occupancy perception is critical for mobile robots operating in pedestrian-rich environments, yet it remains underexplored compared to its application in autonomous driving. To address this gap, we present MobileOcc, a semantic occupancy dataset for mobile robots operating in crowded human environments. Our dataset is built using an annotation pipeline that incorporates static object occupancy annotations and a novel mesh optimization framework explicitly designed for human occupancy modeling. It reconstructs deformable human geometry from 2D images and subsequently refines and optimizes it using associated LiDAR point data. Using MobileOcc, we establish benchmarks for two tasks, i) Occupancy prediction and ii) Pedestrian velocity prediction, using different methods including monocular, stereo, and panoptic occupancy, with metrics and baseline implementations for reproducible comparison. Beyond occupancy prediction, we further assess our annotation method on 3D human pose estimation datasets. Results demonstrate that our method exhibits robust performance across different datasets.",
            "headline_zh": "提出MobileOcc数据集以解决移动机器人在拥挤环境中语义占用感知不足的问题",
            "intro_zh": [
                "核心问题：密集3D语义占用感知在移动机器人领域研究不足，尤其在行人密集环境",
                "方法要点：结合静态对象标注和新型网格优化框架，从2D图像重建可变形人体几何",
                "实验或效果：建立占用和行人速度预测基准，并在3D人体姿态估计中验证鲁棒性"
            ],
            "tags_zh": [
                "语义占用感知",
                "移动机器人",
                "人体几何重建",
                "LiDAR优化",
                "基准数据集",
                "3D人体姿态估计"
            ],
            "_index": 122
        },
        {
            "title": "Flow-Guided Implicit Neural Representation for Motion-Aware Dynamic MRI Reconstruction",
            "authors": [
                "Baoqing Li",
                "Yuanyuan Liu",
                "Congcong Liu",
                "Qingyong Zhu",
                "Jing Cheng",
                "Yihang Zhou",
                "Hao Chen",
                "Zhuo-Xu Cui",
                "Dong Liang"
            ],
            "arxiv_id": "2511.16948v1",
            "summary": "Dynamic magnetic resonance imaging (dMRI) captures temporally-resolved anatomy but is often challenged by limited sampling and motion-induced artifacts. Conventional motion-compensated reconstructions typically rely on pre-estimated optical flow, which is inaccurate under undersampling and degrades reconstruction quality. In this work, we propose a novel implicit neural representation (INR) framework that jointly models both the dynamic image sequence and its underlying motion field. Specifically, one INR is employed to parameterize the spatiotemporal image content, while another INR represents the optical flow. The two are coupled via the optical flow equation, which serves as a physics-inspired regularization, in addition to a data consistency loss that enforces agreement with k-space measurements. This joint optimization enables simultaneous recovery of temporally coherent images and motion fields without requiring prior flow estimation. Experiments on dynamic cardiac MRI datasets demonstrate that the proposed method outperforms state-of-the-art motion-compensated and deep learning approaches, achieving superior reconstruction quality, accurate motion estimation, and improved temporal fidelity. These results highlight the potential of implicit joint modeling with flow-regularized constraints for advancing dMRI reconstruction.",
            "headline_zh": "提出流引导隐式神经表示框架，联合重建动态MRI图像与运动场",
            "intro_zh": [
                "动态MRI面临采样不足和运动伪影问题，传统方法依赖预估计光流易不准确",
                "使用两个隐式神经表示分别建模图像序列和光流，通过光流方程耦合作为正则化",
                "实验显示在心脏MRI上优于现有方法，提升重建质量、运动估计精度和时间一致性"
            ],
            "tags_zh": [
                "动态MRI重建",
                "隐式神经表示",
                "光流估计",
                "联合优化",
                "运动补偿"
            ],
            "_index": 123
        },
        {
            "title": "MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models",
            "authors": [
                "Xiongtao Sun",
                "Hui Li",
                "Jiaming Zhang",
                "Yujie Yang",
                "Kaili Liu",
                "Ruxin Feng",
                "Wen Jun Tan",
                "Wei Yang Bryan Lim"
            ],
            "arxiv_id": "2511.16940v1",
            "summary": "Modern Vision-Language Models (VLMs) demonstrate sophisticated reasoning, escalating privacy risks beyond simple attribute perception to individual-level linkage. Current privacy benchmarks are structurally insufficient for this new threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM's ability to infer and link distributed information to construct individual profiles. To address this critical gap, we propose \\textbf{MultiPriv}, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the \\textbf{Privacy Perception and Reasoning (PPR)} framework and construct a novel, bilingual multimodal dataset to support it. The dataset uniquely features a core component of synthetic individual profiles where identifiers (e.g., faces, names) are meticulously linked to sensitive attributes. This design enables nine challenging tasks evaluating the full PPR spectrum, from attribute detection to cross-image re-identification and chained inference. We conduct a large-scale evaluation of over 50 foundational and commercial VLMs. Our analysis reveals: (1) Many VLMs possess significant, unmeasured reasoning-based privacy risks. (2) Perception-level metrics are poor predictors of these reasoning risks, revealing a critical evaluation gap. (3) Existing safety alignments are inconsistent and ineffective against such reasoning-based attacks. MultiPriv exposes systemic vulnerabilities and provides the necessary framework for developing robust, privacy-preserving VLMs.",
            "headline_zh": "提出MultiPriv基准以评估视觉语言模型中的个体级隐私推理风险",
            "intro_zh": [
                "核心问题：现有隐私基准无法评估VLMs从分布式信息推断个体档案的推理风险",
                "方法要点：构建双语多模态数据集，包含合成个体档案，支持九项隐私推理任务",
                "实验或效果：评估50多个VLMs，揭示推理风险高、感知指标预测差、安全对齐无效"
            ],
            "tags_zh": [
                "视觉语言模型",
                "隐私推理基准",
                "个体档案构建",
                "多模态数据集",
                "安全对齐评估",
                "隐私风险分析"
            ],
            "_index": 124
        },
        {
            "title": "OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios",
            "authors": [
                "Hong Gao",
                "Jingyu Wu",
                "Xiangkai Xu",
                "Kangni Xie",
                "Yunchen Zhang",
                "Bin Zhong",
                "Xurui Gao",
                "Min-Ling Zhang"
            ],
            "arxiv_id": "2511.16937v1",
            "summary": "Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\\_tIoU and m\\_vIoU on OmniGround with consistent gains across four benchmarks.",
            "headline_zh": "提出OmniGround基准和PG-TAF框架以解决复杂场景时空视频定位问题",
            "intro_zh": [
                "核心问题：现有STVG模型在真实复杂场景中表现不佳，存在类别偏见和推理简化问题",
                "方法要点：引入OmniGround基准和PG-TAF训练免费框架，分解时空定位任务",
                "实验或效果：PG-TAF在OmniGround上m_tIoU和m_vIoU分别提升25.6%和35.6%"
            ],
            "tags_zh": [
                "时空视频定位",
                "基准构建",
                "训练免费框架",
                "多模态语言模型",
                "复杂查询处理"
            ],
            "_index": 125
        },
        {
            "title": "Shape-preserving Tooth Segmentation from CBCT Images Using Deep Learning with Semantic and Shape Awareness",
            "authors": [
                "Zongrui Ji",
                "Zhiming Cui",
                "Na Li",
                "Qianhan Zheng",
                "Miaojing Shi",
                "Ke Deng",
                "Jingyang Zhang",
                "Chaoyuan Li",
                "Xuepeng Chen",
                "Yi Dong",
                "Lei Ma"
            ],
            "arxiv_id": "2511.16936v1",
            "summary": "Background:Accurate tooth segmentation from cone beam computed tomography (CBCT) images is crucial for digital dentistry but remains challenging in cases of interdental adhesions, which cause severe anatomical shape distortion.\n  Methods:\n  To address this, we propose a deep learning framework that integrates semantic and shape awareness for shape-preserving segmentation. Our method introduces a target-tooth-centroid prompted multi-label learning strategy to model semantic relationships between teeth, reducing shape ambiguity. Additionally, a tooth-shape-aware learning mechanism explicitly enforces morphological constraints to preserve boundary integrity. These components are unified via multi-task learning, jointly optimizing segmentation and shape preservation.\n  Results: Extensive evaluations on internal and external datasets demonstrate that our approach significantly outperforms existing methods.\n  Conclusions: Our approach effectively mitigates shape distortions and providing anatomically faithful tooth boundaries.",
            "headline_zh": "提出语义与形状感知深度学习框架以解决CBCT图像中牙齿粘连导致的形状失真问题",
            "intro_zh": [
                "核心问题：CBCT图像中牙齿粘连导致解剖形状失真，影响分割准确性",
                "方法要点：集成语义关系建模与形态约束，通过多任务学习优化分割与形状保持",
                "实验或效果：在内外数据集上显著优于现有方法，有效减少形状失真"
            ],
            "tags_zh": [
                "牙齿分割",
                "CBCT图像",
                "深度学习",
                "形状保持",
                "语义建模",
                "多任务学习"
            ],
            "_index": 126
        },
        {
            "title": "Rethinking Diffusion Model-Based Video Super-Resolution: Leveraging Dense Guidance from Aligned Features",
            "authors": [
                "Jingyi Xu",
                "Meisong Zheng",
                "Ying Chen",
                "Minglang Qiao",
                "Xin Deng",
                "Mai Xu"
            ],
            "arxiv_id": "2511.16928v1",
            "summary": "Diffusion model (DM) based Video Super-Resolution (VSR) approaches achieve impressive perceptual quality. However, they suffer from error accumulation, spatial artifacts, and a trade-off between perceptual quality and fidelity, primarily caused by inaccurate alignment and insufficient compensation between video frames. In this paper, within the DM-based VSR pipeline, we revisit the role of alignment and compensation between adjacent video frames and reveal two crucial observations: (a) the feature domain is better suited than the pixel domain for information compensation due to its stronger spatial and temporal correlations, and (b) warping at an upscaled resolution better preserves high-frequency information, but this benefit is not necessarily monotonic. Therefore, we propose a novel Densely Guided diffusion model with Aligned Features for Video Super-Resolution (DGAF-VSR), with an Optical Guided Warping Module (OGWM) to maintain high-frequency details in the aligned features and a Feature-wise Temporal Condition Module (FTCM) to deliver dense guidance in the feature domain. Extensive experiments on synthetic and real-world datasets demonstrate that DGAF-VSR surpasses state-of-the-art methods in key aspects of VSR, including perceptual quality (35.82\\% DISTS reduction), fidelity (0.20 dB PSNR gain), and temporal consistency (30.37\\% tLPIPS reduction).",
            "headline_zh": "提出DGAF-VSR模型，利用对齐特征密集引导解决视频超分辨率中的误差累积与保真度权衡问题",
            "intro_zh": [
                "核心问题：基于扩散模型的视频超分辨率存在误差累积、空间伪影及感知质量与保真度权衡问题",
                "方法要点：引入光学引导变形模块和特征域时序条件模块，在特征域进行密集引导与对齐",
                "实验或效果：在合成和真实数据集上，感知质量、保真度和时序一致性均优于现有方法"
            ],
            "tags_zh": [
                "视频超分辨率",
                "扩散模型",
                "特征对齐",
                "时序一致性",
                "光学引导变形"
            ],
            "_index": 127
        },
        {
            "title": "DeltaDeno: Zero-Shot Anomaly Generation via Delta-Denoising Attribution",
            "authors": [
                "Chaoran Xu",
                "Chengkan Lv",
                "Qiyu Chen",
                "Yunkang Cao",
                "Feng Zhang",
                "Zhengtao Zhang"
            ],
            "arxiv_id": "2511.16920v1",
            "summary": "Anomaly generation is often framed as few-shot fine-tuning with anomalous samples, which contradicts the scarcity that motivates generation and tends to overfit category priors. We tackle the setting where no real anomaly samples or training are available. We propose Delta-Denoising (DeltaDeno), a training-free zero-shot anomaly generation method that localizes and edits defects by contrasting two diffusion branches driven by a minimal prompt pair under a shared schedule. By accumulating per-step denoising deltas into an image-specific localization map, we obtain a mask to guide the latent inpainting during later diffusion steps and preserve the surrounding context while generating realistic local defects. To improve stability and control, DeltaDeno performs token-level prompt refinement that aligns shared content and strengthens anomaly tokens, and applies a spatial attention bias restricted to anomaly tokens in the predicted region. Experiments on public datasets show that DeltaDeno achieves great generation, realism and consistent gains in downstream detection performance. Code will be made publicly available.",
            "headline_zh": "提出DeltaDeno方法以在零样本场景下生成异常样本",
            "intro_zh": [
                "核心问题：异常样本稀缺，传统方法依赖少量样本微调易过拟合",
                "方法要点：基于扩散模型对比分支，通过去噪差异定位并编辑缺陷",
                "实验效果：在公共数据集上生成真实异常，提升下游检测性能"
            ],
            "tags_zh": [
                "零样本异常生成",
                "扩散模型",
                "去噪对比",
                "图像定位",
                "潜在修复"
            ],
            "_index": 128
        },
        {
            "title": "UniModel: A Visual-Only Framework for Unified Multimodal Understanding and Generation",
            "authors": [
                "Chi Zhang",
                "Jiepeng Wang",
                "Youming Wang",
                "Yuanzhi Liang",
                "Xiaoyan Yang",
                "Zuoxin Li",
                "Haibin Huang",
                "Xuelong Li"
            ],
            "arxiv_id": "2511.16917v1",
            "summary": "We present UniModel, a unified generative model that jointly supports visual understanding and visual generation within a single pixel-to-pixel diffusion framework. Our goal is to achieve unification along three axes: the model, the tasks, and the representations. At the representation level, we eliminate modality discrepancies by mapping both text and images into a shared visual space: textual prompts are rendered as painted text images on a clean canvas, and all inputs and outputs are treated purely as RGB pixels. This yields a fully vision-native formulation of multimodal learning. At the task level, a broad range of vision-language problems are cast as pixel-to-pixel transformations in this visual space. For understanding tasks, the model takes an RGB image and produces a painted text image that visually encodes the semantic prediction. For generation tasks, painted text images serve as visual conditions that guide realistic and semantically aligned image synthesis. Captioning and text-to-image generation thus become different directions of the same underlying visual translation process. At the model level, we instantiate a single Unified Diffusion Transformer trained with rectified flow in pixel space. A shared backbone jointly learns bidirectional mappings between natural images and painted text images, with lightweight task embeddings to specify the desired direction. Experiments on text-to-image synthesis and image-to-text understanding demonstrate strong cross-modal alignment and emergent controllability such as cycle-consistent image-caption-image loops. Our initial exploration suggests that unifying model, tasks, and representations in a single visual space is a promising paradigm for general-purpose multimodal intelligence.",
            "headline_zh": "提出UniModel以统一视觉理解与生成，通过像素级扩散框架实现多模态学习。",
            "intro_zh": [
                "核心问题：多模态学习中模态差异阻碍统一模型开发。",
                "方法要点：将文本和图像映射到共享视觉空间，使用像素到像素变换。",
                "实验或效果：在文本到图像和图像到文本任务中展示强对齐和可控性。"
            ],
            "tags_zh": [
                "统一多模态模型",
                "像素级扩散",
                "视觉空间映射",
                "生成与理解",
                "扩散变换器"
            ],
            "_index": 129
        },
        {
            "title": "Multi-UAV Swarm Obstacle Avoidance Based on Potential Field Optimization",
            "authors": [
                "Yendo Hu",
                "Yiliang Wu",
                "Weican Chen"
            ],
            "arxiv_id": "2511.16911v1",
            "summary": "In multi UAV scenarios,the traditional Artificial Potential Field (APF) method often leads to redundant flight paths and frequent abrupt heading changes due to unreasonable obstacle avoidance path planning,and is highly prone to inter UAV collisions during the obstacle avoidance process.To address these issues,this study proposes a novel hybrid algorithm that combines the improved Multi-Robot Formation Obstacle Avoidance (MRF IAPF) algorithm with an enhanced APF optimized for single UAV path planning.Its core ideas are as follows:first,integrating three types of interaction forces from MRF IAPF obstacle repulsion force,inter UAV interaction force,and target attraction force;second,incorporating a refined single UAV path optimization mechanism,including collision risk assessment and an auxiliary sub goal strategy.When a UAV faces a high collision threat,temporary waypoints are generated to guide obstacle avoidance,ensuring eventual precise arrival at the actual target.Simulation results demonstrate that compared with traditional APF based formation algorithms,the proposed algorithm achieves significant improvements in path length optimization and heading stability,can effectively avoid obstacles and quickly restore the formation configuration,thus verifying its applicability and effectiveness in static environments with unknown obstacles.",
            "headline_zh": "提出混合算法以解决多无人机编队避障中的路径冗余和碰撞问题",
            "intro_zh": [
                "传统人工势场法导致路径冗余、航向突变和无人机间碰撞",
                "结合改进编队避障算法与单机路径优化，引入交互力和辅助子目标策略",
                "仿真显示路径长度和航向稳定性显著提升，有效避障并快速恢复编队"
            ],
            "tags_zh": [
                "多无人机编队",
                "人工势场优化",
                "避障算法",
                "路径规划",
                "碰撞风险评估"
            ],
            "_index": 130
        },
        {
            "title": "Q-REAL: Towards Realism and Plausibility Evaluation for AI-Generated Content",
            "authors": [
                "Shushi Wang",
                "Zicheng Zhang",
                "Chunyi Li",
                "Wei Wang",
                "Liya Ma",
                "Fengjiao Chen",
                "Xiaoyu Li",
                "Xuezhi Cao",
                "Guangtao Zhai",
                "Xiaohong Liu"
            ],
            "arxiv_id": "2511.16908v1",
            "summary": "Quality assessment of AI-generated content is crucial for evaluating model capability and guiding model optimization. However, most existing quality assessment datasets and models provide only a single quality score, which is too coarse to offer targeted guidance for improving generative models. In current applications of AI-generated images, realism and plausibility are two critical dimensions, and with the emergence of unified generation-understanding models, fine-grained evaluation along these dimensions becomes especially effective for improving generative performance. Therefore, we introduce Q-Real, a novel dataset for fine-grained evaluation of realism and plausibility in AI-generated images. Q-Real consists of 3,088 images generated by popular text-to-image models. For each image, we annotate the locations of major entities and provide a set of judgment questions and attribution descriptions for these along the dimensions of realism and plausibility. Considering that recent advances in multi-modal large language models (MLLMs) enable fine-grained evaluation of AI-generated images, we construct Q-Real Bench to evaluate them on two tasks: judgment and grounding with reasoning. Finally, to enhance MLLM capabilities, we design a fine-tuning framework and conduct experiments on multiple MLLMs using our dataset. Experimental results demonstrate the high quality and significance of our dataset and the comprehensiveness of the benchmark. Dataset and code will be released upon publication.",
            "headline_zh": "提出Q-Real数据集以精细评估AI生成图像的真实性与合理性",
            "intro_zh": [
                "现有AI生成内容质量评估仅提供粗粒度分数，无法针对性指导模型优化",
                "构建包含3088张图像的Q-Real数据集，标注实体位置并提供真实性与合理性判断问题",
                "实验验证数据集高质量，并设计微调框架提升多模态大语言模型评估能力"
            ],
            "tags_zh": [
                "AI生成内容评估",
                "真实性评估",
                "合理性评估",
                "多模态大语言模型",
                "图像生成",
                "数据集构建"
            ],
            "_index": 131
        },
        {
            "title": "Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models",
            "authors": [
                "Hao-Chien Hsueh",
                "Chi-En Yen",
                "Wen-Hsiao Peng",
                "Ching-Chun Huang"
            ],
            "arxiv_id": "2511.16904v1",
            "summary": "Diffusion probabilistic models have achieved remarkable success in generative tasks across diverse data types. While recent studies have explored alternative degradation processes beyond Gaussian noise, this paper bridges two key diffusion paradigms: hot diffusion, which relies entirely on noise, and cold diffusion, which uses only blurring without noise. We argue that hot diffusion fails to exploit the strong correlation between high-frequency image detail and low-frequency structures, leading to random behaviors in the early steps of generation. Conversely, while cold diffusion leverages image correlations for prediction, it neglects the role of noise (randomness) in shaping the data manifold, resulting in out-of-manifold issues and partially explaining its performance drop. To integrate both strengths, we propose Warm Diffusion, a unified Blur-Noise Mixture Diffusion Model (BNMD), to control blurring and noise jointly. Our divide-and-conquer strategy exploits the spectral dependency in images, simplifying score model estimation by disentangling the denoising and deblurring processes. We further analyze the Blur-to-Noise Ratio (BNR) using spectral analysis to investigate the trade-off between model learning dynamics and changes in the data manifold. Extensive experiments across benchmarks validate the effectiveness of our approach for image generation.",
            "headline_zh": "提出Warm Diffusion以结合噪声与模糊扩散优势，提升图像生成质量",
            "intro_zh": [
                "核心问题：热扩散忽略图像相关性，冷扩散缺乏噪声导致数据流形问题",
                "方法要点：联合控制模糊和噪声，利用谱依赖简化去噪与去模糊过程",
                "实验或效果：在多个基准测试中验证模型有效性，改善生成性能"
            ],
            "tags_zh": [
                "扩散模型",
                "图像生成",
                "模糊噪声混合",
                "谱分析",
                "去噪去模糊"
            ],
            "_index": 132
        },
        {
            "title": "R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios",
            "authors": [
                "Lu Zhu",
                "Tiantian Geng",
                "Yangye Chen",
                "Teng Wang",
                "Ping Lu",
                "Feng Zheng"
            ],
            "arxiv_id": "2511.16901v1",
            "summary": "Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.",
            "headline_zh": "提出R-AVST数据集与AVST-Zero模型以增强复杂视听场景中的细粒度时空推理",
            "intro_zh": [
                "当前多模态大模型在复杂真实世界视听事件中表现不足，缺乏细粒度时空推理能力。",
                "构建R-AVST数据集，含5K视频与27K对象，并定义三个核心任务生成8K问答对。",
                "提出AVST-Zero模型，基于强化学习优化行为，实验显示在R-AVST上性能优越。"
            ],
            "tags_zh": [
                "视听时空推理",
                "多模态大语言模型",
                "强化学习",
                "数据集构建",
                "视频理解",
                "细粒度标注"
            ],
            "_index": 133
        },
        {
            "title": "Single-Pixel Tactile Skin via Compressive Sampling",
            "authors": [
                "Ariel Slepyan",
                "Laura Xing",
                "Rudy Zhang",
                "Nitish Thakor"
            ],
            "arxiv_id": "2511.16898v1",
            "summary": "Development of large-area, high-speed electronic skins is a grand challenge for robotics, prosthetics, and human-machine interfaces, but is fundamentally limited by wiring complexity and data bottlenecks. Here, we introduce Single-Pixel Tactile Skin (SPTS), a paradigm that uses compressive sampling to reconstruct rich tactile information from an entire sensor array via a single output channel. This is achieved through a direct circuit-level implementation where each sensing element, equipped with a miniature microcontroller, contributes a dynamically weighted analog signal to a global sum, performing distributed compressed sensing in hardware. Our flexible, daisy-chainable design simplifies wiring to a few input lines and one output, and significantly reduces measurement requirements compared to raster scanning methods. We demonstrate the system's performance by achieving object classification at an effective 3500 FPS and by capturing transient dynamics, resolving an 8 ms projectile impact into 23 frames. A key feature is the support for adaptive reconstruction, where sensing fidelity scales with measurement time. This allows for rapid contact localization using as little as 7% of total data, followed by progressive refinement to a high-fidelity image - a capability critical for responsive robotic systems. This work offers an efficient pathway towards large-scale tactile intelligence for robotics and human-machine interfaces.",
            "headline_zh": "提出单像素触觉皮肤以解决电子皮肤布线复杂和数据瓶颈问题",
            "intro_zh": [
                "核心问题：大范围高速电子皮肤受限于布线复杂性和数据瓶颈",
                "方法要点：采用压缩采样通过单输出通道重构触觉信息，硬件实现分布式压缩感知",
                "实验或效果：实现3500 FPS物体分类，捕获8 ms瞬态动态，支持自适应重建"
            ],
            "tags_zh": [
                "触觉皮肤",
                "压缩采样",
                "分布式压缩感知",
                "机器人接口",
                "自适应重建"
            ],
            "_index": 134
        },
        {
            "title": "Glass Surface Detection: Leveraging Reflection Dynamics in Flash/No-flash Imagery",
            "authors": [
                "Tao Yan",
                "Hao Huang",
                "Yiwei Lu",
                "Zeyu Wang",
                "Ke Xu",
                "Yinghui Wang",
                "Xiaojun Chang",
                "Rynson W. H. Lau"
            ],
            "arxiv_id": "2511.16887v1",
            "summary": "Glass surfaces are ubiquitous in daily life, typically appearing colorless, transparent, and lacking distinctive features. These characteristics make glass surface detection a challenging computer vision task. Existing glass surface detection methods always rely on boundary cues (e.g., window and door frames) or reflection cues to locate glass surfaces, but they fail to fully exploit the intrinsic properties of the glass itself for accurate localization. We observed that in most real-world scenes, the illumination intensity in front of the glass surface differs from that behind it, which results in variations in the reflections visible on the glass surface. Specifically, when standing on the brighter side of the glass and applying a flash towards the darker side, existing reflections on the glass surface tend to disappear. Conversely, while standing on the darker side and applying a flash towards the brighter side, distinct reflections will appear on the glass surface. Based on this phenomenon, we propose NFGlassNet, a novel method for glass surface detection that leverages the reflection dynamics present in flash/no-flash imagery. Specifically, we propose a Reflection Contrast Mining Module (RCMM) for extracting reflections, and a Reflection Guided Attention Module (RGAM) for fusing features from reflection and glass surface for accurate glass surface detection. For learning our network, we also construct a dataset consisting of 3.3K no-flash and flash image pairs captured from various scenes with corresponding ground truth annotations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code, model, and dataset will be available upon acceptance of the manuscript.",
            "headline_zh": "提出NFGlassNet方法，利用闪光/无闪光图像中的反射动态检测玻璃表面。",
            "intro_zh": [
                "玻璃表面检测因无色透明特性而具挑战性，现有方法依赖边界或反射线索但未充分利用玻璃内在属性。",
                "基于闪光/无闪光图像中反射动态，设计反射对比挖掘模块和反射引导注意力模块以提升检测精度。",
                "构建3.3K图像对数据集，实验表明方法优于现有先进技术，代码和数据集将公开。"
            ],
            "tags_zh": [
                "玻璃表面检测",
                "闪光/无闪光图像",
                "反射动态",
                "NFGlassNet",
                "计算机视觉"
            ],
            "_index": 135
        }
    ]
}