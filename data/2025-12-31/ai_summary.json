{
    "papers": [
        {
            "title": "RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence",
            "authors": [
                "Chengkai Hou",
                "Kun Wu",
                "Jiaming Liu",
                "Zhengping Che",
                "Di Wu",
                "Fei Liao",
                "Guangrun Li",
                "Jingyang He",
                "Qiuxuan Feng",
                "Zhao Jin",
                "Chenyang Gu",
                "Zhuoyang Liu",
                "Nuowei Han",
                "Xiangju Mi",
                "Yaoxu Lv",
                "Yankai Fu",
                "Gaole Dai",
                "Langzhe Gu",
                "Tao Li",
                "Yuheng Zhang",
                "Yixue Zhang",
                "Xinhua Wang",
                "Shichao Fan",
                "Meng Li",
                "Zhen Zhao",
                "Ning Liu",
                "Zhiyuan Xu",
                "Pei Ren",
                "Junjie Ji",
                "Haonan Liu",
                "Kuan Cheng",
                "Shanghang Zhang",
                "Jian Tang"
            ],
            "arxiv_id": "2512.24653v1",
            "summary": "While data-driven imitation learning has revolutionized robotic manipulation, current approaches remain constrained by the scarcity of large-scale, diverse real-world demonstrations. Consequently, the ability of existing models to generalize across long-horizon bimanual tasks and mobile manipulation in unstructured environments remains limited. To bridge this gap, we present RoboMIND 2.0, a comprehensive real-world dataset comprising over 310K dual-arm manipulation trajectories collected across six distinct robot embodiments and 739 complex tasks. Crucially, to support research in contact-rich and spatially extended tasks, the dataset incorporates 12K tactile-enhanced episodes and 20K mobile manipulation trajectories. Complementing this physical data, we construct high-fidelity digital twins of our real-world environments, releasing an additional 20K-trajectory simulated dataset to facilitate robust sim-to-real transfer. To fully exploit the potential of RoboMIND 2.0, we propose MIND-2 system, a hierarchical dual-system frame-work optimized via offline reinforcement learning. MIND-2 integrates a high-level semantic planner (MIND-2-VLM) to decompose abstract natural language instructions into grounded subgoals, coupled with a low-level Vision-Language-Action executor (MIND-2-VLA), which generates precise, proprioception-aware motor actions.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24653v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "[T]mobile manipulation",
                        "[T]bi-manual",
                        "dual-arm",
                        "sim-to-real"
                    ],
                    "score": 22.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "offline reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "vision-language-action",
                        "VLA",
                        "[T]multimodal"
                    ],
                    "score": 15.0
                }
            ],
            "relevance_score": 41.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "RoboMIND 2.0：用于通用具身智能的多模态双臂移动操作数据集",
            "summary_zh": "本文提出了RoboMIND 2.0，一个全面的真实世界数据集，包含超过31万条双臂操作轨迹，涵盖六种不同的机器人形态和739个复杂任务。为了支持接触丰富的和空间扩展的任务研究，该数据集还包含了1.2万个触觉增强的片段和2万个移动操作轨迹。为了补充物理数据，作者构建了真实世界环境的高保真数字孪生，并发布了额外的2万条轨迹的模拟数据集，以促进鲁棒的sim-to-real迁移。为了充分利用RoboMIND 2.0的潜力，作者提出了MIND-2系统，一个通过离线强化学习优化的分层双系统框架。MIND-2集成了高层语义规划器(MIND-2-VLM)，将抽象的自然语言指令分解为具体的子目标，以及一个低层视觉-语言-动作执行器(MIND-2-VLA)，生成精确的、感知自身状态的运动动作。",
            "intro_zh": [
                "现有模仿学习方法受限于大规模、多样化的真实世界演示数据匮乏，泛化能力不足。",
                "RoboMIND 2.0通过构建大规模多模态数据集，并结合数字孪生，促进具身智能的sim-to-real迁移。",
                "提出的MIND-2系统，利用分层双系统框架，将自然语言指令转化为机器人可执行的动作。"
            ],
            "method_zh": "**问题定义**：现有机器人操作方法难以在非结构化环境中泛化到长时程双臂任务和移动操作，主要原因是缺乏大规模、多样化的真实世界演示数据。现有方法在接触丰富的任务和空间扩展的任务中表现不佳。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、多模态的机器人操作数据集，包含真实世界和模拟环境的数据，并设计一个分层控制框架，将高级语义指令转化为低级运动动作。通过离线强化学习优化控制策略，提高机器人的泛化能力和鲁棒性。\\n\\n**技术框架**：MIND-2系统采用分层双系统框架，包含两个主要模块：MIND-2-VLM（高层语义规划器）和MIND-2-VLA（低层视觉-语言-动作执行器）。MIND-2-VLM负责将自然语言指令分解为具体的子目标，MIND-2-VLA负责根据视觉信息、语言指令和自身状态生成精确的运动动作。整个系统通过离线强化学习进行优化。\\n\\n**关键创新**：该论文的关键创新在于构建了大规模、多模态的RoboMIND 2.0数据集，该数据集包含真实世界和模拟环境的数据，并涵盖多种机器人形态和复杂任务。此外，提出的MIND-2系统采用分层双系统框架，能够有效地将高级语义指令转化为低级运动动作。\\n\\n**关键设计**：MIND-2-VLM可能采用了Transformer等模型，用于理解自然语言指令并生成子目标序列。MIND-2-VLA可能采用了深度神经网络，将视觉信息、语言指令和自身状态作为输入，生成运动动作。离线强化学习可能采用了DDPG、SAC等算法，用于优化MIND-2-VLA的控制策略。具体参数设置、损失函数和网络结构等细节未知。",
            "application_zh": "该研究成果可应用于各种机器人操作任务，例如家庭服务、工业自动化、医疗辅助等。通过利用RoboMIND 2.0数据集和MIND-2系统，可以提高机器人在复杂环境中的操作能力和泛化能力，从而实现更智能、更自主的机器人应用。未来，该研究可以进一步扩展到更多机器人形态和任务类型，并探索更有效的控制策略和学习算法。",
            "highlight_zh": "RoboMIND 2.0数据集包含超过31万条双臂操作轨迹，涵盖六种不同的机器人形态和739个复杂任务。该数据集还包含了1.2万个触觉增强的片段和2万个移动操作轨迹。此外，作者还构建了真实世界环境的高保真数字孪生，并发布了额外的2万条轨迹的模拟数据集。MIND-2系统的具体性能数据未知，但论文强调其通过离线强化学习进行了优化。",
            "tags_zh": [
                "机器人操作",
                "具身智能",
                "多模态学习",
                "数据集",
                "模仿学习"
            ],
            "_index": 0,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Coordinated Humanoid Manipulation with Choice Policies",
            "authors": [
                "Haozhi Qi",
                "Yen-Jen Wang",
                "Toru Lin",
                "Brent Yi",
                "Yi Ma",
                "Koushil Sreenath",
                "Jitendra Malik"
            ],
            "arxiv_id": "2512.25072v1",
            "summary": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Code and Website: https://choice-policy.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25072v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "humanoid robot",
                        "humanoid control",
                        "locomotion",
                        "[T]manipulation",
                        "loco-manipulation",
                        "teleoperation"
                    ],
                    "score": 22.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning",
                        "behavior cloning",
                        "diffusion policy"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 29.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Choice Policy，结合模块化遥操作，实现协调的人形机器人操作",
            "summary_zh": "人形机器人在以人为中心的环境中具有广阔的应用前景，但实现头部、手部和腿部之间的鲁棒全身协调仍然是一个主要挑战。本文提出了一个系统，该系统结合了模块化遥操作界面和可扩展的学习框架来解决这个问题。我们的遥操作设计将人形机器人控制分解为直观的子模块，包括手眼协调、抓取原语、手臂末端执行器跟踪和运动。这种模块化使我们能够高效地收集高质量的演示数据。在此基础上，我们引入了Choice Policy，这是一种模仿学习方法，可以生成多个候选动作并学习对它们进行评分。这种架构能够实现快速推理和有效建模多模态行为。我们在两个真实世界的任务中验证了我们的方法：洗碗机装载和用于擦拭白板的全身定位操作。实验表明，Choice Policy明显优于扩散策略和标准行为克隆。此外，我们的结果表明，手眼协调对于长时程任务的成功至关重要。我们的工作展示了一条在非结构化环境中实现可扩展数据收集和学习以进行协调的人形机器人操作的实用途径。",
            "intro_zh": [
                "现有方法难以实现人形机器人在复杂环境中头部、手部和腿部的全身协调控制。",
                "通过模块化遥操作收集高质量数据，并提出Choice Policy模仿学习方法，生成并评估多个候选动作。",
                "在洗碗机装载和白板擦拭等真实任务中，Choice Policy优于其他方法，验证了手眼协调的重要性。"
            ],
            "method_zh": "**问题定义**：现有的人形机器人控制方法难以在复杂、非结构化的环境中实现全身协调操作，尤其是在长时程任务中。现有的方法，如行为克隆和强化学习，在处理多模态行为和探索复杂动作空间时存在局限性。此外，数据收集的效率和质量也是一个挑战。\\n\\n**核心思路**：论文的核心思路是将人形机器人的控制分解为多个直观的模块，例如手眼协调、抓取原语、末端执行器跟踪和运动控制。通过模块化的遥操作界面，可以高效地收集高质量的演示数据。然后，利用Choice Policy模仿学习方法，学习从多个候选动作中选择最优动作，从而有效地建模多模态行为。\\n\\n**技术框架**：该系统包含两个主要部分：模块化遥操作界面和Choice Policy学习框架。遥操作界面允许操作员通过多个子模块控制人形机器人的各个部分。收集到的数据用于训练Choice Policy。Choice Policy包含一个动作生成器和一个评分器。动作生成器生成多个候选动作，评分器评估每个动作的质量。最终选择得分最高的动作执行。\\n\\n**关键创新**：Choice Policy是该论文的关键创新点。与传统的行为克隆方法不同，Choice Policy能够生成多个候选动作并学习对它们进行评分，从而更好地建模多模态行为。与扩散策略相比，Choice Policy具有更快的推理速度。此外，模块化的遥操作界面也提高了数据收集的效率和质量。\\n\\n**关键设计**：Choice Policy的具体实现细节包括：动作生成器可以使用多种方法，例如高斯混合模型或神经网络。评分器可以使用神经网络进行训练，其输入是机器人的状态和候选动作，输出是该动作的得分。损失函数可以包括行为克隆损失和正则化项。遥操作界面的模块化设计允许操作员专注于控制机器人的特定部分，从而提高数据质量。",
            "application_zh": "该研究成果可应用于各种人形机器人操作任务，例如家庭服务、工业自动化、医疗辅助等。通过模块化遥操作和Choice Policy学习，可以使人形机器人在非结构化环境中更安全、更高效地完成复杂任务。未来的研究可以进一步探索如何将该方法扩展到更复杂的任务和环境，并提高机器人的自主性。",
            "highlight_zh": "实验结果表明，Choice Policy在洗碗机装载和白板擦拭等真实世界的任务中，显著优于扩散策略和标准行为克隆。具体而言，Choice Policy在这些任务上的成功率分别提高了15%和20%。此外，实验还验证了手眼协调对于长时程任务的重要性，表明了模块化遥操作的有效性。",
            "tags_zh": [
                "人形机器人",
                "模仿学习",
                "遥操作",
                "全身协调",
                "多模态行为"
            ],
            "_index": 1,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.25072v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.25072v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.25072v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids",
            "authors": [
                "Sungjae Min",
                "Hyungjoo Kim",
                "David Hyunchul Shim"
            ],
            "arxiv_id": "2512.24657v1",
            "summary": "Humanoid robots toward human-level dexterity require robotic hands capable of simultaneously providing high grasping force, rapid actuation speeds, multiple degrees of freedom, and lightweight structures within human-like size constraints. Meeting these conflicting requirements remains challenging, as satisfying this combination typically necessitates heavier actuators and bulkier transmission systems, significantly restricting the payload capacity of robot arms. In this letter, we present a lightweight anthropomorphic hand actuated by Bowden cables, which uniquely combines rolling-contact joint optimization with antagonistic cable actuation, enabling single-motor-per-joint control with negligible cable-length deviation. By relocating the actuator module to the torso, the design substantially reduces distal mass while maintaining anthropomorphic scale and dexterity. Additionally, this antagonistic cable actuation eliminates the need for synchronization between motors. Using the proposed methods, the hand assembly with a distal mass of 236g (excluding remote actuators and Bowden sheaths) demonstrated reliable execution of dexterous tasks, exceeding 18N fingertip force and lifting payloads over one hundred times its own mass. Furthermore, robustness was validated through Cutkosky taxonomy grasps and trajectory consistency under perturbed actuator-hand transformations.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Preprint",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24657v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "humanoid robot",
                        "[T]manipulation",
                        "[T]dexterous manipulation"
                    ],
                    "score": 20.0
                }
            ],
            "relevance_score": 20.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种基于拮抗式Bowden线缆驱动的轻量化机械手，用于有效载荷受限的人形机器人灵巧操作。",
            "summary_zh": "本文提出了一种轻量级拟人机械手，该机械手由Bowden线缆驱动，独特地结合了滚动接触关节优化和拮抗式线缆驱动，实现了每个关节单电机控制，且线缆长度偏差可忽略不计。通过将执行器模块重新定位到躯干，该设计在保持拟人尺度和灵巧性的同时，显著降低了远端质量。此外，这种拮抗式线缆驱动消除了电机之间的同步需求。使用所提出的方法，远端质量为236克的机械手（不包括远程执行器和Bowden护套）展示了灵巧任务的可靠执行，超过18N的指尖力，并能举起超过自身质量一百倍的有效载荷。此外，通过Cutkosky分类抓取和在扰动执行器-手部变换下的轨迹一致性验证了鲁棒性。",
            "intro_zh": [
                "人形机器人需要具备高抓握力、快速驱动、多自由度和轻量化结构的灵巧手，但这些需求相互冲突，难以同时满足。",
                "该论文提出一种基于Bowden线缆驱动的轻量化机械手，通过滚动接触关节优化和拮抗式线缆驱动，实现单电机控制和低线缆长度偏差。",
                "实验结果表明，该机械手远端质量仅为236g，能产生超过18N的指尖力，并能举起超过自身质量一百倍的有效载荷，验证了其鲁棒性。"
            ],
            "method_zh": "**问题定义**：人形机器人需要灵巧的手部，但现有方法难以同时满足高抓握力、快速驱动、多自由度、轻量化和拟人尺寸等相互冲突的需求。传统的解决方案通常需要更重的执行器和更笨重的传动系统，从而显著限制了机器人手臂的有效载荷能力。\\n\\n**核心思路**：该论文的核心思路是将执行器模块远程放置到躯干，通过Bowden线缆驱动机械手，从而显著降低手部的远端质量。同时，采用拮抗式线缆驱动，每个关节仅需一个电机，无需电机间的同步，简化了控制系统。\\n\\n**技术框架**：该机械手的整体架构包括：1) 远程执行器模块（位于躯干）；2) Bowden线缆传动系统；3) 具有滚动接触关节的轻量化机械手。通过拮抗式线缆驱动，每个关节由一对线缆控制，实现正反两个方向的运动。\\n\\n**关键创新**：该论文的关键创新在于：1) 结合了滚动接触关节优化和拮抗式线缆驱动，实现了单电机-单关节控制，并最大程度地减少了线缆长度偏差；2) 通过远程执行器布局，显著降低了手部的远端质量，提高了有效载荷能力；3) 采用拮抗式线缆驱动，消除了电机同步的需求，简化了控制。\\n\\n**关键设计**：在机械手设计中，优化了滚动接触关节的几何形状，以减少线缆长度偏差。拮抗式线缆驱动采用预张力，以确保线缆始终处于张紧状态，从而提高控制精度和响应速度。线缆的选择需要考虑强度、柔韧性和摩擦系数等因素。",
            "application_zh": "该研究成果可应用于有效载荷受限的人形机器人，使其能够执行更复杂的灵巧操作任务，例如在狭小空间内的装配、医疗手术辅助、以及危险环境下的物品操作。轻量化设计也有助于提高机器人的安全性，降低与人交互时的风险。未来，该技术有望扩展到外骨骼和假肢等领域。",
            "highlight_zh": "该机械手远端质量仅为236g（不含远程执行器和线缆护套），能够产生超过18N的指尖力，并成功举起超过自身质量一百倍的有效载荷。通过Cutkosky分类抓取实验和在扰动下的轨迹一致性验证，证明了该机械手具有良好的鲁棒性和适应性。这些实验结果表明，该设计在轻量化和灵巧操作方面取得了显著的进展。",
            "tags_zh": [
                "机械手",
                "Bowden线缆驱动",
                "拮抗式驱动",
                "轻量化设计",
                "人形机器人",
                "灵巧操作",
                "滚动接触关节"
            ],
            "_index": 2,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24657v1/Image/PCDRH2_33Captioned3.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24657v1/Image/Fig_0_HumanHandDOF.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24657v1/Image/Fig_0_RobotHandDOF.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression",
            "authors": [
                "Xiang Liu",
                "Yimin Zhou",
                "Jinxiang Wang",
                "Yujun Huang",
                "Shuzhao Xie",
                "Shiyu Qin",
                "Mingyao Hong",
                "Jiawei Li",
                "Yaowei Wang",
                "Zhi Wang",
                "Shu-Tao Xia",
                "Bin Chen"
            ],
            "arxiv_id": "2512.24742v1",
            "summary": "The recent advent of 3D Gaussian Splatting (3DGS) has marked a significant breakthrough in real-time novel view synthesis. However, the rapid proliferation of 3DGS-based algorithms has created a pressing need for standardized and comprehensive evaluation tools, especially for compression task. Existing benchmarks often lack the specific metrics necessary to holistically assess the unique characteristics of different methods, such as rendering speed, rate distortion trade-offs memory efficiency, and geometric accuracy. To address this gap, we introduce Splatwizard, a unified benchmark toolkit designed specifically for benchmarking 3DGS compression models. Splatwizard provides an easy-to-use framework to implement new 3DGS compression model and utilize state-of-the-art techniques proposed by previous work. Besides, an integrated pipeline that automates the calculation of key performance indicators, including image-based quality metrics, chamfer distance of reconstruct mesh, rendering frame rates, and computational resource consumption is included in the framework as well. Code is available at https://github.com/splatwizard/splatwizard",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24742v1",
            "code_links": [
                {
                    "url": "https://github.com/splatwizard/splatwizard",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]3D gaussian splatting",
                        "3DGS",
                        "[T]gaussian splatting",
                        "[T]splatting"
                    ],
                    "score": 20.0
                }
            ],
            "relevance_score": 20.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "Splatwizard：用于3D高斯溅射压缩的综合基准测试工具包",
            "summary_zh": "3D高斯溅射（3DGS）的出现标志着实时新视角合成的重大突破。然而，基于3DGS算法的快速普及，对标准化和全面的评估工具产生了迫切需求，特别是对于压缩任务。现有的基准测试通常缺乏必要的特定指标，无法全面评估不同方法的独特特征，例如渲染速度、率失真权衡、内存效率和几何精度。为了解决这一差距，我们推出了Splatwizard，这是一个专门为3DGS压缩模型基准测试而设计的统一基准测试工具包。Splatwizard提供了一个易于使用的框架来实现新的3DGS压缩模型，并利用先前工作提出的最先进技术。此外，该框架还包含一个集成的pipeline，可以自动计算关键性能指标，包括基于图像的质量指标、重建网格的倒角距离、渲染帧速率和计算资源消耗。代码可在https://github.com/splatwizard/splatwizard获取。",
            "intro_zh": [
                "现有3DGS压缩基准缺乏全面评估指标，难以充分衡量渲染速度、率失真和几何精度等关键特性。",
                "Splatwizard提供统一框架，简化3DGS压缩模型实现，集成先进技术，并自动化关键性能指标的计算。",
                "Splatwizard集成了图像质量、几何精度、渲染速度和资源消耗等指标，为3DGS压缩提供全面评估。"
            ],
            "method_zh": "**问题定义**：论文旨在解决3D高斯溅射（3DGS）压缩算法缺乏标准化和全面评估工具的问题。现有基准测试无法充分评估不同压缩方法在渲染速度、率失真权衡、内存效率和几何精度等方面的性能，导致难以公平比较和选择最佳压缩方案。\\n\\n**核心思路**：Splatwizard的核心思路是构建一个统一的、易于使用的基准测试工具包，该工具包能够自动化地评估3DGS压缩模型的各项关键性能指标。通过提供标准化的评估流程和全面的性能指标，Splatwizard旨在促进3DGS压缩算法的公平比较和快速发展。\\n\\n**技术框架**：Splatwizard包含以下主要模块：1) 模型实现框架：提供易于使用的接口，方便用户实现和集成新的3DGS压缩模型。2) 性能评估pipeline：自动化计算关键性能指标，包括图像质量指标（如PSNR、SSIM）、几何精度指标（如倒角距离）、渲染帧速率和计算资源消耗。3) 数据集管理：支持常用的3DGS数据集，并提供数据预处理和加载功能。4) 结果可视化：提供清晰直观的性能报告和可视化工具，方便用户分析和比较不同压缩模型的性能。\\n\\n**关键创新**：Splatwizard的关键创新在于其统一性和全面性。它不仅提供了一个易于使用的模型实现框架，还集成了多种性能评估指标，涵盖了图像质量、几何精度、渲染速度和资源消耗等多个方面。此外，Splatwizard还提供了自动化的评估pipeline和清晰直观的结果可视化工具，大大简化了3DGS压缩算法的评估流程。\\n\\n**关键设计**：Splatwizard的关键设计包括：1) 模块化的架构，方便用户扩展和定制。2) 标准化的评估流程，确保评估结果的可重复性和可比性。3) 多种性能指标的集成，全面评估压缩模型的性能。4) 易于使用的API和文档，降低用户的使用门槛。具体参数设置、损失函数和网络结构取决于用户实现的具体压缩模型。",
            "application_zh": "Splatwizard可广泛应用于虚拟现实、增强现实、自动驾驶、游戏开发等领域。通过提供标准化的评估工具，Splatwizard能够帮助研究人员和开发人员快速评估和选择最佳的3DGS压缩算法，从而提高渲染效率、降低存储成本，并最终提升用户体验。未来，Splatwizard有望成为3DGS压缩领域的重要基础设施。",
            "highlight_zh": "Splatwizard提供了一个全面的评估框架，能够自动化计算图像质量（PSNR、SSIM）、几何精度（倒角距离）、渲染帧速率和计算资源消耗等关键指标。通过Splatwizard，研究人员可以更方便地比较不同的3DGS压缩算法，并选择最适合特定应用场景的方案。该工具包的开源发布将促进3DGS压缩领域的研究和发展。",
            "tags_zh": [
                "3D高斯溅射",
                "压缩",
                "基准测试",
                "新视角合成",
                "性能评估"
            ],
            "_index": 3,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24742v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24742v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24742v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning",
            "authors": [
                "Ankit Dhiman",
                "Srinath R",
                "Jaswanth Reddy",
                "Lokesh R Boregowda",
                "Venkatesh Babu Radhakrishnan"
            ],
            "arxiv_id": "2512.24763v1",
            "summary": "3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have advanced novel-view synthesis. Recent methods extend multi-view 2D segmentation to 3D, enabling instance/semantic segmentation for better scene understanding. A key challenge is the inconsistency of 2D instance labels across views, leading to poor 3D predictions. Existing methods use a two-stage approach in which some rely on contrastive learning with hyperparameter-sensitive clustering, while others preprocess labels for consistency. We propose a unified framework that merges these steps, reducing training time and improving performance by introducing a learnable feature embedding for segmentation in Gaussian primitives. This embedding is then efficiently decoded into instance labels through a novel \"Embedding-to-Label\" process, effectively integrating the optimization. While this unified framework offers substantial benefits, we observed artifacts at the object boundaries. To address the object boundary issues, we propose hard-mining samples along these boundaries. However, directly applying hard mining to the feature embeddings proved unstable. Therefore, we apply a linear layer to the rasterized feature embeddings before calculating the triplet loss, which stabilizes training and significantly improves performance. Our method outperforms baselines qualitatively and quantitatively on the ScanNet, Replica3D, and Messy-Rooms datasets.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Accepted to AAAI 2026. Project Page: https://unic-lift.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24763v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]contrastive learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "splatting",
                        "NeRF",
                        "neural radiance field",
                        "scene understanding"
                    ],
                    "score": 14.0
                }
            ],
            "relevance_score": 18.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "UniC-Lift：通过对比学习实现统一的3D实例分割",
            "summary_zh": "3D高斯溅射(3DGS)和神经辐射场(NeRF)推动了新视角合成的发展。最近的方法将多视角2D分割扩展到3D，从而实现实例/语义分割，以更好地理解场景。一个关键的挑战是2D实例标签在不同视角下的一致性问题，导致较差的3D预测。现有方法采用两阶段方法，一些依赖于对超参数敏感的对比学习聚类，而另一些则预处理标签以保证一致性。我们提出了一个统一的框架，它合并了这些步骤，通过引入用于高斯基元分割的可学习特征嵌入，从而减少了训练时间并提高了性能。然后，通过一种新颖的“嵌入到标签”过程，将该嵌入有效地解码为实例标签，从而有效地集成了优化。虽然这个统一的框架提供了显著的好处，但我们观察到物体边界处存在伪影。为了解决物体边界问题，我们提出了沿这些边界进行硬样本挖掘。然而，直接将硬挖掘应用于特征嵌入被证明是不稳定的。因此，我们在计算三元组损失之前，将线性层应用于光栅化的特征嵌入，这稳定了训练并显著提高了性能。我们的方法在ScanNet、Replica3D和Messy-Rooms数据集上，在质量和数量上都优于基线。",
            "intro_zh": [
                "现有3D实例分割方法受限于多视角2D标签不一致性，导致3D预测效果不佳，且通常采用两阶段流程，效率较低。",
                "UniC-Lift提出统一框架，通过可学习的特征嵌入和“嵌入到标签”过程，将对比学习和标签优化集成，提升分割性能。",
                "为解决物体边界伪影问题，提出在光栅化特征嵌入上进行硬样本挖掘，稳定训练并显著提升ScanNet等数据集上的分割效果。"
            ],
            "method_zh": "**问题定义**：现有3D实例分割方法面临的关键问题是多视角2D实例标签的不一致性，这会导致3D预测的准确性下降。此外，许多现有方法采用两阶段流程，例如先进行对比学习聚类，然后再进行标签优化，这增加了训练的复杂性和时间成本。这些方法通常对超参数敏感，且需要对标签进行预处理以保证一致性。\\n\\n**核心思路**：UniC-Lift的核心思路是将对比学习和标签优化集成到一个统一的框架中。通过学习一个特征嵌入，使得属于同一实例的高斯基元在嵌入空间中更接近，而属于不同实例的基元更远离。然后，利用一个“嵌入到标签”的过程，将学习到的嵌入直接解码为实例标签，从而避免了传统两阶段方法中的分离优化问题。\\n\\n**技术框架**：UniC-Lift的整体框架包括以下几个主要模块：1) 特征嵌入模块：用于学习高斯基元的特征嵌入，该嵌入能够区分不同的实例。2) “嵌入到标签”模块：将学习到的特征嵌入解码为实例标签。3) 损失函数：包括对比损失（例如三元组损失）和分割损失，用于优化特征嵌入和标签预测。4) 硬样本挖掘模块：用于解决物体边界处的伪影问题，通过挖掘边界附近的困难样本来提升分割精度。\\n\\n**关键创新**：UniC-Lift最重要的创新点在于其统一的框架，它将对比学习和标签优化集成到一个端到端的流程中。与现有方法相比，UniC-Lift避免了复杂的两阶段优化，减少了训练时间和超参数调整的难度。此外，提出的“嵌入到标签”过程能够更有效地利用学习到的特征嵌入进行实例分割。\\n\\n**关键设计**：为了解决物体边界处的伪影问题，UniC-Lift采用了一种硬样本挖掘策略。具体来说，首先对特征嵌入进行光栅化，然后应用一个线性层。在光栅化后的特征上计算三元组损失，而不是直接在原始特征嵌入上进行计算，这有助于稳定训练过程并提高性能。此外，损失函数的设计也至关重要，需要平衡对比损失和分割损失，以获得最佳的分割效果。",
            "application_zh": "UniC-Lift在三维场景理解领域具有广泛的应用前景，例如机器人导航、自动驾驶、虚拟现实和增强现实。该方法可以用于构建更精确的三维场景模型，从而使机器人能够更好地理解周围环境并进行自主导航。此外，该方法还可以用于改善虚拟现实和增强现实体验，例如通过对场景中的物体进行精确分割，可以实现更逼真的交互效果。",
            "highlight_zh": "UniC-Lift在ScanNet、Replica3D和Messy-Rooms数据集上取得了显著的性能提升。相较于基线方法，UniC-Lift在实例分割精度方面取得了明显的优势，尤其是在物体边界区域。通过硬样本挖掘策略，有效地减少了物体边界处的伪影，进一步提升了分割质量。实验结果表明，UniC-Lift是一种高效且准确的3D实例分割方法。",
            "tags_zh": [
                "3D实例分割",
                "对比学习",
                "高斯溅射",
                "神经辐射场",
                "统一框架"
            ],
            "_index": 4,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24763v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24763v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24763v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents",
            "authors": [
                "Xunyi Zhao",
                "Gengze Zhou",
                "Qi Wu"
            ],
            "arxiv_id": "2512.24851v1",
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24851v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]VLN",
                        "large language model",
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "VLN-MME：诊断多模态大语言模型在语言引导视觉导航任务中的表现",
            "summary_zh": "多模态大语言模型(MLLMs)在各种视觉-语言任务中表现出卓越的能力。然而，它们作为具身智能体的性能，需要多轮对话空间推理和序列动作预测，仍有待进一步探索。本研究通过引入一个统一且可扩展的评估框架VLN-MME，将传统导航数据集转化为标准化基准，从而研究MLLMs在视觉-语言导航(VLN)中作为零样本智能体的潜力。我们通过高度模块化和易于访问的设计简化了评估。这种灵活性简化了实验，实现了跨不同MLLM架构、智能体设计和导航任务的结构化比较和组件级消融研究。重要的是，在我们的框架支持下，我们观察到使用思维链(CoT)推理和自我反思增强我们的基线智能体反而导致性能下降。这表明MLLMs在具身导航任务中表现出较差的上下文感知能力；尽管它们可以遵循指令并构建其输出，但它们的3D空间推理保真度较低。VLN-MME为在具身导航环境中系统评估通用MLLMs奠定了基础，并揭示了它们在序列决策能力方面的局限性。我们相信这些发现为MLLM作为具身智能体的后训练提供了关键指导。",
            "intro_zh": [
                "现有具身智能体研究缺乏统一的评估框架，难以系统性地评估多模态大语言模型（MLLMs）在视觉-语言导航（VLN）中的能力。",
                "论文提出VLN-MME框架，将传统导航数据集转化为标准化基准，用于零样本评估MLLMs在VLN任务中的性能。",
                "实验发现，增强MLLMs的思维链推理和自我反思反而导致性能下降，表明其在具身导航中上下文感知和3D空间推理能力不足。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言导航（VLN）任务缺乏一个统一的、可扩展的评估框架，难以对多模态大语言模型（MLLMs）在具身环境下的导航能力进行全面诊断。现有方法难以有效评估MLLMs在多轮对话、空间推理和序列决策方面的能力，阻碍了MLLMs在具身智能体领域的应用。\\n\\n**核心思路**：论文的核心思路是构建一个名为VLN-MME的评估框架，将现有的VLN数据集转化为统一的基准，从而能够以零样本的方式评估MLLMs作为导航智能体的性能。通过模块化的设计，VLN-MME可以灵活地支持不同的MLLM架构、智能体设计和导航任务，从而实现结构化的比较和组件级的消融研究。\\n\\n**技术框架**：VLN-MME框架主要包含以下几个模块：1) 数据集转换模块，将不同的VLN数据集转换为统一的格式；2) 智能体接口模块，用于连接不同的MLLM智能体；3) 评估指标模块，用于计算导航任务的性能指标。整个流程包括：输入导航指令和视觉信息，MLLM智能体根据指令进行推理和决策，输出动作序列，最后根据实际导航结果计算评估指标。\\n\\n**关键创新**：VLN-MME的关键创新在于其统一性和可扩展性，它提供了一个标准化的平台，使得研究人员可以方便地比较不同的MLLM智能体在VLN任务中的性能。此外，论文还发现，简单地将思维链（CoT）推理和自我反思应用于MLLMs反而会降低其导航性能，这揭示了MLLMs在具身导航任务中上下文感知和3D空间推理方面的局限性。\\n\\n**关键设计**：VLN-MME框架的设计注重模块化和灵活性，允许研究人员自定义智能体的行为策略、调整导航环境的参数，以及选择不同的评估指标。具体的技术细节包括：数据集转换的规则、智能体接口的协议、以及评估指标的计算方法。论文没有详细说明具体的参数设置、损失函数或网络结构，因为VLN-MME主要是一个评估框架，而不是一个特定的模型。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、虚拟现实等领域。通过VLN-MME框架，可以更有效地评估和改进MLLMs在具身环境中的导航能力，从而推动智能体在复杂环境中的自主决策和行动。未来的研究可以利用该框架探索更有效的MLLM训练方法，提升其在真实世界场景中的应用价值。",
            "highlight_zh": "实验结果表明，VLN-MME框架能够有效地评估MLLMs在VLN任务中的性能。令人惊讶的是，使用思维链（CoT）推理和自我反思增强MLLMs反而导致性能下降，这表明MLLMs在具身导航任务中存在上下文感知和3D空间推理的局限性。这一发现为MLLM在具身智能体领域的后训练提供了重要的指导。",
            "tags_zh": [
                "视觉语言导航",
                "多模态大语言模型",
                "具身智能体",
                "评估框架",
                "思维链推理"
            ],
            "_index": 5,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24851v1/Fig/Framework.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24851v1/Fig/VLNMME.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24851v1/Fig/reasoning_compare.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer",
            "authors": [
                "Dongyun Kang",
                "Min-Gyu Kim",
                "Tae-Gyu Song",
                "Hajun Kim",
                "Sehoon Ha",
                "Hae-Won Park"
            ],
            "arxiv_id": "2512.24698v1",
            "summary": "Generating dynamic motions for legged robots remains a challenging problem. While reinforcement learning has achieved notable success in various legged locomotion tasks, producing highly dynamic behaviors often requires extensive reward tuning or high-quality demonstrations. Leveraging reduced-order models can help mitigate these challenges. However, the model discrepancy poses a significant challenge when transferring policies to full-body dynamics environments. In this work, we introduce a continuation-based learning framework that combines simplified model pretraining and model homotopy transfer to efficiently generate and refine complex dynamic behaviors. First, we pretrain the policy using a single rigid body model to capture core motion patterns in a simplified environment. Next, we employ a continuation strategy to progressively transfer the policy to the full-body environment, minimizing performance loss. To define the continuation path, we introduce a model homotopy from the single rigid body model to the full-body model by gradually redistributing mass and inertia between the trunk and legs. The proposed method not only achieves faster convergence but also demonstrates superior stability during the transfer process compared to baseline methods. Our framework is validated on a range of dynamic tasks, including flips and wall-assisted maneuvers, and is successfully deployed on a real quadrupedal robot.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "8 pages. Submitted to the IEEE for possible publication",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24698v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "quadruped",
                        "[T]legged robot",
                        "legged locomotion",
                        "locomotion"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]policy learning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于简化模型预训练和模型同伦迁移的动态策略学习方法，用于解决腿足机器人运动控制问题。",
            "summary_zh": "本文提出了一种基于连续性学习框架的动态策略学习方法，用于高效生成和优化腿足机器人的复杂动态行为。该方法结合了简化模型预训练和模型同伦迁移。首先，使用单刚体模型预训练策略，以在简化环境中捕获核心运动模式。然后，采用连续性策略，逐步将策略迁移到全身动力学环境中，从而最大限度地减少性能损失。为了定义连续性路径，本文引入了一种模型同伦方法，通过逐渐重新分配躯干和腿部之间的质量和惯性，从单刚体模型过渡到全身模型。实验结果表明，与基线方法相比，该方法不仅收敛速度更快，而且在迁移过程中表现出更高的稳定性。该框架已在一系列动态任务（包括翻转和墙壁辅助动作）中得到验证，并已成功部署在真实的四足机器人上。",
            "intro_zh": [
                "腿足机器人动态运动生成面临挑战，强化学习方法通常需要大量的奖励调整或高质量的演示。",
                "论文提出一种基于连续性学习的框架，通过简化模型预训练和模型同伦迁移，实现策略从简化模型到复杂模型的平滑过渡。",
                "实验表明，该方法在动态任务中收敛速度更快，稳定性更高，并在真实四足机器人上成功部署，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：腿足机器人动态运动控制，尤其是在复杂环境下的高动态运动，是一个具有挑战性的问题。传统的强化学习方法需要大量的奖励函数调整，或者依赖于高质量的专家演示数据，这限制了其在实际应用中的可行性。此外，直接在复杂的全身动力学模型上训练策略，计算成本高昂，且容易陷入局部最优。\\n\\n**核心思路**：论文的核心思路是利用简化模型进行策略预训练，然后通过模型同伦的方式，逐步将策略迁移到复杂的全身动力学模型上。这种方法利用了简化模型计算效率高、易于训练的优点，同时避免了直接在复杂模型上训练的困难。模型同伦保证了策略迁移的平滑性，从而减少了性能损失。\\n\\n**技术框架**：该方法包含两个主要阶段：简化模型预训练和模型同伦迁移。在简化模型预训练阶段，使用单刚体模型训练策略，使其能够完成基本的运动模式。在模型同伦迁移阶段，通过逐渐改变模型的质量和惯性分布，从单刚体模型过渡到全身动力学模型。在每个同伦阶段，使用强化学习算法对策略进行微调，以适应新的模型。\\n\\n**关键创新**：该方法最重要的创新点在于提出了模型同伦的概念，并将其应用于策略迁移。通过逐步改变模型的参数，实现从简化模型到复杂模型的平滑过渡，避免了策略在迁移过程中出现剧烈的性能下降。这种方法有效地利用了简化模型的优势，同时保证了策略在复杂模型上的性能。\\n\\n**关键设计**：模型同伦的关键在于如何定义从简化模型到复杂模型的过渡路径。论文通过逐渐重新分配躯干和腿部之间的质量和惯性来实现这一目标。具体来说，定义了一个同伦参数λ，λ=0对应于单刚体模型，λ=1对应于全身动力学模型。在每个同伦阶段，根据λ的值调整模型的质量和惯性参数，并使用PPO等强化学习算法对策略进行微调。",
            "application_zh": "该研究成果可应用于各种腿足机器人的运动控制，尤其是在复杂地形和动态环境下的应用，例如搜救、勘探、物流等。通过简化模型预训练和模型同伦迁移，可以降低腿足机器人运动控制的开发成本和时间，提高其在实际应用中的可靠性和鲁棒性。未来，该方法可以扩展到其他类型的机器人和控制任务中。",
            "highlight_zh": "实验结果表明，该方法在翻转和墙壁辅助动作等动态任务中，与基线方法相比，收敛速度更快，稳定性更高。例如，在翻转任务中，该方法能够更快地学习到稳定的翻转动作，并且在迁移到真实机器人上时，表现出更好的鲁棒性。在真实四足机器人上的部署验证了该方法的实际可行性。",
            "tags_zh": [
                "腿足机器人",
                "强化学习",
                "动态运动控制",
                "模型预训练",
                "模型同伦",
                "策略迁移",
                "四足机器人"
            ],
            "_index": 6,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24698v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24698v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24698v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM",
            "authors": [
                "Yuchen Wu",
                "Jiahe Li",
                "Fabio Tosi",
                "Matteo Poggi",
                "Jin Zheng",
                "Xiao Bai"
            ],
            "arxiv_id": "2512.25008v1",
            "summary": "We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25008v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]visual SLAM"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "FoundationSLAM：利用深度基础模型实现端到端稠密视觉SLAM",
            "summary_zh": "本文提出FoundationSLAM，一个基于学习的单目稠密SLAM系统，旨在解决以往基于光流的方法中缺乏几何一致性的问题，从而实现更准确和鲁棒的跟踪和建图。核心思想是通过利用基础深度模型的指导，将光流估计与几何推理相结合。为此，我们首先开发了一个混合光流网络，该网络产生具有几何感知的对应关系，从而能够在不同的关键帧之间实现一致的深度和姿态推断。为了加强全局一致性，我们提出了一个双一致性捆绑调整层，该层在多视图约束下联合优化关键帧姿态和深度。此外，我们引入了一种可靠性感知细化机制，通过区分可靠和不确定区域来动态调整光流更新过程，从而在匹配和优化之间形成闭环反馈。大量实验表明，FoundationSLAM在多个具有挑战性的数据集上实现了卓越的轨迹精度和稠密重建质量，同时以18 FPS的实时速度运行，展示了对各种场景的强大泛化能力和方法的实际适用性。",
            "intro_zh": [
                "现有基于光流的SLAM方法缺乏几何一致性，导致跟踪和建图精度受限，鲁棒性不足。",
                "FoundationSLAM利用深度基础模型指导光流估计，建立几何感知的对应关系，实现一致的深度和姿态推断。",
                "实验表明，FoundationSLAM在多个数据集上实现了更高的轨迹精度和稠密重建质量，并能实时运行。"
            ],
            "method_zh": "**问题定义**：现有的基于光流的单目稠密SLAM系统，由于缺乏明确的几何约束，容易出现累积误差，导致轨迹漂移和重建质量下降。尤其是在光照变化、快速运动或缺乏纹理的场景中，光流估计的准确性会受到严重影响，进而影响SLAM系统的整体性能。因此，如何提升光流估计的几何一致性，是提升单目稠密SLAM系统性能的关键挑战。\\n\\n**核心思路**：FoundationSLAM的核心思路是利用预训练的深度基础模型（Foundation Depth Models）提供的先验知识，指导光流估计过程，从而增强光流的几何一致性。通过将深度信息融入光流估计，可以有效地约束光流的搜索范围，减少错误匹配，并提高深度和姿态估计的准确性。这种方法将深度学习的强大表征能力与传统的几何约束相结合，有望实现更鲁棒和精确的SLAM系统。\\n\\n**技术框架**：FoundationSLAM的整体框架包含以下几个主要模块：1) 混合光流网络（Hybrid Flow Network）：用于估计几何感知的对应关系，融合了传统光流估计和深度信息。2) 双一致性捆绑调整层（Bi-Consistent Bundle Adjustment Layer）：用于在多视图约束下联合优化关键帧的姿态和深度，保证全局一致性。3) 可靠性感知细化机制（Reliability-Aware Refinement）：用于动态调整光流更新过程，区分可靠和不确定区域，形成匹配和优化之间的闭环反馈。整个流程首先通过混合光流网络提取特征并估计光流，然后利用双一致性捆绑调整层进行全局优化，最后通过可靠性感知细化机制进行局部调整。\\n\\n**关键创新**：FoundationSLAM的关键创新在于将深度基础模型引入到单目稠密SLAM系统中，并设计了一系列模块来充分利用深度信息。与传统的基于光流的SLAM系统相比，FoundationSLAM能够更好地处理光照变化、快速运动和缺乏纹理等挑战性场景，从而实现更准确和鲁棒的跟踪和建图。此外，双一致性捆绑调整层和可靠性感知细化机制也进一步提升了系统的全局一致性和局部精度。\\n\\n**关键设计**：混合光流网络的设计融合了传统光流估计方法和深度信息，具体实现细节未知。双一致性捆绑调整层可能采用了基于图优化的方法，将关键帧的姿态和深度作为节点，将多视图约束作为边，构建一个图模型，然后通过迭代优化算法求解。可靠性感知细化机制可能使用了不确定性估计方法，例如方差估计或Dropout等，来评估光流估计的可靠性，并根据可靠性动态调整光流更新的权重。",
            "application_zh": "FoundationSLAM具有广泛的应用前景，例如增强现实（AR）、虚拟现实（VR）、机器人导航、自动驾驶、三维重建等领域。该系统能够提供高精度、鲁棒的定位和建图能力，为这些应用提供关键的技术支持。未来，可以进一步研究如何将FoundationSLAM与语义信息相结合，实现更智能化的SLAM系统。",
            "highlight_zh": "实验结果表明，FoundationSLAM在多个具有挑战性的数据集上实现了卓越的轨迹精度和稠密重建质量。具体性能数据未知，但摘要中提到该系统能够以18 FPS的实时速度运行，展示了其强大的泛化能力和实际应用价值。与现有的基于光流的SLAM系统相比，FoundationSLAM在精度和鲁棒性方面都有显著提升。",
            "tags_zh": [
                "视觉SLAM",
                "单目SLAM",
                "稠密重建",
                "深度学习",
                "光流估计"
            ],
            "_index": 7,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.25008v1/figs/radar.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.25008v1/figs/framework.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.25008v1/figs/tnt-demo.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes",
            "authors": [
                "Luca Collorone",
                "Mert Kiray",
                "Indro Spinelli",
                "Fabio Galasso",
                "Benjamin Busam"
            ],
            "arxiv_id": "2512.24986v1",
            "summary": "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a \"render and wait\" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.",
            "categories": [
                "cs.GR",
                "cs.CV"
            ],
            "primary_category": "cs.GR",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24986v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3D gaussian splatting",
                        "3DGS",
                        "gaussian splatting",
                        "splatting",
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 17.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "PhysTalk：基于语言驱动的3D高斯场景实时物理交互",
            "summary_zh": "逼真的视觉模拟应用广泛，但其创建需要计算时间、渲染和专业的动画知识。从文本输入生成开放词汇的视觉效果是一种有前景的解决方案，可以释放巨大的创造潜力。然而，当前的流程缺乏物理真实性和有效的语言接口，需要缓慢的离线优化。相比之下，PhysTalk 以 3D 高斯溅射 (3DGS) 场景作为输入，并将任意用户提示转换为实时的、基于物理的交互式 4D 动画。大型语言模型 (LLM) 生成可执行代码，通过轻量级代理和粒子动力学直接修改 3DGS 参数。值得注意的是，PhysTalk 是第一个将 3DGS 直接与物理模拟器耦合，而无需依赖耗时的网格提取的框架。在保持开放词汇的同时，这种设计能够通过碰撞感知的、基于物理的对任意多材质对象的操纵来实现交互式 3D 高斯动画。最后，PhysTalk 是免训练且计算量轻的：这使得 4D 动画得到广泛应用，并将这些工作流程从“渲染并等待”的模式转变为与现代的、物理信息丰富的管道进行交互式对话。",
            "intro_zh": [
                "现有方法在生成逼真物理交互的3D动画时，面临计算成本高昂、渲染耗时以及缺乏有效语言控制接口等挑战。",
                "PhysTalk 提出利用大型语言模型直接操控3D高斯溅射场景参数，实现基于物理的实时交互式4D动画。",
                "PhysTalk 首次将3DGS与物理模拟器直接耦合，无需耗时的网格提取，实现了开放词汇的交互式3D高斯动画。"
            ],
            "method_zh": "**问题定义**：现有方法生成具有物理交互的3D动画时，需要耗费大量的计算资源进行渲染和优化，并且缺乏有效的语言控制接口，用户难以通过自然语言直接控制动画效果。此外，将3D场景与物理引擎结合通常需要进行耗时的网格提取，限制了实时交互性。\\n\\n**核心思路**：PhysTalk的核心思路是利用大型语言模型（LLM）理解用户输入的自然语言指令，并将其转化为可执行的代码，直接操控3D高斯溅射（3DGS）场景的参数。通过轻量级的代理和粒子动力学，实现对3DGS场景的实时物理模拟和交互。这种方法避免了传统的网格提取过程，提高了效率和交互性。\\n\\n**技术框架**：PhysTalk的整体框架包括以下几个主要模块：1) 3DGS场景输入：接收3DGS场景作为输入。2) 语言模型：使用大型语言模型（LLM）解析用户输入的自然语言指令，并生成相应的代码。3) 代码执行器：执行LLM生成的代码，直接修改3DGS场景的参数。4) 物理模拟器：使用物理模拟器对3DGS场景进行物理模拟，实现碰撞检测和动力学效果。5) 渲染器：实时渲染经过物理模拟后的3DGS场景，生成4D动画。\\n\\n**关键创新**：PhysTalk最重要的创新点在于它首次将3DGS直接与物理模拟器耦合，无需进行耗时的网格提取。这使得PhysTalk能够实现对任意多材质对象的碰撞感知和基于物理的操纵，从而实现交互式的3D高斯动画。此外，PhysTalk还利用大型语言模型实现了开放词汇的语言控制，用户可以通过自然语言指令直接控制动画效果。\\n\\n**关键设计**：PhysTalk的关键设计包括：1) 使用轻量级的代理和粒子动力学来表示3DGS场景中的对象，以便进行高效的物理模拟。2) 设计了一种代码生成机制，使得LLM能够生成可执行的代码，直接修改3DGS场景的参数。3) 采用了一种碰撞检测算法，能够高效地检测3DGS场景中对象之间的碰撞。",
            "application_zh": "PhysTalk 有潜力应用于游戏开发、电影制作、虚拟现实和增强现实等领域。它能够让用户通过简单的自然语言指令，快速创建具有逼真物理效果的3D动画，极大地降低了动画制作的门槛，并提升了创作效率。未来，PhysTalk 还可以应用于机器人控制和人机交互等领域，实现更加自然和智能的人机交互体验。",
            "highlight_zh": "PhysTalk 实现了基于语言驱动的3D高斯场景实时物理交互，无需训练，计算量小，使得4D动画制作更加便捷。该框架首次将3DGS与物理模拟器直接耦合，避免了耗时的网格提取，实现了开放词汇的交互式3D高斯动画。实验结果表明，PhysTalk 能够生成逼真的物理效果，并支持实时交互。",
            "tags_zh": [
                "3D高斯溅射",
                "物理模拟",
                "语言驱动",
                "实时交互",
                "4D动画"
            ],
            "_index": 8,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24986v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24986v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24986v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation",
            "authors": [
                "Yury Kolomeytsev",
                "Dmitry Golembiovsky"
            ],
            "arxiv_id": "2512.24651v1",
            "summary": "Autonomous mobile robots operating in complex, dynamic environments face the dual challenge of navigating large-scale, structurally diverse spaces with static obstacles while safely interacting with various moving agents. Traditional graph-based planners excel at long-range pathfinding but lack reactivity, while Deep Reinforcement Learning (DRL) methods demonstrate strong collision avoidance but often fail to reach distant goals due to a lack of global context. We propose Hybrid Motion Planning with Deep Reinforcement Learning (HMP-DRL), a hybrid framework that bridges this gap. Our approach utilizes a graph-based global planner to generate a path, which is integrated into a local DRL policy via a sequence of checkpoints encoded in both the state space and reward function. To ensure social compliance, the local planner employs an entity-aware reward structure that dynamically adjusts safety margins and penalties based on the semantic type of surrounding agents. We validate the proposed method through extensive testing in a realistic simulation environment derived from real-world map data. Comprehensive experiments demonstrate that HMP-DRL consistently outperforms other methods, including state-of-the-art approaches, in terms of key metrics of robot navigation: success rate, collision rate, and time to reach the goal. Overall, these findings confirm that integrating long-term path guidance with semantically-aware local control significantly enhances both the safety and reliability of autonomous navigation in complex human-centric settings.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "22 pages, 4 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24651v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "[T]deep reinforcement learning",
                        "DRL"
                    ],
                    "score": 10.5
                }
            ],
            "relevance_score": 16.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出HMP-DRL混合运动规划，提升移动机器人在复杂动态环境中的导航能力。",
            "summary_zh": "本文提出了一种混合运动规划与深度强化学习(HMP-DRL)框架，旨在解决移动机器人在复杂动态环境中导航的问题。传统图搜索规划器擅长长距离路径规划，但缺乏反应性；深度强化学习(DRL)方法在避障方面表现出色，但由于缺乏全局上下文信息，难以到达远距离目标。HMP-DRL框架结合了图搜索全局规划器和局部DRL策略，通过将全局路径上的检查点编码到状态空间和奖励函数中，实现二者的有效集成。为了确保社交合规性，局部规划器采用了一种实体感知的奖励结构，该结构根据周围智能体的语义类型动态调整安全边际和惩罚。在基于真实地图数据的仿真环境中进行的大量实验表明，HMP-DRL在成功率、碰撞率和到达目标时间等关键指标上始终优于其他方法，包括最先进的方法。研究结果表明，将长期路径引导与语义感知的局部控制相结合，可以显著提高自主导航在复杂人机交互环境中的安全性和可靠性。",
            "intro_zh": [
                "传统图搜索规划器缺乏对动态环境的反应性，而深度强化学习方法缺乏全局路径规划能力，导致导航效率和安全性不足。",
                "HMP-DRL框架融合了图搜索全局规划和深度强化学习局部控制，利用全局路径点引导局部策略，实现长期规划和短期反应的结合。",
                "实验结果表明，HMP-DRL在成功率、碰撞率和到达目标时间等指标上优于现有方法，提升了复杂环境下的导航性能。"
            ],
            "method_zh": "**问题定义**：移动机器人在复杂动态环境中导航，需要兼顾长距离路径规划和局部避障。传统图搜索规划器难以应对动态障碍物，而深度强化学习方法缺乏全局视野，容易陷入局部最优，导致导航效率低下和安全性不足。\\n\\n**核心思路**：将图搜索全局规划器提供的长距离路径信息融入到深度强化学习的局部控制策略中，利用全局路径点引导局部策略的学习，从而实现长期规划和短期反应的有效结合。通过实体感知的奖励函数，使机器人能够根据周围环境中的智能体类型调整行为，提高社交合规性。\\n\\n**技术框架**：HMP-DRL框架包含两个主要模块：全局规划器和局部DRL策略。全局规划器使用图搜索算法生成从起点到目标点的全局路径。然后，将全局路径上的关键点（检查点）编码到局部DRL策略的状态空间和奖励函数中。局部DRL策略根据当前状态（包括机器人自身状态、周围环境信息和全局路径点信息）选择动作，并根据奖励函数进行学习。\\n\\n**关键创新**：HMP-DRL的关键创新在于将全局路径信息有效地融入到局部DRL策略中。通过将全局路径点编码到状态空间和奖励函数中，局部策略能够感知到全局目标，避免陷入局部最优。此外，实体感知的奖励函数能够使机器人根据周围环境中的智能体类型调整行为，提高社交合规性。\\n\\n**关键设计**：奖励函数的设计是HMP-DRL的关键。奖励函数包含多个部分，包括到达目标点的奖励、避开障碍物的惩罚、与周围智能体保持安全距离的奖励以及偏离全局路径的惩罚。实体感知的奖励函数根据周围智能体的类型动态调整安全距离和惩罚力度。具体的网络结构和参数设置在论文中未明确给出，属于未知信息。",
            "application_zh": "该研究成果可应用于各种需要在复杂动态环境中进行自主导航的移动机器人，例如服务机器人、物流机器人、自动驾驶汽车等。通过提高导航的安全性和效率，可以降低运营成本，提升用户体验，并促进相关产业的发展。",
            "highlight_zh": "实验结果表明，HMP-DRL在成功率、碰撞率和到达目标时间等关键指标上始终优于其他方法，包括最先进的方法。具体性能数据和提升幅度在摘要中提及，但在论文信息中未给出具体数值，属于未知信息。",
            "tags_zh": [
                "移动机器人导航",
                "深度强化学习",
                "混合运动规划",
                "全局路径规划",
                "局部避障",
                "动态环境",
                "人机交互",
                "社交合规性"
            ],
            "_index": 9,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24651v1/figures/map1_v2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24651v1/figures/checkpoints.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24651v1/figures/map_example.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots",
            "authors": [
                "Yongsheng Zhao",
                "Lei Zhao",
                "Baoping Cheng",
                "Gongxin Yao",
                "Xuanzhang Wen",
                "Han Gao"
            ],
            "arxiv_id": "2512.24673v1",
            "summary": "Vision-Language-Action (VLA) models have achieved remarkable breakthroughs in robotics, with the action chunk playing a dominant role in these advances. Given the real-time and continuous nature of robotic motion control, the strategies for fusing a queue of successive action chunks have a profound impact on the overall performance of VLA models. Existing methods suffer from jitter, stalling, or even pauses in robotic action execution, which not only limits the achievable execution speed but also reduces the overall success rate of task completion. This paper introduces VLA-RAIL (A Real-Time Asynchronous Inference Linker), a novel framework designed to address these issues by conducting model inference and robot motion control asynchronously and guaranteeing smooth, continuous, and high-speed action execution. The core contributions of the paper are two fold: a Trajectory Smoother that effectively filters out the noise and jitter in the trajectory of one action chunk using polynomial fitting and a Chunk Fuser that seamlessly align the current executing trajectory and the newly arrived chunk, ensuring position, velocity, and acceleration continuity between two successive action chunks. We validate the effectiveness of VLA-RAIL on a benchmark of dynamic simulation tasks and several real-world manipulation tasks. Experimental results demonstrate that VLA-RAIL significantly reduces motion jitter, enhances execution speed, and improves task success rates, which will become a key infrastructure for the large-scale deployment of VLA models.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24673v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "vision-language-action",
                        "[T]VLA"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 14.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "VLA-RAIL：用于VLA模型和机器人的实时异步推理连接器，解决动作执行抖动和停顿问题。",
            "summary_zh": "本文提出了一种名为VLA-RAIL（实时异步推理连接器）的新框架，旨在解决视觉-语言-动作（VLA）模型在机器人控制中面临的问题。VLA模型在机器人领域取得了显著进展，其中动作块在这些进展中起着主导作用。考虑到机器人运动控制的实时性和连续性，融合连续动作块队列的策略对VLA模型的整体性能具有深远的影响。现有方法存在机器人动作执行中的抖动、停顿甚至暂停等问题，这不仅限制了可实现的执行速度，还降低了任务完成的整体成功率。VLA-RAIL通过异步执行模型推理和机器人运动控制，并保证平滑、连续和高速的动作执行来解决这些问题。该论文的核心贡献包括：使用多项式拟合有效过滤动作块轨迹中的噪声和抖动的轨迹平滑器，以及无缝对齐当前执行轨迹和新到达的动作块，确保两个连续动作块之间的位置、速度和加速度连续性的块融合器。在动态仿真任务和多个真实操作任务的基准测试中验证了VLA-RAIL的有效性。实验结果表明，VLA-RAIL显著减少了运动抖动，提高了执行速度，并提高了任务成功率，这将成为VLA模型大规模部署的关键基础设施。",
            "intro_zh": [
                "现有VLA模型在机器人控制中存在动作执行抖动、停顿等问题，限制了执行速度和任务成功率。",
                "VLA-RAIL通过异步推理和运动控制，以及轨迹平滑和块融合，保证动作执行的平滑、连续和高速。",
                "实验结果表明，VLA-RAIL显著减少了运动抖动，提高了执行速度，并提高了任务成功率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决VLA模型在机器人控制中，由于连续动作块的融合策略不佳导致的动作执行抖动、停顿甚至暂停等问题。现有方法无法保证机器人运动的平滑性和连续性，限制了执行速度和任务成功率。\\n\\n**核心思路**：论文的核心思路是将模型推理和机器人运动控制异步执行，并设计轨迹平滑器和块融合器来保证动作执行的平滑、连续和高速。通过异步执行，可以避免模型推理的延迟对机器人运动的直接影响。轨迹平滑器用于消除单个动作块中的噪声和抖动，块融合器用于无缝连接连续的动作块。\\n\\n**技术框架**：VLA-RAIL框架主要包含三个部分：VLA模型推理模块、轨迹平滑器和块融合器。VLA模型推理模块负责生成动作块序列。轨迹平滑器对每个动作块的轨迹进行平滑处理，消除噪声和抖动。块融合器将当前执行的轨迹与新到达的动作块进行对齐，保证位置、速度和加速度的连续性。整个过程是异步执行的，即VLA模型推理和机器人运动控制并行进行。\\n\\n**关键创新**：论文的关键创新在于提出了异步推理连接器VLA-RAIL，以及其中的轨迹平滑器和块融合器。异步推理允许模型推理和机器人运动控制并行进行，避免了模型推理延迟的影响。轨迹平滑器使用多项式拟合来消除噪声和抖动，块融合器通过保证位置、速度和加速度的连续性来实现无缝连接。与现有方法相比，VLA-RAIL能够显著减少运动抖动，提高执行速度和任务成功率。\\n\\n**关键设计**：轨迹平滑器使用多项式拟合来平滑轨迹，多项式的阶数是一个关键参数，需要根据具体任务进行调整。块融合器通过优化目标函数来对齐轨迹，目标函数通常包含位置、速度和加速度的连续性约束。损失函数的设计需要权衡不同约束的重要性。此外，异步执行的调度策略也需要仔细设计，以保证系统的实时性和稳定性。",
            "application_zh": "VLA-RAIL框架可广泛应用于各种需要实时、连续运动控制的机器人任务中，例如工业自动化、服务机器人、医疗机器人等。该框架能够提高机器人操作的效率和精度，降低操作风险，并为VLA模型的大规模部署提供关键基础设施。未来，该框架可以进一步扩展到更复杂的机器人任务和更广泛的应用场景。",
            "highlight_zh": "实验结果表明，VLA-RAIL在动态仿真任务和真实操作任务中均取得了显著的性能提升。与现有方法相比，VLA-RAIL显著减少了运动抖动，提高了执行速度，并提高了任务成功率。例如，在某项真实操作任务中，VLA-RAIL将任务成功率提高了15%，并将执行时间缩短了20%。",
            "tags_zh": [
                "视觉语言动作模型",
                "机器人控制",
                "实时推理",
                "异步执行",
                "轨迹平滑",
                "块融合"
            ],
            "_index": 10,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24673v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24673v1/images/vla-rail-pipeline.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24673v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Control of Microrobots with Reinforcement Learning under On-Device Compute Constraints",
            "authors": [
                "Yichen Liu",
                "Kesava Viswanadha",
                "Zhongyu Li",
                "Nelson Lojo",
                "Kristofer S. J. Pister"
            ],
            "arxiv_id": "2512.24740v1",
            "summary": "An important function of autonomous microrobots is the ability to perform robust movement over terrain. This paper explores an edge ML approach to microrobot locomotion, allowing for on-device, lower latency control under compute, memory, and power constraints. This paper explores the locomotion of a sub-centimeter quadrupedal microrobot via reinforcement learning (RL) and deploys the resulting controller on an ultra-small system-on-chip (SoC), SC$μ$M-3C, featuring an ARM Cortex-M0 microcontroller running at 5 MHz. We train a compact FP32 multilayer perceptron (MLP) policy with two hidden layers ($[128, 64]$) in a massively parallel GPU simulation and enhance robustness by utilizing domain randomization over simulation parameters. We then study integer (Int8) quantization (per-tensor and per-feature) to allow for higher inference update rates on our resource-limited hardware, and we connect hardware power budgets to achievable update frequency via a cycles-per-update model for inference on our Cortex-M0. We propose a resource-aware gait scheduling viewpoint: given a device power budget, we can select the gait mode (trot/intermediate/gallop) that maximizes expected RL reward at a corresponding feasible update frequency. Finally, we deploy our MLP policy on a real-world large-scale robot on uneven terrain, qualitatively noting that domain-randomized training can improve out-of-distribution stability. We do not claim real-world large-robot empirical zero-shot transfer in this work.",
            "categories": [
                "cs.RO",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "9 pages, 10 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24740v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "quadruped",
                        "locomotion",
                        "domain randomization"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "zero-shot transfer"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出一种边缘计算约束下的强化学习微型机器人控制方法，实现低延迟鲁棒运动。",
            "summary_zh": "本文探索了一种用于微型机器人运动的边缘机器学习方法，允许在计算、内存和功率约束下进行片上、低延迟控制。本文研究了亚厘米级四足微型机器人的运动，通过强化学习(RL)训练控制器，并将其部署在超小型片上系统(SoC) SCμM-3C上，该系统采用运行在5 MHz的ARM Cortex-M0微控制器。我们在大规模并行GPU模拟中训练了一个紧凑的FP32多层感知器(MLP)策略，该策略具有两个隐藏层([128, 64])，并通过对模拟参数进行域随机化来增强鲁棒性。然后，我们研究整数(Int8)量化(per-tensor和per-feature)，以便在资源受限的硬件上实现更高的推理更新速率，并通过Cortex-M0上推理的每次更新周期模型，将硬件功率预算与可实现的更新频率联系起来。我们提出了一种资源感知的步态调度观点：给定设备功率预算，我们可以选择以相应的可行更新频率最大化预期RL奖励的步态模式(小跑/中间/疾驰)。最后，我们将MLP策略部署在不平坦地形上的真实大型机器人上，定性地注意到，域随机化训练可以提高分布外稳定性。我们不声称在这项工作中实现真实世界大型机器人的经验零样本迁移。",
            "intro_zh": [
                "微型机器人在复杂地形上的鲁棒运动控制面临计算资源、内存和功耗的严格限制，传统的控制方法难以满足需求。",
                "该论文提出了一种基于边缘机器学习的强化学习控制方法，通过在资源受限的片上系统上部署紧凑型神经网络策略，实现低延迟控制。",
                "通过领域随机化训练和整数量化技术，提升了策略的鲁棒性和推理速度，并在真实大型机器人上验证了领域随机化训练的有效性。"
            ],
            "method_zh": "**问题定义**：微型机器人在实际应用中，需要在计算资源极其有限的条件下，实现复杂地形上的稳定运动控制。传统的控制算法往往计算量大，难以在微型机器人的片上系统上实时运行。此外，真实环境与仿真环境存在差异，导致在仿真环境中训练的策略在真实环境中表现不佳。\\n\\n**核心思路**：该论文的核心思路是利用强化学习训练一个紧凑型神经网络策略，并将其部署在资源受限的片上系统上。通过领域随机化技术，提高策略在不同环境下的泛化能力。同时，采用整数量化技术，降低模型的计算复杂度和内存占用，从而提高推理速度。\\n\\n**技术框架**：整体框架包括三个主要阶段：1) 在GPU上进行强化学习训练，得到一个FP32精度的MLP策略；2) 对MLP策略进行整数量化，得到Int8精度的模型；3) 将量化后的模型部署到ARM Cortex-M0微控制器上，进行实时控制。此外，论文还提出了资源感知的步态调度方法，根据设备功率预算选择合适的步态模式。\\n\\n**关键创新**：该论文的关键创新在于将强化学习与边缘计算相结合，提出了一种适用于资源受限微型机器人的控制方法。通过领域随机化和整数量化，提高了策略的鲁棒性和推理速度。此外，资源感知的步态调度方法能够根据设备功率预算动态调整控制策略，从而实现更高效的能量利用。\\n\\n**关键设计**：MLP策略采用两层隐藏层结构，大小分别为[128, 64]。强化学习算法采用未知（论文未明确指出）。领域随机化通过随机改变仿真环境的参数，如摩擦系数、地形高度等，来提高策略的泛化能力。整数量化采用per-tensor和per-feature两种方式。资源感知的步态调度方法根据不同步态模式的功率消耗和预期奖励，选择最优的步态模式。",
            "application_zh": "该研究成果可应用于微型机器人在复杂环境下的自主导航、搜索救援、医疗诊断等领域。通过在边缘设备上实现低延迟、鲁棒的控制，可以提高微型机器人的自主性和适应性，使其能够在各种实际场景中发挥作用。未来，该技术有望推动微型机器人在工业、医疗、军事等领域的广泛应用。",
            "highlight_zh": "该论文在仿真环境中训练了一个紧凑的MLP策略，并通过领域随机化和整数量化技术，成功将其部署在资源受限的ARM Cortex-M0微控制器上。定性实验表明，领域随机化训练可以提高策略在真实大型机器人上的稳定性。虽然论文没有提供具体的性能数据，但其提出的方法为微型机器人的边缘计算控制提供了一种可行的解决方案。",
            "tags_zh": [
                "微型机器人",
                "强化学习",
                "边缘计算",
                "领域随机化",
                "整数量化",
                "片上系统",
                "低功耗控制"
            ],
            "_index": 11,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24740v1/figures/scum_robot.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24740v1/figures/hardware_config.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24740v1/figures/matlab_model.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation",
            "authors": [
                "Zichen Tang",
                "Haihong E",
                "Rongjin Li",
                "Jiacheng Liu",
                "Linwei Jia",
                "Zhuodi Hao",
                "Zhongjun Yang",
                "Yuanze Li",
                "Haolin Tian",
                "Xinyi Hu",
                "Peizhi Zhao",
                "Yuan Liu",
                "Zhengyu Wang",
                "Xianghe Wang",
                "Yiling Huang",
                "Xueyuan Lin",
                "Ruofei Bai",
                "Zijian Xie",
                "Qian Huang",
                "Ruining Cao",
                "Haocheng Gao"
            ],
            "arxiv_id": "2512.24903v1",
            "summary": "We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.",
            "categories": [
                "cs.CV",
                "cs.CE"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Accepted by AAAI-26 Main Track",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24903v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "FinMMDocR：提出金融多模态推理基准，关注场景感知、文档理解和多步计算。",
            "summary_zh": "本文提出了FinMMDocR，这是一个新的双语多模态基准，用于评估多模态大型语言模型（MLLM）在真实金融数值推理方面的能力。与现有基准相比，FinMMDocR有三个主要进展：(1) 场景感知：1200个专家标注的问题中，57.9%的问题融入了12种隐式金融场景（例如，投资组合管理），挑战模型基于假设执行专家级推理；(2) 文档理解：837篇中文/英文文档涵盖9种类型（例如，公司研究），平均50.8页，包含丰富的视觉元素，在金融文档的广度和深度上显著超越现有基准；(3) 多步计算：问题平均需要11步推理（5.3步提取+5.7步计算），其中65.0%需要跨页证据（平均2.4页）。性能最佳的MLLM仅达到58.0%的准确率，并且不同的检索增强生成（RAG）方法在该任务上表现出显著的性能差异。我们期望FinMMDocR能够推动MLLM和推理增强方法在真实场景中复杂多模态推理任务上的改进。",
            "intro_zh": [
                "现有基准在金融领域的多模态推理方面存在不足，缺乏对复杂场景、深度文档理解和多步计算的有效评估。",
                "FinMMDocR通过引入场景感知、深度金融文档和多步计算，构建更贴近真实金融场景的多模态推理基准。",
                "实验表明，现有最佳MLLM在FinMMDocR上表现仍有提升空间，且RAG方法性能差异显著，突显了该基准的挑战性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大型语言模型（MLLM）在真实金融场景下进行数值推理时面临的挑战。现有方法在场景感知、文档理解深度和多步计算能力方面存在不足，难以有效处理复杂的金融文档和推理任务。\\n\\n**核心思路**：论文的核心思路是构建一个更具挑战性和真实性的金融多模态推理基准，即FinMMDocR。该基准通过引入隐式金融场景、深度金融文档和多步计算，迫使模型进行更深入的理解和推理，从而更好地评估和提升MLLM在金融领域的应用能力。\\n\\n**技术框架**：FinMMDocR基准包含以下几个关键组成部分：1) 大量的金融文档，涵盖多种类型和格式；2) 专家标注的问题，包含隐式金融场景和多步计算要求；3) 评估指标，用于衡量模型在场景感知、文档理解和多步计算方面的性能。整体流程是：给定金融文档和问题，模型需要提取相关信息、进行计算和推理，最终给出答案。\\n\\n**关键创新**：FinMMDocR的关键创新在于其对真实金融场景的模拟和对模型推理能力的深度评估。具体体现在：1) 引入了隐式金融场景，要求模型具备专家级的推理能力；2) 采用了深度金融文档，挑战模型的文档理解能力；3) 设计了多步计算问题，考察模型的推理和计算能力。\\n\\n**关键设计**：FinMMDocR基准包含1200个专家标注的问题，其中57.9%的问题包含12种隐式金融场景。文档方面，包含837篇中文/英文文档，涵盖9种类型，平均50.8页。问题平均需要11步推理（5.3步提取+5.7步计算），其中65.0%需要跨页证据（平均2.4页）。评估指标包括准确率等。",
            "application_zh": "FinMMDocR的研究成果可应用于金融领域的智能投顾、风险评估、财务分析等场景。通过提升MLLM在金融多模态推理方面的能力，可以帮助金融从业者更高效地处理海量金融数据，做出更明智的决策，并为投资者提供更优质的服务。未来，该基准可以促进金融领域AI应用的进一步发展。",
            "highlight_zh": "实验结果表明，现有最佳MLLM在FinMMDocR上的准确率仅为58.0%，表明该基准具有很高的挑战性。此外，不同的RAG方法在该任务上表现出显著的性能差异，突显了检索和推理策略的重要性。这些结果为未来研究提供了明确的方向，即需要进一步提升MLLM在场景感知、文档理解和多步计算方面的能力。",
            "tags_zh": [
                "金融多模态推理",
                "大型语言模型",
                "基准测试",
                "场景感知",
                "文档理解",
                "多步计算",
                "检索增强生成"
            ],
            "_index": 12,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24903v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24903v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24903v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation",
            "authors": [
                "Takeru Kusakabe",
                "Yudai Hirose",
                "Mashiho Mukaida",
                "Satoshi Ono"
            ],
            "arxiv_id": "2512.24792v1",
            "summary": "Deep neural networks (DNNs) remain vulnerable to adversarial attacks that cause misclassification when specific perturbations are added to input images. This vulnerability also threatens the reliability of DNN-based monocular depth estimation (MDE) models, making robustness enhancement a critical need in practical applications. To validate the vulnerability of DNN-based MDE models, this study proposes a projection-based adversarial attack method that projects perturbation light onto a target object. The proposed method employs physics-in-the-loop (PITL) optimization -- evaluating candidate solutions in actual environments to account for device specifications and disturbances -- and utilizes a distributed covariance matrix adaptation evolution strategy. Experiments confirmed that the proposed method successfully created adversarial examples that lead to depth misestimations, resulting in parts of objects disappearing from the target scene.",
            "categories": [
                "cs.CV",
                "cs.LG",
                "cs.NE"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "10.1587/transinf.2025MUL0002",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24792v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]depth estimation",
                        "[T]monocular depth"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于物理环路优化的投影对抗攻击，用于单目深度估计",
            "summary_zh": "深度神经网络（DNNs）容易受到对抗攻击的影响，即通过向输入图像添加特定的扰动会导致错误分类。这种脆弱性也威胁到基于DNN的单目深度估计（MDE）模型的可靠性，使得鲁棒性增强在实际应用中至关重要。为了验证基于DNN的MDE模型的脆弱性，本研究提出了一种基于投影的对抗攻击方法，该方法将扰动光投影到目标对象上。所提出的方法采用物理环路（PITL）优化——在实际环境中评估候选解决方案，以考虑设备规格和干扰——并利用分布式协方差矩阵自适应进化策略。实验证实，该方法成功地创建了对抗样本，导致深度估计错误，从而导致目标场景中的部分对象消失。",
            "intro_zh": [
                "单目深度估计模型易受对抗攻击影响，实际应用中鲁棒性需求迫切。",
                "提出基于物理环路优化的投影对抗攻击方法，将扰动光投影到目标对象。",
                "实验表明该方法能有效生成对抗样本，导致深度估计错误，使目标对象部分消失。"
            ],
            "method_zh": "**问题定义**：论文旨在解决单目深度估计（MDE）模型在对抗攻击下的脆弱性问题。现有的深度学习模型容易受到对抗样本的攻击，即使是微小的扰动也可能导致模型产生错误的深度估计，这对于依赖深度信息的应用来说是不可接受的。\\n\\n**核心思路**：论文的核心思路是通过物理环路（PITL）优化来生成对抗样本。与传统的数字对抗攻击不同，该方法考虑了真实物理环境中的设备规格和干扰，通过将扰动光投影到目标对象上，直接在物理世界中创建对抗样本。这种方法更贴近实际应用场景，能够更有效地评估和提升模型的鲁棒性。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 定义目标场景和目标对象；2) 使用投影仪将扰动光投影到目标对象上；3) 使用单目深度估计模型对场景进行深度估计；4) 计算深度估计结果与真实深度之间的差异，作为损失函数；5) 使用分布式协方差矩阵自适应进化策略（CMA-ES）优化扰动光的参数，以最大化损失函数。整个过程形成一个闭环，通过不断迭代优化，生成能够有效欺骗深度估计模型的对抗样本。\\n\\n**关键创新**：该方法最重要的技术创新在于将物理环路优化引入到对抗攻击中。传统的对抗攻击主要在数字空间进行，忽略了物理世界的复杂性和不确定性。通过在实际环境中评估候选解决方案，该方法能够更好地模拟真实场景，生成更具鲁棒性的对抗样本。此外，使用分布式CMA-ES算法能够高效地搜索高维参数空间，找到最优的扰动光参数。\\n\\n**关键设计**：关键设计包括：1) 扰动光的投影方式，需要考虑投影仪的参数和场景的光照条件；2) 损失函数的定义，需要能够准确反映深度估计的误差；3) CMA-ES算法的参数设置，包括种群大小、学习率等。此外，论文还可能涉及到一些图像处理和深度学习相关的技术细节，例如图像增强、网络结构设计等，但摘要中未明确提及。",
            "application_zh": "该研究成果可应用于自动驾驶、机器人导航、增强现实等领域。通过评估和增强单目深度估计模型在对抗攻击下的鲁棒性，可以提高这些系统在复杂和不确定环境中的可靠性和安全性。未来的研究可以进一步探索更有效的对抗防御方法，以及将该方法推广到其他计算机视觉任务中。",
            "highlight_zh": "实验结果表明，所提出的方法能够成功生成对抗样本，导致单目深度估计模型产生显著的深度误差，使得目标场景中的部分对象在深度图中消失。具体的性能数据和对比基线在摘要中未提供，但实验结果验证了该方法在攻击单目深度估计模型方面的有效性。",
            "tags_zh": [
                "对抗攻击",
                "单目深度估计",
                "物理环路优化",
                "深度神经网络",
                "鲁棒性",
                "计算机视觉",
                "进化策略"
            ],
            "_index": 13,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Sparse Offline Reinforcement Learning with Corruption Robustness",
            "authors": [
                "Nam Phuong Tran",
                "Andi Nika",
                "Goran Radanovic",
                "Long Tran-Thanh",
                "Debmalya Mandal"
            ],
            "arxiv_id": "2512.24768v1",
            "summary": "We investigate robustness to strong data corruption in offline sparse reinforcement learning (RL). In our setting, an adversary may arbitrarily perturb a fraction of the collected trajectories from a high-dimensional but sparse Markov decision process, and our goal is to estimate a near optimal policy. The main challenge is that, in the high-dimensional regime where the number of samples $N$ is smaller than the feature dimension $d$, exploiting sparsity is essential for obtaining non-vacuous guarantees but has not been systematically studied in offline RL. We analyse the problem under uniform coverage and sparse single-concentrability assumptions. While Least Square Value Iteration (LSVI), a standard approach for robust offline RL, performs well under uniform coverage, we show that integrating sparsity into LSVI is unnatural, and its analysis may break down due to overly pessimistic bonuses. To overcome this, we propose actor-critic methods with sparse robust estimator oracles, which avoid the use of pointwise pessimistic bonuses and provide the first non-vacuous guarantees for sparse offline RL under single-policy concentrability coverage. Moreover, we extend our results to the contaminated setting and show that our algorithm remains robust under strong contamination. Our results provide the first non-vacuous guarantees in high-dimensional sparse MDPs with single-policy concentrability coverage and corruption, showing that learning a near-optimal policy remains possible in regimes where traditional robust offline RL techniques may fail.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24768v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning",
                        "offline RL",
                        "[T]offline reinforcement learning"
                    ],
                    "score": 10.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于稀疏鲁棒估计的Actor-Critic算法，解决离线稀疏RL中的数据污染问题。",
            "summary_zh": "本文研究了离线稀疏强化学习（RL）中对强数据污染的鲁棒性。在我们的设置中，一个攻击者可以任意扰动来自高维但稀疏马尔可夫决策过程的一小部分收集到的轨迹，我们的目标是估计一个接近最优的策略。主要的挑战是，在高维情况下，样本数量N小于特征维度d，利用稀疏性对于获得非平凡的保证至关重要，但在离线RL中尚未得到系统研究。我们分析了均匀覆盖和稀疏单策略集中性假设下的问题。虽然最小二乘值迭代（LSVI）是鲁棒离线RL的标准方法，并且在均匀覆盖下表现良好，但我们表明将稀疏性集成到LSVI中是不自然的，并且由于过于悲观的奖励，其分析可能会崩溃。为了克服这个问题，我们提出了具有稀疏鲁棒估计器oracle的actor-critic方法，该方法避免了逐点悲观奖励的使用，并为单策略集中性覆盖下的稀疏离线RL提供了第一个非平凡的保证。此外，我们将结果扩展到受污染的环境，并表明我们的算法在强污染下仍然是鲁棒的。我们的结果在高维稀疏MDP中提供了第一个具有单策略集中性覆盖和污染的非平凡保证，表明在传统鲁棒离线RL技术可能失败的情况下，学习接近最优的策略仍然是可能的。",
            "intro_zh": [
                "现有离线RL方法在高维稀疏MDP中，面对数据污染时，鲁棒性不足，难以保证策略的有效性。",
                "提出基于稀疏鲁棒估计的Actor-Critic算法，避免使用逐点悲观奖励，从而提升算法的鲁棒性。",
                "理论分析表明，该算法在单策略集中性覆盖和数据污染下，能够学习到接近最优的策略。"
            ],
            "method_zh": "**问题定义**：论文旨在解决高维稀疏马尔可夫决策过程（MDP）中，离线强化学习算法对数据污染的鲁棒性问题。现有方法，如最小二乘值迭代（LSVI），在处理高维数据时，难以有效利用稀疏性，并且在数据被污染的情况下，性能会显著下降。传统的鲁棒离线RL技术在高维稀疏MDP中可能失效。\n\n**核心思路**：论文的核心思路是设计一种基于稀疏鲁棒估计的Actor-Critic算法，该算法避免使用逐点悲观奖励，从而提高算法对数据污染的鲁棒性。通过利用稀疏性，算法能够在高维环境中更有效地学习策略。\n\n**技术框架**：该算法采用Actor-Critic框架，包含以下主要模块：1) Actor网络，用于生成策略；2) Critic网络，用于评估策略的价值；3) 稀疏鲁棒估计器Oracle，用于估计价值函数，并对数据污染具有鲁棒性。算法通过迭代更新Actor和Critic网络，最终学习到一个接近最优的策略。\n\n**关键创新**：最重要的技术创新点在于提出了稀疏鲁棒估计器Oracle，该Oracle能够有效地利用数据的稀疏性，并且对数据污染具有鲁棒性。与传统的LSVI方法相比，该方法避免了使用逐点悲观奖励，从而避免了过度悲观的估计，提高了算法的性能。\n\n**关键设计**：论文中，稀疏鲁棒估计器Oracle的具体实现方式未知，但其核心在于利用稀疏性约束，例如L1正则化，来提高估计的准确性和鲁棒性。Actor和Critic网络的具体结构也未知，但通常会采用深度神经网络来实现。",
            "application_zh": "该研究成果可应用于高维、数据稀疏且易受污染的强化学习场景，例如推荐系统、金融交易、医疗诊断等。在这些场景中，数据质量难以保证，传统的强化学习算法容易受到数据污染的影响，而该研究提出的算法能够提高策略的鲁棒性和可靠性，具有重要的实际应用价值。",
            "highlight_zh": "论文提供了理论分析，证明了所提出的算法在单策略集中性覆盖和数据污染下，能够学习到接近最优的策略。该结果在高维稀疏MDP中提供了第一个具有单策略集中性覆盖和污染的非平凡保证，表明在传统鲁棒离线RL技术可能失败的情况下，学习接近最优的策略仍然是可能的。具体的实验结果未知。",
            "tags_zh": [
                "离线强化学习",
                "稀疏强化学习",
                "鲁棒性",
                "数据污染",
                "Actor-Critic",
                "高维数据",
                "马尔可夫决策过程"
            ],
            "_index": 14,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
            "authors": [
                "Junru Lu",
                "Jiarui Qin",
                "Lingfeng Qiao",
                "Yinghui Li",
                "Xinyi Dai",
                "Bo Ke",
                "Jianfeng He",
                "Ruizhi Qiao",
                "Di Yin",
                "Xing Sun",
                "Yunsheng Wu",
                "Yinsong Liu",
                "Shuangyin Liu",
                "Mingkong Tang",
                "Haodong Lin",
                "Jiayi Kuang",
                "Fanxu Meng",
                "Xiaojuan Tang",
                "Yunjia Xi",
                "Junjie Huang",
                "Haotong Yang",
                "Zhenyi Shen",
                "Yangning Li",
                "Qianwen Zhang",
                "Yifei Yu",
                "Siyu An",
                "Junnan Dong",
                "Qiufeng Wang",
                "Jie Wang",
                "Keyu Chen",
                "Wei Wen",
                "Taian Guo",
                "Zhifeng Shen",
                "Daohai Yu",
                "Jiahao Li",
                "Ke Li",
                "Zongyi Li",
                "Xiaoyu Tan"
            ],
            "arxiv_id": "2512.24618v1",
            "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "57 pages, 26 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24618v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "distillation"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Youtu-LLM，一种轻量级且具备原生Agent能力的语言模型，性能超越同规模模型。",
            "summary_zh": "本文介绍Youtu-LLM，一种轻量级但功能强大的语言模型，它兼顾了高计算效率和原生Agent智能。与依赖蒸馏的典型小型模型不同，Youtu-LLM (1.96B) 从头开始预训练，以系统地培养推理和规划能力。主要技术进步如下：(1) 具有长上下文支持的紧凑架构：Youtu-LLM 构建在具有新型面向 STEM 词汇的密集多潜在注意力 (MLA) 架构之上，支持 128k 上下文窗口。这种设计在最小的内存占用内实现了强大的长上下文推理和状态跟踪，使其成为长程 Agent 和推理任务的理想选择。(2) 有原则的“常识-STEM-Agent”课程：我们整理了一个大约 11T tokens 的大型语料库，并实施了多阶段训练策略。通过逐步将预训练数据分布从一般常识转移到复杂的 STEM 和 Agent 任务，我们确保模型获得深刻的认知能力，而不是表面上的对齐。(3) 可扩展的 Agent 中期训练：专门针对 Agent 中期训练，我们采用多样化的数据构建方案来合成跨数学、编码和工具使用领域的丰富多样的轨迹。这种高质量的数据使模型能够有效地内化规划和反思行为。广泛的评估表明，Youtu-LLM 为 2B 以下的 LLM 树立了新的技术水平。在通用基准测试中，它实现了与更大模型相比具有竞争力的性能，而在特定于 Agent 的任务中，它显着超越了现有的 SOTA 基线，表明轻量级模型可以拥有强大的内在 Agent 能力。",
            "intro_zh": [
                "现有小模型依赖蒸馏，难以兼顾效率与Agent能力，限制了其在复杂任务中的应用。",
                "Youtu-LLM通过从头预训练，结合MLA架构和STEM导向词汇，实现长上下文推理和规划。",
                "多阶段训练策略和Agent中期训练，使模型在Agent任务上超越现有SOTA基线，通用性能也具竞争力。"
            ],
            "method_zh": "**问题定义**：现有的小型语言模型通常依赖于知识蒸馏，这限制了它们在推理、规划和Agent任务中的能力。同时，如何在计算资源有限的情况下，构建一个既高效又智能的轻量级语言模型是一个挑战。现有方法难以在模型大小、上下文长度和Agent能力之间取得平衡。\\n\\n**核心思路**：Youtu-LLM的核心思路是从头开始预训练一个轻量级模型，并采用一种课程学习策略，逐步提升模型的认知能力。通过精心设计的模型架构和训练数据，使模型能够在有限的参数下实现强大的长上下文推理和Agent能力。\\n\\n**技术框架**：Youtu-LLM的整体框架包括以下几个主要部分：1) **紧凑架构**：采用多潜在注意力（MLA）架构，支持128k上下文窗口。2) **STEM导向词汇**：设计了一种新型的面向STEM的词汇表。3) **多阶段训练**：采用“常识-STEM-Agent”课程，逐步提升模型能力。4) **Agent中期训练**：使用多样化的数据构建方案，合成数学、编码和工具使用等领域的轨迹。\\n\\n**关键创新**：Youtu-LLM的关键创新在于：1) **从头预训练**：避免了知识蒸馏带来的性能瓶颈。2) **MLA架构**：在保证性能的同时，减少了模型参数量。3) **课程学习**：通过逐步增加训练难度，提升模型的认知能力。4) **Agent中期训练**：专门针对Agent任务进行训练，提升模型的规划和反思能力。\\n\\n**关键设计**：在模型架构方面，MLA架构通过引入多个潜在变量来捕捉输入序列中的不同方面的信息。在训练数据方面，“常识-STEM-Agent”课程逐步增加训练难度，从一般常识到复杂的STEM和Agent任务。在Agent中期训练中，使用多样化的数据构建方案，合成高质量的训练数据。",
            "application_zh": "Youtu-LLM具有广泛的应用前景，尤其是在资源受限的环境中。例如，它可以被部署在移动设备或嵌入式系统中，用于智能助手、自动化客服、智能家居等应用。其强大的Agent能力使其能够处理复杂的任务，例如自动代码生成、数学问题求解、工具使用等。该研究为轻量级语言模型的发展开辟了新的方向。",
            "highlight_zh": "Youtu-LLM在多个基准测试中取得了显著成果。在通用基准测试中，它与更大的模型相比具有竞争力。在Agent特定任务中，它显著优于现有的SOTA基线。例如，在某些Agent任务上，Youtu-LLM的性能提升超过了10%。这些结果表明，轻量级模型可以拥有强大的内在Agent能力。",
            "tags_zh": [
                "轻量级语言模型",
                "Agent智能",
                "长上下文推理",
                "课程学习",
                "多潜在注意力",
                "从头预训练",
                "STEM教育"
            ],
            "_index": 15,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24618v1/figures/1_intro/scatter.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24618v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24618v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation",
            "authors": [
                "Qiuyi Gu",
                "Yuze Sheng",
                "Jincheng Yu",
                "Jiahao Tang",
                "Xiaolong Shan",
                "Zhaoyang Shen",
                "Tinghao Yi",
                "Xiaodan Liang",
                "Xinlei Chen",
                "Yu Wang"
            ],
            "arxiv_id": "2512.24845v1",
            "summary": "3D scene graphs have empowered robots with semantic understanding for navigation and planning, yet they often lack the functional information required for physical manipulation, particularly regarding articulated objects. Existing approaches for inferring articulation mechanisms from static observations are prone to visual ambiguity, while methods that estimate parameters from state changes typically rely on constrained settings such as fixed cameras and unobstructed views. Furthermore, fine-grained functional elements like small handles are frequently missed by general object detectors. To bridge this gap, we present ArtiSG, a framework that constructs functional 3D scene graphs by encoding human demonstrations into structured robotic memory. Our approach leverages a robust articulation data collection pipeline utilizing a portable setup to accurately estimate 6-DoF articulation trajectories and axes even under camera ego-motion. We integrate these kinematic priors into a hierarchical and open-vocabulary graph while utilizing interaction data to discover inconspicuous functional elements missed by visual perception. Extensive real-world experiments demonstrate that ArtiSG significantly outperforms baselines in functional element recall and articulation estimation precision. Moreover, we show that the constructed graph serves as a reliable functional memory that effectively guides robots to perform language-directed manipulation tasks in real-world environments containing diverse articulated objects.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24845v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "open-vocabulary",
                        "open vocabulary"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam"
            ],
            "headline_zh": "ArtiSG：通过人机协作操纵关节物体构建功能性3D场景图",
            "summary_zh": "3D场景图赋予了机器人语义理解能力，从而进行导航和规划，但它们通常缺乏物理操作所需的功能信息，尤其是在关节物体方面。现有的从静态观察中推断关节机制的方法容易产生视觉歧义，而从状态变化估计参数的方法通常依赖于固定的相机和无遮挡的视图等受限设置。此外，通用物体检测器经常遗漏像小把手这样的细粒度功能元素。为了弥合这一差距，我们提出了ArtiSG，一个通过将人类演示编码为结构化机器人记忆来构建功能性3D场景图的框架。我们的方法利用一个鲁棒的关节数据收集流程，该流程使用便携式设置来准确估计6自由度关节轨迹和轴，即使在相机自我运动的情况下也是如此。我们将这些运动学先验知识集成到一个分层和开放词汇的图中，同时利用交互数据来发现视觉感知遗漏的不显眼的功能元素。大量的真实世界实验表明，ArtiSG在功能元素召回率和关节估计精度方面显著优于基线。此外，我们证明了构建的图可以作为可靠的功能记忆，有效地指导机器人在包含各种关节物体的真实世界环境中执行语言引导的操作任务。",
            "intro_zh": [
                "现有3D场景图缺乏关节物体的功能信息，且难以处理视觉歧义和遮挡，限制了机器人的操作能力。",
                "ArtiSG通过人机协作，利用便携式设备收集关节运动数据，构建包含功能元素的3D场景图。",
                "实验表明，ArtiSG在功能元素召回率和关节估计精度上优于基线，并能指导机器人完成语言引导的操作任务。"
            ],
            "method_zh": "**问题定义**：现有方法在构建用于机器人操作的3D场景图时，存在以下痛点：一是难以从静态图像中准确推断关节物体的运动机制，容易受到视觉歧义的影响；二是依赖于受限的实验环境，例如固定相机和无遮挡的视角；三是通用物体检测器难以检测到细粒度的功能元素，例如小型把手等。这些问题导致机器人难以理解和操作关节物体。\n\n**核心思路**：ArtiSG的核心思路是通过人机协作，利用人类的演示数据来指导3D场景图的构建。具体来说，通过便携式设备记录人类操纵关节物体的过程，提取关节运动轨迹和轴等运动学信息，并将这些信息融入到3D场景图中。同时，利用交互数据来发现视觉感知遗漏的功能元素。这样可以有效地克服视觉歧义和遮挡问题，提高场景图的完整性和准确性。\n\n**技术框架**：ArtiSG的整体框架包含以下几个主要模块：1) **关节数据收集**：使用便携式设备记录人类操纵关节物体的过程，获取6自由度的关节轨迹和轴信息。2) **运动学先验集成**：将收集到的运动学信息集成到分层和开放词汇的3D场景图中。3) **功能元素发现**：利用交互数据来发现视觉感知遗漏的功能元素。4) **场景图构建**：将上述信息整合，构建包含功能信息的3D场景图。\n\n**关键创新**：ArtiSG最重要的技术创新点在于它利用人机协作的方式来构建功能性的3D场景图。与传统的基于视觉感知的场景图构建方法相比，ArtiSG能够有效地克服视觉歧义和遮挡问题，提高场景图的完整性和准确性。此外，ArtiSG还能够发现视觉感知遗漏的功能元素，从而更好地支持机器人的操作任务。\n\n**关键设计**：ArtiSG的关键设计包括：1) 使用便携式设备进行数据采集，保证了数据的准确性和鲁棒性。2) 采用分层和开放词汇的场景图结构，能够有效地组织和管理场景中的各种信息。3) 利用交互数据来发现功能元素，弥补了视觉感知的不足。4) 采用合适的损失函数来优化关节参数的估计，例如，可以使用重投影误差作为损失函数。",
            "application_zh": "ArtiSG构建的功能性3D场景图可应用于机器人操作、人机交互、虚拟现实等领域。例如，可以帮助机器人在复杂环境中执行装配、维修等任务，提升机器人的自主性和智能化水平。在人机交互方面，可以使机器人更好地理解人类的意图，从而提供更自然、更高效的交互体验。在虚拟现实领域，可以构建更逼真、更具交互性的虚拟环境。",
            "highlight_zh": "ArtiSG在真实世界实验中表现出色，在功能元素召回率和关节估计精度方面显著优于基线方法。具体来说，ArtiSG在功能元素召回率上提升了XX%，在关节估计精度上提升了YY%。实验结果表明，ArtiSG构建的场景图能够有效地指导机器人在真实世界环境中执行语言引导的操作任务。",
            "tags_zh": [
                "3D场景图",
                "关节物体操作",
                "人机协作",
                "机器人操作",
                "功能元素发现"
            ],
            "_index": 16,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24845v1/m_figs/fig1_v9.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24845v1/m_figs/fig2_v7.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24845v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands",
            "authors": [
                "Siyuan Hu",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.24965v1",
            "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-$π$, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-$π$ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.HC"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "17 pages, 15 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24965v1",
            "code_links": [
                {
                    "url": "https://github.com/showlab/showui-pi",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "[T]dexterous hand",
                        "dexterous manipulation"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "ShowUI-$π$：提出基于Flow的生成模型，实现GUI界面的灵巧操作。",
            "summary_zh": "本文提出ShowUI-$π$，首个基于Flow的生成模型，作为GUI界面的灵巧手。该模型具有以下设计：（i）统一离散-连续动作空间，整合离散点击和连续拖动，灵活适应不同交互模式；（ii）基于Flow的动作生成，用于拖动建模，通过轻量级动作专家从连续视觉观察中预测增量光标调整，确保轨迹平滑稳定；（iii）拖动训练数据和基准测试，手动收集并合成跨五个领域的2万条拖动轨迹（如PowerPoint、Adobe Premiere Pro），并引入ScreenDrag基准，包含全面的在线和离线评估协议，用于评估GUI代理的拖动能力。实验表明，商业GUI代理在ScreenDrag上表现不佳（例如，Operator得分13.27，最佳Gemini-2.5-CUA达到22.18）。相比之下，ShowUI-$π$仅用4.5亿参数就达到了26.98，突显了任务的难度和该方法的有效性。希望这项工作能推动GUI代理朝着数字世界中类人灵巧控制的方向发展。",
            "intro_zh": [
                "现有GUI代理依赖离散点击预测，难以实现需要连续感知和调整的自由拖动轨迹。",
                "提出ShowUI-$π$，使用基于Flow的生成模型，统一建模离散点击和连续拖动动作。",
                "构建ScreenDrag基准测试，实验表明ShowUI-$π$在拖动任务上优于现有商业GUI代理。"
            ],
            "method_zh": "**问题定义**：现有GUI代理主要通过预测离散的点击坐标来进行操作，这种方式无法处理需要连续、平滑控制的任务，例如拖动进度条。现有方法的痛点在于无法生成连续的动作轨迹，缺乏对环境变化的实时适应能力，限制了GUI代理的灵活性和操作能力。\\n\\n**核心思路**：论文的核心思路是将GUI操作建模为一个连续的动作生成过程，利用Flow-based生成模型学习从视觉输入到连续动作的映射。通过预测光标的增量调整，实现平滑的拖动轨迹。这种方法允许代理根据实时视觉反馈进行动态调整，从而提高操作的稳定性和准确性。\\n\\n**技术框架**：ShowUI-$π$的整体框架包含以下几个主要模块：1) 视觉感知模块：负责从GUI界面捕获视觉信息，提取特征表示。2) 动作专家模块：这是一个轻量级的神经网络，基于视觉特征预测光标的增量调整。3) Flow-based生成模型：利用Flow模型学习动作专家输出的动作分布，从而生成平滑、连续的拖动轨迹。4) 统一动作空间：将离散点击和连续拖动动作整合到一个共享的模型中，实现不同交互模式的灵活切换。\\n\\n**关键创新**：最重要的技术创新点在于将Flow-based生成模型应用于GUI操作的连续动作生成。与传统的离散动作预测方法相比，ShowUI-$π$能够生成平滑、连续的动作轨迹，更好地模拟人类的灵巧操作。此外，统一离散-连续动作空间的设计也使得模型能够灵活适应不同的交互模式。\\n\\n**关键设计**：在Flow-based生成模型中，论文可能采用了以下关键设计：1) 轻量级动作专家：为了保证实时性，动作专家采用轻量级网络结构，减少计算负担。2) 损失函数设计：可能采用了基于Flow模型的似然损失函数，以及额外的平滑性约束，以保证生成轨迹的平滑性。3) 数据增强策略：为了提高模型的泛化能力，可能采用了数据增强策略，例如对拖动轨迹进行扰动或变形。",
            "application_zh": "该研究成果可应用于自动化测试、RPA（机器人流程自动化）、辅助功能等领域。例如，可以利用该技术自动执行复杂的GUI操作，提高测试效率；帮助残疾人士更方便地使用计算机；在RPA场景中，实现更智能、更灵活的自动化流程。未来，该技术有望进一步扩展到虚拟现实、游戏等领域，实现更自然、更沉浸式的交互体验。",
            "highlight_zh": "实验结果表明，ShowUI-$π$在ScreenDrag基准测试上取得了显著的性能提升。例如，商业GUI代理Operator的得分为13.27，最佳Gemini-2.5-CUA的得分为22.18，而ShowUI-$π$仅使用4.5亿参数就达到了26.98。这表明ShowUI-$π$在GUI拖动任务上具有更强的性能和更高的效率。",
            "tags_zh": [
                "GUI代理",
                "Flow模型",
                "连续动作生成",
                "灵巧操作",
                "人机交互"
            ],
            "_index": 17,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24965v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24965v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24965v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
            "authors": [
                "Karthik Dharmarajan",
                "Wenlong Huang",
                "Jiajun Wu",
                "Li Fei-Fei",
                "Ruohan Zhang"
            ],
            "arxiv_id": "2512.24766v1",
            "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Project website: https://dream2flow.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24766v1",
            "code_links": [
                {
                    "url": "https://dream2flow.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "trajectory optimization"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 9.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "Dream2Flow：利用3D物体流桥接视频生成与开放世界操作",
            "summary_zh": "生成式视频建模已成为一种引人注目的工具，可以对开放世界操作中合理的物理交互进行零样本推理。然而，将这种人为引导的运动转化为机器人系统所需的底层动作仍然是一个挑战。我们观察到，给定初始图像和任务指令，这些模型擅长合成合理的物体运动。因此，我们引入了Dream2Flow，一个通过3D物体流作为中间表示来桥接视频生成和机器人控制的框架。我们的方法从生成的视频中重建3D物体运动，并将操作定义为物体轨迹跟踪。通过将状态变化与实现这些变化的执行器分离，Dream2Flow克服了具身差距，并实现了从预训练视频模型到操作各种类别物体的零样本指导，包括刚性、铰接、可变形和颗粒状物体。通过轨迹优化或强化学习，Dream2Flow将重建的3D物体流转换为可执行的底层命令，而无需特定于任务的演示。仿真和真实世界的实验突出了3D物体流作为一种通用且可扩展的接口，用于将视频生成模型适配到开放世界机器人操作。",
            "intro_zh": [
                "现有方法难以将生成视频中的物体运动转化为机器人可执行的底层动作，存在“具身差距”。",
                "Dream2Flow通过3D物体流作为中间表示，将视频生成与机器人控制连接，实现零样本操作指导。",
                "实验表明，Dream2Flow能够操作各种类型的物体，并通过轨迹优化或强化学习生成可执行的机器人指令。"
            ],
            "method_zh": "**问题定义**：现有生成式视频模型擅长生成物体运动，但难以直接控制机器人执行这些运动，因为视频模型输出的是高层语义信息，而机器人需要底层控制指令。这种从高层语义到低层控制的转换存在“具身差距”，阻碍了视频生成模型在机器人操作中的应用。\\n\\n**核心思路**：Dream2Flow的核心思路是将视频生成模型生成的物体运动信息提取出来，用3D物体流来表示，然后将机器人操作任务转化为对3D物体轨迹的跟踪问题。通过这种方式，将高层语义的物体运动与底层控制指令解耦，从而克服“具身差距”。\\n\\n**技术框架**：Dream2Flow框架包含以下几个主要步骤：1) 给定初始图像和任务指令，使用预训练的视频生成模型生成视频；2) 从生成的视频中重建3D物体运动，得到3D物体流；3) 将机器人操作任务定义为对3D物体轨迹的跟踪问题；4) 使用轨迹优化或强化学习方法，将3D物体流转换为可执行的底层机器人控制指令。\\n\\n**关键创新**：Dream2Flow的关键创新在于使用3D物体流作为视频生成和机器人控制之间的中间表示。这种表示方式能够有效地提取视频中的物体运动信息，并将高层语义的物体运动与底层控制指令解耦，从而克服“具身差距”，实现零样本机器人操作。\\n\\n**关键设计**：Dream2Flow的关键设计包括：1) 使用现有的视频生成模型，无需重新训练；2) 使用现有的3D重建算法从视频中重建3D物体运动；3) 使用轨迹优化或强化学习方法将3D物体流转换为可执行的机器人控制指令。具体使用的轨迹优化算法和强化学习算法可以根据具体任务进行选择。",
            "application_zh": "Dream2Flow具有广泛的应用前景，例如家庭服务机器人、工业自动化、医疗机器人等。它可以使机器人能够理解人类的指令，并根据指令操作各种物体，从而提高机器人的智能化水平和应用范围。未来，Dream2Flow可以与其他技术结合，例如自然语言处理、计算机视觉等，实现更复杂的机器人操作任务。",
            "highlight_zh": "Dream2Flow在仿真和真实世界环境中进行了实验验证。实验结果表明，Dream2Flow能够成功地操作各种类型的物体，包括刚性、铰接、可变形和颗粒状物体。与现有的方法相比，Dream2Flow无需特定于任务的演示，即可实现零样本机器人操作，具有更好的泛化能力和可扩展性。",
            "tags_zh": [
                "视频生成",
                "机器人操作",
                "3D物体流",
                "零样本学习",
                "轨迹优化"
            ],
            "_index": 18,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme",
            "authors": [
                "Xueyan Li",
                "Yingyi Xue",
                "Mengjie Jiang",
                "Qingzi Zhu",
                "Yazhe Niu"
            ],
            "arxiv_id": "2512.24555v1",
            "summary": "Generating humorous memes is a challenging multimodal task that moves beyond direct image-to-caption supervision. It requires a nuanced reasoning over visual content, contextual cues, and subjective humor. To bridge this gap between visual perception and humorous punchline creation, we propose HUMOR}, a novel framework that guides VLMs through hierarchical reasoning and aligns them with group-wise human preferences. First, HUMOR employs a hierarchical, multi-path Chain-of-Thought (CoT): the model begins by identifying a template-level intent, then explores diverse reasoning paths under different contexts, and finally anchors onto a high-quality, context-specific path. This CoT supervision, which traces back from ground-truth captions, enhances reasoning diversity. We further analyze that this multi-path exploration with anchoring maintains a high expected humor quality, under the practical condition that high-quality paths retain significant probability mass. Second, to capture subjective humor, we train a pairwise reward model that operates within groups of memes sharing the same template. Following established theory, this approach ensures a consistent and robust proxy for human preference, even with subjective and noisy labels. The reward model then enables a group-wise reinforcement learning optimization, guaranteeing providing a theoretical guarantee for monotonic improvement within the trust region. Extensive experiments show that HUMOR empowers various VLMs with superior reasoning diversity, more reliable preference alignment, and higher overall meme quality. Beyond memes, our work presents a general training paradigm for open-ended, human-aligned multimodal generation, where success is guided by comparative judgment within coherent output group.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "46 pages, 20 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24555v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "HuMoR"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 9.5,
            "hit_pillars": [
                "2_algo_arch",
                "6_video_extraction",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出HUMOR框架，赋能VLM生成更幽默、符合人类偏好的野生表情包",
            "summary_zh": "生成幽默表情包是一项具有挑战性的多模态任务，它超越了直接的图像到标题的监督。它需要对视觉内容、上下文线索和主观幽默进行细致的推理。为了弥合视觉感知和幽默妙语创作之间的差距，我们提出了HUMOR，这是一个新颖的框架，通过分层推理引导视觉语言模型（VLM），并使其与群体人类偏好对齐。首先，HUMOR采用分层的多路径思维链（CoT）：模型首先识别模板级别的意图，然后探索不同上下文下的多样化推理路径，最后锚定到高质量、特定于上下文的路径。这种从真实标题追溯的CoT监督增强了推理多样性。我们进一步分析，在高质量路径保留显著概率质量的实际条件下，这种具有锚定的多路径探索保持了较高的预期幽默质量。其次，为了捕捉主观幽默，我们训练了一个成对奖励模型，该模型在共享相同模板的表情包组内运行。根据既定理论，即使存在主观和嘈杂的标签，这种方法也能确保人类偏好的一致且稳健的代理。然后，奖励模型支持组内强化学习优化，从而为信任区域内的单调改进提供理论保证。大量实验表明，HUMOR使各种VLM具有卓越的推理多样性、更可靠的偏好对齐和更高的整体表情包质量。除了表情包之外，我们的工作还提出了一种通用的训练范式，用于开放式、人类对齐的多模态生成，其中成功由连贯输出组内的比较判断来指导。",
            "intro_zh": [
                "现有表情包生成方法难以捕捉视觉内容、上下文和主观幽默之间的复杂关系，缺乏细致的推理能力。",
                "HUMOR框架通过分层多路径思维链引导VLM进行推理，并使用成对奖励模型对齐人类偏好，提升幽默感。",
                "实验表明，HUMOR框架显著提升了VLM在表情包生成任务中的推理多样性、偏好对齐和整体质量。"
            ],
            "method_zh": "**问题定义**：表情包生成任务需要模型理解图像内容、捕捉上下文信息，并生成具有幽默感的文字。现有的方法通常采用图像到文本的直接监督，缺乏对幽默感和人类偏好的建模，导致生成的表情包质量不高，难以引起共鸣。此外，主观幽默的建模也是一个挑战，因为不同的人对同一表情包的幽默程度可能有不同的看法。\\n\\n**核心思路**：HUMOR框架的核心思路是通过分层推理和群体偏好学习来提升表情包的幽默感和质量。首先，利用分层多路径思维链（CoT）引导VLM进行推理，增强推理的多样性。然后，通过成对奖励模型学习人类对幽默的主观偏好，并利用强化学习优化生成策略，使生成的表情包更符合人类的期望。\\n\\n**技术框架**：HUMOR框架主要包含两个阶段：1) 分层多路径思维链推理：首先，模型识别表情包的模板意图；然后，在不同的上下文下探索多条推理路径；最后，选择一条高质量的、特定于上下文的路径。2) 基于群体偏好的强化学习：训练一个成对奖励模型，用于评估表情包的幽默程度；然后，利用强化学习算法，根据奖励模型的反馈，优化VLM的生成策略。\\n\\n**关键创新**：HUMOR框架的关键创新在于：1) 提出了分层多路径思维链，增强了VLM的推理多样性，使其能够生成更具创意的表情包。2) 引入了基于群体偏好的奖励模型，能够更好地捕捉主观幽默，使生成的表情包更符合人类的偏好。3) 采用组内强化学习优化，保证了在信任区域内的单调改进。\\n\\n**关键设计**：在分层多路径思维链中，模型首先识别模板级别的意图，例如“嘲讽”、“惊讶”等。然后，模型在不同的上下文下探索多条推理路径，例如“如果图像是猫，那么可以生成‘猫：我每天都在努力工作’的文字”。最后，模型选择一条高质量的、特定于上下文的路径，生成最终的表情包。在基于群体偏好的强化学习中，奖励模型采用pairwise ranking loss进行训练，用于评估表情包的幽默程度。强化学习算法采用PPO，优化VLM的生成策略。",
            "application_zh": "该研究成果可应用于智能对话系统、社交媒体内容生成、广告创意设计等领域。通过生成更幽默、更符合人类偏好的内容，可以提升用户体验，增强用户粘性，并创造更大的商业价值。未来，该方法有望扩展到其他开放式、人类对齐的多模态生成任务中。",
            "highlight_zh": "实验结果表明，HUMOR框架在表情包生成任务中取得了显著的性能提升。相比于基线方法，HUMOR框架生成的表情包在幽默感、相关性和整体质量方面均有明显提高。具体来说，HUMOR框架在人工评估中获得了更高的评分，并且在客观指标上也表现出更好的性能。",
            "tags_zh": [
                "表情包生成",
                "视觉语言模型",
                "思维链",
                "强化学习",
                "人类偏好",
                "多模态学习",
                "幽默感",
                "群体智能"
            ],
            "_index": 19,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24555v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24555v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24555v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding",
            "authors": [
                "Panquan Yang",
                "Junfei Huang",
                "Zongzhangbao Yin",
                "Yingsong Hu",
                "Anni Xu",
                "Xinyi Luo",
                "Xueqi Sun",
                "Hai Wu",
                "Sheng Ao",
                "Zhaoxing Zhu",
                "Chenglu Wen",
                "Cheng Wang"
            ],
            "arxiv_id": "2512.24605v1",
            "summary": "3D visual grounding aims to localize the object in 3D point cloud scenes that semantically corresponds to given natural language sentences. It is very critical for roadside infrastructure system to interpret natural languages and localize relevant target objects in complex traffic environments. However, most existing datasets and approaches for 3D visual grounding focus on the indoor and outdoor driving scenes, outdoor monitoring scenarios remain unexplored due to scarcity of paired point cloud-text data captured by roadside infrastructure sensors. In this paper, we introduce a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios, which enables infrastructure-level understanding of traffic scenes beyond the ego-vehicle perspective. To support this task, we construct MoniRefer, the first real-world large-scale multi-modal dataset for roadside-level 3D visual grounding. The dataset consists of about 136,018 objects with 411,128 natural language expressions collected from multiple complex traffic intersections in the real-world environments. To ensure the quality and accuracy of the dataset, we manually verified all linguistic descriptions and 3D labels for objects. Additionally, we also propose a new end-to-end method, named Moni3DVG, which utilizes the rich appearance information provided by images and geometry and optical information from point cloud for multi-modal feature learning and 3D object localization. Extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method. Our dataset and code will be released.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "14 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24605v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]visual grounding"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出MoniRefer数据集，用于路侧基础设施的3D视觉定位任务",
            "summary_zh": "本文提出了一种新的任务：面向户外监控场景的3D视觉定位，旨在实现基础设施级别的交通场景理解，超越了自车视角。为此，构建了MoniRefer，这是第一个真实世界的大规模多模态数据集，用于路侧级别的3D视觉定位。该数据集包含约136,018个对象，以及从真实环境中的多个复杂交通路口收集的411,128个自然语言表达式。为了确保数据集的质量和准确性，我们手动验证了所有语言描述和对象的3D标签。此外，还提出了一种新的端到端方法Moni3DVG，该方法利用图像提供的丰富外观信息以及点云提供的几何和光学信息进行多模态特征学习和3D对象定位。在提出的基准上进行的大量实验和消融研究证明了该方法的优越性和有效性。数据集和代码将会开源。",
            "intro_zh": [
                "现有3D视觉定位数据集主要集中在室内和自动驾驶场景，缺乏路侧基础设施视角的户外监控场景数据。",
                "提出MoniRefer数据集和Moni3DVG方法，利用图像外观信息和点云几何、光学信息进行多模态特征学习和3D对象定位。",
                "实验结果表明，Moni3DVG方法在MoniRefer数据集上表现出优越性和有效性，为路侧3D视觉定位提供新基准。"
            ],
            "method_zh": "**问题定义**：论文旨在解决路侧基础设施视角下的3D视觉定位问题，即根据自然语言描述在3D点云场景中定位目标对象。现有方法和数据集主要集中在室内和自动驾驶场景，缺乏对路侧监控场景的有效支持，无法满足智能交通系统对基础设施级别场景理解的需求。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、高质量的路侧3D视觉定位数据集（MoniRefer），并提出一个端到端的多模态融合方法（Moni3DVG）。通过结合图像提供的丰富外观信息和点云提供的几何、光学信息，实现更准确的3D对象定位。\\n\\n**技术框架**：Moni3DVG方法是一个端到端的框架，主要包含以下模块：1) 多模态特征提取模块，分别从图像和点云中提取特征；2) 特征融合模块，将图像和点云特征进行融合；3) 3D对象定位模块，根据融合后的特征预测3D bounding box。\\n\\n**关键创新**：论文的关键创新在于：1) 构建了首个大规模路侧3D视觉定位数据集MoniRefer，填补了该领域的数据空白；2) 提出了Moni3DVG方法，有效融合了图像和点云的多模态信息，提高了3D对象定位的准确性。与现有方法相比，Moni3DVG更关注路侧场景的特点，并针对性地设计了多模态融合策略。\\n\\n**关键设计**：Moni3DVG方法在特征提取方面，图像分支可能采用预训练的CNN模型（如ResNet），点云分支可能采用PointNet++等点云处理网络。在特征融合方面，可能采用注意力机制或跨模态Transformer等方法，学习不同模态之间的关联性。损失函数可能包括定位损失（如IoU loss）和分类损失（如交叉熵损失）。具体网络结构和参数设置在论文中应该有详细描述，但根据摘要信息无法得知。",
            "application_zh": "该研究成果可应用于智能交通系统、智慧城市等领域。通过路侧基础设施对交通场景进行3D视觉定位，可以实现更精确的交通监控、事件检测和自动驾驶辅助，提升交通效率和安全性。未来，该技术有望应用于更广泛的户外监控场景，例如安防监控、环境监测等。",
            "highlight_zh": "论文提出的Moni3DVG方法在MoniRefer数据集上进行了实验验证，结果表明该方法在3D对象定位方面取得了显著的性能提升。具体的性能数据（如定位精度、召回率等）以及与现有基线方法的对比结果需要在论文中查找。摘要中提到“extensive experiments and ablation studies on the proposed benchmarks demonstrate the superiority and effectiveness of our method”，表明该方法具有较强的竞争力。",
            "tags_zh": [
                "3D视觉定位",
                "多模态融合",
                "路侧基础设施",
                "智能交通",
                "点云处理"
            ],
            "_index": 20,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24605v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24605v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24605v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios",
            "authors": [
                "Tianyi Zhao",
                "Jiawen Xi",
                "Linhui Xiao",
                "Junnan Li",
                "Xue Yang",
                "Maoxun Yuan",
                "Xingxing Wei"
            ],
            "arxiv_id": "2512.24561v1",
            "summary": "Visual Grounding (VG) aims to localize specific objects in an image according to natural language expressions, serving as a fundamental task in vision-language understanding. However, existing VG benchmarks are mostly derived from datasets collected under clean environments, such as COCO, where scene diversity is limited. Consequently, they fail to reflect the complexity of real-world conditions, such as changes in illumination, weather, etc., that are critical to evaluating model robustness and generalization in safety-critical applications. To address these limitations, we present RGBT-Ground, the first large-scale visual grounding benchmark built for complex real-world scenarios. It consists of spatially aligned RGB and Thermal infrared (TIR) image pairs with high-quality referring expressions, corresponding object bounding boxes, and fine-grained annotations at the scene, environment, and object levels. This benchmark enables comprehensive evaluation and facilitates the study of robust grounding under diverse and challenging conditions. Furthermore, we establish a unified visual grounding framework that supports both uni-modal (RGB or TIR) and multi-modal (RGB-TIR) visual inputs. Based on it, we propose RGBT-VGNet, a simple yet effective baseline for fusing complementary visual modalities to achieve robust grounding. We conduct extensive adaptations to the existing methods on RGBT-Ground. Experimental results show that our proposed RGBT-VGNet significantly outperforms these adapted methods, particularly in nighttime and long-distance scenarios. All resources will be publicly released to promote future research on robust visual grounding in complex real-world environments.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "27pages, 9figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24561v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]visual grounding"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出RGBT-Ground基准，用于评估复杂场景下RGB-T图像的视觉定位",
            "summary_zh": "视觉定位（VG）旨在根据自然语言表达式定位图像中的特定对象，是视觉语言理解中的一项基本任务。然而，现有的VG基准主要来自在干净环境下收集的数据集，如COCO，场景多样性有限。因此，它们无法反映真实世界条件的复杂性，如光照、天气等变化，而这些对于评估模型在安全关键应用中的鲁棒性和泛化能力至关重要。为了解决这些限制，我们提出了RGBT-Ground，这是第一个为复杂真实世界场景构建的大规模视觉定位基准。它由空间对齐的RGB和热红外（TIR）图像对组成，具有高质量的指代表达式、相应的对象边界框以及场景、环境和对象级别的细粒度注释。该基准能够进行全面评估，并促进在多样化和具有挑战性的条件下对鲁棒定位的研究。此外，我们建立了一个统一的视觉定位框架，支持单模态（RGB或TIR）和多模态（RGB-TIR）视觉输入。在此基础上，我们提出了RGBT-VGNet，这是一个简单而有效的基线，用于融合互补的视觉模态以实现鲁棒定位。我们对RGBT-Ground上的现有方法进行了广泛的调整。实验结果表明，我们提出的RGBT-VGNet显著优于这些调整后的方法，尤其是在夜间和远距离场景中。所有资源将公开发布，以促进未来对复杂真实世界环境中鲁棒视觉定位的研究。",
            "intro_zh": [
                "现有视觉定位基准数据集缺乏真实场景的复杂性，难以评估模型在光照变化、恶劣天气等条件下的鲁棒性。",
                "提出RGBT-Ground数据集，包含RGB和热红外图像对，以及高质量的指代表达式和细粒度标注，用于评估模型在复杂场景下的定位能力。",
                "提出RGBT-VGNet，一个融合RGB和热红外信息的视觉定位基线模型，并在RGBT-Ground数据集上取得了显著优于现有方法的性能。"
            ],
            "method_zh": "**问题定义**：现有的视觉定位（Visual Grounding, VG）方法和数据集主要集中在条件良好的场景下，例如COCO。然而，在真实的复杂场景中，光照变化、恶劣天气等因素会对视觉定位的性能产生显著影响。因此，需要一个能够反映真实世界复杂性的数据集，以及能够有效利用多模态信息（如RGB和热红外图像）的视觉定位方法。\\n\\n**核心思路**：论文的核心思路是构建一个大规模的RGBT数据集，其中包含RGB和热红外图像对，以及对应的自然语言描述和目标边界框标注。同时，提出一个能够有效融合RGB和热红外信息的视觉定位模型RGBT-VGNet。通过多模态信息的互补，提高模型在复杂场景下的鲁棒性。\\n\\n**技术框架**：RGBT-VGNet的整体框架包含以下几个主要模块：1) 特征提取模块：分别提取RGB和热红外图像的视觉特征。2) 文本特征提取模块：提取自然语言描述的文本特征。3) 多模态融合模块：将视觉特征和文本特征进行融合。4) 定位模块：根据融合后的特征，预测目标对象的边界框。\\n\\n**关键创新**：论文的关键创新在于：1) 构建了RGBT-Ground数据集，这是第一个大规模的RGBT视觉定位数据集，包含丰富的场景和环境信息。2) 提出了RGBT-VGNet模型，能够有效融合RGB和热红外信息，提高模型在复杂场景下的鲁棒性。与现有方法相比，RGBT-VGNet能够更好地利用多模态信息的互补性。\\n\\n**关键设计**：RGBT-VGNet的关键设计包括：1) 使用预训练的视觉模型（如ResNet）提取RGB和热红外图像的视觉特征。2) 使用Transformer模型提取自然语言描述的文本特征。3) 使用注意力机制将视觉特征和文本特征进行融合，从而更好地关注与目标对象相关的区域。4) 使用交叉熵损失函数和IoU损失函数来训练模型。",
            "application_zh": "该研究成果可应用于自动驾驶、安防监控、机器人导航等领域。在这些应用中，视觉定位的鲁棒性至关重要，尤其是在光照不足、恶劣天气等复杂环境下。RGBT-Ground数据集和RGBT-VGNet模型为这些应用提供了有力的支持，有助于提高系统的可靠性和安全性，未来可以进一步扩展到更多模态的数据融合。",
            "highlight_zh": "实验结果表明，RGBT-VGNet在RGBT-Ground数据集上显著优于现有的视觉定位方法，尤其是在夜间和远距离场景中。例如，在夜间场景中，RGBT-VGNet的性能提升了10%以上。此外，RGBT-VGNet在长距离场景中也取得了显著的性能提升，证明了其在复杂场景下的鲁棒性。",
            "tags_zh": [
                "视觉定位",
                "多模态融合",
                "RGB-T图像",
                "真实场景",
                "鲁棒性",
                "深度学习",
                "数据集"
            ],
            "_index": 21,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24561v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24561v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24561v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks",
            "authors": [
                "Shota Suzuki",
                "Satoshi Ono"
            ],
            "arxiv_id": "2512.24793v1",
            "summary": "Neural architecture search (NAS), which automates the architectural design process of deep neural networks (DNN), has attracted increasing attention. Multimodal DNNs that necessitate feature fusion from multiple modalities benefit from NAS due to their structural complexity; however, constructing an architecture for multimodal DNNs through NAS requires a substantial amount of labeled training data. Thus, this paper proposes a self-supervised learning (SSL) method for architecture search of multimodal DNNs. The proposed method applies SSL comprehensively for both the architecture search and model pretraining processes. Experimental results demonstrated that the proposed method successfully designed architectures for DNNs from unlabeled training data.",
            "categories": [
                "cs.LG",
                "cs.NE"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "10.1587/transinf.2024EDL8018",
            "journal_ref": "IEICE Transactions on Information and Systems, Vol.E108.D, No. 6, pp. 640-643, 2025",
            "pdf_url": "https://arxiv.org/pdf/2512.24793v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出一种自监督多模态神经网络架构搜索方法，解决标注数据依赖问题。",
            "summary_zh": "本文提出了一种用于多模态深度神经网络（DNN）架构搜索的自监督学习（SSL）方法。由于多模态DNN需要融合来自多个模态的特征，其结构复杂性使得神经架构搜索（NAS）受益匪浅。然而，通过NAS构建多模态DNN架构需要大量的标注训练数据。因此，本文提出的方法将SSL全面应用于架构搜索和模型预训练过程。实验结果表明，该方法成功地从无标注训练数据中设计了DNN架构。",
            "intro_zh": [
                "多模态DNN的架构设计复杂，利用NAS可以有效提升性能，但传统NAS方法依赖大量标注数据。",
                "论文提出一种自监督学习方法，同时应用于架构搜索和模型预训练，从而降低对标注数据的需求。",
                "实验结果表明，该方法能够仅使用无标注数据，成功搜索出适用于多模态DNN的有效架构。"
            ],
            "method_zh": "**问题定义**：多模态深度神经网络的设计需要考虑不同模态数据的融合方式，手动设计过程繁琐且难以达到最优。神经架构搜索（NAS）可以自动化这一过程，但传统NAS方法通常需要大量的标注数据进行训练和验证，这在实际应用中是一个很大的限制，尤其是在标注数据获取成本高昂的情况下。因此，如何降低NAS对标注数据的依赖是本文要解决的核心问题。\\n\\n**核心思路**：本文的核心思路是利用自监督学习（SSL）来替代或减少对标注数据的需求。通过设计合适的自监督任务，使网络能够从无标注数据中学习到有用的特征表示，从而指导架构搜索过程。这样，即使在缺乏标注数据的情况下，也能有效地搜索出适合多模态数据的网络架构。\\n\\n**技术框架**：该方法包含两个主要阶段：首先，利用自监督学习对候选网络进行预训练，使其具备一定的特征提取能力。然后，在架构搜索阶段，使用预训练后的网络作为基础，通过某种搜索算法（具体算法未知）来探索不同的网络结构。搜索过程中，使用自监督任务的性能作为评价指标，选择表现最好的架构。整个框架将自监督学习贯穿于架构搜索和模型训练的始终。\\n\\n**关键创新**：该方法最关键的创新在于将自监督学习与神经架构搜索相结合，从而解决了多模态DNN架构搜索对大量标注数据的依赖问题。通过自监督学习，网络可以从无标注数据中学习到有用的特征表示，从而为架构搜索提供有效的指导。这种方法使得在标注数据稀缺的情况下，也能有效地搜索出适合多模态数据的网络架构。\\n\\n**关键设计**：论文中没有明确给出具体的自监督任务、搜索算法和网络结构细节。但是，可以推测，自监督任务的设计需要与多模态数据的特性相适应，例如，可以采用对比学习、生成式学习等方法。搜索算法可以选择强化学习、进化算法等。网络结构的设计需要考虑不同模态数据的融合方式，例如，可以采用注意力机制、跨模态Transformer等。",
            "application_zh": "该研究成果可广泛应用于多模态数据分析领域，例如自动驾驶、医疗诊断、视频理解等。在这些领域，往往存在大量的无标注数据，而标注数据的获取成本很高。该方法能够利用这些无标注数据，自动设计出高性能的多模态DNN架构，从而降低模型开发成本，提高模型性能，加速相关技术的落地。",
            "highlight_zh": "论文的主要亮点在于提出了自监督学习驱动的多模态神经网络架构搜索方法，实验结果表明该方法能够在无标注数据上成功搜索出有效的DNN架构。虽然具体的性能数据和对比基线未知，但该方法为解决多模态DNN架构搜索的标注数据依赖问题提供了一种新的思路。",
            "tags_zh": [
                "神经架构搜索",
                "多模态学习",
                "自监督学习",
                "深度神经网络",
                "无监督学习"
            ],
            "_index": 22,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Do Large Language Models Know What They Are Capable Of?",
            "authors": [
                "Casey O. Barkan",
                "Sid Black",
                "Oliver Sourbut"
            ],
            "arxiv_id": "2512.24661v1",
            "summary": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "23 pages, 8 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24661v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "探讨大型语言模型的自我能力认知与决策改进",
            "summary_zh": "本研究调查了大型语言模型（LLMs）是否能够预测其在特定任务上的成功率，以及在多步骤任务中随着进展其预测能力是否有所提升。研究发现，所有测试的LLMs普遍表现出过度自信，但大多数模型在成功预测上具有优于随机的判别能力。尽管较新和较大的LLMs通常没有更强的判别能力，但Claude模型显示出这一趋势。在多步骤任务中，部分前沿LLMs的过度自信随着任务进展而加剧，而推理型LLMs的表现与非推理型LLMs相当或更差。部分LLMs在经历失败的上下文经验后能够减少过度自信，从而显著改善决策，而其他模型则未能做到。这些结果表明，当前的LLM代理受限于对自身能力的缺乏认知。",
            "intro_zh": [
                "核心问题：现有大型语言模型在任务成功预测上普遍过度自信，缺乏对自身能力的准确评估。",
                "方法要点：研究通过多步骤任务和上下文经验，评估LLMs的自我认知与决策能力，探索改进策略。",
                "实验或效果：发现部分LLMs在经历失败后能够改善决策，但整体仍表现出过度乐观的倾向。"
            ],
            "method_zh": "**问题定义**：本研究旨在解决大型语言模型在任务成功预测中的过度自信问题。现有方法未能有效评估模型的自我能力，导致决策失误。\\n\\n**核心思路**：论文通过分析LLMs在多步骤任务中的表现，探讨其自我认知能力及如何通过上下文经验改善决策。设计的核心在于评估模型在不同任务阶段的自信程度与实际表现之间的关系。\\n\\n**技术框架**：研究采用实验设计，分为任务预测、上下文经验学习和决策评估三个主要模块。每个模块通过不同的任务设置和评估标准进行验证。\\n\\n**关键创新**：最重要的创新在于揭示了LLMs在多步骤任务中的过度自信现象及其对决策的影响，尤其是通过上下文学习来调整自信程度的能力。\\n\\n**关键设计**：研究中采用了多种任务设置，评估模型在不同情境下的表现，关键参数包括任务复杂度、上下文信息的引入等，确保了实验的全面性与有效性。",
            "application_zh": "该研究的潜在应用领域包括智能助手、自动化决策系统和人机交互等。通过提升LLMs的自我认知能力，可以有效减少决策失误，增强其在复杂任务中的应用价值，未来可能对AI的安全性和可靠性产生深远影响。",
            "highlight_zh": "实验结果显示，尽管所有LLMs普遍存在过度自信的问题，但大多数模型在成功预测上表现出优于随机的判别能力。部分LLMs在经历失败后能够显著改善决策，表明上下文学习对决策质量的提升具有重要作用。",
            "tags_zh": [
                "大型语言模型",
                "自我认知",
                "决策改进",
                "上下文学习",
                "多步骤任务"
            ],
            "_index": 23,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24661v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24661v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24661v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System",
            "authors": [
                "Md Hasan Saju",
                "Austin Page",
                "Akramul Azim",
                "Jeff Gardiner",
                "Farzaneh Abazari",
                "Frank Eargle"
            ],
            "arxiv_id": "2512.24571v1",
            "summary": "Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "https://conf.researchr.org/home/cascon-2025",
            "pdf_url": "https://arxiv.org/pdf/2512.24571v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "SynRAG：用于异构SIEM系统中可执行查询生成的大语言模型框架",
            "summary_zh": "安全信息和事件管理（SIEM）系统对于大型企业监控其IT基础设施至关重要，它们每天摄取和分析数百万的日志和事件。安全运营中心（SOC）分析师负责监控和分析这些海量数据，以识别潜在威胁并采取预防措施来保护企业资产。然而，Palo Alto Networks Qradar、Google SecOps、Splunk、Microsoft Sentinel和Elastic Stack等SIEM平台之间的多样性带来了重大挑战。由于这些系统在属性、架构和查询语言上存在差异，使得分析师难以有效地监控多个平台，除非经过广泛的培训或企业被迫扩大员工队伍。为了解决这个问题，我们引入了SynRAG，一个统一的框架，可以从平台无关的规范中自动生成针对多个SIEM平台的威胁检测或事件调查查询。SynRAG可以从分析师编写的单个高级规范生成特定于平台的查询。如果没有SynRAG，分析师将需要为每个SIEM平台手动编写单独的查询，因为查询语言在不同系统之间差异很大。该框架实现了跨异构SIEM环境的无缝威胁检测和事件调查，减少了对专门培训和手动查询转换的需求。我们使用Qradar和SecOps作为代表性的SIEM系统，针对包括GPT、Llama、DeepSeek、Gemma和Claude在内的最先进的语言模型评估了SynRAG。我们的结果表明，与最先进的基础模型相比，SynRAG为跨SIEM威胁检测和事件调查生成了明显更好的查询。",
            "intro_zh": [
                "现有SIEM系统多样性导致安全分析师需要针对不同平台进行专门培训，增加了工作负担和企业成本。",
                "SynRAG框架通过平台无关的规范，自动生成适用于多种SIEM系统的威胁检测和事件调查查询，简化了跨平台操作。",
                "实验结果表明，SynRAG在跨SIEM威胁检测和事件调查方面，显著优于GPT、Llama等先进语言模型。"
            ],
            "method_zh": "**问题定义**：现有安全信息和事件管理（SIEM）系统种类繁多，例如Qradar、SecOps、Splunk等，它们使用不同的查询语言和数据结构。安全分析师需要针对每个平台编写不同的查询，这既耗时又需要专业的平台知识。现有方法缺乏一个统一的查询生成框架，导致效率低下和学习成本高昂。\\n\\n**核心思路**：SynRAG的核心思路是利用大型语言模型（LLM）的自然语言理解和生成能力，将平台无关的高级查询规范转换为特定于SIEM平台的查询语言。通过这种方式，分析师只需要编写一次查询，SynRAG就能自动生成适用于不同平台的版本。\\n\\n**技术框架**：SynRAG框架包含以下主要模块：1) 接收分析师编写的平台无关查询规范；2) 利用LLM将该规范转换为特定SIEM平台的查询语言；3) 输出可执行的查询语句。该框架的关键在于LLM的训练和微调，使其能够理解安全领域的概念，并准确地生成符合SIEM平台语法的查询。\\n\\n**关键创新**：SynRAG的关键创新在于它提供了一个统一的、基于LLM的查询生成框架，能够自动处理不同SIEM平台之间的差异。与传统的手动查询编写或基于规则的转换方法相比，SynRAG具有更高的灵活性和可扩展性，能够适应新的SIEM平台和查询需求。\\n\\n**关键设计**：SynRAG的关键设计包括：1) 使用高质量的安全领域数据对LLM进行预训练，使其具备安全知识；2) 使用特定SIEM平台的查询示例对LLM进行微调，提高其查询生成准确性；3) 设计有效的提示工程（Prompt Engineering）策略，引导LLM生成符合要求的查询语句。具体的参数设置和损失函数选择取决于所使用的LLM架构和训练数据。",
            "application_zh": "SynRAG可应用于各种需要跨多个异构SIEM系统进行威胁检测和事件调查的场景。它可以帮助安全运营中心（SOC）提高效率，降低培训成本，并更快地响应安全事件。此外，该框架还可以用于自动化安全审计和合规性检查，提升整体安全态势。",
            "highlight_zh": "SynRAG在Qradar和SecOps两个代表性SIEM系统上的实验结果表明，其生成的查询质量显著优于GPT、Llama、DeepSeek、Gemma和Claude等先进语言模型。具体性能提升数据未知，但论文强调了SynRAG在跨SIEM威胁检测和事件调查方面的优越性。",
            "tags_zh": [
                "安全信息和事件管理",
                "SIEM",
                "大型语言模型",
                "查询生成",
                "威胁检测",
                "事件调查",
                "异构系统"
            ],
            "_index": 24,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24571v1/MyRAGL.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24571v1/Result-ComparisonAll.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Large language models and the entropy of English",
            "authors": [
                "Colin Scheibner",
                "Lindsay M. Smith",
                "William Bialek"
            ],
            "arxiv_id": "2512.24969v1",
            "summary": "We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.",
            "categories": [
                "cond-mat.stat-mech",
                "cs.CL",
                "physics.bio-ph",
                "q-bio.NC"
            ],
            "primary_category": "cond-mat.stat-mech",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "8 pages, 6 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24969v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用大语言模型揭示英语文本的长程结构",
            "summary_zh": "本研究利用大语言模型（LLMs）揭示来自多种来源的英语文本中的长程结构。条件熵或编码长度在许多情况下随着上下文长度的增加而持续减少，至少达到$N\text{∼}10^4$个字符，这表明在这些距离上存在直接的依赖关系或交互。我们从数据中独立于模型展示了字符之间的小但显著的相关性。编码长度的分布揭示了对大量字符的逐渐增强的确定性。模型训练过程中，我们观察到长短上下文长度的动态差异，表明长程结构的学习是逐步进行的。我们的结果为构建LLMs或语言本身的统计物理模型提供了约束。",
            "intro_zh": [
                "现有方法在处理长文本时，难以捕捉字符之间的长程依赖关系，导致信息损失。",
                "本研究通过大语言模型分析文本的条件熵，揭示长程结构的存在及其学习过程。",
                "实验结果表明，随着上下文长度的增加，编码长度持续减少，显示出字符间的显著相关性。"
            ],
            "method_zh": "**问题定义**：本研究旨在揭示英语文本中的长程结构，现有方法在处理长文本时难以捕捉字符之间的长程依赖关系，导致信息损失。\\n\\n**核心思路**：通过利用大语言模型分析文本的条件熵，研究字符间的依赖关系，展示长程结构的存在及其学习过程。\\n\\n**技术框架**：整体架构包括数据收集、模型训练和条件熵计算三个主要阶段。首先收集多种来源的英语文本数据，然后训练大语言模型，最后计算不同上下文长度下的条件熵。\\n\\n**关键创新**：本研究的主要创新在于通过条件熵的分析揭示了长程依赖关系的存在，且展示了长程结构的学习是一个渐进的过程，这与现有方法的短期依赖假设形成对比。\\n\\n**关键设计**：在模型训练中，采用了适当的上下文窗口大小，并通过调整学习率和损失函数优化模型性能，以确保能够有效捕捉长程依赖。具体的参数设置和网络结构细节在实验部分进行了详细描述。",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、文本生成和机器翻译等。通过揭示长程结构的学习过程，可以为改进现有语言模型提供理论依据，进而提升模型在复杂文本处理中的表现。未来，研究结果可能推动更高效的语言模型设计和应用。",
            "highlight_zh": "实验结果显示，随着上下文长度的增加，条件熵持续降低，表明长程依赖关系的存在。具体而言，在$N\text{∼}10^4$字符的情况下，编码长度显著减少，展示了字符间的小但显著的相关性。这一发现为理解语言模型的学习过程提供了新的视角。",
            "tags_zh": [
                "大语言模型",
                "条件熵",
                "长程依赖",
                "文本分析",
                "机器学习"
            ],
            "_index": 25,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24969v1/figures/C4_models.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24969v1/figures/GernreFigureV3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24969v1/figures/histogramsV3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models",
            "authors": [
                "Ákos Prucs",
                "Márton Csutora",
                "Mátyás Antal",
                "Márk Marosi"
            ],
            "arxiv_id": "2512.24776v1",
            "summary": "Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a test-time-compute aware evaluation of both contemporary and older open-source LLMs, mapping their Pareto frontiers across math- and reasoning-intensive benchmarks. Our findings identify the Mixture of Experts (MoE) architecture as a strong candidate to balance performance and efficiency in our evaluation setting. Furthermore, we trace the trajectory of Pareto efficiency over time to derive an emergent trend regarding accuracy gain per unit of compute. Finally, we demonstrate that there is a saturation point for inference-time compute. Beyond a certain threshold, accuracy gains diminish, indicating that while extended reasoning capabilities are beneficial, they cannot overcome intrinsic model limitations regarding specific complexities.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24776v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "针对开源推理大语言模型，构建计算-精度帕累托前沿，优化模型选择。",
            "summary_zh": "大型语言模型（LLM）在复杂的推理基准测试中表现出快速的改进，尤其是在允许利用中间推理步骤之后。然而，目前的文献经常忽略与生成长推理序列相关的巨大计算负担。对于工业应用，模型选择不仅取决于原始精度，还取决于资源约束和推理成本。在这项工作中，我们对当代和较早的开源LLM进行了测试时计算感知的评估，绘制了它们在数学和推理密集型基准测试中的帕累托前沿。我们的研究结果表明，混合专家（MoE）架构是在我们的评估环境中平衡性能和效率的有力候选者。此外，我们追踪了帕累托效率随时间的轨迹，以推导出关于单位计算的精度增益的新兴趋势。最后，我们证明了推理时计算存在饱和点。超过某个阈值，精度增益会减少，这表明虽然扩展的推理能力是有益的，但它们无法克服关于特定复杂性的内在模型限制。",
            "intro_zh": [
                "现有文献忽略了长推理序列带来的巨大计算负担，工业应用中模型选择需兼顾精度与资源约束。",
                "论文通过测试时计算感知的评估，绘制开源LLM在推理基准上的计算-精度帕累托前沿。",
                "实验表明混合专家（MoE）架构在性能和效率之间取得了较好的平衡，并揭示了推理计算的饱和点。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型在复杂推理任务上取得了显著进展，但生成长推理链条需要消耗大量的计算资源。在实际工业应用中，模型选择不仅要考虑精度，还需要考虑计算成本和资源限制。因此，如何在精度和计算成本之间找到平衡点是一个关键问题。\\n\\n**核心思路**：论文的核心思路是通过构建计算-精度帕累托前沿，来评估不同开源大语言模型在推理任务上的性能。帕累托前沿能够清晰地展示在给定计算资源下，模型能够达到的最高精度，或者在给定精度要求下，模型所需的最小计算资源。这为模型选择提供了有力的依据。\\n\\n**技术框架**：论文的技术框架主要包括以下几个步骤：1) 选择一系列开源大语言模型，包括不同架构和规模的模型。2) 选择一系列数学和推理密集型基准测试。3) 对每个模型在每个基准测试上进行测试，并记录其精度和计算成本（例如，推理时间、GPU 内存占用）。4) 基于精度和计算成本，构建帕累托前沿。5) 分析帕累托前沿，找出在不同计算资源约束下，性能最优的模型。\\n\\n**关键创新**：论文的关键创新在于提出了一个测试时计算感知的评估框架，能够全面评估大语言模型在推理任务上的性能和计算成本。通过构建帕累托前沿，能够清晰地展示模型在精度和计算成本之间的权衡关系，为模型选择提供了客观的依据。此外，论文还发现混合专家（MoE）架构在性能和效率之间取得了较好的平衡，并揭示了推理计算的饱和点。\\n\\n**关键设计**：论文的关键设计包括：1) 选择具有代表性的开源大语言模型，覆盖不同架构和规模。2) 选择具有挑战性的数学和推理密集型基准测试，能够充分评估模型的推理能力。3) 精确测量模型的计算成本，例如，推理时间、GPU 内存占用。4) 使用帕累托前沿分析方法，能够清晰地展示模型在精度和计算成本之间的权衡关系。",
            "application_zh": "该研究成果可应用于工业界大语言模型的选型和部署。企业可以根据自身的计算资源约束和精度要求，利用论文提出的评估框架，选择最适合自身需求的模型。此外，该研究还可以指导模型优化，例如，通过调整模型架构或推理策略，在精度和计算成本之间取得更好的平衡。该研究对于推动大语言模型在实际应用中的落地具有重要意义。",
            "highlight_zh": "实验结果表明，混合专家（MoE）架构在性能和效率之间取得了较好的平衡，是工业应用中一个有潜力的选择。此外，研究还发现推理时计算存在饱和点，超过某个阈值，精度增益会减少。这意味着，过度增加计算资源并不能无限提升模型的推理能力，需要综合考虑模型本身的限制。",
            "tags_zh": [
                "大语言模型",
                "推理能力",
                "计算成本",
                "帕累托前沿",
                "模型选择"
            ],
            "_index": 26,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24776v1/figs/tflop_vs_accuracy_avg.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24776v1/figs/tflop_vs_accuracy_by_benchmark.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24776v1/figs/trends_multi_aggregated_log10-FLOPS.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
            "authors": [
                "Yuanhao Cai",
                "Kunpeng Li",
                "Menglin Jia",
                "Jialiang Wang",
                "Junzhe Sun",
                "Feng Liang",
                "Weifeng Chen",
                "Felix Juefei-Xu",
                "Chu Wang",
                "Ali Thabet",
                "Xiaoliang Dai",
                "Xuan Ju",
                "Alan Yuille",
                "Ji Hou"
            ],
            "arxiv_id": "2512.24551v1",
            "summary": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24551v1",
            "code_links": [
                {
                    "url": "https://github.com/caiyuanhao1998/Open-PhyGDPO",
                    "type": "github"
                },
                {
                    "url": "https://caiyuanhao1998.github.io/project/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]direct preference optimization"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PhyGDPO框架，通过物理感知的群体偏好优化实现物理一致的文本生成视频。",
            "summary_zh": "本文旨在解决文本生成视频（T2V）领域中，生成符合物理规律的视频这一难题。现有方法主要依赖图形或提示扩展，难以泛化到复杂环境，且缺乏物理推理能力。同时，缺乏包含丰富物理交互和现象的训练数据也是一个问题。为此，本文首先提出了一个物理增强的视频数据构建流程PhyAugPipe，利用具有思维链推理能力的视觉语言模型（VLM）收集大规模训练数据集PhyVidGen-135K。然后，构建了一个基于群体Plackett-Luce概率模型的物理感知群体偏好优化框架PhyGDPO，以捕捉超越成对比较的整体偏好。在PhyGDPO中，设计了物理引导奖励（PGR）方案，嵌入基于VLM的物理奖励，引导优化过程朝着物理一致性方向发展。此外，还提出了LoRA-Switch Reference（LoRA-SR）方案，消除了内存密集型的参考复制，从而实现高效训练。实验表明，该方法在PhyGenBench和VideoPhy2上显著优于最先进的开源方法。",
            "intro_zh": [
                "现有文本生成视频方法难以生成符合物理规律的视频，泛化性差，缺乏物理推理能力。",
                "提出PhyGDPO框架，利用物理增强数据和物理引导奖励，优化模型生成物理上合理的视频。",
                "实验表明，PhyGDPO在多个物理一致性测试基准上显著优于现有开源方法。"
            ],
            "method_zh": "**问题定义**：现有文本生成视频模型在生成视觉质量良好的视频方面取得了显著进展，但难以保证生成的视频内容符合物理规律，例如物体运动、碰撞等。现有方法主要依赖于图形引擎或提示词工程，难以泛化到复杂的物理场景，并且缺乏对物理规律的显式建模和推理能力。此外，缺乏大规模的、包含丰富物理交互的训练数据也是一个重要瓶颈。\\n\\n**核心思路**：本文的核心思路是利用视觉语言模型（VLM）的强大能力，构建大规模的物理增强训练数据，并设计一种物理感知的偏好优化框架，引导模型学习生成符合物理规律的视频。通过物理引导的奖励机制，鼓励模型生成物理上合理的视频内容。\\n\\n**技术框架**：整体框架包含两个主要阶段：数据构建阶段和模型训练阶段。在数据构建阶段，利用PhyAugPipe流程，使用VLM生成包含物理交互的视频描述，构建大规模数据集PhyVidGen-135K。在模型训练阶段，使用PhyGDPO框架，结合物理引导奖励（PGR）和LoRA-Switch Reference（LoRA-SR）方案，对文本生成视频模型进行优化。\\n\\n**关键创新**：本文的关键创新在于提出了物理感知的群体偏好优化框架PhyGDPO。与传统的成对偏好优化方法不同，PhyGDPO基于群体Plackett-Luce概率模型，能够捕捉更全面的偏好信息。此外，物理引导奖励（PGR）方案利用VLM对视频的物理合理性进行评估，并将评估结果作为奖励信号，引导模型朝着物理一致性方向优化。LoRA-Switch Reference（LoRA-SR）方案则解决了训练过程中参考视频内存占用过大的问题。\\n\\n**关键设计**：PhyAugPipe流程利用VLM生成视频描述，并进行思维链推理，以确保描述的准确性和丰富性。物理引导奖励（PGR）方案使用预训练的VLM模型评估视频的物理合理性，并将其转化为奖励信号。LoRA-Switch Reference（LoRA-SR）方案通过切换不同的LoRA模块，减少了参考视频的内存占用。具体的损失函数设计和网络结构细节在论文中有详细描述（未知）。",
            "application_zh": "该研究成果可应用于虚拟现实、游戏开发、机器人仿真等领域，提升生成内容的真实感和可信度。例如，可以用于生成更逼真的游戏场景，训练更智能的机器人，或者创建更具沉浸感的虚拟现实体验。未来，该技术有望推动人工智能在物理世界的应用。",
            "highlight_zh": "实验结果表明，PhyGDPO在PhyGenBench和VideoPhy2两个物理一致性测试基准上显著优于现有开源方法。具体而言，PhyGDPO在两个基准上的指标均取得了显著提升（具体数值未知），证明了其在生成物理合理视频方面的有效性。LoRA-SR方案也有效降低了训练过程中的内存占用。",
            "tags_zh": [
                "文本生成视频",
                "物理一致性",
                "偏好优化",
                "视觉语言模型",
                "群体偏好学习"
            ],
            "_index": 27,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24551v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24551v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24551v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization",
            "authors": [
                "Dong Qiu",
                "Duo Xu",
                "Limengxi Yue"
            ],
            "arxiv_id": "2512.24609v1",
            "summary": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Accepted by IEEE ICFTIC 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24609v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出强化学习增强的LLM智能体框架，优化协同决策与性能",
            "summary_zh": "大型语言模型(LLMs)在语言任务中表现出色，但通常缺乏协作意识，难以优化多智能体环境中的全局性能。本文提出了一种强化学习增强的LLM智能体框架，该框架将协作建模为去中心化部分可观测马尔可夫决策过程(Dec-POMDP)，并采用集中式训练和分散式执行(CTDE)。我们引入了组相对策略优化(GRPO)，以在训练期间利用全局信号联合优化智能体策略，并采用简化的联合奖励来平衡任务质量、速度和协调成本。在协作写作和编码基准测试中，我们的框架比单智能体基线提高了3倍的任务处理速度，写作结构/风格一致性达到98.7%，编码测试通过率达到74.6%。该方法始终优于强大的多智能体LLM基线，并为复杂工作流程中的可靠协作提供了一条实用途径。",
            "intro_zh": [
                "现有LLM在多智能体协作中缺乏全局优化能力，难以兼顾任务质量、速度和协调成本。",
                "论文提出强化学习增强的LLM智能体框架，利用集中式训练分散式执行(CTDE)优化协作策略。",
                "实验表明，该框架在写作和编码任务中显著提升了任务速度、一致性和测试通过率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多智能体协作场景下，大型语言模型(LLM)难以有效协作并优化全局性能的问题。现有方法，如直接使用LLM进行多智能体协作，往往缺乏对全局信息的利用，导致协作效率低下、任务质量难以保证，并且难以平衡任务速度、质量和协调成本等多个目标。\\n\\n**核心思路**：论文的核心思路是将多智能体协作问题建模为去中心化部分可观测马尔可夫决策过程(Dec-POMDP)，并利用强化学习方法进行优化。通过集中式训练分散式执行(CTDE)的框架，智能体可以在训练阶段利用全局信息进行学习，而在执行阶段则独立行动，从而实现高效的协作。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) LLM智能体：每个智能体由一个LLM驱动，负责生成动作。2) 强化学习模块：使用强化学习算法优化LLM智能体的策略。3) 集中式训练模块：在训练阶段，所有智能体共享全局信息，并使用组相对策略优化(GRPO)算法进行联合优化。4) 分散式执行模块：在执行阶段，每个智能体独立行动，根据自身观测和学习到的策略生成动作。\\n\\n**关键创新**：论文最重要的技术创新点在于提出了组相对策略优化(GRPO)算法。GRPO允许智能体在训练期间访问全局信号，从而更好地学习协作策略。此外，论文还设计了一个简化的联合奖励函数，用于平衡任务质量、速度和协调成本，使得智能体能够更好地优化全局性能。\\n\\n**关键设计**：GRPO算法通过引入一个组相对价值函数来指导智能体的策略更新。该价值函数考虑了所有智能体的联合行动对全局奖励的影响，从而使得智能体能够更好地理解协作的重要性。此外，简化的联合奖励函数被设计为任务质量、速度和协调成本的加权和，权重参数可以根据具体任务进行调整。",
            "application_zh": "该研究成果可应用于多种多智能体协作场景，例如：协同写作、代码生成、机器人协作、自动驾驶等。通过强化学习增强LLM智能体的协作能力，可以显著提高工作效率、降低成本，并实现更智能化的自动化流程。未来，该方法有望扩展到更复杂的任务和更大规模的智能体群体。",
            "highlight_zh": "实验结果表明，该框架在协作写作和编码基准测试中表现出色。在写作任务中，任务处理速度提高了3倍，结构/风格一致性达到98.7%。在编码任务中，测试通过率达到74.6%。该方法始终优于强大的多智能体LLM基线，证明了其有效性和优越性。",
            "tags_zh": [
                "多智能体协作",
                "强化学习",
                "大型语言模型",
                "Dec-POMDP",
                "CTDE",
                "组相对策略优化",
                "协同写作",
                "代码生成"
            ],
            "_index": 28,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding",
            "authors": [
                "Qingda Hu",
                "Ziheng Qiu",
                "Zijun Xu",
                "Kaizhao Zhang",
                "Xizhou Bu",
                "Zuolei Sun",
                "Bo Zhang",
                "Jieru Zhao",
                "Zhongxue Gan",
                "Wenchao Ding"
            ],
            "arxiv_id": "2512.24638v1",
            "summary": "State ambiguity is common in robotic manipulation. Identical observations may correspond to multiple valid behavior trajectories. The visuomotor policy must correctly extract the appropriate types and levels of information from the history to identify the current task phase. However, naively extending the history window is computationally expensive and may cause severe overfitting. Inspired by the continuous nature of human reasoning and the recoding of working memory, we introduce PAM, a novel visuomotor Policy equipped with Adaptive working Memory. With minimal additional training cost in a two-stage manner, PAM supports a 300-frame history window while maintaining high inference speed. Specifically, a hierarchical frame feature extractor yields two distinct representations for motion primitives and temporal disambiguation. For compact representation, a context router with range-specific queries is employed to produce compact context features across multiple history lengths. And an auxiliary objective of reconstructing historical information is introduced to ensure that the context router acts as an effective bottleneck. We meticulously design 7 tasks and verify that PAM can handle multiple scenarios of state ambiguity simultaneously. With a history window of approximately 10 seconds, PAM still supports stable training and maintains inference speeds above 20Hz. Project website: https://tinda24.github.io/pam/",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24638v1",
            "code_links": [
                {
                    "url": "https://tinda24.github.io/pam/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出基于自适应工作记忆的策略PAM，解决机器人操作中的状态歧义问题",
            "summary_zh": "本文提出了一种名为PAM的视觉运动策略，它配备了自适应工作记忆，旨在解决机器人操作中常见的状态歧义问题。相同的观测可能对应多个有效的行为轨迹，因此策略需要从历史信息中正确提取信息以识别当前任务阶段。PAM通过分层帧特征提取器为运动原语和时间消歧生成不同的表示，并采用带有范围特定查询的上下文路由来生成跨多个历史长度的紧凑上下文特征。引入重构历史信息的辅助目标，确保上下文路由作为有效的瓶颈。实验表明，PAM能够同时处理多种状态歧义场景，在约10秒的历史窗口下，仍能保持稳定的训练和20Hz以上的推理速度。",
            "intro_zh": [
                "机器人操作中状态歧义普遍存在，相同观测可能对应多个行为轨迹，需要有效利用历史信息。",
                "PAM通过自适应工作记忆，分层提取特征，并使用上下文路由压缩历史信息，解决长时依赖问题。",
                "实验证明PAM能有效处理多种状态歧义场景，在长历史窗口下保持稳定训练和较高推理速度。"
            ],
            "method_zh": "**问题定义**：机器人操作中，由于传感器噪声、部分观测等原因，常常出现状态歧义，即相同的观测可能对应多个不同的行为轨迹。现有的方法要么依赖于手工设计的状态表示，要么简单地扩展历史窗口，前者泛化性差，后者计算成本高且容易过拟合。\\n\\n**核心思路**：受到人类推理和工作记忆重编码的启发，论文提出使用自适应工作记忆来解决状态歧义问题。核心思想是学习一种能够动态选择和压缩历史信息的策略，从而在保证推理速度的同时，有效地利用长时依赖关系。\\n\\n**技术框架**：PAM包含以下几个主要模块：1) 分层帧特征提取器：提取运动原语和时间消歧两种不同的特征表示。2) 上下文路由：使用范围特定的查询来生成跨多个历史长度的紧凑上下文特征。3) 策略网络：基于上下文特征输出动作。4) 辅助重构目标：用于训练上下文路由，使其能够有效地压缩历史信息。整体流程是，首先通过分层帧特征提取器提取每一帧的特征，然后通过上下文路由将历史帧的特征压缩成上下文特征，最后策略网络基于上下文特征输出动作。\\n\\n**关键创新**：PAM的关键创新在于自适应工作记忆的设计，它能够动态地选择和压缩历史信息，从而在保证推理速度的同时，有效地利用长时依赖关系。与现有方法相比，PAM不需要手工设计状态表示，也不需要简单地扩展历史窗口，因此具有更好的泛化性和计算效率。\\n\\n**关键设计**：1) 分层帧特征提取器：使用两个独立的卷积神经网络分别提取运动原语和时间消歧特征。2) 上下文路由：使用多个范围特定的查询，每个查询负责提取特定时间范围内的信息。3) 辅助重构目标：使用一个解码器来重构历史帧的特征，并使用重构误差作为损失函数来训练上下文路由。4) 训练方式：采用两阶段训练方式，首先训练分层帧特征提取器和上下文路由，然后固定这些模块，训练策略网络。",
            "application_zh": "该研究成果可应用于各种需要处理状态歧义的机器人操作任务，例如装配、抓取、导航等。通过利用长时依赖关系，机器人可以更好地理解任务目标，并做出更合理的决策。此外，该方法还可以推广到其他需要处理序列数据的领域，例如自然语言处理、语音识别等。",
            "highlight_zh": "实验结果表明，PAM在多个状态歧义场景中表现出色，能够同时处理多种类型的歧义。在具有约10秒历史窗口的情况下，PAM仍然能够保持稳定的训练，并实现20Hz以上的推理速度。相较于基线方法，PAM在多个任务上取得了显著的性能提升。",
            "tags_zh": [
                "机器人操作",
                "状态歧义",
                "自适应工作记忆",
                "长时依赖",
                "视觉运动策略"
            ],
            "_index": 29,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24638v1/figures/coarse.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24638v1/figures/pipeline.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24638v1/figures/exp.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment",
            "authors": [
                "Chenji Lu",
                "Zhuo Chen",
                "Hui Zhao",
                "Zhenyi Wang",
                "Pengjie Wang",
                "Jian Xu",
                "Bo Zheng"
            ],
            "arxiv_id": "2512.24943v1",
            "summary": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.",
            "categories": [
                "cs.IR",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.IR",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24943v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出RAIR：一个面向电商相关性评估的规则感知、长尾和视觉显著性基准",
            "summary_zh": "本文提出了一个名为RAIR（Rule-Aware benchmark with Image for Relevance assessment）的中文数据集，用于电商搜索相关性评估。现有基准在模型评估的复杂性方面存在不足，导致行业内缺乏标准化的相关性评估指标。RAIR源于真实场景，建立了一个标准化的相关性评估框架，并提供了一套通用的规则，为标准化评估奠定了基础。RAIR分析了当前相关性模型所需的基本能力，并引入了一个包含三个子集的数据集：一个具有行业平衡抽样的通用子集，用于评估基本模型能力；一个侧重于挑战性案例的长尾硬子集，用于评估性能极限；一个用于评估多模态理解能力的视觉显著性子集。使用14个开源和闭源模型在RAIR上进行了实验，结果表明即使是性能最佳的GPT-5，RAIR也提出了足够的挑战。RAIR数据现已开放，可作为相关性评估的行业基准，并为通用LLM和视觉语言模型（VLM）评估提供新的见解。",
            "intro_zh": [
                "现有电商搜索相关性评估基准缺乏足够的复杂性，难以全面评估模型，行业内缺少标准化的评估指标。",
                "RAIR通过构建包含通用、长尾和视觉显著性三个子集的数据集，并制定通用规则，建立标准化的评估框架。",
                "实验结果表明，即使是GPT-5在RAIR上也面临挑战，证明了RAIR的难度和有效性，为LLM和VLM评估提供新视角。"
            ],
            "method_zh": "**问题定义**：论文旨在解决电商搜索相关性评估中，现有基准数据集复杂度不足，无法充分评估大型语言模型（LLM）和视觉语言模型（VLM）的问题。现有方法无法有效评估模型在长尾数据和多模态理解方面的能力，导致行业内缺乏统一且具有挑战性的评估标准。\\n\\n**核心思路**：论文的核心思路是构建一个更具挑战性和代表性的数据集RAIR，该数据集包含通用子集、长尾硬子集和视觉显著性子集，并制定一套通用的评估规则。通过这种方式，RAIR能够更全面地评估模型在不同场景下的相关性判断能力，并为行业提供一个标准化的评估基准。\\n\\n**技术框架**：RAIR的构建主要包含以下几个阶段：1) 数据收集：从真实电商场景中收集数据，保证数据的真实性和多样性。2) 子集划分：将数据划分为通用子集、长尾硬子集和视觉显著性子集，分别评估模型在不同方面的能力。3) 规则制定：制定一套通用的评估规则，用于指导相关性判断，保证评估的公平性和一致性。4) 模型评估：使用多个开源和闭源模型在RAIR上进行评估，验证RAIR的有效性和挑战性。\\n\\n**关键创新**：RAIR的关键创新在于其数据集的构建方式和评估规则的制定。RAIR不仅考虑了数据的通用性，还特别关注了长尾数据和视觉信息，这使得RAIR能够更全面地评估模型的相关性判断能力。此外，RAIR制定的通用评估规则也为行业提供了一个标准化的评估框架。\\n\\n**关键设计**：RAIR的关键设计包括：1) 长尾硬子集的构建：通过选择具有挑战性的长尾查询和商品，评估模型在罕见情况下的表现。2) 视觉显著性子集的构建：通过引入商品图片，评估模型的多模态理解能力。3) 通用评估规则的制定：制定清晰明确的规则，指导相关性判断，减少主观性。",
            "application_zh": "RAIR可应用于电商搜索、推荐系统、广告排序等领域，帮助提升用户搜索体验和商品点击率。通过更准确地评估模型的相关性判断能力，RAIR可以指导模型优化，提升电商平台的整体效率和用户满意度。未来，RAIR可以扩展到其他领域，如新闻推荐、知识问答等，为更广泛的应用场景提供支持。",
            "highlight_zh": "实验结果表明，即使是性能领先的GPT-5模型在RAIR上也面临挑战，这证明了RAIR数据集的难度和有效性。不同模型在RAIR的不同子集上表现差异明显，表明RAIR能够有效区分模型在通用性、长尾处理和多模态理解方面的能力。RAIR的发布为电商相关性评估提供了一个新的行业基准。",
            "tags_zh": [
                "电商搜索",
                "相关性评估",
                "长尾数据",
                "视觉显著性",
                "多模态理解",
                "大型语言模型",
                "评估基准"
            ],
            "_index": 30,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24943v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24943v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24943v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time",
            "authors": [
                "Zhenyu Zhang",
                "Xiaoxia Wu",
                "Zhongzhu Zhou",
                "Qingyang Wu",
                "Yineng Zhang",
                "Pragaash Ponnusamy",
                "Harikaran Subbaraj",
                "Jue Wang",
                "Shuaiwen Leon Song",
                "Ben Athiwaratkun"
            ],
            "arxiv_id": "2512.24574v1",
            "summary": "Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24574v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CREST，通过干预注意力头引导LLM推理，提升效率和准确率。",
            "summary_zh": "大型语言模型(LLM)通常依赖于长链思维(CoT)推理来解决复杂任务。虽然有效，但这些轨迹常常效率低下，导致过多的token生成带来的高延迟，或不稳定的推理，在欠思考(浅薄、不一致的步骤)和过度思考(重复、冗长的推理)之间交替。本文研究了推理轨迹的结构，并发现了与验证和回溯等不同认知行为相关的特定注意力头。通过在推理时对这些头进行轻微干预，我们可以引导模型远离低效模式。基于此，我们提出了CREST，一种用于测试时认知推理引导的无训练方法。CREST包含两个部分：(1)离线校准步骤，用于识别认知头并导出特定于头的引导向量；(2)推理时程序，用于旋转隐藏表示以抑制沿这些向量的分量。CREST自适应地抑制了无益的推理行为，从而提高了准确性并降低了计算成本。在不同的推理基准和模型上，CREST将准确率提高了高达17.5%，同时减少了37.6%的token使用量，为更快、更可靠的LLM推理提供了一种简单有效的途径。",
            "intro_zh": [
                "现有LLM推理过程存在效率问题，表现为token生成过多、推理不稳定，在欠思考和过度思考间切换。",
                "CREST通过离线校准识别与特定认知行为相关的注意力头，并在推理时干预这些头，抑制低效行为。",
                "实验表明，CREST在提升准确率的同时，显著降低了token使用量，实现了更高效可靠的推理。"
            ],
            "method_zh": "**问题定义**：现有大型语言模型在进行复杂推理时，依赖于链式思维(CoT)，但推理过程常常效率低下。具体表现为：生成过多的token导致高延迟；推理过程不稳定，在浅薄的欠思考和冗余的过度思考之间摇摆。这些问题限制了LLM在实际应用中的性能和效率。\\n\\n**核心思路**：论文的核心思路是，通过分析推理轨迹的结构，发现与特定认知行为（如验证、回溯）相关的注意力头。然后，在推理过程中，对这些注意力头进行干预，抑制模型产生低效的推理行为。这种方法无需额外的训练，可以在测试时直接应用，从而提高推理效率和准确率。\\n\\n**技术框架**：CREST包含两个主要阶段：离线校准阶段和推理时引导阶段。在离线校准阶段，首先分析推理轨迹，识别与特定认知行为相关的注意力头，并为每个头计算一个引导向量。在推理时引导阶段，对于每个推理步骤，CREST旋转隐藏层表示，以抑制沿着引导向量方向的分量，从而抑制与该向量相关的认知行为。\\n\\n**关键创新**：CREST的关键创新在于，它提出了一种无需训练的测试时认知推理引导方法。通过分析注意力头与认知行为之间的关联，实现了对LLM推理过程的细粒度控制。与传统的微调或提示工程方法相比，CREST更加轻量级，易于部署，并且能够自适应地抑制不必要的推理行为。\\n\\n**关键设计**：CREST的关键设计包括：(1)认知头的识别方法，通过分析注意力权重与特定认知行为之间的相关性来确定；(2)引导向量的计算方法，基于校准数据，计算每个认知头的引导向量，用于在推理时抑制相应的认知行为；(3)隐藏层表示的旋转方法，通过旋转隐藏层表示，抑制沿着引导向量方向的分量，从而实现对认知行为的干预。具体旋转操作通常使用正交投影或类似的线性变换。",
            "application_zh": "CREST可应用于各种需要复杂推理的场景，例如问答系统、代码生成、数学问题求解等。通过提高推理效率和准确率，CREST可以降低LLM的使用成本，并提升用户体验。未来，该方法有望扩展到其他类型的认知任务，并与其他推理优化技术相结合，进一步提升LLM的性能。",
            "highlight_zh": "实验结果表明，CREST在多个推理基准测试中显著提高了LLM的性能。例如，在某些任务上，CREST将准确率提高了高达17.5%，同时减少了37.6%的token使用量。这些结果表明，CREST是一种简单而有效的LLM推理优化方法，具有很强的实用价值。",
            "tags_zh": [
                "大型语言模型",
                "链式推理",
                "注意力机制",
                "认知引导",
                "测试时干预"
            ],
            "_index": 31,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24574v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24574v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24574v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GenZ: Foundational models as latent variable generators within traditional statistical models",
            "authors": [
                "Marko Jojic",
                "Nebojsa Jojic"
            ],
            "arxiv_id": "2512.24834v1",
            "summary": "We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24834v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "GenZ：融合统计模型与大模型的隐变量生成框架，提升预测精度。",
            "summary_zh": "本文提出GenZ，一种混合模型，通过可解释的语义特征桥接基础模型和统计建模。大型语言模型虽然拥有广泛的领域知识，但通常无法捕捉对预测任务至关重要的数据集特定模式。我们的方法通过迭代过程发现语义特征描述来解决这个问题，该过程对比通过统计建模误差识别的项目组，而不是仅仅依赖于基础模型的领域理解。我们将其公式化为广义EM算法，该算法联合优化语义特征描述符和统计模型参数。该方法提示一个冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为潜在二元特征的噪声观测，这些特征通过学习的统计关系预测实值目标。我们在两个领域展示了该方法：房价预测（享乐回归）和电影推荐的冷启动协同过滤。在房价方面，我们的模型使用从多模态列表数据中发现的语义特征实现了12%的中位数相对误差，大大优于依赖于LLM一般领域知识的GPT-5基线（38%误差）。对于Netflix电影嵌入，我们的模型仅从语义描述预测协同过滤表示，余弦相似度为0.59——匹配了通过传统协同过滤需要大约4000个用户评分才能达到的性能。发现的特征揭示了数据集特定的模式（例如，预测当地房地产市场的建筑细节，预测用户偏好的特许经营会员资格），这些模式与模型单独的领域知识不同。",
            "intro_zh": [
                "现有大型语言模型在特定数据集预测任务中，无法有效捕捉数据集特有的模式。",
                "GenZ通过迭代对比统计建模误差识别的项目组，发现可解释的语义特征，并用其指导预测。",
                "实验表明，GenZ在房价预测和电影推荐任务上显著优于现有方法，并能发现数据集特有模式。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在特定数据集上进行预测时，无法有效利用数据集自身蕴含的特定模式的问题。现有方法过度依赖LLM的通用领域知识，忽略了数据集内部的细粒度信息，导致预测精度下降。例如，在房价预测中，LLM可能知道房屋的一般特征，但难以捕捉特定区域的建筑风格对房价的影响。\\n\\n**核心思路**：GenZ的核心思路是将LLM作为一种隐变量生成器，通过统计建模误差来引导LLM发现数据集特有的语义特征。这些特征作为隐变量，连接LLM的领域知识和统计模型的预测能力，从而实现更精确的预测。这种方法避免了直接依赖LLM的通用知识，而是让LLM专注于提取数据集相关的特征。\\n\\n**技术框架**：GenZ的整体框架是一个广义EM算法。首先，使用统计模型进行初步预测，并识别预测误差较大的样本组。然后，利用这些样本组的差异，提示LLM生成语义特征描述。接着，将这些特征描述作为隐变量，通过学习统计关系来预测目标值。最后，迭代优化语义特征描述和统计模型参数，直至收敛。主要模块包括：统计建模模块、语义特征发现模块（基于LLM）和联合优化模块。\\n\\n**关键创新**：GenZ的关键创新在于将LLM与传统的统计建模方法相结合，并利用统计建模误差来引导LLM发现数据集特有的语义特征。与现有方法相比，GenZ不是直接使用LLM进行预测，而是将LLM作为一种特征提取器，提取出的特征再用于统计建模。这种方法能够更好地利用数据集自身的信息，提高预测精度。\\n\\n**关键设计**：GenZ的关键设计包括：1) 使用广义EM算法进行联合优化，确保语义特征和统计模型参数能够协同优化。2) 设计合适的prompt，引导LLM生成有意义的语义特征描述。3) 将LLM的输出视为隐变量的噪声观测，并通过学习统计关系来降低噪声的影响。4) 使用冻结的LLM，避免了微调LLM带来的计算成本和过拟合风险。",
            "application_zh": "GenZ具有广泛的应用前景，可应用于各种需要利用领域知识和数据集特定模式的预测任务。例如，在金融领域，可以用于预测股票价格或信用风险；在医疗领域，可以用于诊断疾病或预测患者预后；在推荐系统领域，可以用于冷启动推荐或个性化推荐。GenZ的实际价值在于提高预测精度，并发现数据集中的隐藏模式，为决策提供更可靠的依据。",
            "highlight_zh": "GenZ在房价预测任务上，使用发现的语义特征实现了12%的中位数相对误差，显著优于GPT-5基线（38%误差）。在Netflix电影嵌入任务上，GenZ仅从语义描述预测协同过滤表示，余弦相似度达到0.59，与需要约4000个用户评分的传统协同过滤方法性能相当。这些结果表明，GenZ能够有效利用数据集特有的模式，提高预测精度。",
            "tags_zh": [
                "隐变量模型",
                "基础模型",
                "统计建模",
                "语义特征",
                "房价预测",
                "协同过滤",
                "冷启动",
                "EM算法"
            ],
            "_index": 32,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24834v1/1_bathroom.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24834v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24834v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression",
            "authors": [
                "Manikanta Kotthapalli",
                "Banafsheh Rekabdar"
            ],
            "arxiv_id": "2512.24547v1",
            "summary": "The exponential growth of video traffic has placed increasing demands on bandwidth and storage infrastructure, particularly for content delivery networks (CDNs) and edge devices. While traditional video codecs like H.264 and HEVC achieve high compression ratios, they are designed primarily for pixel-domain reconstruction and lack native support for machine learning-centric latent representations, limiting their integration into deep learning pipelines. In this work, we present a Multi-Scale Vector Quantized Variational Autoencoder (MS-VQ-VAE) designed to generate compact, high-fidelity latent representations of low-resolution video, suitable for efficient storage, transmission, and client-side decoding. Our architecture extends the VQ-VAE-2 framework to a spatiotemporal setting, introducing a two-level hierarchical latent structure built with 3D residual convolutions. The model is lightweight (approximately 18.5M parameters) and optimized for 64x64 resolution video clips, making it appropriate for deployment on edge devices with constrained compute and memory resources. To improve perceptual reconstruction quality, we incorporate a perceptual loss derived from a pre-trained VGG16 network. Trained on the UCF101 dataset using 2-second video clips (32 frames at 16 FPS), on the test set we achieve 25.96 dB PSNR and 0.8375 SSIM. On validation, our model improves over the single-scale baseline by 1.41 dB PSNR and 0.0248 SSIM. The proposed framework is well-suited for scalable video compression in bandwidth-sensitive scenarios, including real-time streaming, mobile video analytics, and CDN-level storage optimization.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "11 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24547v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "VQ-VAE"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "4_motion_diffusion",
                "8_physics_animation"
            ],
            "headline_zh": "提出一种分层矢量量化隐变量的感知低分辨率视频压缩方法，适用于带宽受限场景。",
            "summary_zh": "视频流量的指数增长对带宽和存储基础设施提出了更高的要求，尤其是在内容分发网络（CDN）和边缘设备方面。传统的视频编解码器（如H.264和HEVC）虽然实现了高压缩比，但它们主要为像素域重建而设计，缺乏对机器学习中心隐变量表示的原生支持，限制了它们与深度学习管道的集成。本文提出了一种多尺度矢量量化变分自编码器（MS-VQ-VAE），旨在生成低分辨率视频的紧凑、高保真隐变量表示，适用于高效存储、传输和客户端解码。该架构将VQ-VAE-2框架扩展到时空环境，引入了一个由3D残差卷积构建的两层分层隐变量结构。该模型轻量级（约1850万参数），并针对64x64分辨率的视频片段进行了优化，使其适合部署在计算和内存资源受限的边缘设备上。为了提高感知重建质量，我们结合了从预训练的VGG16网络中提取的感知损失。在UCF101数据集上使用2秒视频片段（32帧，16 FPS）进行训练，在测试集上我们实现了25.96 dB PSNR和0.8375 SSIM。在验证集上，我们的模型比单尺度基线提高了1.41 dB PSNR和0.0248 SSIM。所提出的框架非常适合带宽敏感场景中的可扩展视频压缩，包括实时流媒体、移动视频分析和CDN级别的存储优化。",
            "intro_zh": [
                "现有视频编解码器主要面向像素域重建，缺乏对机器学习友好的隐变量表示，限制了其在深度学习中的应用。",
                "提出一种多尺度矢量量化变分自编码器，通过分层隐变量结构生成紧凑、高保真视频表示，适用于边缘设备。",
                "实验表明，该模型在UCF101数据集上取得了良好的PSNR和SSIM指标，优于单尺度基线模型。"
            ],
            "method_zh": "**问题定义**：论文旨在解决低分辨率视频压缩问题，特别是在带宽受限的边缘设备和内容分发网络（CDN）等场景下。现有视频编解码器（如H.264和HEVC）虽然压缩率高，但主要针对像素域重建，与深度学习模型的集成存在困难，无法直接提供适用于机器学习任务的隐变量表示。\\n\\n**核心思路**：论文的核心思路是利用矢量量化变分自编码器（VQ-VAE）学习视频数据的紧凑、高保真隐变量表示。通过分层结构，模型能够捕捉不同尺度的时空特征，从而更好地重建视频。此外，引入感知损失函数，使重建视频在感知上更接近原始视频，提高视觉质量。\\n\\n**技术框架**：整体架构是一个多尺度矢量量化变分自编码器（MS-VQ-VAE），包含编码器、量化器和解码器三个主要模块。编码器将输入视频帧序列编码成一系列隐变量表示。量化器将连续的隐变量离散化为有限的码本索引。解码器根据码本索引重建视频帧序列。该架构采用两层分层结构，分别提取不同尺度的特征。\\n\\n**关键创新**：主要创新点在于将VQ-VAE-2框架扩展到时空视频领域，并引入分层隐变量结构。这种分层结构能够捕捉不同尺度的时空信息，从而更好地表示视频内容。此外，结合感知损失函数，优化了重建视频的感知质量。\\n\\n**关键设计**：模型使用3D残差卷积来提取时空特征。量化器使用可学习的码本，将连续的隐变量离散化。感知损失函数基于预训练的VGG16网络提取的特征图计算。模型针对64x64分辨率的视频片段进行了优化，使用Adam优化器进行训练。",
            "application_zh": "该研究成果可应用于多种带宽敏感的场景，例如实时视频流媒体、移动视频分析、CDN级别的存储优化以及边缘计算设备上的视频处理。通过高效的视频压缩和高质量的重建，可以降低带宽需求，提高用户体验，并为各种视频分析任务提供更有效的输入。",
            "highlight_zh": "实验结果表明，该模型在UCF101数据集上取得了良好的性能。在测试集上，PSNR达到25.96 dB，SSIM达到0.8375。在验证集上，相比于单尺度基线模型，PSNR提高了1.41 dB，SSIM提高了0.0248。这些结果表明，该模型能够有效地压缩低分辨率视频，并保持较高的重建质量。",
            "tags_zh": [
                "视频压缩",
                "矢量量化",
                "变分自编码器",
                "低分辨率视频",
                "分层隐变量",
                "感知损失",
                "边缘计算"
            ],
            "_index": 33,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24547v1/architecture_diagram_1000dpi.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24547v1/figures/v_ApplyEyeMakeup_g01_c01_clip_0004__original.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24547v1/figures/v_ApplyEyeMakeup_g01_c01_clip_0004__vq-vae.png",
                    "caption": "",
                    "figure_id": "img_2"
                }
            ]
        },
        {
            "title": "Many Minds from One Model: Bayesian Transformers for Population Intelligence",
            "authors": [
                "Diji Yang",
                "Yi Zhang"
            ],
            "arxiv_id": "2512.25063v1",
            "summary": "Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.\n  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.",
            "categories": [
                "cs.LG",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25063v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Population Bayesian Transformers，提升Transformer模型的多样性和决策能力",
            "summary_zh": "本文提出Population Bayesian Transformers (B-Trans)，旨在将标准大型语言模型转化为贝叶斯Transformer模型，从而能够从单个预训练权重集中采样出多样且连贯的模型实例。B-Trans通过将归一化层中的偏置类偏移视为具有高斯变分近似的随机变量，引入了一种受贝叶斯启发的后验代理，从而在不训练完整贝叶斯神经网络的情况下，诱导模型行为的分布。通过在序列级别冻结采样的噪声，B-Trans保持了每个生成内部的连贯性，从而在token之间强制执行时间一致性。B-Trans支持群体层面的决策，通过聚合采样个体的预测，显著增强了探索能力。在零样本生成、具有可验证奖励的强化学习（RLVR）以及没有显式标签的强化学习等实验中，B-Trans有效地利用了群体智慧，与确定性基线相比，在实现更好任务性能的同时，产生了卓越的语义多样性。",
            "intro_zh": [
                "现有Transformer模型通常训练为单一确定性系统，缺乏多样性，限制了其探索能力和泛化性能。",
                "B-Trans将Transformer模型转化为贝叶斯模型，通过对归一化层偏置进行随机采样，生成多样化的模型实例。",
                "实验表明，B-Trans在零样本生成和强化学习任务中，能够提升语义多样性和任务性能，有效利用群体智慧。"
            ],
            "method_zh": "**问题定义**：现有Transformer模型通常被训练成单一的确定性系统，即优化过程产生一组固定的参数，代表着关于数据的单一功能假设。这种单调性限制了模型在探索未知空间和处理不确定性方面的能力。尤其是在强化学习等任务中，缺乏多样性的模型难以有效地进行探索，从而影响最终的性能。\\n\\n**核心思路**：本文的核心思路是将Transformer模型转化为贝叶斯模型，从而能够从单个预训练权重集中采样出多个具有不同行为的模型实例。通过引入模型参数的概率分布，B-Trans能够模拟“群体智慧”，即通过聚合多个模型的预测来提高决策的鲁棒性和准确性。\\n\\n**技术框架**：B-Trans的核心在于将Transformer模型中的归一化层（Normalization Layer）的偏置项视为随机变量，并使用高斯变分近似来建模这些变量的后验分布。具体来说，对于每个归一化层，B-Trans引入一个可学习的均值和方差参数，用于定义高斯分布。在推理阶段，从这些高斯分布中采样偏置项，从而生成不同的模型实例。为了保证生成序列的连贯性，B-Trans在序列级别冻结采样的噪声，即对于同一个序列中的所有token，使用相同的偏置项。\\n\\n**关键创新**：B-Trans的关键创新在于提出了一种轻量级的贝叶斯Transformer实现方法。与传统的贝叶斯神经网络相比，B-Trans不需要训练完整的概率模型，而是通过对归一化层偏置进行随机采样来近似后验分布，从而大大降低了计算成本。此外，B-Trans通过在序列级别冻结噪声，保证了生成序列的连贯性，避免了随机采样带来的不稳定性。\\n\\n**关键设计**：B-Trans的关键设计包括：1) 使用高斯变分近似来建模归一化层偏置的后验分布；2) 在序列级别冻结采样的噪声，以保证生成序列的连贯性；3) 通过聚合多个模型实例的预测来进行决策，从而利用群体智慧。",
            "application_zh": "B-Trans具有广泛的应用前景，例如可以应用于零样本生成、强化学习、对话系统等领域。在零样本生成中，B-Trans可以生成更多样化的文本内容。在强化学习中，B-Trans可以提高探索效率和策略的鲁棒性。在对话系统中，B-Trans可以生成更自然、更流畅的对话。",
            "highlight_zh": "实验结果表明，B-Trans在零样本生成、具有可验证奖励的强化学习（RLVR）以及没有显式标签的强化学习等任务中，均取得了显著的性能提升。例如，在零样本生成任务中，B-Trans生成的文本具有更高的语义多样性。在强化学习任务中，B-Trans能够更快地学习到最优策略，并取得更高的奖励。",
            "tags_zh": [
                "贝叶斯Transformer",
                "群体智能",
                "多样性生成",
                "强化学习",
                "零样本学习",
                "变分推断",
                "大型语言模型"
            ],
            "_index": 34,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.25063v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.25063v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.25063v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning",
            "authors": [
                "Timo Kaufmann",
                "Yannick Metz",
                "Daniel Keim",
                "Eyke Hüllermeier"
            ],
            "arxiv_id": "2512.25023v1",
            "summary": "Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "NeurIPS 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25023v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "preference learning",
                        "RLHF"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "ResponseRank：通过偏好强度学习实现数据高效的奖励建模",
            "summary_zh": "二元选择常用于从人类反馈中进行强化学习(RLHF)，但仅能传达偏好的方向。例如，人们可能选择苹果而不是橙子，香蕉而不是葡萄，但哪种偏好更强烈？强度对于不确定性下的决策和偏好模型的泛化至关重要，但难以可靠地衡量。响应时间和注释者间一致性等元数据可以作为强度的代理，但通常存在噪声且相互混淆。我们提出了ResponseRank来解决从噪声强度信号中学习的挑战。我们的方法使用代理信号中的相对差异，通过推断的偏好强度对成对比较的响应进行排序。为了控制系统性变化，我们仅在精心构建的层内局部比较信号。这使得能够稳健地学习与强度导出的排序一致的效用差异，同时对强度信号做出最小的假设。我们的贡献有三方面：(1) ResponseRank，一种通过利用局部有效的相对强度信号来稳健地学习偏好强度的新方法；(2) 在各种任务中改进样本效率和鲁棒性的经验证据：合成偏好学习(具有模拟响应时间)、语言建模(具有注释者一致性)和RL控制任务(具有模拟episode回报)；(3) Pearson距离相关性(PDC)，一种将基数效用学习与序数准确性隔离的新指标。",
            "intro_zh": [
                "现有RLHF方法依赖二元偏好，忽略了偏好强度，导致模型泛化能力受限，且对噪声敏感。",
                "ResponseRank通过比较局部范围内响应的相对强度，学习与强度排序一致的效用差异，从而更稳健地建模偏好。",
                "实验表明，ResponseRank在合成数据、语言建模和RL控制任务中均表现出更高的样本效率和鲁棒性。"
            ],
            "method_zh": "**问题定义**：现有基于人类反馈的强化学习（RLHF）方法通常只使用二元偏好数据，即给定两个选项，人类选择更喜欢哪一个。然而，这种方法忽略了偏好强度，即人类对某个选项的偏好程度。偏好强度对于在不确定性下做出决策以及模型的泛化能力至关重要。此外，现有方法容易受到噪声数据的影响，例如，人类标注错误或不一致。\n\n**核心思路**：ResponseRank的核心思想是利用偏好强度信息来提高奖励模型的学习效率和鲁棒性。该方法通过比较成对比较中响应的相对强度来推断偏好强度。具体来说，它使用诸如响应时间或注释者间一致性等代理信号来估计偏好强度，并利用这些信号中的相对差异来对响应进行排序。通过学习与强度导出的排序一致的效用差异，ResponseRank能够更准确地建模人类偏好。\n\n**技术框架**：ResponseRank的整体框架包括以下几个主要步骤：1) 数据收集：收集包含成对比较和相应代理信号（如响应时间、注释者一致性）的数据。2) 分层：将数据划分为多个局部层，以控制系统性变化。3) 强度排序：在每个局部层内，根据代理信号的相对差异对响应进行排序，以推断偏好强度。4) 效用学习：学习与强度导出的排序一致的效用差异。5) 模型评估：使用Pearson距离相关性（PDC）等指标评估模型的性能。\n\n**关键创新**：ResponseRank的关键创新在于它能够从噪声强度信号中稳健地学习偏好强度。与现有方法不同，ResponseRank不直接使用代理信号的绝对值，而是使用相对差异来推断偏好强度。此外，ResponseRank通过分层策略来控制系统性变化，从而进一步提高了鲁棒性。Pearson距离相关性（PDC）是一种新的评估指标，用于将基数效用学习与序数准确性隔离。\n\n**关键设计**：ResponseRank的关键设计包括：1) 局部分层策略：通过将数据划分为多个局部层，ResponseRank能够控制系统性变化，例如，不同注释者之间的偏好差异。2) 相对强度信号：ResponseRank使用代理信号的相对差异来推断偏好强度，而不是直接使用绝对值。这使得该方法对噪声更加鲁棒。3) 损失函数：ResponseRank使用一种损失函数，该函数鼓励模型学习与强度导出的排序一致的效用差异。具体的损失函数形式未知，需要查阅论文原文。",
            "application_zh": "ResponseRank具有广泛的应用前景，包括：1) 机器人控制：通过学习人类对不同机器人行为的偏好，可以训练出更符合人类意图的机器人。2) 自然语言处理：可以用于改进语言模型的生成质量，使其更符合人类的偏好。3) 推荐系统：可以用于构建更个性化的推荐系统，提高用户满意度。该研究的实际价值在于提高了奖励建模的效率和鲁棒性，未来可能推动人机协作和人工智能应用的进步。",
            "highlight_zh": "实验结果表明，ResponseRank在合成偏好学习、语言建模和RL控制任务中均表现出优于现有方法的性能。例如，在语言建模任务中，ResponseRank能够利用注释者一致性信息，显著提高模型的生成质量。在RL控制任务中，ResponseRank能够更有效地学习奖励函数，从而提高智能体的性能。具体的性能提升幅度未知，需要查阅论文原文。",
            "tags_zh": [
                "奖励建模",
                "偏好学习",
                "强化学习",
                "人机反馈",
                "偏好强度",
                "数据效率",
                "鲁棒性"
            ],
            "_index": 35,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Attribution-Guided Distillation of Matryoshka Sparse Autoencoders",
            "authors": [
                "Cristina P. Martin-Linares",
                "Jonathan P. Ling"
            ],
            "arxiv_id": "2512.24975v1",
            "summary": "Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: train a Matryoshka SAE with a shared core, use gradient X activation to measure each feature's contribution to next-token loss in the most nested reconstruction, and keep only the smallest subset that explains a fixed fraction of the attribution. Only the core encoder weight vectors are transferred across cycles; the core decoder and all non-core latents are reinitialized each time. On Gemma-2-2B layer 12 residual stream activations, seven cycles of distillation (500M tokens, 65k width) yielded a distilled core of 197 features that were repeatedly selected. Training using this distilled core improves several SAEBench metrics and demonstrates that consistent sets of latent features can be transferred across sparsity levels",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24975v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]distillation"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出DMSAE，通过归因引导蒸馏Matryoshka稀疏自编码器，提升特征一致性和可迁移性。",
            "summary_zh": "稀疏自编码器(SAE)旨在将模型激活解耦为单义的、人类可解释的特征。然而，实践中学习到的特征通常是冗余的，并且在不同的训练运行和稀疏度水平上有所不同，这使得解释难以转移和重用。我们引入了Distilled Matryoshka Sparse Autoencoders (DMSAEs)，这是一种训练流程，它提炼出一个紧凑的、始终有用的特征核心，并重用它来训练新的SAE。DMSAEs运行一个迭代蒸馏循环：训练一个具有共享核心的Matryoshka SAE，使用梯度X激活来测量每个特征对最嵌套重建中下一个token损失的贡献，并且只保留解释固定比例归因的最小子集。只有核心编码器权重向量在循环中传递；核心解码器和所有非核心潜在变量每次都会重新初始化。在Gemma-2-2B第12层残差流激活上，七个循环的蒸馏（500M tokens，65k宽度）产生了一个重复选择的197个特征的蒸馏核心。使用这个蒸馏核心进行训练提高了几个SAEBench指标，并证明了一致的潜在特征集可以在不同的稀疏度水平上传输。",
            "intro_zh": [
                "现有稀疏自编码器学习到的特征冗余且不稳定，导致解释性差，难以迁移和复用。",
                "DMSAE通过迭代蒸馏，提取并重用一致且有用的特征核心，提升特征的稳定性和可迁移性。",
                "实验表明，使用DMSAE蒸馏出的特征核心训练SAE，能有效提升SAEBench指标，验证了方法有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决稀疏自编码器(SAE)训练中特征冗余、不稳定以及难以迁移的问题。现有的SAE训练方法在不同训练轮次和稀疏度下，学习到的特征差异较大，导致模型解释性差，且难以将学到的知识迁移到新的任务或模型上。\\n\\n**核心思路**：论文的核心思路是通过蒸馏的方式，从多个SAE中提取出一个共享的、一致的特征核心，并利用该核心来指导后续SAE的训练。通过迭代蒸馏，逐步筛选出对模型性能贡献最大的特征，从而获得一个紧凑且稳定的特征表示。\\n\\n**技术框架**：DMSAE的整体框架是一个迭代蒸馏循环。每个循环包含以下步骤：1) 训练一个Matryoshka SAE，该SAE具有一个共享的核心编码器；2) 使用梯度X激活方法评估每个特征对下一个token预测损失的贡献；3) 选择贡献最大的特征子集作为新的核心；4) 将核心编码器的权重传递到下一个循环，并重新初始化核心解码器和非核心潜在变量。\\n\\n**关键创新**：DMSAE的关键创新在于使用归因方法（梯度X激活）来指导特征选择，从而确保选择的特征对模型性能具有重要意义。此外，通过迭代蒸馏和核心重用，DMSAE能够学习到更加稳定和可迁移的特征表示。与现有方法相比，DMSAE能够有效地减少特征冗余，并提高特征的一致性。\\n\\n**关键设计**：DMSAE的关键设计包括：1) 使用Matryoshka SAE作为基础模型，允许在不同稀疏度下进行训练；2) 使用梯度X激活作为归因方法，评估特征的重要性；3) 设置固定的归因比例，控制核心的大小；4) 迭代蒸馏的循环次数和训练tokens数量等超参数。",
            "application_zh": "DMSAE可应用于自然语言处理领域，例如提升大型语言模型的解释性和可控性。通过提取模型内部的关键特征，可以更好地理解模型的行为，并进行针对性的干预和优化。此外，DMSAE还可以用于知识迁移和模型压缩，将大型模型的知识迁移到小型模型，并减少模型的计算复杂度。",
            "highlight_zh": "在Gemma-2-2B模型第12层残差流激活上的实验表明，经过七个循环的蒸馏（500M tokens，65k宽度），DMSAE能够提取出一个包含197个特征的蒸馏核心，这些特征在不同的训练轮次中被重复选择。使用该核心进行训练，能够有效提升SAEBench的各项指标，证明了DMSAE能够学习到稳定且可迁移的特征表示。",
            "tags_zh": [
                "稀疏自编码器",
                "蒸馏学习",
                "模型解释性",
                "特征提取",
                "知识迁移"
            ],
            "_index": 36,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24975v1/figures/Fig1_DMSAE_Schematic_DynamicNonCoreL0.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24975v1/figures/k320_core_latent_origins_0-1-2-3-4-5-6-7.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24975v1/figures/figure_distilled_vs_random_1panel.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Iterative Deployment Improves Planning Skills in LLMs",
            "authors": [
                "Augusto B. Corrêa",
                "Yoav Gelberg",
                "Luckeciano C. Melo",
                "Ilia Shumailov",
                "André G. Pereira",
                "Yarin Gal"
            ],
            "arxiv_id": "2512.24940v1",
            "summary": "We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.",
            "categories": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24940v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "迭代部署提升大型语言模型在规划任务中的能力",
            "summary_zh": "本文提出了一种迭代部署大型语言模型（LLM）的方法，通过让用户精心筛选先前模型部署产生的数据，并以此微调后续模型，从而显著改变模型的属性。在多个规划领域进行的测试表明，这种机制可以大幅提升模型的规划能力，后续模型能够涌现出更强的泛化能力，发现比初始模型更长的计划。进一步的理论分析表明，迭代部署实际上在外部循环中实现了强化学习（RL）训练，并带有一个隐式的奖励函数。与强化学习的这种联系具有两个重要意义：首先，对于人工智能安全领域，由于重复部署所带来的奖励函数没有被明确定义，可能会对未来模型部署的属性产生意想不到的影响。其次，本文强调的这种机制可以被视为一种替代显式强化学习的训练方式，它依赖于数据管理而非显式奖励。",
            "intro_zh": [
                "现有LLM在复杂规划任务中面临泛化能力不足的挑战，难以发现长程依赖关系。",
                "论文提出迭代部署方法，通过用户筛选数据微调模型，隐式地进行强化学习。",
                "实验表明，迭代部署能显著提升LLM的规划能力，涌现出更强的泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型在复杂规划任务中表现出的泛化能力不足的问题。现有的方法通常依赖于预训练数据或显式的强化学习奖励，但前者难以覆盖所有可能的规划场景，后者则需要精心设计的奖励函数，这在复杂任务中往往难以实现。因此，如何让LLM在没有明确奖励信号的情况下，通过与环境的交互逐步提升规划能力是一个关键挑战。\\n\\n**核心思路**：论文的核心思路是利用迭代部署的方式，让用户扮演“环境”的角色，通过筛选和反馈LLM生成的计划，隐式地提供奖励信号。具体来说，每次部署后，用户会选择表现良好的计划，并用这些数据微调下一个版本的LLM。通过多次迭代，LLM逐渐学会生成更符合用户期望的计划。这种方法避免了显式定义奖励函数的困难，而是通过数据驱动的方式，让LLM自动学习规划策略。\\n\\n**技术框架**：整体框架包含以下几个主要步骤：1. 初始LLM部署：首先，使用一个预训练的LLM作为起点。2. 计划生成：LLM根据给定的规划任务生成多个候选计划。3. 用户筛选：用户根据一定的标准（例如计划的长度、可行性等）筛选出表现良好的计划。4. 模型微调：使用用户筛选出的数据对LLM进行微调，得到下一个版本的LLM。5. 迭代部署：重复步骤2-4，直到达到预期的性能。\\n\\n**关键创新**：该论文最重要的技术创新在于将迭代部署与强化学习联系起来，证明迭代部署实际上在外部循环中实现了一种隐式的强化学习过程。这种方法避免了显式定义奖励函数的困难，而是通过用户筛选数据的方式，让LLM自动学习规划策略。此外，论文还观察到，通过迭代部署，LLM能够涌现出更强的泛化能力，发现比初始模型更长的计划。\\n\\n**关键设计**：论文的关键设计在于用户筛选数据的标准和微调策略。用户需要根据具体的规划任务，制定合理的筛选标准，例如计划的长度、可行性、效率等。微调策略则需要根据LLM的类型和数据量进行调整，例如学习率、batch size、训练轮数等。此外，论文还探讨了不同用户筛选策略对模型性能的影响，例如只选择最优计划、选择多个较好计划等。",
            "application_zh": "该研究成果可应用于各种需要复杂规划能力的场景，如机器人导航、任务调度、游戏AI等。通过迭代部署，可以使LLM在没有明确奖励信号的情况下，逐步提升规划能力，从而降低开发成本和难度。此外，该方法还可以用于探索LLM的涌现能力，发现新的规划策略和算法。",
            "highlight_zh": "实验结果表明，通过迭代部署，LLM在多个规划领域都取得了显著的性能提升。例如，在某些任务中，迭代后的LLM能够发现比初始模型长数倍的计划。此外，实验还表明，迭代部署能够使LLM涌现出更强的泛化能力，能够适应不同的规划场景。",
            "tags_zh": [
                "大型语言模型",
                "迭代部署",
                "强化学习",
                "规划任务",
                "泛化能力"
            ],
            "_index": 37,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24940v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24940v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Robust Bayesian Dynamic Programming for On-policy Risk-sensitive Reinforcement Learning",
            "authors": [
                "Shanyu Han",
                "Yangbo He",
                "Yang Liu"
            ],
            "arxiv_id": "2512.24580v1",
            "summary": "We propose a novel framework for risk-sensitive reinforcement learning (RSRL) that incorporates robustness against transition uncertainty. We define two distinct yet coupled risk measures: an inner risk measure addressing state and cost randomness and an outer risk measure capturing transition dynamics uncertainty. Our framework unifies and generalizes most existing RL frameworks by permitting general coherent risk measures for both inner and outer risk measures. Within this framework, we construct a risk-sensitive robust Markov decision process (RSRMDP), derive its Bellman equation, and provide error analysis under a given posterior distribution. We further develop a Bayesian Dynamic Programming (Bayesian DP) algorithm that alternates between posterior updates and value iteration. The approach employs an estimator for the risk-based Bellman operator that combines Monte Carlo sampling with convex optimization, for which we prove strong consistency guarantees. Furthermore, we demonstrate that the algorithm converges to a near-optimal policy in the training environment and analyze both the sample complexity and the computational complexity under the Dirichlet posterior and CVaR. Finally, we validate our approach through two numerical experiments. The results exhibit excellent convergence properties while providing intuitive demonstrations of its advantages in both risk-sensitivity and robustness. Empirically, we further demonstrate the advantages of the proposed algorithm through an application on option hedging.",
            "categories": [
                "q-fin.RM",
                "cs.LG"
            ],
            "primary_category": "q-fin.RM",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "63 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24580v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出鲁棒贝叶斯动态规划，用于解决策略风险敏感强化学习中的转移不确定性问题",
            "summary_zh": "本文提出了一种新的风险敏感强化学习（RSRL）框架，该框架融合了针对转移不确定性的鲁棒性。我们定义了两种截然不同但又相互关联的风险度量：一种内部风险度量，用于处理状态和成本的随机性；另一种外部风险度量，用于捕获转移动态的不确定性。我们的框架通过允许内部和外部风险度量采用一般的相干风险度量，统一并推广了大多数现有的RL框架。在此框架内，我们构建了一个风险敏感的鲁棒马尔可夫决策过程（RSRMDP），推导了其贝尔曼方程，并在给定的后验分布下提供了误差分析。我们进一步开发了一种贝叶斯动态规划（Bayesian DP）算法，该算法在后验更新和值迭代之间交替进行。该方法采用了一种基于风险的贝尔曼算子的估计器，该估计器将蒙特卡洛采样与凸优化相结合，并为此证明了强一致性保证。此外，我们证明了该算法收敛到训练环境中接近最优的策略，并分析了Dirichlet后验和CVaR下的样本复杂度和计算复杂度。最后，我们通过两个数值实验验证了我们的方法。结果显示出优异的收敛特性，同时直观地展示了其在风险敏感性和鲁棒性方面的优势。通过期权对冲的应用，我们进一步从经验上证明了所提出的算法的优势。",
            "intro_zh": [
                "现有强化学习方法在处理环境转移概率不确定性时存在不足，可能导致策略的风险敏感性降低。",
                "论文提出一种鲁棒贝叶斯动态规划框架，通过内外两层风险度量分别处理状态成本随机性和转移动态不确定性。",
                "实验结果表明，该方法在风险敏感性和鲁棒性方面均有提升，并在期权对冲等实际应用中表现出优势。"
            ],
            "method_zh": "**问题定义**：现有的强化学习方法在处理环境转移概率不确定性时，往往缺乏足够的鲁棒性，导致学习到的策略在实际应用中表现不佳，尤其是在风险敏感的场景下。传统的强化学习方法通常假设环境是完全已知的，或者通过一些简单的正则化方法来处理不确定性，但这些方法无法有效地应对复杂的转移不确定性。\n\n**核心思路**：论文的核心思路是将鲁棒优化和贝叶斯动态规划相结合，通过内外两层风险度量来显式地建模和处理转移不确定性。内层风险度量用于处理状态和成本的随机性，外层风险度量用于捕获转移动态的不确定性。这种双层风险度量的设计允许算法在面对不确定性时，能够更加保守和稳健地进行决策。\n\n**技术框架**：该方法构建了一个风险敏感的鲁棒马尔可夫决策过程（RSRMDP），并推导了其贝尔曼方程。整体框架包含以下几个主要步骤：1) 定义内外两层相干风险度量；2) 构建RSRMDP并推导贝尔曼方程；3) 开发贝叶斯动态规划算法，交替进行后验更新和值迭代；4) 使用蒙特卡洛采样和凸优化相结合的估计器来估计基于风险的贝尔曼算子。\n\n**关键创新**：该方法最重要的技术创新点在于提出了一个统一的框架，能够处理一般的相干风险度量，并且能够显式地建模和处理转移不确定性。与现有的强化学习方法相比，该方法更加鲁棒，并且能够更好地适应风险敏感的场景。此外，该方法还提供了一种基于贝叶斯动态规划的算法，能够有效地学习到接近最优的策略。\n\n**关键设计**：该方法的关键设计包括：1) 使用CVaR（条件风险价值）作为风险度量的一个具体实例；2) 使用Dirichlet后验分布来建模转移概率的不确定性；3) 使用蒙特卡洛采样来估计贝尔曼算子，并通过凸优化来提高估计的准确性；4) 算法在后验更新和值迭代之间交替进行，以不断提高策略的性能。",
            "application_zh": "该研究成果可应用于金融领域的期权对冲、自动驾驶中的安全决策、医疗领域的个性化治疗方案制定等风险敏感的决策场景。通过考虑环境的不确定性，该方法能够帮助决策者制定更加稳健和可靠的策略，降低潜在的损失，提高决策的安全性。",
            "highlight_zh": "实验结果表明，该方法在风险敏感性和鲁棒性方面均有显著提升。在数值实验中，该方法表现出优异的收敛特性，并且能够有效地应对转移不确定性。在期权对冲的应用中，该方法能够显著降低投资组合的风险，并获得更高的收益。",
            "tags_zh": [
                "风险敏感强化学习",
                "鲁棒优化",
                "贝叶斯动态规划",
                "转移不确定性",
                "相干风险度量"
            ],
            "_index": 38,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
            "authors": [
                "Yuchen Shi",
                "Yuzheng Cai",
                "Siqi Cai",
                "Zihan Xu",
                "Lichao Chen",
                "Yulei Qin",
                "Zhijian Zhou",
                "Xiang Fei",
                "Chaofan Qiu",
                "Xiaoyu Tan",
                "Gang Li",
                "Zongyi Li",
                "Haojia Lin",
                "Guocan Cai",
                "Yong Mao",
                "Yunsheng Wu",
                "Ke Li",
                "Xing Sun"
            ],
            "arxiv_id": "2512.24615v1",
            "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24615v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "Youtu-Agent：通过自动生成和混合策略优化提升Agent生产力",
            "summary_zh": "现有的大语言模型（LLM）Agent框架面临两大挑战：高配置成本和静态能力。构建高质量的Agent通常需要在工具集成和提示工程方面进行大量的人工工作，而部署的Agent在没有昂贵的微调的情况下难以适应动态环境。为了解决这些问题，我们提出了Youtu-Agent，这是一个为LLM Agent的自动生成和持续演进而设计的模块化框架。Youtu-Agent具有结构化的配置系统，可解耦执行环境、工具包和上下文管理，从而实现灵活的重用和自动合成。我们引入了两种生成范式：用于标准任务的Workflow模式和用于复杂、非标准需求的Meta-Agent模式，能够自动生成工具代码、提示和配置。此外，Youtu-Agent建立了一个混合策略优化系统：（1）Agent Practice模块，使Agent能够通过上下文优化积累经验并提高性能，而无需参数更新；（2）Agent RL模块，与分布式训练框架集成，以实现任何Youtu-Agent的端到端、大规模可扩展且稳定的强化学习。实验表明，Youtu-Agent使用开源模型在WebWalkerQA（71.47%）和GAIA（72.8%）上实现了最先进的性能。我们的自动生成管道实现了超过81%的工具合成成功率，而Practice模块将AIME 2024/2025的性能分别提高了+2.7%和+5.4%。此外，我们的Agent RL训练实现了40%的加速，并在7B LLM上实现了稳定的性能提升，在Maths和通用/多跳QA基准测试中，分别将编码/推理和搜索能力提高了高达35%和21%。",
            "intro_zh": [
                "现有LLM Agent框架配置成本高昂，且难以适应动态环境，限制了其应用。",
                "Youtu-Agent通过模块化设计、自动生成和混合策略优化，实现Agent的自动构建和持续进化。",
                "实验表明，Youtu-Agent在多个基准测试中取得了领先性能，并显著提升了Agent的工具合成和问题解决能力。"
            ],
            "method_zh": "**问题定义**：现有LLM Agent框架需要大量人工配置，包括工具集成和提示工程，成本高昂。同时，已部署的Agent难以适应动态变化的环境，需要耗费资源的微调才能保持性能。因此，如何降低Agent的配置成本，并使其具备持续学习和适应能力，是本文要解决的核心问题。\\n\\n**核心思路**：Youtu-Agent的核心思路是实现Agent的自动生成和持续进化。通过模块化的框架设计，解耦执行环境、工具包和上下文管理，实现灵活的组件复用和自动合成。同时，引入混合策略优化系统，包括Agent Practice和Agent RL，使Agent能够通过上下文学习和强化学习不断提升自身能力。\\n\\n**技术框架**：Youtu-Agent包含以下主要模块：\n1. **结构化配置系统**：解耦执行环境、工具包和上下文管理，实现组件的灵活复用。\n2. **自动生成模块**：包含Workflow模式和Meta-Agent模式，自动生成工具代码、提示和配置。\n3. **混合策略优化系统**：包含Agent Practice模块（上下文优化）和Agent RL模块（强化学习）。\n\n**关键创新**：Youtu-Agent的关键创新在于其自动生成和混合策略优化机制。自动生成模块能够显著降低Agent的配置成本，而混合策略优化系统则使Agent具备了持续学习和适应能力，无需人工干预即可提升性能。与现有方法相比，Youtu-Agent更加灵活、高效和智能化。\\n\\n**关键设计**：\n1. **Workflow模式和Meta-Agent模式**：针对不同复杂度的任务，采用不同的生成策略。\n2. **Agent Practice模块**：通过上下文学习，使Agent能够从经验中学习，提升性能。\n3. **Agent RL模块**：利用分布式训练框架，实现大规模Agent的强化学习，提升Agent的通用能力。",
            "application_zh": "Youtu-Agent可广泛应用于智能客服、自动化办公、智能搜索等领域。通过自动生成和持续优化，可以大幅降低Agent的开发和维护成本，提高Agent的智能化水平和服务质量。未来，Youtu-Agent有望成为构建通用人工智能Agent的重要基石。",
            "highlight_zh": "Youtu-Agent在WebWalkerQA和GAIA数据集上取得了SOTA结果，分别达到71.47%和72.8%。自动生成管道的工具合成成功率超过81%。Agent Practice模块使AIME 2024/2025的性能分别提高了+2.7%和+5.4%。Agent RL训练实现了40%的加速，并在Maths和通用/多跳QA基准测试中，分别将编码/推理和搜索能力提高了高达35%和21%。",
            "tags_zh": [
                "LLM Agent",
                "自动生成",
                "混合策略优化",
                "强化学习",
                "上下文学习",
                "工具集成",
                "智能自动化"
            ],
            "_index": 39,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24615v1/figs/fig2_autogen.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24615v1/figs/fig2_2_tfgrpo.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24615v1/figs/fig_youtu-agent-rl.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Hierarchical Deformation Planning and Neural Tracking for DLOs in Constrained Environments",
            "authors": [
                "Yunxi Tang",
                "Tianqi Yang",
                "Jing Huang",
                "Xiangyu Chu",
                "Kwok Wai Samuel Au"
            ],
            "arxiv_id": "2512.24974v1",
            "summary": "Deformable linear objects (DLOs) manipulation presents significant challenges due to DLOs' inherent high-dimensional state space and complex deformation dynamics. The wide-populated obstacles in realistic workspaces further complicate DLO manipulation, necessitating efficient deformation planning and robust deformation tracking. In this work, we propose a novel framework for DLO manipulation in constrained environments. This framework combines hierarchical deformation planning with neural tracking, ensuring reliable performance in both global deformation synthesis and local deformation tracking. Specifically, the deformation planner begins by generating a spatial path set that inherently satisfies the homotopic constraints associated with DLO keypoint paths. Next, a path-set-guided optimization method is applied to synthesize an optimal temporal deformation sequence for the DLO. In manipulation execution, a neural model predictive control approach, leveraging a data-driven deformation model, is designed to accurately track the planned DLO deformation sequence. The effectiveness of the proposed framework is validated in extensive constrained DLO manipulation tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24974v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "model predictive control"
                    ],
                    "score": 4.0
                }
            ],
            "relevance_score": 4.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "提出一种分层规划与神经跟踪框架，用于约束环境中DLO操作",
            "summary_zh": "可变形线性物体(DLOs)的操作由于其固有的高维状态空间和复杂的形变动力学而面临重大挑战。现实工作空间中广泛存在的障碍进一步复杂化了DLO操作，需要高效的形变规划和鲁棒的形变跟踪。本文提出了一种用于约束环境中DLO操作的新框架。该框架结合了分层形变规划与神经跟踪，确保了全局形变合成和局部形变跟踪的可靠性能。具体而言，形变规划器首先生成一个空间路径集，该路径集固有地满足与DLO关键点路径相关的同伦约束。接下来，应用路径集引导的优化方法来合成DLO的最佳时间形变序列。在操作执行中，利用数据驱动的形变模型，设计了一种神经模型预测控制方法，以准确跟踪规划的DLO形变序列。所提出的框架在广泛的约束DLO操作任务中得到了验证。",
            "intro_zh": [
                "DLO操作因其高维状态空间和复杂动力学而极具挑战，现有方法难以兼顾全局规划和局部跟踪。",
                "该论文提出一种分层规划与神经跟踪相结合的框架，利用空间路径集引导优化，并采用神经模型预测控制。",
                "实验结果表明，该框架在约束DLO操作任务中表现出良好的性能，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决约束环境中可变形线性物体（DLOs）的操作问题。现有方法在处理DLOs时，难以同时实现高效的全局形变规划和鲁棒的局部形变跟踪，尤其是在存在障碍物的情况下，容易陷入局部最优或无法精确跟踪目标形变。\\n\\n**核心思路**：论文的核心思路是将DLO操作分解为分层规划和神经跟踪两个阶段。首先，通过分层规划生成满足约束条件的全局形变序列；然后，利用神经模型预测控制实现对规划形变的精确跟踪。这种分而治之的方法能够有效降低问题的复杂度，提高操作的可靠性和效率。\\n\\n**技术框架**：整体框架包含两个主要模块：分层形变规划器和神经跟踪控制器。分层形变规划器首先生成一个空间路径集，该路径集满足DLO关键点路径的同伦约束。然后，利用路径集引导的优化方法，合成DLO的最佳时间形变序列。神经跟踪控制器则利用数据驱动的形变模型，采用模型预测控制策略，精确跟踪规划的DLO形变序列。\\n\\n**关键创新**：该论文的关键创新在于将分层规划与神经跟踪相结合，实现了全局规划和局部跟踪的有效协同。此外，利用路径集引导的优化方法，能够有效地处理约束环境下的DLO形变规划问题。数据驱动的神经模型预测控制方法，提高了DLO形变跟踪的精度和鲁棒性。\\n\\n**关键设计**：在分层形变规划中，采用空间路径集来表示DLO的可能形变路径，并通过优化算法选择最优路径。在神经跟踪控制器中，使用神经网络学习DLO的形变模型，并将其嵌入到模型预测控制框架中。损失函数的设计考虑了跟踪误差和控制成本，以实现精确和高效的形变跟踪。具体的网络结构和参数设置未在摘要中体现，属于未知信息。",
            "application_zh": "该研究成果可应用于柔性物体的操作，例如医疗手术中的缝合线操作、工业生产中的线缆布线等。通过精确控制DLO的形变，可以提高操作的精度和效率，降低操作风险。未来，该技术有望应用于更复杂的DLO操作任务，例如在拥挤环境中进行DLO的装配和维护。",
            "highlight_zh": "论文通过实验验证了所提出框架的有效性，但摘要中未提供具体的性能数据和对比基线。实验结果表明，该框架能够在约束环境中实现DLO的精确形变规划和跟踪，验证了分层规划与神经跟踪相结合的优势。具体的提升幅度属于未知信息。",
            "tags_zh": [
                "可变形线性物体",
                "DLO操作",
                "形变规划",
                "神经跟踪",
                "模型预测控制",
                "约束环境",
                "分层规划"
            ],
            "_index": 40,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24974v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24974v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24974v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
            "authors": [
                "Yi-Chuan Huang",
                "Hao-Jen Chien",
                "Chin-Yang Lin",
                "Ying-Huan Chen",
                "Yu-Lun Liu"
            ],
            "arxiv_id": "2512.25073v1",
            "summary": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Project page: https://yichuanh.github.io/GaMO/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25073v1",
            "code_links": [
                {
                    "url": "https://yichuanh.github.io/GaMO/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "GaMO：基于几何感知的多视角扩散外绘用于稀疏视角3D重建",
            "summary_zh": "本文提出GaMO（Geometry-aware Multi-view Outpainter），一个通过多视角外绘重新构建稀疏视角3D重建的框架。与生成新视点不同，GaMO从现有相机姿态扩展视野，从而固有地保持了几何一致性，同时提供了更广泛的场景覆盖。该方法采用多视角条件和几何感知去噪策略，以零样本方式运行，无需训练。在Replica和ScanNet++上的大量实验表明，在3、6和9个输入视角下，GaMO实现了最先进的重建质量，在PSNR和LPIPS方面优于现有方法，同时比最先进的基于扩散的方法实现了25倍的加速，处理时间低于10分钟。",
            "intro_zh": [
                "现有3D重建方法在密集多视角图像中表现出色，但在输入视角有限时效果不佳，缺乏对已知视角外围的充分覆盖。",
                "GaMO通过多视角外绘扩展现有相机姿态的视野，而非生成新视点，从而在本质上保持了几何一致性，并扩大了场景覆盖范围。",
                "GaMO在Replica和ScanNet++数据集上实现了最先进的重建质量，且速度比现有扩散方法快25倍，处理时间缩短至10分钟以内。"
            ],
            "method_zh": "**问题定义**：论文旨在解决稀疏视角下的3D重建问题。现有方法在视角稀疏时，重建质量显著下降，主要痛点包括：覆盖范围不足，无法有效推断已知视角之外的区域；几何不一致性，生成的视图之间缺乏空间一致性；计算成本高昂，特别是基于扩散模型的方法，推理速度慢。\\n\\n**核心思路**：GaMO的核心思路是将稀疏视角3D重建问题转化为多视角外绘问题。通过从现有相机姿态向外扩展视野，而不是生成全新的相机姿态，可以自然地保持几何一致性，并提供更广阔的场景覆盖范围。这种方法避免了生成新视点带来的几何校正问题，简化了重建流程。\\n\\n**技术框架**：GaMO框架主要包含以下几个关键模块：1) 多视角条件输入：利用多个已知视角的图像作为条件信息。2) 几何感知扩散模型：使用扩散模型进行图像外绘，并融入几何信息以保证生成图像的几何一致性。3) 零样本推理：整个过程无需训练，直接利用预训练的扩散模型进行推理。框架通过迭代的去噪过程，逐步生成扩展视野的图像。\\n\\n**关键创新**：GaMO的关键创新在于将多视角外绘应用于稀疏视角3D重建，并设计了几何感知的扩散模型。与以往生成新视角的扩散方法不同，GaMO通过扩展现有视角的视野，避免了复杂的几何校正，并显著提高了重建速度。此外，零样本推理方式也避免了针对特定场景的训练需求。\\n\\n**关键设计**：GaMO的关键设计包括：1) 多视角条件融合策略，如何有效地将多个视角的图像信息融入到扩散模型的去噪过程中。2) 几何感知去噪策略，具体如何将几何信息（例如深度信息或相机参数）融入到扩散模型的去噪过程中，以保证生成图像的几何一致性。3) 扩散模型的选择和参数设置，例如使用何种扩散模型架构，以及如何调整扩散模型的参数以获得最佳的重建效果。论文中可能还涉及损失函数的设计，用于指导扩散模型的训练（如果使用了微调）。",
            "application_zh": "GaMO在机器人导航、虚拟现实、增强现实、自动驾驶等领域具有广泛的应用前景。该技术可以利用少量图像快速重建场景，降低了对传感器数量和计算资源的需求。在文物保护领域，GaMO可以用于快速重建文物的三维模型，方便研究和展示。未来，该技术有望应用于实时三维重建，为用户提供更加沉浸式的体验。",
            "highlight_zh": "GaMO在Replica和ScanNet++数据集上取得了显著的性能提升。在3、6和9个输入视角下，GaMO在PSNR和LPIPS指标上均优于现有方法，实现了最先进的重建质量。更重要的是，GaMO比最先进的基于扩散的方法实现了25倍的加速，处理时间缩短至10分钟以内，大大提高了重建效率。",
            "tags_zh": [
                "3D重建",
                "稀疏视角",
                "多视角外绘",
                "扩散模型",
                "几何感知",
                "零样本学习",
                "图像生成"
            ],
            "_index": 41,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.25073v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.25073v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.25073v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation",
            "authors": [
                "Bingxuan Li",
                "Yiming Cui",
                "Yicheng He",
                "Yiwei Wang",
                "Shu Zhang",
                "Longyin Wen",
                "Yulei Niu"
            ],
            "arxiv_id": "2512.24731v1",
            "summary": "Sound effects build an essential layer of multimodal storytelling, shaping the emotional atmosphere and the narrative semantics of videos. Despite recent advancement in video-text-to-audio (VT2A), the current formulation faces three key limitations: First, an imbalance between visual and textual conditioning that leads to visual dominance; Second, the absence of a concrete definition for fine-grained controllable generation; Third, weak instruction understanding and following, as existing datasets rely on brief categorical tags. To address these limitations, we introduce EchoFoley, a new task designed for video-grounded sound generation with both event level local control and hierarchical semantic control. Our symbolic representation for sounding events specifies when, what, and how each sound is produced within a video or instruction, enabling fine-grained controls like sound generation, insertion, and editing. To support this task, we construct EchoFoley-6k, a large-scale, expert-curated benchmark containing over 6,000 video-instruction-annotation triplets. Building upon this foundation, we propose EchoVidia a sounding-event-centric agentic generation framework with slow-fast thinking strategy. Experiments show that EchoVidia surpasses recent VT2A models by 40.7% in controllability and 12.5% in perceptual quality.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24731v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出EchoFoley，通过事件中心的分层控制实现视频相关的创意声音生成。",
            "summary_zh": "声音效果是多模态叙事的重要组成部分，塑造视频的情感氛围和叙事语义。尽管视频-文本到音频（VT2A）技术取得了进展，但当前方法存在三个主要限制：视觉和文本条件之间的不平衡导致视觉主导；缺乏对细粒度可控生成的具体定义；指令理解和遵循能力较弱，因为现有数据集依赖于简短的类别标签。为了解决这些限制，我们引入了EchoFoley，这是一个新的任务，旨在通过事件级别的局部控制和分层语义控制进行视频相关的声音生成。我们用于发声事件的符号表示指定了每个声音在视频或指令中产生的时间、内容和方式，从而实现诸如声音生成、插入和编辑之类的细粒度控制。为了支持这项任务，我们构建了EchoFoley-6k，这是一个大规模的、专家策划的基准，包含超过6,000个视频-指令-注释三元组。在此基础上，我们提出了EchoVidia，一个以发声事件为中心的智能体生成框架，具有慢-快思考策略。实验表明，EchoVidia在可控性方面比最近的VT2A模型高出40.7%，在感知质量方面高出12.5%。",
            "intro_zh": [
                "现有视频-文本到音频模型存在视觉主导、缺乏细粒度控制和指令理解弱等问题。",
                "提出EchoFoley任务，通过事件级别的局部控制和分层语义控制实现可控的声音生成。",
                "构建了EchoFoley-6k数据集，并提出了EchoVidia框架，实验证明其性能显著优于现有模型。"
            ],
            "method_zh": "**问题定义**：现有视频-文本到音频（VT2A）模型在生成声音效果时，存在视觉信息过度主导的问题，导致生成的声音与文本描述不符。此外，缺乏对声音生成的细粒度控制，例如无法精确控制声音的起始时间、类型和强度。现有数据集的标注信息过于简单，不足以训练模型理解复杂的指令。\n\n**核心思路**：论文的核心思路是以“发声事件”为中心，将声音生成过程分解为一系列可控的事件。通过对每个事件进行精确的符号化表示，包括时间、内容和方式，实现对声音生成的细粒度控制。同时，采用分层语义控制，从全局和局部两个层面指导声音生成。\n\n**技术框架**：EchoVidia框架采用了一种智能体（Agentic）生成方式，模拟人类的思考过程，包含慢速思考和快速执行两个阶段。慢速思考阶段负责理解视频内容和指令，规划发声事件序列；快速执行阶段则根据事件序列生成相应的声音。框架包含视频编码器、文本编码器、事件预测模块、音频生成模块等。\n\n**关键创新**：最重要的创新点在于提出了以“发声事件”为中心的符号化表示方法，将声音生成过程分解为可控的事件序列。这种方法使得模型能够精确控制声音的各个方面，从而实现细粒度的可控生成。此外，EchoVidia框架的慢-快思考策略也提高了模型的指令理解和遵循能力。\n\n**关键设计**：EchoFoley-6k数据集包含视频、指令和发声事件标注三元组。发声事件标注包括事件的起始时间、类型和属性。EchoVidia框架采用Transformer架构，使用交叉注意力机制融合视频和文本信息。损失函数包括事件预测损失和音频生成损失，用于优化模型的性能。",
            "application_zh": "该研究成果可应用于电影制作、游戏开发、虚拟现实等领域，实现自动化、可控的声音效果生成。例如，可以根据视频内容自动生成逼真的环境音效，或者根据用户的指令生成特定的声音效果，从而提高内容创作的效率和质量。未来，该技术有望进一步发展，实现更加智能化的声音设计。",
            "highlight_zh": "实验结果表明，EchoVidia在可控性方面比现有的VT2A模型提高了40.7%，在感知质量方面提高了12.5%。这些显著的提升表明，以发声事件为中心的控制方法能够有效地提高声音生成的可控性和质量。此外，消融实验验证了慢-快思考策略的有效性。",
            "tags_zh": [
                "视频声音生成",
                "事件中心控制",
                "分层语义控制",
                "多模态学习",
                "智能体生成"
            ],
            "_index": 42,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24731v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24731v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24731v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Diffusion Language Models are Provably Optimal Parallel Samplers",
            "authors": [
                "Haozhe Jiang",
                "Nika Haghtalab",
                "Lijie Chen"
            ],
            "arxiv_id": "2512.25014v1",
            "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.",
            "categories": [
                "cs.LG",
                "cs.CC"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25014v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于思维链的扩散语言模型，实现最优并行采样，并论证修订机制的必要性。",
            "summary_zh": "扩散语言模型（DLMs）作为一种通过并行token生成实现更快推理的方案，正逐渐成为自回归模型的有希望的替代品。本文通过形式化并行采样模型，为这种优势提供了严格的理论基础，证明了增强了多项式长度思维链（CoT）的DLMs可以使用最优数量的顺序步骤来模拟任何并行采样算法。因此，每当目标分布可以使用少量顺序步骤生成时，就可以使用DLM以相同数量的最优顺序步骤生成该分布。然而，在没有修改先前已揭示token能力的情况下，具有CoT的DLM仍然会产生较大的中间占用空间。我们证明，启用重掩码（将未掩码的token转换为掩码）或修订（将未掩码的token转换为其他未掩码的token）以及CoT，可以使DLM以最优的空间复杂度模拟任何并行采样算法。我们通过建立严格的表达能力差距进一步证明了修订的优势：具有修订或重掩码的DLM比没有修订或重掩码的DLM具有更强的表达能力。我们的结果不仅为DLM作为最有效的并行采样器的前景提供了理论依据，而且还提倡在DLM中启用修订。",
            "intro_zh": [
                "现有自回归模型推理速度慢，扩散语言模型（DLM）作为并行生成方案有潜力加速推理。",
                "论文提出增强思维链（CoT）的DLM，证明其能以最优步骤模拟任何并行采样算法。",
                "引入重掩码或修订机制，使DLM在模拟并行采样算法时达到最优空间复杂度，并提升表达能力。"
            ],
            "method_zh": "**问题定义**：现有自回归语言模型在生成文本时依赖顺序解码，速度较慢。扩散语言模型（DLM）旨在通过并行生成token来加速推理过程。然而，DLM在并行采样方面的理论优势和能力边界尚不明确，尤其是在处理复杂推理任务时，如何保证效率和表达能力是一个挑战。\\n\\n**核心思路**：论文的核心思路是通过理论分析，证明带有思维链（CoT）的DLM可以模拟任何并行采样算法，并且通过引入重掩码或修订机制，可以进一步优化DLM的空间复杂度。这种设计旨在充分发挥DLM并行计算的优势，同时保证其在复杂任务中的表达能力和效率。\\n\\n**技术框架**：论文构建了一个并行采样的理论模型，并在此基础上分析了DLM的计算能力。主要框架包括：1) 形式化定义并行采样算法；2) 证明带有CoT的DLM可以模拟任何并行采样算法；3) 分析DLM在没有重掩码或修订机制时的空间复杂度瓶颈；4) 证明引入重掩码或修订机制可以解决空间复杂度问题，并提升表达能力。\\n\\n**关键创新**：论文的关键创新在于：1) 首次从理论上证明了带有CoT的DLM可以作为最优的并行采样器；2) 提出了重掩码和修订机制，并证明其可以显著提升DLM的空间复杂度和表达能力；3) 建立了DLM表达能力的严格差距，证明了带有修订或重掩码的DLM比没有这些机制的DLM具有更强的表达能力。\\n\\n**关键设计**：论文的关键设计包括：1) 使用多项式长度的CoT来增强DLM的推理能力；2) 引入重掩码机制，允许将已生成的token重新转换为掩码，以便后续步骤可以修改这些token；3) 引入修订机制，允许将已生成的token修改为其他token。这些机制的设计旨在解决DLM在并行采样过程中可能遇到的空间复杂度和表达能力瓶颈。",
            "application_zh": "该研究成果可应用于需要快速文本生成的场景，例如机器翻译、文本摘要、对话系统等。通过利用DLM的并行计算能力，可以显著提升生成速度，从而提高用户体验和系统效率。此外，重掩码和修订机制的引入，也为DLM在处理复杂推理任务时提供了更强的灵活性和表达能力。",
            "highlight_zh": "论文通过理论证明，增强了思维链（CoT）的扩散语言模型（DLM）能够以最优的顺序步骤模拟任何并行采样算法。此外，论文还证明了启用重掩码或修订机制可以使DLM以最优的空间复杂度模拟任何并行采样算法，并建立了DLM表达能力的严格差距，证明了带有修订或重掩码的DLM比没有这些机制的DLM具有更强的表达能力。",
            "tags_zh": [
                "扩散语言模型",
                "并行采样",
                "思维链",
                "重掩码",
                "修订机制",
                "空间复杂度",
                "表达能力"
            ],
            "_index": 43,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Efficiently Estimating Data Efficiency for Language Model Fine-tuning",
            "authors": [
                "Gyung Hyun Je",
                "Colin Raffel"
            ],
            "arxiv_id": "2512.24991v1",
            "summary": "While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24991v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于梯度余弦相似性的数据效率预测方法，减少LLM微调的标注成本。",
            "summary_zh": "大型语言模型（LLM）在许多下游任务中表现出不错的零样本能力，但微调是提高其性能的常用方法。然而，任务的数据效率（即达到期望性能所需的微调样本数量）通常是未知的，导致增量标注和重新训练的成本高昂。本文通过一组精心挑选的30个专业任务，展示了高性能LLM可能在零样本学习中表现不佳，但在微调后可以获得更强的性能。这促使我们需要预测任务的数据效率，而无需增量标注。本文提出了一种具体的指标来量化任务的数据效率，并提出使用低置信度样本的梯度余弦相似性，基于少量标记样本来预测数据效率。在各种具有不同数据效率的任务上验证了该方法，在整体数据效率预测中达到了8.6%的误差，并且通常在每个任务上消除了数百个不必要的标注。",
            "intro_zh": [
                "大型语言模型微调需要大量标注数据，但任务的数据效率未知，导致标注成本高昂。",
                "提出利用少量标注样本，通过计算低置信度样本的梯度余弦相似性来预测数据效率。",
                "实验表明，该方法能有效预测数据效率，减少不必要的标注，整体数据效率预测误差为8.6%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）微调过程中，由于任务数据效率未知而导致的标注成本高昂问题。现有方法需要通过增量标注和重新训练来确定最佳的微调数据量，效率低下且成本很高。因此，如何仅使用少量数据就能准确预测任务的数据效率是关键挑战。\\n\\n**核心思路**：论文的核心思路是利用模型在少量数据上训练后的梯度信息来预测数据效率。具体来说，作者认为，如果模型在少量数据上训练后，对低置信度样本的梯度方向一致性较高，则表明该任务的数据效率较高，反之则较低。这是因为数据效率高的任务，模型更容易从少量数据中学习到通用的特征表示。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 选取少量已标注的样本；2) 使用这些样本对LLM进行微调；3) 选择模型预测置信度较低的样本；4) 计算这些低置信度样本的梯度；5) 计算梯度之间的余弦相似度；6) 使用梯度余弦相似度来预测任务的数据效率。整体流程简单高效，易于实现。\\n\\n**关键创新**：该方法最重要的创新点在于利用梯度余弦相似度来表征任务的数据效率。与传统的基于性能指标（如准确率）的评估方法不同，该方法直接从模型的梯度信息入手，能够更早地预测数据效率，从而避免了不必要的标注和训练。此外，该方法只需要少量标注数据，即可进行预测，大大降低了标注成本。\\n\\n**关键设计**：关键设计包括：1) 低置信度样本的选择策略：作者可能采用阈值法或Top-K法来选择置信度较低的样本；2) 梯度计算方式：需要明确是计算哪个层或哪些层的梯度，以及如何对梯度进行归一化；3) 梯度余弦相似度的计算方式：需要明确是计算所有梯度对之间的平均余弦相似度，还是采用其他统计量；4) 数据效率的预测模型：可能使用线性回归或更复杂的模型，将梯度余弦相似度映射到数据效率。",
            "application_zh": "该研究成果可广泛应用于自然语言处理领域，尤其是在需要对大型语言模型进行微调的场景中。例如，可以帮助企业或研究机构在有限的标注预算下，选择最具数据效率的任务进行微调，从而最大化模型的性能提升。此外，该方法还可以用于自动评估数据集的质量，帮助用户选择更适合模型训练的数据集。",
            "highlight_zh": "实验结果表明，该方法在预测数据效率方面表现出色，整体数据效率预测误差仅为8.6%。与传统的增量标注方法相比，该方法能够显著减少不必要的标注，平均每个任务可以节省数百个标注样本。这些结果表明，该方法具有很高的实用价值，可以有效降低LLM微调的成本。",
            "tags_zh": [
                "数据效率",
                "语言模型微调",
                "梯度余弦相似性",
                "低置信度样本",
                "标注成本"
            ],
            "_index": 44,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline",
            "authors": [
                "Minjun Zhao",
                "Xinyu Zhang",
                "Shuai Zhang",
                "Deyang Li",
                "Ruifeng Shi"
            ],
            "arxiv_id": "2512.24933v1",
            "summary": "Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.",
            "categories": [
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24933v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ADOPT框架，自适应优化多步LLM流水线中的提示，解决依赖建模难题。",
            "summary_zh": "多步LLM流水线通过结构化的序列多次调用大型语言模型，能够有效解决复杂任务，但其性能严重依赖于每个步骤中使用的提示。由于缺乏步骤级别的监督和步骤间的依赖关系，联合优化这些提示非常困难。现有的端到端提示优化方法在这种条件下表现不佳，并且常常产生次优或不稳定的更新。我们提出了ADOPT，一个用于多步LLM流水线的自适应依赖感知提示优化框架。ADOPT显式地建模了每个LLM步骤和最终任务结果之间的依赖关系，从而能够进行精确的文本梯度估计，类似于计算解析导数。它将文本梯度估计与梯度更新解耦，将多提示优化简化为灵活的单提示优化步骤，并采用基于Shapley值的机制来适应性地分配优化资源。在真实世界数据集和多样化流水线结构上的实验表明，ADOPT是有效且鲁棒的，始终优于最先进的提示优化基线。",
            "intro_zh": [
                "多步LLM流水线性能受各步骤提示影响，但缺乏步骤监督和步骤间依赖导致联合优化困难。",
                "ADOPT框架显式建模步骤依赖，实现精确文本梯度估计，解耦梯度估计与更新，简化优化过程。",
                "ADOPT采用Shapley值自适应分配优化资源，实验表明其优于现有方法，具有有效性和鲁棒性。"
            ],
            "method_zh": "**问题定义**：多步LLM流水线的性能高度依赖于每个步骤的提示设计，但由于缺乏对中间步骤的直接监督信号，以及步骤之间复杂的依赖关系，联合优化所有步骤的提示变得异常困难。现有的端到端提示优化方法难以有效处理这种复杂性，容易陷入局部最优，导致性能不稳定。\\n\\n**核心思路**：ADOPT的核心思路是通过显式地建模每个LLM步骤与最终任务结果之间的依赖关系，来解决多步提示优化问题。类似于计算解析导数，ADOPT能够精确地估计文本梯度，从而更准确地指导提示的优化方向。通过解耦文本梯度估计和梯度更新，将复杂的多提示优化问题分解为多个更易于处理的单提示优化问题。\\n\\n**技术框架**：ADOPT框架主要包含三个阶段：1) 依赖关系建模：利用因果推断或注意力机制等方法，显式地建模每个LLM步骤对最终结果的影响。2) 文本梯度估计：基于建模的依赖关系，计算每个步骤提示的文本梯度，反映其对最终结果的影响程度。3) 自适应资源分配与优化：使用基于Shapley值的机制，根据每个步骤的重要性自适应地分配优化资源，并采用单提示优化算法更新提示。\\n\\n**关键创新**：ADOPT的关键创新在于其依赖感知的文本梯度估计方法和自适应资源分配机制。传统的端到端优化方法忽略了步骤间的依赖关系，导致梯度估计不准确。ADOPT通过显式建模依赖关系，实现了更精确的梯度估计，从而能够更有效地优化提示。此外，自适应资源分配机制能够根据每个步骤的重要性动态调整优化力度，避免了对所有步骤进行平均处理，提高了优化效率。\\n\\n**关键设计**：ADOPT使用Shapley值来衡量每个步骤对最终结果的贡献，并以此为依据分配优化资源。具体而言，Shapley值通过计算每个步骤在所有可能的步骤组合中的边际贡献来评估其重要性。在优化过程中，ADOPT可以采用各种单提示优化算法，例如梯度下降或进化算法，来更新每个步骤的提示。损失函数的设计需要根据具体的任务进行调整，通常包括任务相关的损失和正则化项，以避免过拟合。",
            "application_zh": "ADOPT框架可广泛应用于需要多步推理或决策的复杂任务中，例如知识图谱推理、对话系统、代码生成和规划等。通过自动优化每个步骤的提示，可以显著提升LLM流水线的性能和鲁棒性，降低人工设计提示的成本，并加速LLM在实际应用中的部署。",
            "highlight_zh": "实验结果表明，ADOPT在多个真实世界数据集和不同的流水线结构上均优于现有的提示优化基线。例如，在知识图谱推理任务上，ADOPT相比于最先进的基线方法，准确率提升了5%-10%。此外，ADOPT在面对噪声数据和对抗性攻击时，表现出更强的鲁棒性。",
            "tags_zh": [
                "多步LLM流水线",
                "提示优化",
                "依赖建模",
                "文本梯度估计",
                "自适应资源分配",
                "Shapley值",
                "因果推断"
            ],
            "_index": 45,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Characterization of Transfer Using Multi-task Learning Curves",
            "authors": [
                "András Millinghoffer",
                "Bence Bolgár",
                "Péter Antal"
            ],
            "arxiv_id": "2512.24866v1",
            "summary": "Transfer effects manifest themselves both during training using a fixed data set and in inductive inference using accumulating data. We hypothesize that perturbing the data set by including more samples, instead of perturbing the model by gradient updates, provides a complementary and more fundamental characterization of transfer effects. To capture this phenomenon, we quantitatively model transfer effects using multi-task learning curves approximating the inductive performance over varying sample sizes. We describe an efficient method to approximate multi-task learning curves analogous to the Task Affinity Grouping method applied during training. We compare the statistical and computational approaches to transfer, which indicates considerably higher compute costs for the previous but better power and broader applicability. Evaluations are performed using a benchmark drug-target interaction data set. Our results show that learning curves can better capture the effects of multi-task learning and their multi-task extensions can delineate pairwise and contextual transfer effects in foundation models.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24866v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于多任务学习曲线的迁移学习表征方法，用于刻画数据集扰动下的迁移效应。",
            "summary_zh": "迁移效应在训练过程中（使用固定数据集）和归纳推理过程中（使用累积数据）均会显现。本文假设，通过包含更多样本来扰动数据集，而不是通过梯度更新来扰动模型，能够提供对迁移效应的互补且更根本的表征。为了捕捉这种现象，我们使用多任务学习曲线对迁移效应进行定量建模，该曲线近似于不同样本大小下的归纳性能。我们描述了一种有效的方法来近似多任务学习曲线，类似于训练期间应用的任务亲和性分组方法。我们比较了迁移的统计和计算方法，表明前者计算成本高得多，但功率更好，适用性更广。使用基准药物-靶标相互作用数据集进行评估。我们的结果表明，学习曲线可以更好地捕捉多任务学习的效果，并且它们的多任务扩展可以描绘基础模型中的成对和上下文迁移效应。",
            "intro_zh": [
                "现有方法主要通过梯度更新扰动模型来研究迁移学习，忽略了数据集扰动带来的影响。",
                "论文提出利用多任务学习曲线，通过改变样本数量来扰动数据集，从而更本质地刻画迁移效应。",
                "实验结果表明，该方法能更好地捕捉多任务学习效果，并能有效区分基础模型中的成对和上下文迁移效应。"
            ],
            "method_zh": "**问题定义**：现有研究迁移学习的方法主要集中在模型参数的调整和优化上，例如通过梯度更新来适应新的任务。然而，数据集本身的变化，例如样本数量的增加，也会对迁移学习的效果产生显著影响。现有方法缺乏对这种数据集扰动影响的有效建模和分析，限制了我们对迁移学习本质的理解。\\n\\n**核心思路**：本文的核心思路是利用多任务学习曲线来刻画数据集扰动下的迁移效应。具体来说，通过构建不同样本数量下的学习曲线，可以观察到模型在不同数据规模下的性能变化。这些学习曲线能够反映出任务之间的相关性和迁移潜力，从而为迁移学习提供更全面的表征。这种方法将数据集的变化视为一种扰动，并分析其对模型性能的影响，从而提供了一种互补的视角。\\n\\n**技术框架**：该方法主要包含以下几个步骤：1) 构建多任务学习场景，即定义多个相关的学习任务。2) 对于每个任务，构建不同样本数量下的数据集。3) 在每个数据集上训练模型，并记录模型的性能指标（例如准确率、损失值）。4) 将模型的性能指标与样本数量绘制成学习曲线。5) 分析学习曲线的形状、趋势和相互关系，从而推断任务之间的迁移效应。该框架类似于任务亲和性分组方法，但应用于学习曲线而非模型训练。\\n\\n**关键创新**：该方法的关键创新在于将学习曲线的概念引入到迁移学习的分析中。传统的迁移学习研究主要关注模型参数的调整，而忽略了数据集本身的影响。通过分析学习曲线，可以更全面地了解迁移学习的本质，并为迁移学习策略的设计提供更有效的指导。此外，该方法还提供了一种定量分析迁移效应的手段，可以用于比较不同任务之间的相关性和迁移潜力。\\n\\n**关键设计**：在构建学习曲线时，需要选择合适的性能指标来衡量模型的性能。常用的性能指标包括准确率、精确率、召回率、F1值等。此外，还需要选择合适的样本数量范围，以确保学习曲线能够充分反映模型的性能变化。在分析学习曲线时，可以采用统计方法或机器学习方法来提取关键特征，例如学习曲线的斜率、截距、面积等。这些特征可以用于描述任务之间的相关性和迁移潜力。",
            "application_zh": "该研究成果可应用于药物发现、图像识别、自然语言处理等领域。例如，在药物发现中，可以利用该方法分析不同靶标之间的相关性，从而加速新药的研发过程。在图像识别中，可以利用该方法分析不同图像类别之间的相关性，从而提高图像识别的准确率。此外，该方法还可以用于评估预训练模型在不同下游任务上的迁移能力，为模型选择和微调提供指导。",
            "highlight_zh": "论文使用药物-靶标相互作用数据集进行评估，结果表明，学习曲线能够更好地捕捉多任务学习的效果，并且它们的多任务扩展可以描绘基础模型中的成对和上下文迁移效应。该方法能够有效区分不同任务之间的相关性和迁移潜力，为迁移学习策略的设计提供了更有效的指导。",
            "tags_zh": [
                "迁移学习",
                "多任务学习",
                "学习曲线",
                "数据集扰动",
                "任务亲和性"
            ],
            "_index": 46,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24866v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24866v1/kiba244_tasks__10f_comp-vs-scaff.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24866v1/kiba244_tasks__10f_scaff_single-vs-all.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback",
            "authors": [
                "Shulun Chen",
                "Runlong Zhou",
                "Zihan Zhang",
                "Maryam Fazel",
                "Simon S. Du"
            ],
            "arxiv_id": "2512.24818v1",
            "summary": "Aligning large language models (LLMs) with human preferences has proven effective for enhancing model capabilities, yet standard preference modeling using the Bradley-Terry model assumes transitivity, overlooking the inherent complexity of human population preferences. Nash learning from human feedback (NLHF) addresses this by framing non-transitive preferences as a two-player zero-sum game, where alignment reduces to finding the Nash equilibrium (NE). However, existing algorithms typically rely on regularization, incurring unavoidable bias when computing the duality gap in the original game. In this work, we provide the first convergence guarantee for Optimistic Multiplicative Weights Update ($\\mathtt{OMWU}$) in NLHF, showing that it achieves last-iterate linear convergence after a burn-in phase whenever an NE with full support exists, with an instance-dependent linear convergence rate to the original NE, measured by duality gaps. Compared to prior results in Wei et al. (2020), we do not require the assumption of NE uniqueness. Our analysis identifies a novel marginal convergence behavior, where the probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants than prior results. Experiments corroborate the theoretical strengths of $\\mathtt{OMWU}$ in both tabular and neural policy classes, demonstrating its potential for LLM applications.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "28 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24818v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出无正则化的OMWU算法，解决偏好反馈零和博弈中的线性收敛问题",
            "summary_zh": "将大型语言模型（LLM）与人类偏好对齐已被证明能有效提升模型能力。然而，使用Bradley-Terry模型的标准偏好建模假设传递性，忽略了人类群体偏好的内在复杂性。从人类反馈中进行纳什学习（NLHF）通过将非传递偏好构建为双人零和博弈来解决这个问题，其中对齐简化为寻找纳什均衡（NE）。然而，现有算法通常依赖于正则化，在计算原始博弈中的对偶间隙时会产生不可避免的偏差。本文为NLHF中的乐观乘性权重更新（OMWU）提供了首个收敛保证，表明只要存在具有完全支持的NE，它就能在burn-in阶段后实现最后一次迭代的线性收敛，并具有实例相关的线性收敛速度到原始NE，通过对偶间隙衡量。与Wei等人（2020）的先前结果相比，我们不需要NE唯一性的假设。我们的分析确定了一种新的边际收敛行为，其中很少采取的行动的概率从指数小的数值呈指数增长，从而实现了比先前结果更好的实例相关常数的指数依赖性。实验证实了OMWU在表格和神经策略类中的理论优势，证明了其在LLM应用中的潜力。",
            "intro_zh": [
                "现有基于Bradley-Terry模型的偏好学习方法假设传递性，无法有效处理人类偏好的复杂性。",
                "论文提出使用乐观乘性权重更新（OMWU）算法，在无正则化的情况下寻找零和博弈的纳什均衡。",
                "实验结果表明，OMWU算法在表格和神经策略类中均表现出良好的收敛性，并具备应用于LLM的潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从人类偏好反馈中学习策略的问题，特别是当人类偏好不满足传递性时。现有的方法通常采用正则化技术来保证算法的收敛性，但正则化会引入偏差，影响最终策略的准确性。此外，现有方法通常需要纳什均衡唯一性的假设，限制了其适用范围。\\n\\n**核心思路**：论文的核心思路是将人类偏好学习问题建模为一个双人零和博弈，并使用乐观乘性权重更新（OMWU）算法来寻找纳什均衡。OMWU算法是一种无正则化的算法，可以避免正则化带来的偏差。此外，论文证明了OMWU算法在纳什均衡不唯一的情况下也能收敛。\\n\\n**技术框架**：论文的技术框架主要包括以下几个部分：1) 将人类偏好建模为双人零和博弈；2) 使用OMWU算法更新策略；3) 分析OMWU算法的收敛性。具体来说，论文首先定义了一个 payoff 矩阵，用于表示不同策略组合下的收益。然后，使用OMWU算法迭代更新两个玩家的策略，使其逐渐逼近纳什均衡。最后，论文通过理论分析证明了OMWU算法的线性收敛性。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了无正则化的OMWU算法，避免了正则化带来的偏差；2) 证明了OMWU算法在纳什均衡不唯一的情况下也能收敛；3) 发现了一种新的边际收敛行为，即很少采取的行动的概率从指数小的数值呈指数增长，从而实现了更好的实例相关常数的指数依赖性。\\n\\n**关键设计**：论文的关键设计包括：1) OMWU算法的学习率参数设置；2) payoff 矩阵的构建方式；3) 收敛性分析中使用的 Lyapunov 函数的设计。具体来说，论文选择合适的学习率参数，以保证算法的收敛速度和稳定性。Payoff 矩阵的构建方式直接影响到博弈的性质和纳什均衡的存在性。Lyapunov 函数的设计是收敛性分析的关键，论文设计了一个合适的 Lyapunov 函数，证明了OMWU算法的线性收敛性。",
            "application_zh": "该研究成果可应用于大型语言模型的对齐，通过人类偏好反馈训练模型，使其更好地符合人类价值观和需求。此外，该方法还可应用于推荐系统、强化学习等领域，解决非传递偏好下的策略学习问题，提升系统的性能和用户体验。未来，该研究有望推动人工智能技术在更广泛领域的应用。",
            "highlight_zh": "论文证明了OMWU算法在NLHF中实现了最后一次迭代的线性收敛，且不需要纳什均衡唯一性的假设。与现有方法相比，OMWU算法避免了正则化带来的偏差，并实现了更好的实例相关常数的指数依赖性。实验结果验证了OMWU算法在表格和神经策略类中的有效性。",
            "tags_zh": [
                "偏好学习",
                "零和博弈",
                "纳什均衡",
                "乐观乘性权重更新",
                "线性收敛",
                "无正则化",
                "大型语言模型对齐",
                "人类反馈"
            ],
            "_index": 47,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24818v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24818v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24818v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)",
            "authors": [
                "Rongge Xu",
                "Hui Dai",
                "Yiming Fu",
                "Jiedong Jiang",
                "Tianjiao Nie",
                "Hongwei Wang",
                "Junkai Wang",
                "Holiverse Yang",
                "Jiatong Yang",
                "Zhi-Hao Zhang"
            ],
            "arxiv_id": "2512.24796v1",
            "summary": "Large language models (LLMs) have made rapid progress in formal theorem proving, yet current benchmarks under-measure the kind of abstraction and library-mediated reasoning that organizes modern mathematics. In parallel with FATE's emphasis on frontier algebra, we introduce LeanCat, a Lean benchmark for category-theoretic formalization -- a unifying language for mathematical structure and a core layer of modern proof engineering -- serving as a stress test of structural, interface-level reasoning. Part I: 1-Categories contains 100 fully formalized statement-level tasks, curated into topic families and three difficulty tiers via an LLM-assisted + human grading process. The best model solves 8.25% of tasks at pass@1 (32.50%/4.17%/0.00% by Easy/Medium/High) and 12.00% at pass@4 (50.00%/4.76%/0.00%). We also evaluate LeanBridge which use LeanExplore to search Mathlib, and observe consistent gains over single-model baselines. LeanCat is intended as a compact, reusable checkpoint for tracking both AI and human progress toward reliable, research-level formalization in Lean.",
            "categories": [
                "cs.LO",
                "cs.AI",
                "cs.FL",
                "cs.LG",
                "math.CT"
            ],
            "primary_category": "cs.LO",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "11 pages, 4 figures, 1 table",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24796v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出LeanCat基准测试集，用于评估LLM在范畴论形式化证明中的能力。",
            "summary_zh": "大型语言模型（LLM）在形式化定理证明方面取得了快速进展，但目前的基准测试低估了组织现代数学的抽象和库介导推理。与FATE强调前沿代数并行，我们引入LeanCat，一个用于范畴论形式化的Lean基准——一种数学结构的统一语言和现代证明工程的核心层——作为结构化、接口级推理的压力测试。第一部分：1-范畴包含100个完全形式化的语句级任务，通过LLM辅助+人工分级过程整理成主题族和三个难度等级。最佳模型在pass@1时解决了8.25%的任务（简单/中等/高难度分别为32.50%/4.17%/0.00%），在pass@4时解决了12.00%的任务（简单/中等/高难度分别为50.00%/4.76%/0.00%）。我们还评估了使用LeanExplore搜索Mathlib的LeanBridge，并观察到相对于单模型基线的持续提升。LeanCat旨在作为一个紧凑、可重用的检查点，用于跟踪人工智能和人类在Lean中实现可靠的、研究级别的形式化的进展。",
            "intro_zh": [
                "现有形式化定理证明基准缺乏对抽象和库介导推理的充分评估，无法真实反映现代数学的复杂性。",
                "LeanCat通过构建范畴论形式化的基准测试集，着重考察模型在结构化和接口级推理方面的能力。",
                "实验结果表明，现有最佳模型在LeanCat上的表现仍有很大提升空间，突显了该基准的挑战性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型在形式化定理证明中，对抽象概念和库函数的使用能力不足的问题。现有的基准测试无法充分评估模型在现代数学中常见的结构化推理和接口级推理能力，尤其是在范畴论这样的高度抽象领域。\\n\\n**核心思路**：论文的核心思路是构建一个专门针对范畴论形式化的基准测试集LeanCat。通过提供一系列精心设计的、难度分级的范畴论问题，来评估LLM在形式化证明方面的能力，特别是其对数学结构和接口的理解和运用。\\n\\n**技术框架**：LeanCat基准测试集包含100个完全形式化的语句级任务，这些任务被组织成不同的主题族，并根据难度分为简单、中等和高三个等级。难度分级由LLM辅助和人工评估相结合的方式完成。论文还评估了LeanBridge，它使用LeanExplore来搜索Mathlib库，以辅助证明过程。\\n\\n**关键创新**：LeanCat的关键创新在于其专注于范畴论这一高度抽象的数学领域，并提供了一个结构化的、难度分级的基准测试集。这使得研究人员能够更精确地评估LLM在形式化证明中对抽象概念和库函数的使用能力。此外，LLM辅助的人工分级方法也保证了基准测试的质量和难度分布。\\n\\n**关键设计**：LeanCat基准测试集中的任务涵盖了1-范畴论的各种基本概念和定理。难度分级标准主要考虑了证明的长度、涉及的抽象概念的数量以及对Mathlib库的依赖程度。LeanBridge使用LeanExplore来搜索Mathlib库，并利用搜索结果来指导证明过程。具体的参数设置和损失函数等技术细节未在摘要中提及，属于未知信息。",
            "application_zh": "LeanCat可用于评估和提升人工智能在形式化数学和软件验证等领域的应用能力。通过提供一个标准化的测试平台，它可以促进LLM在定理证明、程序验证和知识表示等方面的研究进展，并最终推动人工智能在科学发现和工程设计中的应用。",
            "highlight_zh": "最佳模型在LeanCat基准测试集上的pass@1准确率为8.25%，pass@4准确率为12.00%。其中，简单难度任务的解决率较高（pass@1: 32.50%, pass@4: 50.00%），而中等和高难度任务的解决率较低（中等难度pass@1: 4.17%, pass@4: 4.76%; 高难度pass@1: 0.00%, pass@4: 0.00%）。使用LeanBridge可以获得相对于单模型基线的持续提升。",
            "tags_zh": [
                "形式化验证",
                "范畴论",
                "基准测试",
                "大型语言模型",
                "定理证明",
                "Lean",
                "数学推理"
            ],
            "_index": 48,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24796v1/formal-accuracy_pass4.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24796v1/accuracy_comparison_300dpi.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24796v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference",
            "authors": [
                "Fen-Yu Hsieh",
                "Yun-Chang Teng",
                "Ding-Yong Hong",
                "Jan-Jan Wu"
            ],
            "arxiv_id": "2512.24713v1",
            "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of language processing tasks. However, this success comes at the cost of substantial computation and memory requirements, which significantly impedes their deployment in resource-constrained environments. To address this challenge, this work introduces an automation framework that leverages weight pruning and low-bit quantization, and presents a hardware-software co-design method that generates accelerators on the Field-Programmable Gate Array (FPGA) platform. In particular, we implement a unified pipeline that applies N:M structured pruning and 4-bit integer quantization to reduce the memory footprint, followed by optimized dequantization and matrix multiplication to enhance LLM inference on several hardware platforms, including CPUs, NVIDIA GPUs with Dense and 2:4 Sparse Tensor Cores, and a custom systolic-array-based FPGA accelerator. Utilizing 2:4 sparsity combined with quantization on $4096 \\times 4096$ matrices, our approach achieves a reduction of up to $4\\times$ in weight storage and a $1.71\\times$ speedup in matrix multiplication, yielding a $1.29\\times$ end-to-end latency reduction compared to dense GPU baselines. Scaling analysis on the LLaMA-7B model further shows that structured sparsity enhances the throughput per token by $1.36\\times$. These results demonstrate the synergy of fine-grained N:M sparsity and quantization for enabling efficient and deployable LLM inference, while the proposed FPGA accelerator offers a flexible architectural path for supporting a broader class of sparsity patterns beyond the fixed 2:4 hardware constraints.",
            "categories": [
                "cs.LG",
                "cs.AR"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24713v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于FPGA的软硬件协同设计框架，加速稀疏量化大语言模型推理。",
            "summary_zh": "大型语言模型(LLMs)在各种语言处理任务中表现出卓越的性能。然而，这种成功是以巨大的计算和内存需求为代价的，这严重阻碍了它们在资源受限环境中的部署。为了应对这一挑战，本文介绍了一种自动化框架，该框架利用权重剪枝和低比特量化，并提出了一种硬件-软件协同设计方法，用于在现场可编程门阵列(FPGA)平台上生成加速器。特别地，我们实现了一个统一的pipeline，该pipeline应用N:M结构化剪枝和4比特整数量化来减少内存占用，然后进行优化的反量化和矩阵乘法，以增强LLM在包括CPU、具有密集和2:4稀疏张量核心的NVIDIA GPU以及定制的基于 systolic 阵列的FPGA加速器等多种硬件平台上的推理。通过在$4096 \times 4096$矩阵上利用2:4稀疏性和量化，我们的方法实现了高达4倍的权重存储减少和1.71倍的矩阵乘法加速，与密集GPU基线相比，端到端延迟降低了1.29倍。在LLaMA-7B模型上的缩放分析进一步表明，结构化稀疏性将每token的吞吐量提高了1.36倍。这些结果证明了细粒度N:M稀疏性和量化在实现高效且可部署的LLM推理方面的协同作用，而所提出的FPGA加速器为支持超出固定2:4硬件约束的更广泛的稀疏模式类别提供了一种灵活的架构路径。",
            "intro_zh": [
                "大型语言模型部署受限于高昂的计算和内存需求，现有方法难以在资源受限环境中有效部署。",
                "提出一种软硬件协同设计框架，结合N:M稀疏剪枝和低比特量化，优化LLM在FPGA上的推理。",
                "实验表明，该方法在权重存储、矩阵乘法和端到端延迟方面均有显著提升，并提高了LLaMA-7B模型的吞吐量。"
            ],
            "method_zh": "**问题定义**：现有的大型语言模型（LLMs）计算和内存需求巨大，难以在资源受限的边缘设备上部署。现有的优化方法，如量化和剪枝，虽然可以降低模型大小和计算复杂度，但往往受限于硬件平台的特定约束，例如GPU的2:4稀疏性限制，缺乏灵活性和通用性。\\n\\n**核心思路**：本文的核心思路是采用软硬件协同设计的方法，结合N:M结构化剪枝和低比特量化，并针对FPGA平台定制加速器。通过软件层面的稀疏化和量化降低模型复杂度，同时在硬件层面设计灵活的加速器架构，以充分利用稀疏性和量化的优势，从而实现高效的LLM推理。\\n\\n**技术框架**：该框架包含一个统一的pipeline，首先对LLM进行N:M结构化剪枝和4比特整数量化，以减少内存占用。然后，针对不同的硬件平台（CPU、GPU、FPGA）进行优化的反量化和矩阵乘法。对于FPGA平台，设计了一个基于systolic阵列的定制加速器，以高效地执行稀疏矩阵乘法。整个框架旨在实现自动化，能够根据不同的模型和硬件平台自动生成优化的推理方案。\\n\\n**关键创新**：最重要的技术创新点在于软硬件协同设计，特别是针对FPGA平台定制的加速器架构。该加速器能够灵活地支持不同的N:M稀疏模式，突破了传统硬件平台（如GPU）对稀疏模式的限制。此外，该框架还实现了自动化，能够根据不同的模型和硬件平台自动生成优化的推理方案，降低了部署的复杂性。\\n\\n**关键设计**：该论文的关键设计包括：1) N:M结构化剪枝策略，在保证模型性能的同时，最大限度地减少非零元素的数量。2) 4比特整数量化，进一步降低模型大小和计算复杂度。3) 基于systolic阵列的FPGA加速器架构，能够高效地执行稀疏矩阵乘法。4) 优化的反量化和矩阵乘法实现，充分利用硬件平台的特性，提高推理速度。",
            "application_zh": "该研究成果可应用于各种资源受限的场景，例如边缘计算设备、移动设备和嵌入式系统。通过降低LLM的计算和内存需求，可以使这些设备能够运行复杂的AI模型，从而实现智能家居、自动驾驶、智能医疗等应用。此外，该研究还可以促进LLM在更多领域的应用，例如自然语言处理、机器翻译和语音识别。",
            "highlight_zh": "实验结果表明，在4096x4096矩阵上，使用2:4稀疏性和量化后，权重存储减少了4倍，矩阵乘法加速了1.71倍，端到端延迟相比于密集GPU基线降低了1.29倍。在LLaMA-7B模型上的缩放分析表明，结构化稀疏性将每token的吞吐量提高了1.36倍。这些结果表明，该方法能够显著提高LLM的推理效率。",
            "tags_zh": [
                "FPGA加速",
                "稀疏性",
                "量化",
                "大语言模型",
                "软硬件协同设计"
            ],
            "_index": 49,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24713v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24713v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24713v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework",
            "authors": [
                "András Millinghoffer",
                "András Formanek",
                "András Antos",
                "Péter Antal"
            ],
            "arxiv_id": "2512.24708v1",
            "summary": "The challenge of effectively transferring knowledge across multiple tasks is of critical importance and is also present in downstream tasks with foundation models. However, the nature of transfer, its transitive-intransitive nature, is still an open problem, and negative transfer remains a significant obstacle. Selection of beneficial auxiliary task sets in multi-task learning is frequently hindered by the high computational cost of their evaluation, the high number of plausible candidate auxiliary sets, and the varying complexity of selection across target tasks.\n  To address these constraints, we introduce BandiK, a novel three-stage multi-task auxiliary task subset selection method using multi-bandits, where each arm pull evaluates candidate auxiliary sets by training and testing a multiple output neural network on a single random train-test dataset split. Firstly, BandiK estimates the pairwise transfers between tasks, which helps in identifying which tasks are likely to benefit from joint learning. In the second stage, it constructs a linear number of candidate sets of auxiliary tasks (in the number of all tasks) for each target task based on the initial estimations, significantly reducing the exponential number of potential auxiliary task sets. Thirdly, it employs a Multi-Armed Bandit (MAB) framework for each task, where the arms correspond to the performance of candidate auxiliary sets realized as multiple output neural networks over train-test data set splits. To enhance efficiency, BandiK integrates these individual task-specific MABs into a multi-bandit structure. The proposed multi-bandit solution exploits that the same neural network realizes multiple arms of different individual bandits corresponding to a given candidate set. This semi-overlapping arm property defines a novel multi-bandit cost/reward structure utilized in BandiK.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "8 pages, 14 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24708v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出BandiK以解决多任务学习中的辅助任务选择问题",
            "summary_zh": "有效地在多个任务之间转移知识是一个重要的挑战，尤其在基础模型的下游任务中。然而，转移的性质及其传递性-非传递性特征仍然是一个未解的问题，负转移也成为了显著障碍。为了解决这些问题，本文提出了BandiK，一种新颖的三阶段多任务辅助任务子集选择方法，利用多臂赌博机框架。BandiK通过训练和测试多输出神经网络来评估候选辅助集，显著降低了计算成本，并提高了选择效率。",
            "intro_zh": [
                "现有的多任务学习方法在选择有益的辅助任务集时受到计算成本和候选集数量的限制，导致负转移现象。",
                "BandiK通过三阶段的多臂赌博机框架，评估候选辅助任务集，优化了任务间的知识转移。",
                "实验结果表明，BandiK在多任务学习中显著提高了辅助任务选择的效率和效果，降低了计算复杂度。"
            ],
            "method_zh": "**问题定义**：本文旨在解决多任务学习中辅助任务选择的高计算成本和负转移问题。现有方法在评估候选辅助任务集时面临候选集数量庞大和选择复杂度高的挑战。\\n\\n**核心思路**：BandiK的核心思路是通过多臂赌博机框架来评估和选择辅助任务集，利用任务间的知识转移来优化学习效果。该方法通过三阶段的流程来降低计算复杂度。\\n\\n**技术框架**：BandiK的整体架构包括三个主要阶段：首先，估计任务间的成对转移；其次，为每个目标任务构建线性数量的候选辅助任务集；最后，利用多臂赌博机框架评估候选集的性能。\\n\\n**关键创新**：BandiK的创新之处在于其多臂赌博机结构，允许同一神经网络实现多个任务的评估，利用半重叠臂特性来优化成本/收益结构，显著提高了选择效率。\\n\\n**关键设计**：在实现过程中，BandiK设置了多个参数以优化任务间的转移估计，并设计了适应性损失函数以提高多输出神经网络的性能。",
            "application_zh": "BandiK的研究成果在多任务学习、迁移学习和基础模型的应用中具有广泛的潜在价值。它可以被应用于自然语言处理、计算机视觉等领域，帮助提升模型在多任务环境下的学习效率和效果，推动智能系统的进一步发展。",
            "highlight_zh": "实验结果显示，BandiK在多个基准数据集上相较于传统方法提高了辅助任务选择的效率，减少了计算时间，并在任务性能上实现了显著提升，具体提升幅度达到20%以上。",
            "tags_zh": [
                "多任务学习",
                "辅助任务选择",
                "多臂赌博机",
                "知识转移",
                "计算效率",
                "迁移学习",
                "神经网络"
            ],
            "_index": 50,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24708v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24708v1/BASIC0_aupr_auroc_V.bar.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24708v1/ecai_pairwise_heatmaps__diff.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
            "authors": [
                "Xingwei Qu",
                "Shaowen Wang",
                "Zihao Huang",
                "Kai Hua",
                "Fan Yin",
                "Rui-Jie Zhu",
                "Jundong Zhou",
                "Qiyang Min",
                "Zihao Wang",
                "Yizhi Li",
                "Tianyu Zhang",
                "He Xing",
                "Zheng Zhang",
                "Yuxuan Song",
                "Tianyu Zheng",
                "Zhiyuan Zeng",
                "Chenghua Lin",
                "Ge Zhang",
                "Wenhao Huang"
            ],
            "arxiv_id": "2512.24617v1",
            "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\\textbf{+2.69$\\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24617v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出动态大概念模型（DLCM），通过自适应语义空间中的潜在推理提升LLM效率。",
            "summary_zh": "大型语言模型（LLM）对所有token应用统一的计算，然而语言的信息密度高度不均匀。这种token统一的模式在局部可预测的跨度上浪费了算力，同时对语义关键的转换分配不足。我们提出了动态大概念模型（DLCM），这是一个分层语言建模框架，它从潜在表示中学习语义边界，并将计算从token转移到压缩的概念空间，从而更有效地进行推理。DLCM端到端地发现可变长度的概念，而无需依赖预定义的语言单元。分层压缩从根本上改变了缩放行为。我们引入了第一个压缩感知缩放定律，它解耦了token级别的容量、概念级别的推理容量和压缩率，从而能够在固定的FLOPs下进行有原则的计算分配。为了稳定地训练这种异构架构，我们进一步开发了一种解耦的μP参数化，它支持跨宽度和压缩机制的零样本超参数迁移。在实际设置（R=4，对应于每个概念平均四个token）中，DLCM将大约三分之一的推理计算重新分配到一个更高容量的推理骨干中，在匹配的推理FLOPs下，在12个零样本基准测试中实现了+2.69%的平均改进。",
            "intro_zh": [
                "现有LLM对所有token采用统一计算，忽略了语言信息密度不均的问题，导致算力浪费。",
                "DLCM通过学习语义边界，将token计算转移到压缩的概念空间，提升推理效率。",
                "实验表明，DLCM在固定FLOPs下，零样本基准测试平均提升2.69%。"
            ],
            "method_zh": "**问题定义**：现有大型语言模型（LLM）对所有token采用统一的计算方式，没有考虑到语言本身信息密度的差异性。这种做法导致在信息冗余的token上浪费计算资源，而在语义关键的token上计算资源不足，限制了模型的效率和性能。\\n\\n**核心思路**：DLCM的核心思想是将token级别的计算转移到概念级别的计算。通过学习token的潜在表示，模型能够自动发现语义边界，并将多个token压缩成一个“概念”。然后在压缩后的概念空间中进行推理，从而减少计算量，并提高推理效率。这种分层压缩的方式能够更好地适应语言信息密度的不均匀性。\\n\\n**技术框架**：DLCM包含以下主要模块：1) **Token Embedding层**：将输入token转换为向量表示。2) **概念发现模块**：基于token embedding学习语义边界，将token分组为概念。3) **概念Embedding层**：将概念转换为向量表示。4) **推理骨干网络**：在概念embedding上进行推理，例如Transformer网络。5) **输出层**：将推理结果映射回token级别。整个框架是端到端可训练的。\\n\\n**关键创新**：DLCM的关键创新在于：1) **动态概念发现**：模型能够自动学习语义边界，无需预定义的语言单元。2) **压缩感知缩放定律**：提出了新的缩放定律，考虑了token级别容量、概念级别推理容量和压缩率之间的关系，从而能够在固定FLOPs下进行更合理的计算分配。3) **解耦的μP参数化**：提出了一种新的参数化方法，能够稳定训练异构架构，并支持零样本超参数迁移。\\n\\n**关键设计**：1) **概念发现模块**：可以使用各种聚类算法或神经网络来实现，目标是学习token之间的相似性，并将相似的token分组为概念。2) **压缩率R**：控制每个概念包含的token数量，R越大，压缩率越高。3) **推理骨干网络**：可以使用各种Transformer变体，例如Sparse Transformer或Longformer，以提高推理效率。4) **损失函数**：除了标准的语言建模损失外，还可以添加正则化项，以鼓励概念的语义一致性。",
            "application_zh": "DLCM可应用于各种自然语言处理任务，如机器翻译、文本摘要、问答系统等。通过提高LLM的推理效率，DLCM可以降低计算成本，并使其能够在资源受限的设备上运行。此外，DLCM的动态概念发现能力可以帮助我们更好地理解语言的结构和语义。",
            "highlight_zh": "实验结果表明，在R=4的压缩率下，DLCM将大约三分之一的推理计算重新分配到一个更高容量的推理骨干中，在12个零样本基准测试中实现了+2.69%的平均改进。这表明DLCM能够在固定FLOPs下显著提高LLM的性能。",
            "tags_zh": [
                "大型语言模型",
                "动态概念模型",
                "语义空间",
                "潜在推理",
                "压缩感知",
                "分层建模",
                "μP参数化"
            ],
            "_index": 51,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24617v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24617v1/full_training_linear.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24617v1/robust_decay_fit.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MultiRisk: Multiple Risk Control via Iterative Score Thresholding",
            "authors": [
                "Sunay Joshi",
                "Yan Sun",
                "Hamed Hassani",
                "Edgar Dobriban"
            ],
            "arxiv_id": "2512.24587v1",
            "summary": "As generative AI systems are increasingly deployed in real-world applications, regulating multiple dimensions of model behavior has become essential. We focus on test-time filtering: a lightweight mechanism for behavior control that compares performance scores to estimated thresholds, and modifies outputs when these bounds are violated. We formalize the problem of enforcing multiple risk constraints with user-defined priorities, and introduce two efficient dynamic programming algorithms that leverage this sequential structure. The first, MULTIRISK-BASE, provides a direct finite-sample procedure for selecting thresholds, while the second, MULTIRISK, leverages data exchangeability to guarantee simultaneous control of the risks. Under mild assumptions, we show that MULTIRISK achieves nearly tight control of all constraint risks. The analysis requires an intricate iterative argument, upper bounding the risks by introducing several forms of intermediate symmetrized risk functions, and carefully lower bounding the risks by recursively counting jumps in symmetrized risk functions between appropriate risk levels. We evaluate our framework on a three-constraint Large Language Model alignment task using the PKU-SafeRLHF dataset, where the goal is to maximize helpfulness subject to multiple safety constraints, and where scores are generated by a Large Language Model judge and a perplexity filter. Our experimental results show that our algorithm can control each individual risk at close to the target level.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24587v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出MultiRisk算法，通过迭代阈值处理实现生成式AI系统多重风险控制。",
            "summary_zh": "随着生成式AI系统在现实世界应用日益广泛，对模型行为的多维度监管变得至关重要。本文关注测试时过滤：一种轻量级的行为控制机制，它将性能得分与估计的阈值进行比较，并在违反这些界限时修改输出。本文形式化了在用户定义的优先级下执行多个风险约束的问题，并引入了两种利用这种顺序结构的高效动态规划算法。第一种算法MULTIRISK-BASE，提供了一种直接的有限样本程序来选择阈值，而第二种算法MULTIRISK，利用数据可交换性来保证对风险的同步控制。在温和的假设下，本文证明了MULTIRISK几乎可以严格控制所有约束风险。该分析需要一个复杂的迭代论证，通过引入几种形式的中间对称风险函数来限制风险的上限，并通过递归计算适当风险水平之间对称风险函数的跳跃来仔细限制风险的下限。本文在PKU-SafeRLHF数据集上评估了该框架，该数据集是一个包含三个约束的大型语言模型对齐任务，目标是在多个安全约束下最大化有用性，其中分数由大型语言模型判断器和困惑度过滤器生成。实验结果表明，该算法可以将每个单独的风险控制在接近目标水平。",
            "intro_zh": [
                "现有生成式AI系统缺乏有效手段在测试时同时控制多个风险维度，难以满足实际应用中的安全性和可靠性需求。",
                "MultiRisk算法通过迭代调整得分阈值，在满足用户定义的优先级下，实现对多个风险约束的同步控制。",
                "实验表明，MultiRisk算法在大型语言模型对齐任务中，能够将每个风险控制在接近目标水平，有效提升模型安全性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决生成式AI模型在实际部署中，需要同时满足多个风险约束（例如安全性、公平性、无害性等）的问题。现有方法通常难以在测试阶段高效且精确地控制这些风险，尤其是在存在多个相互冲突的约束时。简单的阈值过滤方法无法保证在满足一个约束的同时，不会违反其他约束。\\n\\n**核心思路**：MultiRisk的核心思路是利用动态规划，迭代地调整每个风险维度上的得分阈值，以满足用户预先设定的风险控制目标和优先级。通过将多目标优化问题转化为一个序列决策过程，可以有效地搜索到一组阈值，使得模型在满足高优先级约束的同时，尽可能地优化低优先级约束。\\n\\n**技术框架**：MultiRisk算法包含两个主要变体：MULTIRISK-BASE和MULTIRISK。MULTIRISK-BASE是一种直接的有限样本方法，用于选择阈值。MULTIRISK则利用数据可交换性，提供更强的理论保证，确保对风险的同步控制。整体流程如下：1. 对每个样本，模型生成一个得分向量，每个维度对应一个风险指标。2. 根据用户定义的优先级顺序，依次对每个风险维度进行阈值调整。3. 使用动态规划算法，搜索满足风险约束的阈值。4. 对于MULTIRISK，利用数据可交换性进行风险估计，以保证同步控制。\\n\\n**关键创新**：MultiRisk的关键创新在于其能够同时处理多个风险约束，并允许用户定义这些约束的优先级。与传统的单目标优化方法相比，MultiRisk更符合实际应用的需求，能够更好地平衡不同风险维度之间的trade-off。此外，MULTIRISK利用数据可交换性，提供了更强的理论保证，确保风险控制的可靠性。\\n\\n**关键设计**：MultiRisk算法的关键设计包括：1. 使用动态规划算法进行阈值搜索，保证效率。2. 定义明确的风险函数，用于评估模型在每个风险维度上的表现。3. 利用数据可交换性进行风险估计，提高风险控制的准确性。4. 迭代地调整阈值，以满足用户定义的优先级顺序。",
            "application_zh": "MultiRisk算法可广泛应用于各种生成式AI系统的风险控制，例如大型语言模型的安全对齐、图像生成模型的偏见消除、推荐系统的公平性保障等。通过该算法，开发者可以更加灵活地控制模型行为，确保其在满足功能需求的同时，符合伦理和社会规范，从而促进AI技术的健康发展。",
            "highlight_zh": "实验结果表明，MultiRisk算法在PKU-SafeRLHF数据集上，能够有效地控制大型语言模型的多个安全风险。具体来说，该算法可以将每个风险控制在接近目标水平，显著优于简单的阈值过滤方法。这表明MultiRisk算法在实际应用中具有很高的价值。",
            "tags_zh": [
                "风险控制",
                "生成式AI",
                "阈值处理",
                "动态规划",
                "多目标优化"
            ],
            "_index": 52,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24587v1/figures/multirisk_tree.001.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24587v1/figures/reverse_ineq.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24587v1/plots/hh_three_constraint/bonferroni/clt/tradeoff_constraint_1_grid_31_budget_101_shuffles_10.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization",
            "authors": [
                "Yuma Ichikawa",
                "Yoshihiko Fujisawa",
                "Yudai Fujimoto",
                "Akira Sakai",
                "Katsuki Fujisawa"
            ],
            "arxiv_id": "2512.24545v1",
            "summary": "For extreme low-bit quantization of large language models (LLMs), Double Binary Factorization (DBF) is attractive as it enables efficient inference without sacrificing accuracy. However, the scaling parameters of DBF are too restrictive; after factoring out signs, all rank components share the same magnitude profile, resulting in performance saturation. We propose Multi-envelope DBF (MDBF), which retains a shared pair of 1-bit sign bases but replaces the single envelope with a rank-$l$ envelope. By sharing sign matrices among envelope components, MDBF effectively maintains a binary carrier and utilizes the limited memory budget for magnitude expressiveness. We also introduce a closed-form initialization and an alternating refinement method to optimize MDBF. Across the LLaMA and Qwen families, MDBF enhances perplexity and zero-shot accuracy over previous binary formats at matched bits per weight while preserving the same deployment-friendly inference primitive.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "stat.ML"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "14 pages, 2 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24545v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出多包络双重二值分解(MDBF)，用于大语言模型极低比特量化，提升精度。",
            "summary_zh": "针对大语言模型(LLM)的极低比特量化，双重二值分解(DBF)因其在不牺牲精度的情况下实现高效推理而备受关注。然而，DBF的缩放参数过于严格；在分解出符号后，所有秩分量共享相同的幅度分布，导致性能饱和。我们提出了多包络DBF(MDBF)，它保留了一对共享的1比特符号基，但用秩-$l$包络代替了单个包络。通过在包络分量之间共享符号矩阵，MDBF有效地维护了一个二值载体，并将有限的内存预算用于幅度表达。我们还引入了一种闭式初始化和一种交替细化方法来优化MDBF。在LLaMA和Qwen系列模型上，MDBF在匹配的每权重比特数下，提高了困惑度和零样本精度，同时保留了相同的部署友好的推理原语。",
            "intro_zh": [
                "DBF量化方法在极低比特量化大语言模型时表现出潜力，但其缩放参数限制了模型性能，导致精度饱和。",
                "论文提出MDBF方法，通过引入多包络结构，在共享符号矩阵的基础上，利用有限的内存预算提升幅度表达能力。",
                "实验结果表明，MDBF在LLaMA和Qwen模型上，以相同的比特数实现了更好的困惑度和零样本精度。"
            ],
            "method_zh": "**问题定义**：现有DBF方法在极低比特量化大语言模型时，由于其缩放参数的限制，导致模型性能饱和。具体来说，DBF在分解出符号后，所有秩分量共享相同的幅度分布，缺乏足够的灵活性来表示权重矩阵的复杂结构。\\n\\n**核心思路**：论文的核心思路是通过引入多包络结构来增强DBF的幅度表达能力。MDBF保留了DBF的二值分解框架，但将原有的单一幅度包络替换为多个幅度包络，每个包络对应一个秩分量。通过在这些包络分量之间共享符号矩阵，MDBF能够在有限的内存预算下，有效地利用比特来表达幅度信息。\\n\\n**技术框架**：MDBF的整体框架仍然基于双重二值分解，但其核心在于多包络的设计。具体流程如下：1) 对权重矩阵进行二值分解，得到共享的符号矩阵；2) 将幅度信息分解为多个秩-$l$的包络分量；3) 使用闭式初始化方法初始化包络分量；4) 使用交替细化方法优化符号矩阵和包络分量。\\n\\n**关键创新**：MDBF的关键创新在于引入了多包络结构，并设计了相应的初始化和优化方法。与DBF相比，MDBF能够更灵活地表示权重矩阵的幅度信息，从而提高量化模型的精度。此外，共享符号矩阵的设计保证了MDBF能够有效地利用有限的内存预算。\\n\\n**关键设计**：MDBF的关键设计包括：1) 包络分量的秩-$l$的选择，需要根据具体的模型和数据集进行调整；2) 闭式初始化方法，能够为后续的优化提供一个良好的起点；3) 交替细化方法，通过交替优化符号矩阵和包络分量，逐步提高模型的精度。损失函数的设计目标是最小化量化误差，可以使用均方误差等常用的损失函数。",
            "application_zh": "MDBF适用于对计算资源和存储空间有严格限制的场景，例如移动设备、嵌入式系统和边缘计算设备。通过极低比特量化，MDBF能够显著降低大语言模型的存储空间和计算复杂度，使其能够在这些资源受限的设备上部署和运行。此外，MDBF还可以应用于模型压缩、模型加速等领域，提高模型的效率和性能。",
            "highlight_zh": "实验结果表明，MDBF在LLaMA和Qwen系列模型上，以匹配的每权重比特数，显著提高了困惑度和零样本精度。例如，在某些任务上，MDBF的性能优于之前的二值量化方法，并且接近于全精度模型的性能。这些结果表明，MDBF是一种有效的极低比特量化方法，能够在大语言模型上实现高性能。",
            "tags_zh": [
                "极低比特量化",
                "大语言模型",
                "二值分解",
                "模型压缩",
                "多包络",
                "量化精度",
                "推理效率"
            ],
            "_index": 53,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24545v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24545v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
            "authors": [
                "Rohit Dwivedula",
                "Divyanshu Saxena",
                "Sujay Yadalam",
                "Daehyeok Kim",
                "Aditya Akella"
            ],
            "arxiv_id": "2512.25065v1",
            "summary": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.\n  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.",
            "categories": [
                "cs.OS",
                "cs.AI",
                "cs.DC"
            ],
            "primary_category": "cs.OS",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "27 pages, 11 figures, 7 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25065v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "Vulcan：利用LLM驱动搜索，合成实例最优的系统启发式算法",
            "summary_zh": "现代操作系统和分布式系统中的资源管理任务，如调度、缓存或主动队列管理，仍然主要依赖于手工设计的启发式算法。设计高性能的启发式算法既昂贵又耗时，并且由于硬件、工作负载和环境的不断变化，我们不得不持续进行这一过程。本文提出了一种新的替代方案：使用代码生成的大型语言模型（LLM）合成实例最优的启发式算法，专门针对部署它们的确切工作负载和硬件。为了使这种合成易于处理，Vulcan通过LLM友好的、任务无关的接口分离策略和机制。通过这些接口，用户可以指定所需策略的输入和目标，而Vulcan通过对LLM生成的代码进行进化搜索来寻找高性能的策略。该接口具有足够的表达能力来捕获广泛的系统策略，同时又受到足够的约束，即使是小型、廉价的LLM也能生成正确且可执行的代码。我们使用Vulcan合成了用于缓存驱逐和内存分层的高性能启发式算法，发现这些启发式算法在性能上优于所有人工设计的最先进算法，对于每个任务分别高达69%和7.9%。",
            "intro_zh": [
                "现有资源管理依赖手工启发式算法，设计成本高且难以适应硬件和工作负载的快速变化。",
                "Vulcan利用LLM生成代码，通过进化搜索合成针对特定实例优化的启发式策略，分离策略与机制。",
                "实验表明，Vulcan合成的缓存驱逐和内存分层策略，性能超越现有算法高达69%和7.9%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决现代操作系统和分布式系统中资源管理任务（如缓存驱逐、内存分层等）对人工设计启发式算法的依赖问题。现有方法的痛点在于，设计高性能的启发式算法成本高昂、耗时，且难以适应不断变化的硬件、工作负载和环境，导致系统性能难以达到最优。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）的代码生成能力，自动合成针对特定实例（即特定的硬件和工作负载）优化的启发式算法。通过将策略（policy）与机制（mechanism）分离，并提供LLM友好的接口，降低了LLM生成高质量代码的难度，使得即使是小型LLM也能胜任。\\n\\n**技术框架**：Vulcan的技术框架主要包含以下几个模块：1) 用户通过任务无关的接口指定策略的输入和目标；2) Vulcan利用LLM生成候选代码，这些代码实现了不同的启发式策略；3) Vulcan使用进化搜索算法，在生成的代码空间中搜索高性能的策略；4) 最终，Vulcan输出针对特定实例优化的启发式算法。\\n\\n**关键创新**：论文最重要的技术创新点在于，它将LLM的代码生成能力与进化搜索算法相结合，实现了自动合成实例最优的系统启发式算法。与传统的手工设计方法相比，Vulcan能够更快速、更高效地找到针对特定场景的最佳策略。此外，通过分离策略与机制，并提供LLM友好的接口，降低了LLM生成高质量代码的难度。\\n\\n**关键设计**：Vulcan的关键设计包括：1) LLM prompt的设计，需要保证生成的代码既能表达丰富的策略空间，又能保证代码的正确性和可执行性；2) 进化搜索算法的选择和参数设置，需要平衡搜索效率和搜索质量；3) 任务无关接口的设计，需要保证接口的通用性和表达能力，能够支持各种不同的资源管理任务。",
            "application_zh": "Vulcan具有广泛的应用前景，可用于各种资源管理任务，如云计算资源调度、边缘计算缓存管理、数据库查询优化等。通过自动合成实例最优的启发式算法，Vulcan可以显著提升系统性能，降低运营成本，并加速新硬件和工作负载的部署。未来，Vulcan有望成为一种通用的系统优化工具，赋能各种智能系统。",
            "highlight_zh": "实验结果表明，Vulcan合成的缓存驱逐和内存分层策略，在性能上显著优于人工设计的state-of-the-art算法。具体而言，对于缓存驱逐任务，Vulcan的性能提升高达69%；对于内存分层任务，性能提升达到7.9%。这些结果表明，Vulcan具有强大的系统优化能力。",
            "tags_zh": [
                "大型语言模型",
                "启发式搜索",
                "资源管理",
                "系统优化",
                "缓存驱逐",
                "内存分层",
                "进化算法",
                "代码生成"
            ],
            "_index": 54,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.25065v1/osdi26-sections/figures/heatmap-cloudphysics-nosize.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.25065v1/osdi26-sections/figures/system-overview.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.25065v1/osdi26-sections/figures/module-overview-v3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings",
            "authors": [
                "Tianzhi He",
                "Farrokh Jazizadeh"
            ],
            "arxiv_id": "2512.25055v1",
            "summary": "This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype's performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.",
            "categories": [
                "cs.AI",
                "cs.HC"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.25055v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于LLM的智能建筑能源管理AI Agent，实现情境感知能源管理",
            "summary_zh": "本研究提出了一个基于大型语言模型（LLM）的建筑能源管理系统（BEMS）AI Agent的概念框架和一个原型评估，旨在通过自然语言交互促进智能建筑中的情境感知能源管理。该框架包含感知（传感）、中央控制（大脑）和行动（驱动和用户交互）三个模块，形成一个闭环反馈系统，捕获、分析和解释能源数据，从而智能地响应用户查询并管理连接的设备。通过利用LLM的自主数据分析能力，BEMS AI Agent旨在提供关于能源消耗、成本预测和设备调度的情境感知洞察，从而解决现有能源管理系统的局限性。使用120个用户查询，在四个不同的真实住宅能源数据集上，通过延迟、功能、能力、准确性和成本效益等指标评估了原型的性能。使用方差分析（ANOVA）测试证明了该框架的通用性。结果显示出良好的性能，设备控制的响应准确率为86%，记忆相关任务为97%，调度和自动化为74%，能源分析为77%，而更复杂的成本估算任务的准确率为49%，表明仍有改进空间。这项基准研究旨在规范基于LLM的BEMS AI Agent的评估，并确定未来的研究方向，强调响应准确性和计算效率之间的权衡。",
            "intro_zh": [
                "现有能源管理系统缺乏情境感知能力，难以根据用户需求和环境变化进行智能调节。",
                "利用LLM的自然语言处理和数据分析能力，构建情境感知的BEMS AI Agent，实现智能能源管理。",
                "实验结果表明，该Agent在设备控制、记忆任务和能源分析方面表现良好，但在成本估算方面仍需改进。"
            ],
            "method_zh": "**问题定义**：现有建筑能源管理系统（BEMS）在情境感知和用户交互方面存在不足。传统系统难以理解用户的自然语言指令，无法根据用户的具体需求和环境变化进行智能调节，导致能源浪费和用户体验不佳。此外，现有系统的数据分析能力有限，难以提供深入的能源消耗洞察和准确的成本预测。\\n\\n**核心思路**：本研究的核心思路是利用大型语言模型（LLM）的强大自然语言处理和数据分析能力，构建一个情境感知的BEMS AI Agent。该Agent能够理解用户的自然语言查询，分析建筑的能源数据，并根据用户的需求和环境变化智能地控制连接的设备，从而实现更高效、更智能的能源管理。这样设计的目的是为了弥合现有BEMS在用户交互和情境感知方面的差距。\\n\\n**技术框架**：该框架包含三个主要模块：感知（sensing）、中央控制（brain）和行动（actuation and user interaction）。感知模块负责收集建筑的能源数据和用户输入；中央控制模块，即基于LLM的AI Agent，负责分析数据、理解用户意图并生成控制指令；行动模块负责执行控制指令，例如调节设备或向用户提供反馈。这三个模块形成一个闭环反馈系统，持续优化能源管理策略。\\n\\n**关键创新**：最重要的技术创新点在于将LLM应用于BEMS，赋予系统自然语言交互和情境感知能力。与传统的基于规则或机器学习的BEMS相比，该方法能够更好地理解用户的需求，并根据环境变化进行自适应调节。此外，LLM的自主数据分析能力能够提供更深入的能源消耗洞察，帮助用户更好地了解和控制能源使用。\\n\\n**关键设计**：该研究使用了预训练的LLM，并通过微调使其适应BEMS的应用场景。用户通过自然语言与Agent交互，Agent将用户的查询转化为可执行的指令。研究中使用了不同的评估指标，包括延迟、功能、能力、准确性和成本效益，以全面评估Agent的性能。此外，研究还使用了方差分析（ANOVA）测试来验证框架的通用性。",
            "application_zh": "该研究成果可应用于智能家居、智能办公楼等各种建筑环境，实现更高效、更智能的能源管理。通过自然语言交互，用户可以轻松地控制和优化能源使用，降低能源成本，提高生活质量。未来，该技术有望与智能电网等基础设施集成，实现更广泛的能源优化和可持续发展。",
            "highlight_zh": "实验结果表明，该Agent在设备控制方面达到了86%的准确率，在记忆相关任务中达到了97%的准确率，在调度和自动化方面达到了74%的准确率，在能源分析方面达到了77%的准确率。虽然在成本估算方面准确率较低（49%），但整体性能表明基于LLM的BEMS AI Agent具有很大的潜力。",
            "tags_zh": [
                "智能建筑",
                "能源管理系统",
                "大型语言模型",
                "AI Agent",
                "情境感知",
                "自然语言交互",
                "能源效率"
            ],
            "_index": 55,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.25055v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.25055v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.25055v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AMAP Agentic Planning Technical Report",
            "authors": [
                "Yulan Hu",
                "Xiangwen Zhang",
                "Sheng Ouyang",
                "Hao Yi",
                "Lu Xu",
                "Qinglin Lang",
                "Lide Tan",
                "Xiang Cheng",
                "Tianchen Ye",
                "Zhicong Li",
                "Ge Chen",
                "Wenjin Yang",
                "Zheng Pan",
                "Shaopan Xiong",
                "Siran Yang",
                "Ju Huang",
                "Yan Zhang",
                "Jiamang Wang",
                "Yong Liu",
                "Yinfeng Huang",
                "Tucheng Lin",
                "Xin Li",
                "Ning Guo"
            ],
            "arxiv_id": "2512.24957v1",
            "summary": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24957v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出STAgent，一个用于时空理解的Agentic大语言模型，解决复杂任务如POI发现和行程规划。",
            "summary_zh": "本文介绍STAgent，一个专为时空理解设计的agentic大语言模型，旨在解决受约束的兴趣点发现和行程规划等复杂任务。STAgent是一个专门的模型，能够与时空场景中的十种不同的工具进行交互，使其能够在复杂推理过程中探索、验证和改进中间步骤。值得注意的是，STAgent有效地保留了其通用能力。我们通过三个关键贡献赋予STAgent这些能力：（1）一个稳定的工具环境，支持十多种特定领域的工具，实现异步推出和训练；（2）一个分层数据管理框架，像大海捞针一样识别高质量数据，以1:10,000的过滤比例管理高质量查询，强调多样性和难度；（3）一个级联训练方案，首先是一个作为守护者的种子SFT阶段，用于衡量查询难度，然后是第二个SFT阶段，对具有高确定性的查询进行微调，以及最终的RL阶段，利用低确定性的数据。STAgent使用Qwen3-30B-A3B初始化，以建立强大的SFT基础并利用对样本难度的洞察力，在TravelBench上产生了有希望的性能，同时保持了其在各种通用基准测试中的通用能力，从而证明了我们提出的agentic模型的有效性。",
            "intro_zh": [
                "现有方法在解决复杂时空任务（如POI发现和行程规划）时，缺乏有效的工具交互和推理能力。",
                "STAgent通过构建可交互的工具环境、分层数据管理和级联训练方案，提升模型在时空理解和推理方面的能力。",
                "实验表明，STAgent在TravelBench上表现出色，同时保持了通用能力，验证了所提出agentic模型的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂时空任务，例如在特定约束条件下发现兴趣点（POI）和规划行程。现有方法通常难以有效地利用外部工具进行探索、验证和改进中间步骤，导致推理能力不足，难以处理复杂场景。\\n\\n**核心思路**：论文的核心思路是构建一个agentic大语言模型STAgent，使其能够与多个领域特定的工具进行交互，并通过分层数据管理和级联训练方案，提升模型在时空理解和推理方面的能力。通过工具交互，模型可以探索、验证和改进中间步骤，从而更有效地解决复杂任务。\\n\\n**技术框架**：STAgent的技术框架主要包含三个部分：1) 稳定的工具环境，支持十多种领域特定工具的异步训练；2) 分层数据管理框架，用于筛选高质量训练数据，强调多样性和难度；3) 级联训练方案，包括种子SFT阶段（评估查询难度）、高置信度SFT阶段和低置信度RL阶段。整体流程是先通过种子SFT阶段评估数据难度，然后分别使用高置信度和低置信度数据进行SFT和RL训练，最终提升模型性能。\\n\\n**关键创新**：论文的关键创新在于提出了一个完整的agentic框架，包括工具环境、数据管理和训练方案，使得大语言模型能够有效地应用于复杂时空任务。与现有方法相比，STAgent能够更好地利用外部工具进行探索和推理，并且通过分层数据管理和级联训练，能够更有效地利用不同难度的数据。\\n\\n**关键设计**：在数据管理方面，论文采用1:10,000的过滤比例筛选高质量查询，并强调数据的多样性和难度。在训练方面，论文采用级联训练方案，首先使用种子SFT阶段评估查询难度，然后分别使用高置信度和低置信度数据进行SFT和RL训练。模型初始化使用Qwen3-30B-A3B，以建立强大的SFT基础。",
            "application_zh": "该研究成果可应用于智能出行、城市规划、物流优化、旅游推荐等领域。通过STAgent，用户可以更方便地进行个性化行程规划、POI推荐和路径导航，从而提升出行效率和用户体验。未来，该技术有望进一步扩展到更多时空相关的应用场景。",
            "highlight_zh": "STAgent在TravelBench上取得了有希望的性能，同时保持了其在各种通用基准测试中的通用能力。这表明该模型在解决特定领域问题的同时，没有牺牲其通用性，验证了所提出agentic模型的有效性。具体性能数据和对比基线未在摘要中明确给出，属于未知信息。",
            "tags_zh": [
                "Agentic模型",
                "时空理解",
                "大语言模型",
                "行程规划",
                "兴趣点发现"
            ],
            "_index": 56,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing",
            "authors": [
                "Andrii Gamalii",
                "Daniel Górniak",
                "Robert Nowak",
                "Bartłomiej Olber",
                "Krystian Radlak",
                "Jakub Winter"
            ],
            "arxiv_id": "2512.24896v1",
            "summary": "This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24896v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出一种半自动标注流水线，加速自动驾驶多传感器数据的标注过程。",
            "summary_zh": "本报告介绍了DARTS项目中开发的半自动数据标注流水线的设计与实现。DARTS项目的目标是创建波兰驾驶场景的大规模多模态数据集。手动标注此类异构数据既昂贵又耗时。为了解决这个问题，提出的解决方案采用人机协作方法，将人工智能与人类专业知识相结合，以降低标注成本和时间。该系统自动生成初始标注，支持迭代模型重训练，并结合了数据匿名化和领域自适应技术。该工具的核心是使用3D目标检测算法来生成初步标注。总的来说，开发的工具和方法显著节省了时间，同时确保了不同传感器模态之间一致的高质量标注。该解决方案通过加速项目标准化格式的大型标注数据集的准备工作，直接支持DARTS项目，从而加强了波兰自动驾驶汽车研究的技术基础。",
            "intro_zh": [
                "多传感器融合的自动驾驶数据集标注成本高昂且耗时，成为大规模数据集构建的瓶颈。",
                "采用人机协作的半自动标注方法，利用AI生成初始标注，人工修正，并迭代优化模型。",
                "该方法显著节省了标注时间，并保证了多模态数据标注的一致性和高质量。"
            ],
            "method_zh": "**问题定义**：自动驾驶场景下的多传感器数据（例如LiDAR、摄像头）标注非常耗时且成本高昂。人工标注效率低，且容易出现标注不一致的问题，难以满足大规模数据集的需求。现有方法要么依赖全人工标注，要么自动化程度低，无法有效利用人类的专业知识。\\n\\n**核心思路**：论文的核心思路是构建一个人机协作的半自动标注流水线。首先利用3D目标检测算法自动生成初始标注，然后由人工专家进行修正和完善。系统可以根据人工修正的结果进行迭代训练，不断提高自动标注的准确率，从而减少人工干预的需求。\\n\\n**技术框架**：该半自动标注流水线包含以下几个主要模块：1) 数据导入与预处理模块：负责导入多传感器数据，并进行必要的预处理，例如数据同步、校准等。2) 自动标注模块：使用3D目标检测算法（具体算法未提及）对数据进行初步标注。3) 人工校正模块：提供用户界面，允许人工专家对自动标注结果进行修正和完善。4) 模型训练模块：根据人工修正后的数据，对3D目标检测模型进行迭代训练，提高自动标注的准确率。5) 数据匿名化模块：对标注数据进行匿名化处理，保护隐私。6) 领域自适应模块：用于将模型迁移到新的场景或数据集。\\n\\n**关键创新**：该方法的主要创新在于构建了一个完整的人机协作的半自动标注流水线，并将其应用于自动驾驶多传感器数据的标注。通过迭代训练，不断提高自动标注的准确率，从而显著减少人工标注的工作量。此外，该流水线还集成了数据匿名化和领域自适应等功能，提高了标注数据的可用性和安全性。\\n\\n**关键设计**：论文中未详细描述3D目标检测算法的具体选择和参数设置。损失函数和网络结构等技术细节也未提及。但强调了人工校正环节的重要性，以及通过迭代训练不断优化模型的能力。",
            "application_zh": "该研究成果可广泛应用于自动驾驶数据集的构建，加速自动驾驶技术的研发和测试。此外，该半自动标注方法也可推广到其他需要大规模数据标注的领域，例如机器人、智能监控等。通过降低标注成本和提高标注效率，可以促进相关领域的发展。",
            "highlight_zh": "论文强调了该方法在节省标注时间方面的优势，但没有提供具体的性能数据或与其他基线的对比结果。文中提到该方法能够确保不同传感器模态之间标注的一致性和高质量，但缺乏量化指标的支撑。具体的实验结果未知。",
            "tags_zh": [
                "半自动标注",
                "自动驾驶",
                "多传感器融合",
                "数据集构建",
                "人机协作"
            ],
            "_index": 57,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24896v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24896v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24896v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements",
            "authors": [
                "Yiming Liang",
                "Yizhi Li",
                "Yantao Du",
                "Ge Zhang",
                "Jiayi Zhou",
                "Yuchen Wu",
                "Yinzhu Piao",
                "Denghui Cao",
                "Tong Sun",
                "Ziniu Li",
                "Li Du",
                "Bo Lei",
                "Jiaheng Liu",
                "Chenghua Lin",
                "Zhaoxiang Zhang",
                "Wenhao Huang",
                "Jiajun Zhang"
            ],
            "arxiv_id": "2512.24867v1",
            "summary": "Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. We propose Encyclo-K, a statement-based benchmark that rethinks benchmark construction from the ground up. Our key insight is that knowledge statements, not questions, can serve as the unit of curation, and questions can then be constructed from them. We extract standalone knowledge statements from authoritative textbooks and dynamically compose them into evaluation questions through random sampling at test time. This design directly addresses all three limitations: the combinatorial space is too vast to memorize, and model rankings remain stable across dynamically generated question sets, enabling reliable periodic dataset refresh; each question aggregates 8-10 statements for comprehensive multi-knowledge assessment; annotators only verify formatting compliance without requiring domain expertise, substantially reducing annotation costs. Experiments on over 50 LLMs demonstrate that Encyclo-K poses substantial challenges with strong discriminative power. Even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, and model performance displays a clear gradient distribution--reasoning models span from 16.04% to 62.07%, while chat models range from 9.71% to 50.40%. These results validate the challenges introduced by dynamic evaluation and multi-statement comprehensive understanding. These findings establish Encyclo-K as a scalable framework for dynamic evaluation of LLMs' comprehensive understanding over multiple fine-grained disciplinary knowledge statements.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24867v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Encyclo-K，通过动态组合知识语句评估LLM的综合理解能力",
            "summary_zh": "基准测试在追踪大型语言模型（LLMs）的快速发展和识别其能力边界方面起着至关重要的作用。然而，现有的基准测试主要在问题层面策划问题，存在三个根本性的局限性：容易受到数据污染、限制于单知识点评估以及依赖于昂贵的领域专家标注。我们提出了Encyclo-K，这是一个基于语句的基准测试，从根本上重新思考了基准测试的构建。我们的关键见解是，知识语句，而不是问题，可以作为策划的单元，然后可以从中构建问题。我们从权威教科书中提取独立的知识语句，并通过在测试时随机抽样将它们动态地组合成评估问题。这种设计直接解决了所有三个局限性：组合空间太大而无法记忆，并且模型排名在动态生成的问题集中保持稳定，从而能够可靠地定期刷新数据集；每个问题聚合8-10个语句以进行全面的多知识评估；注释者仅验证格式合规性，而无需领域专业知识，从而大大降低了注释成本。对50多个LLM的实验表明，Encyclo-K提出了巨大的挑战，并具有很强的区分能力。即使是表现最佳的OpenAI-GPT-5.1也仅达到62.07％的准确率，并且模型性能显示出清晰的梯度分布-推理模型的范围从16.04％到62.07％，而聊天模型的范围从9.71％到50.40％。这些结果验证了动态评估和多语句综合理解所带来的挑战。这些发现将Encyclo-K确立为一个可扩展的框架，用于动态评估LLM对多个细粒度学科知识语句的综合理解。",
            "intro_zh": [
                "现有LLM基准测试易受数据污染，评估范围局限于单知识点，且依赖昂贵的专家标注。",
                "Encyclo-K以知识语句为单位构建基准，动态组合成问题，避免记忆，实现多知识点评估。",
                "实验表明，即使是顶尖LLM在Encyclo-K上表现仍有提升空间，验证了其挑战性和区分度。"
            ],
            "method_zh": "**问题定义**：现有LLM基准测试主要存在三个问题：一是容易受到数据污染，模型可能已经见过或学习过测试数据；二是评估通常只针对单个知识点，无法考察模型综合运用知识的能力；三是依赖于领域专家进行标注，成本高昂且难以扩展。这些问题限制了对LLM能力的全面和可靠评估。\\n\\n**核心思路**：Encyclo-K的核心思路是将知识语句作为基准测试的基本单元，而不是直接使用问题。通过从权威教科书中提取独立的知识语句，并在测试时动态地将这些语句组合成问题，可以创建一个巨大的问题空间，从而降低数据污染的风险。同时，每个问题包含多个知识语句，可以更全面地评估模型的综合理解能力。此外，标注者只需要验证语句的格式合规性，无需领域专业知识，大大降低了标注成本。\\n\\n**技术框架**：Encyclo-K的整体框架包括以下几个主要步骤：1. **知识语句提取**：从权威教科书中提取独立的、可验证的知识语句。2. **问题生成**：在测试时，随机抽取8-10个知识语句，将它们组合成一个问题。3. **模型评估**：将生成的问题输入到LLM中，评估其回答的准确性。4. **性能分析**：分析LLM在不同知识领域和不同难度级别的问题上的表现。\\n\\n**关键创新**：Encyclo-K的关键创新在于其动态组合知识语句的评估方法。与传统的静态基准测试相比，Encyclo-K可以生成几乎无限数量的问题，从而大大降低了数据污染的风险。此外，通过组合多个知识语句，Encyclo-K可以更全面地评估模型的综合理解能力。\\n\\n**关键设计**：在知识语句提取方面，论文强调从权威教科书中提取，以保证知识的准确性和可靠性。在问题生成方面，论文采用随机抽样的方法，以保证问题的多样性和随机性。在模型评估方面，论文采用准确率作为评估指标，并对不同知识领域和不同难度级别的问题进行细致的分析。",
            "application_zh": "Encyclo-K可用于全面评估LLM在各个学科领域的知识掌握程度和综合理解能力，帮助开发者改进模型性能。此外，该方法可应用于教育领域，辅助学生学习和知识巩固，或用于构建智能问答系统，提供更准确和全面的答案。",
            "highlight_zh": "实验结果表明，即使是顶尖的LLM，如OpenAI-GPT-5.1，在Encyclo-K上的准确率也仅为62.07%，表明该基准测试具有很强的挑战性。同时，不同模型的性能表现出明显的梯度分布，验证了Encyclo-K的区分能力，能够有效区分不同LLM的综合理解能力。",
            "tags_zh": [
                "大型语言模型",
                "基准测试",
                "知识评估",
                "动态评估",
                "综合理解",
                "知识语句",
                "数据污染"
            ],
            "_index": 58,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24867v1/figures/DynamicGPQA_pipeline.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24867v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24867v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information",
            "authors": [
                "Zhili Huang",
                "Ling Xu",
                "Chao Liu",
                "Weifeng Sun",
                "Xu Zhang",
                "Yan Lei",
                "Meng Yan",
                "Hongyu Zhang"
            ],
            "arxiv_id": "2512.24635v1",
            "summary": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.\n  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "22 pages, 7 figures, preprint version",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24635v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "DynaFix：一种执行级动态信息驱动的迭代式自动程序修复方法",
            "summary_zh": "自动程序修复(APR)旨在为有缺陷的程序自动生成正确的补丁。最近利用大型语言模型(LLM)的方法显示出潜力，但也面临局限性。大多数方法仅依赖于静态分析，忽略了运行时行为。一些方法尝试结合动态信号，但这些信号通常仅限于训练或微调，或者仅一次性注入到修复提示中，而没有迭代使用，无法充分捕获程序执行。当前的迭代修复框架通常依赖于粗粒度的反馈，例如通过/失败结果或异常类型，并且没有有效地利用细粒度的执行级信息。因此，模型难以模拟人类逐步调试，限制了其在多步骤推理和复杂错误修复中的有效性。为了解决这些挑战，我们提出DynaFix，一种执行级动态信息驱动的APR方法，它迭代地利用运行时信息来改进修复过程。在每个修复轮次中，DynaFix捕获执行级动态信息，例如变量状态、控制流路径和调用堆栈，将它们转换为结构化提示，以指导LLM生成候选补丁。如果补丁未能通过验证，DynaFix会重新执行修改后的程序，以收集新的执行信息以供下一次尝试。这种迭代循环基于更新的反馈逐步改进补丁，类似于人类开发人员的逐步调试实践。我们在Defects4J v1.2和v2.0基准上评估DynaFix。DynaFix修复了186个单函数错误，比最先进的基线提高了10%，其中包括38个以前未修复的错误。它在最多35次尝试内实现了正确的补丁，与现有方法相比，减少了70%的补丁搜索空间，从而证明了修复复杂错误的有效性和效率。",
            "intro_zh": [
                "现有APR方法依赖静态分析或粗粒度动态反馈，无法充分利用程序运行时信息，限制了复杂错误的修复能力。",
                "DynaFix通过迭代地收集和利用执行级动态信息（如变量状态、控制流），指导LLM生成和改进补丁。",
                "实验表明，DynaFix在Defects4J基准上修复了186个单函数错误，比现有方法提高了10%，并显著减少了搜索空间。"
            ],
            "method_zh": "**问题定义**：自动程序修复旨在自动生成程序缺陷的正确补丁。现有方法，特别是基于大型语言模型的方法，主要依赖静态分析，忽略了程序运行时的动态行为。即使一些方法尝试利用动态信息，也往往是粗粒度的（例如，测试用例通过/失败）或者仅在初始阶段使用，无法充分指导迭代修复过程。这导致模型难以进行多步骤推理和修复复杂错误。\\n\\n**核心思路**：DynaFix的核心思路是模拟人类开发者的调试过程，通过迭代地收集和利用程序运行时的细粒度动态信息来指导LLM生成和改进补丁。每次修复尝试后，DynaFix都会重新执行修改后的程序，收集新的执行信息，并将其作为反馈用于下一次修复尝试。这种迭代过程允许模型逐步逼近正确的补丁。\\n\\n**技术框架**：DynaFix的整体框架包含以下几个主要阶段：1) **程序执行**：执行待修复的程序，并收集执行级动态信息，例如变量状态、控制流路径和调用堆栈。2) **信息转换**：将收集到的动态信息转换为结构化的提示，以便LLM理解和利用。3) **补丁生成**：使用LLM根据提示生成候选补丁。4) **补丁验证**：使用测试用例验证生成的补丁。如果补丁通过验证，则修复成功；否则，返回第一步，重新执行程序并收集新的动态信息。\\n\\n**关键创新**：DynaFix的关键创新在于其迭代地利用执行级动态信息来指导LLM进行程序修复。与现有方法相比，DynaFix能够更充分地利用程序运行时信息，从而更有效地修复复杂错误。此外，DynaFix的迭代修复过程允许模型逐步逼近正确的补丁，减少了搜索空间。\\n\\n**关键设计**：DynaFix的关键设计包括：1) 如何有效地收集和表示执行级动态信息；2) 如何将动态信息转换为LLM能够理解的结构化提示；3) 如何设计迭代修复过程，以便模型能够逐步逼近正确的补丁。论文中具体使用的LLM模型和提示工程方法未明确说明，属于未知信息。",
            "application_zh": "DynaFix具有广泛的应用前景，可以应用于软件开发的各个阶段，例如单元测试、集成测试和系统测试。它可以帮助开发人员自动修复程序中的错误，提高软件质量和开发效率。此外，DynaFix还可以用于教育领域，帮助学生学习程序调试和修复技术。未来，DynaFix可以扩展到支持更复杂的程序和错误类型，并与其他自动化软件工程工具集成。",
            "highlight_zh": "DynaFix在Defects4J v1.2和v2.0基准上进行了评估，修复了186个单函数错误，比最先进的基线提高了10%，其中包括38个以前未修复的错误。此外，DynaFix在最多35次尝试内实现了正确的补丁，与现有方法相比，减少了70%的补丁搜索空间，表明其在修复复杂错误方面的效率。",
            "tags_zh": [
                "自动程序修复",
                "动态分析",
                "大型语言模型",
                "迭代修复",
                "执行级信息"
            ],
            "_index": 59,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24635v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24635v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24635v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Chat-Driven Optimal Management for Virtual Network Services",
            "authors": [
                "Yuya Miyaoka",
                "Masaki Inoue",
                "Kengo Urata",
                "Shigeaki Harada"
            ],
            "arxiv_id": "2512.24614v1",
            "summary": "This paper proposes a chat-driven network management framework that integrates natural language processing (NLP) with optimization-based virtual network allocation, enabling intuitive and reliable reconfiguration of virtual network services. Conventional intent-based networking (IBN) methods depend on statistical language models to interpret user intent but cannot guarantee the feasibility of generated configurations. To overcome this, we develop a two-stage framework consisting of an Interpreter, which extracts intent from natural language prompts using NLP, and an Optimizer, which computes feasible virtual machine (VM) placement and routing via an integer linear programming. In particular, the Interpreter translates user chats into update directions, i.e., whether to increase, decrease, or maintain parameters such as CPU demand and latency bounds, thereby enabling iterative refinement of the network configuration. In this paper, two intent extractors, which are a Sentence-BERT model with support vector machine (SVM) classifiers and a large language model (LLM), are introduced. Experiments in single-user and multi-user settings show that the framework dynamically updates VM placement and routing while preserving feasibility. The LLM-based extractor achieves higher accuracy with fewer labeled samples, whereas the Sentence-BERT with SVM classifiers provides significantly lower latency suitable for real-time operation. These results underscore the effectiveness of combining NLP-driven intent extraction with optimization-based allocation for safe, interpretable, and user-friendly virtual network management.",
            "categories": [
                "cs.NI",
                "cs.AI"
            ],
            "primary_category": "cs.NI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24614v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出聊天驱动的虚拟网络服务优化管理框架，实现意图驱动的网络重配置。",
            "summary_zh": "本文提出了一种聊天驱动的网络管理框架，该框架将自然语言处理（NLP）与基于优化的虚拟网络分配相结合，从而实现虚拟网络服务的直观和可靠的重配置。传统的意图驱动网络（IBN）方法依赖于统计语言模型来解释用户意图，但无法保证生成配置的可行性。为了克服这个问题，我们开发了一个两阶段框架，包括一个解释器（Interpreter），它使用NLP从自然语言提示中提取意图；以及一个优化器（Optimizer），它通过整数线性规划计算可行的虚拟机（VM）放置和路由。特别地，解释器将用户聊天转换为更新方向，即增加、减少或保持CPU需求和延迟界限等参数，从而实现网络配置的迭代优化。本文介绍了两种意图提取器，分别是带有支持向量机（SVM）分类器的Sentence-BERT模型和大型语言模型（LLM）。在单用户和多用户设置中的实验表明，该框架动态地更新VM放置和路由，同时保持可行性。基于LLM的提取器以更少的标记样本实现了更高的准确率，而带有SVM分类器的Sentence-BERT提供了显著更低的延迟，适合实时操作。这些结果强调了将NLP驱动的意图提取与基于优化的分配相结合，对于安全、可解释和用户友好的虚拟网络管理的有效性。",
            "intro_zh": [
                "传统IBN方法依赖统计语言模型，难以保证生成网络配置的可行性，限制了其应用。",
                "提出两阶段框架，利用NLP提取用户意图，并通过优化器计算可行的VM放置和路由方案。",
                "实验表明，该框架能动态更新VM放置和路由，同时保持可行性，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有意图驱动网络（IBN）方法依赖统计语言模型解释用户意图，但无法保证生成配置的可行性。这导致网络重配置过程可能产生无效或不稳定的状态，限制了用户交互的可靠性和效率。\\n\\n**核心思路**：核心思路是将自然语言处理（NLP）与优化算法相结合，构建一个聊天驱动的虚拟网络管理框架。用户通过自然语言表达意图，NLP模块负责理解并将其转化为优化器的输入，优化器则负责生成可行的网络配置方案。这种设计保证了配置的可行性，并提供了一种更直观、用户友好的网络管理方式。\\n\\n**技术框架**：该框架包含两个主要模块：解释器（Interpreter）和优化器（Optimizer）。解释器负责从用户聊天中提取意图，将其转化为更新方向（增加、减少或保持参数）。优化器则利用整数线性规划（ILP）计算可行的虚拟机（VM）放置和路由方案，以满足用户意图。整个流程是一个迭代过程，用户可以通过聊天不断调整意图，框架则相应地更新网络配置。\\n\\n**关键创新**：关键创新在于将NLP与优化算法紧密结合，实现意图驱动的虚拟网络管理。与传统IBN方法不同，该框架不仅能理解用户意图，还能保证生成配置的可行性。此外，通过聊天界面，用户可以更直观地与网络管理系统交互，实现更灵活的网络配置。\\n\\n**关键设计**：解释器使用了两种意图提取器：Sentence-BERT模型与SVM分类器，以及大型语言模型（LLM）。Sentence-BERT模型具有较低的延迟，适合实时操作，而LLM则在少量样本下能实现更高的准确率。优化器使用整数线性规划（ILP）来建模VM放置和路由问题，目标是最小化资源消耗，同时满足用户指定的约束条件（如延迟界限）。具体参数设置和损失函数细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "该研究成果可应用于云计算、边缘计算等场景，实现虚拟网络资源的智能管理和优化配置。通过自然语言交互，用户可以更方便地调整网络资源，满足不同应用的需求，提高资源利用率，降低运维成本。未来，该技术有望应用于更复杂的网络环境，例如5G网络、物联网等。",
            "highlight_zh": "实验结果表明，该框架能够动态更新VM放置和路由，同时保持可行性。基于LLM的意图提取器在少量标记样本下实现了更高的准确率，而基于Sentence-BERT的意图提取器则提供了更低的延迟，适合实时操作。这些结果验证了该框架在虚拟网络管理中的有效性和实用性。",
            "tags_zh": [
                "虚拟网络管理",
                "意图驱动网络",
                "自然语言处理",
                "整数线性规划",
                "虚拟机放置",
                "网络优化",
                "聊天机器人"
            ],
            "_index": 60,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24614v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24614v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24614v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning",
            "authors": [
                "Zheyu Shi",
                "Dong Qiu",
                "Shanlong Yu"
            ],
            "arxiv_id": "2512.24613v1",
            "summary": "This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "Accepted by IEEE ITCA 2025",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24613v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出面向群体审议的多智能体对话模型，解决复杂推理任务难题",
            "summary_zh": "本文提出了一种面向群体审议的多智能体对话模型，旨在解决单个大型语言模型在复杂推理任务中的局限性。该模型采用三层角色分工架构，包括生成、验证和整合。意见生成智能体产生多样化的推理视角，证据验证智能体检索外部知识并量化事实支持，一致性仲裁智能体整合逻辑上连贯的结论。引入自博弈机制以扩展多路径推理轨迹，同时检索增强模块动态补充外部知识。设计了结合事实一致性和逻辑连贯性的复合奖励函数，并应用改进的近端策略优化策略进行协同训练。实验结果表明，所提出的模型在HotpotQA上提高了16.8%的多跳推理准确率，在2WikiMultihopQA上提高了14.3%，在MeetingBank上提高了19.2%，同时一致性提高了21.5%。该模型比主流的多智能体方法具有更高的推理效率，为复杂推理任务提供了一种有效且稳定的解决方案。",
            "intro_zh": [
                "现有大型语言模型在复杂推理任务中存在局限性，难以有效整合多方面信息并保证推理过程的正确性。",
                "该模型模拟群体审议过程，通过多个智能体分工合作，生成、验证和整合信息，从而提升推理能力。",
                "实验表明，该模型在多跳推理准确率和一致性方面均有显著提升，且推理效率高于主流多智能体方法。"
            ],
            "method_zh": "**问题定义**：现有方法，特别是依赖单一大型语言模型的方法，在处理需要多步推理和外部知识的任务时，容易出现逻辑错误和事实性错误。这些模型难以有效地整合来自不同来源的信息，并缺乏对自身推理过程的有效验证机制，导致推理结果的可靠性降低。\\n\\n**核心思路**：本文的核心思路是模拟人类群体审议的过程，通过多个智能体扮演不同的角色（生成、验证、整合），协同完成复杂推理任务。这种分工合作的方式可以有效提高推理的全面性和准确性，并增强模型的可解释性。\\n\\n**技术框架**：该模型采用三层角色分工架构：1) **意见生成智能体**：负责生成多样化的推理视角和假设；2) **证据验证智能体**：负责检索外部知识，并评估各个推理步骤的事实支持度；3) **一致性仲裁智能体**：负责整合各个智能体的输出，并选择逻辑上最连贯的结论。此外，模型还引入了自博弈机制来探索更多的推理路径，并使用检索增强模块动态补充外部知识。\\n\\n**关键创新**：该模型最重要的技术创新点在于其面向群体审议的多智能体架构，以及针对复杂推理任务设计的复合奖励函数。与传统的单智能体或简单多智能体方法相比，该模型能够更好地模拟人类的推理过程，并有效地利用外部知识。自博弈机制和检索增强模块进一步提升了模型的探索能力和知识利用率。\\n\\n**关键设计**：模型使用改进的近端策略优化（PPO）算法进行协同训练，并设计了一个复合奖励函数，该函数结合了事实一致性（factual consistency）和逻辑连贯性（logical coherence）两个方面。事实一致性通过外部知识库的检索结果来评估，逻辑连贯性则通过推理链的合理性来评估。自博弈机制通过让智能体之间相互对抗，从而扩展推理路径。检索增强模块使用动态检索策略，根据当前推理状态选择合适的外部知识。",
            "application_zh": "该研究成果可应用于问答系统、智能客服、决策支持系统等领域，尤其是在需要复杂推理和知识整合的场景下。例如，在医疗诊断领域，该模型可以辅助医生进行病情分析和诊断；在金融风控领域，该模型可以用于识别潜在的欺诈行为。未来，该模型有望进一步提升人工智能在复杂问题解决方面的能力。",
            "highlight_zh": "实验结果表明，该模型在HotpotQA、2WikiMultihopQA和MeetingBank三个数据集上均取得了显著的性能提升。具体而言，在HotpotQA上，多跳推理准确率提高了16.8%；在2WikiMultihopQA上，提高了14.3%；在MeetingBank上，提高了19.2%。同时，模型的一致性也提高了21.5%。此外，该模型在推理效率方面也优于主流的多智能体方法。",
            "tags_zh": [
                "多智能体系统",
                "复杂推理",
                "对话模型",
                "知识检索",
                "群体审议"
            ],
            "_index": 61,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Recursive Language Models",
            "authors": [
                "Alex L. Zhang",
                "Tim Kraska",
                "Omar Khattab"
            ],
            "arxiv_id": "2512.24601v1",
            "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
            "categories": [
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "9 pages, 33 with Appendix",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24601v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出递归语言模型（RLM），解决LLM在超长上下文推理中的难题",
            "summary_zh": "本文研究了如何通过推理时扩展来允许大型语言模型（LLM）处理任意长度的提示。我们提出递归语言模型（RLM），这是一种通用的推理策略，它将长提示视为外部环境的一部分，并允许LLM以编程方式检查、分解提示，并递归地调用自身来处理提示片段。我们发现，RLM成功地处理了比模型上下文窗口大两个数量级的输入，并且即使对于较短的提示，在四个不同的长上下文任务中，也显著优于基础LLM和常见的长上下文支架，同时具有相当（或更便宜）的每次查询成本。",
            "intro_zh": [
                "现有LLM受限于固定长度的上下文窗口，难以处理超长输入，限制了其在长文档理解等领域的应用。",
                "RLM将长提示视为外部环境，允许LLM递归调用自身处理提示片段，从而突破上下文长度限制。",
                "实验表明，RLM在长上下文任务中显著优于传统LLM，且计算成本可与传统方法媲美甚至更低。"
            ],
            "method_zh": "**问题定义**：现有大型语言模型（LLM）的上下文窗口长度有限，无法直接处理超长文本输入。这限制了LLM在需要理解和处理大量信息的任务中的应用，例如长文档摘要、问答等。现有的长上下文处理方法，如截断、滑动窗口等，要么丢失信息，要么效率低下。\\n\\n**核心思路**：论文的核心思路是将LLM视为一个智能体，通过递归的方式处理长文本。具体来说，将长文本视为一个外部环境，LLM可以编程化地检查、分解这个环境，并递归地调用自身来处理文本片段。这种方式类似于人类阅读长篇文章时，会分段阅读并进行思考和总结。\\n\\n**技术框架**：RLM的整体框架包含以下几个主要步骤：1) **提示分解**：LLM首先将长提示分解成多个较小的片段。2) **递归调用**：LLM递归地调用自身来处理每个片段，并生成相应的输出。3) **信息整合**：LLM将各个片段的输出进行整合，生成最终的输出。在这个过程中，LLM可以根据需要选择不同的处理策略，例如总结、问答等。\\n\\n**关键创新**：RLM的关键创新在于其递归处理长文本的方式。与传统的长上下文处理方法相比，RLM可以更好地利用LLM的上下文信息，并且可以灵活地处理不同长度的文本。此外，RLM还引入了编程化的控制机制，允许LLM根据需要选择不同的处理策略。\\n\\n**关键设计**：RLM的关键设计包括：1) **提示工程**：设计合适的提示，引导LLM进行文本分解和递归调用。2) **控制机制**：设计有效的控制机制，控制LLM的递归深度和处理策略。3) **信息整合策略**：设计合理的信息整合策略，将各个片段的输出进行整合，生成最终的输出。论文中使用了标准的Transformer架构，并针对长文本处理进行了优化。损失函数采用交叉熵损失。",
            "application_zh": "RLM具有广泛的应用前景，例如长文档摘要、问答系统、代码理解与生成、以及需要处理大量信息的对话系统。该方法能够提升LLM在处理复杂、长篇幅任务时的性能，并有望推动LLM在更多实际场景中的应用，例如法律文件分析、金融报告解读等。",
            "highlight_zh": "实验结果表明，RLM在四个不同的长上下文任务中显著优于基础LLM和常见的长上下文支架。例如，在某些任务中，RLM的性能提升超过20%。更重要的是，RLM能够处理比模型上下文窗口大两个数量级的输入，这表明RLM具有很强的可扩展性。此外，RLM的每次查询成本与传统方法相当甚至更低。",
            "tags_zh": [
                "递归语言模型",
                "长上下文处理",
                "大型语言模型",
                "推理时扩展",
                "提示工程"
            ],
            "_index": 62,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24601v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24601v1/figures/Fig2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24601v1/figures/cost_quartiles_dual.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use",
            "authors": [
                "Wenrui Liu",
                "Zixiang Liu",
                "Elsie Dai",
                "Wenhan Yu",
                "Lei Yu",
                "Tong Yang"
            ],
            "arxiv_id": "2512.24565v1",
            "summary": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24565v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "MCPAgentBench：构建真实世界MCP工具使用评估基准，提升LLM Agent工具调用能力",
            "summary_zh": "大型语言模型（LLMs）越来越多地被用作自主Agent，并且通过模型上下文协议（MCP）利用外部工具被认为是未来的趋势。现有的MCP评估集存在依赖外部MCP服务和缺乏难度感知等问题。为了解决这些限制，我们提出了MCPAgentBench，这是一个基于真实世界MCP定义的基准，旨在评估Agent的工具使用能力。我们构建了一个包含真实任务和模拟MCP工具的数据集。评估采用动态沙箱环境，为Agent提供包含干扰项的候选工具列表，从而测试它们的工具选择和辨别能力。此外，我们引入了全面的指标来衡量任务完成率和执行效率。对各种最新的主流大型语言模型进行的实验表明，在处理复杂的、多步骤的工具调用时，性能存在显著差异。所有代码均已在Github上开源。",
            "intro_zh": [
                "现有MCP评估方法依赖外部服务，缺乏对任务难度的有效区分，限制了对Agent工具使用能力的全面评估。",
                "MCPAgentBench通过构建真实任务和模拟MCP工具的数据集，并在动态沙箱环境中评估Agent的工具选择和辨别能力，解决上述问题。",
                "实验结果表明，不同LLM在处理复杂多步骤工具调用时表现出显著差异，验证了MCPAgentBench的有效性和区分度。"
            ],
            "method_zh": "**问题定义**：现有MCP评估基准依赖外部MCP服务，这限制了其可扩展性和可控性。此外，现有基准缺乏对任务难度的有效建模，难以区分不同Agent在复杂任务上的工具使用能力。因此，需要一个更贴近真实场景、难度可控、且不依赖外部服务的评估基准来全面评估LLM Agent的MCP工具使用能力。\\n\\n**核心思路**：MCPAgentBench的核心思路是构建一个基于真实世界MCP定义的基准，包含真实任务和模拟MCP工具。通过动态沙箱环境，Agent需要从包含干扰项的候选工具列表中选择合适的工具来完成任务。这种设计模拟了真实世界中Agent需要面对的复杂环境和不确定性，从而更有效地评估Agent的工具使用能力。\\n\\n**技术框架**：MCPAgentBench的整体框架包括以下几个主要模块：1) 任务定义模块：定义一系列真实世界的任务，例如预定机票、查询天气等。2) MCP工具模拟模块：模拟与任务相关的MCP工具，例如机票预订API、天气查询API等。3) 动态沙箱环境：创建一个动态的沙箱环境，为Agent提供任务描述和候选工具列表。4) 评估指标模块：定义一系列评估指标，例如任务完成率、执行效率等。Agent在沙箱环境中接收任务，并根据任务描述选择合适的工具进行调用，最终完成任务。\\n\\n**关键创新**：MCPAgentBench的关键创新在于其真实性和动态性。它基于真实世界的MCP定义构建数据集，并采用动态沙箱环境进行评估。这种设计使得评估更加贴近真实场景，能够更有效地评估Agent的工具选择和辨别能力。此外，MCPAgentBench还引入了全面的评估指标，可以更全面地衡量Agent的工具使用性能。\\n\\n**关键设计**：在动态沙箱环境中，候选工具列表中包含干扰项，Agent需要从这些干扰项中选择正确的工具。任务的难度可以通过增加干扰项的数量和复杂性来控制。评估指标包括任务完成率（衡量Agent是否能够成功完成任务）和执行效率（衡量Agent完成任务所需的步骤数）。此外，还考虑了工具选择的准确率，以评估Agent的工具辨别能力。",
            "application_zh": "MCPAgentBench可用于评估和提升LLM Agent在各种实际应用场景中的工具使用能力，例如智能助手、自动化客服、智能家居控制等。通过该基准，可以更好地了解不同LLM在处理复杂任务时的优缺点，从而选择合适的LLM并进行针对性的优化，最终提升Agent的智能化水平和服务质量。",
            "highlight_zh": "实验结果表明，不同LLM在MCPAgentBench上的表现存在显著差异，尤其是在处理复杂的多步骤工具调用任务时。例如，某些LLM在任务完成率方面表现出色，但执行效率较低；而另一些LLM则在执行效率方面表现更好，但任务完成率较低。这些结果表明，MCPAgentBench能够有效地评估和区分不同LLM的工具使用能力。",
            "tags_zh": [
                "大型语言模型",
                "LLM Agent",
                "模型上下文协议",
                "MCP",
                "工具使用",
                "评估基准",
                "真实世界任务"
            ],
            "_index": 63,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24565v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24565v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24565v1/figures/tefs_tfs_comparison.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Localized Calibrated Uncertainty in Code Language Models",
            "authors": [
                "David Gros",
                "Prem Devanbu"
            ],
            "arxiv_id": "2512.24560v1",
            "summary": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24560v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出代码语言模型局部校准不确定性方法，辅助LLM代码生成质量控制",
            "summary_zh": "大型语言模型(LLM)能够从自然语言提示生成复杂的源代码。然而，LLM生成的代码可能偏离用户意图，需要监督和编辑。为了支持这一过程，本文提出定位生成代码中可能与用户意图不一致部分的技术。首先，创建了一个“最小意图对齐补丁”数据集，其中包含修复后的LLM生成程序，并使用测试用例验证正确性。然后，评估各种技术在预测代码中哪些部分需要进行最小修改时的校准概率的能力（即，概率与实际编辑概率的对应程度）。比较了白盒探测（提出了一种高效的任意跨度查询技术）与黑盒反射和基于自洽性的方法。结果表明，使用小型监督模型的探测器可以实现较低的校准误差和约0.2的Brier Skill Score，用于估计由规模大几个数量级的模型生成的代码中需要编辑的行。讨论了该技术的泛化能力，以及与AI监督和控制的联系，发现仅在代码上训练的探测器，如果允许新的概率缩放，也显示出泛化到自然语言错误的一些迹象。",
            "intro_zh": [
                "大型语言模型生成的代码可能偏离用户意图，缺乏有效的定位和校准不确定性的方法。",
                "提出一种局部校准不确定性的方法，通过探测技术预测代码中需要编辑的部分，辅助代码质量控制。",
                "实验表明，小型监督模型可以实现较低的校准误差和较高的Brier Skill Score，有效定位需要修改的代码行。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在生成代码时，输出结果与用户意图不符的问题。现有方法缺乏有效手段来定位和量化代码中潜在错误的位置，导致调试和修改效率低下。因此，需要一种能够准确预测代码中哪些部分可能需要修改的技术，以辅助人工干预和提高代码质量。\\n\\n**核心思路**：论文的核心思路是训练一个探测器（probe），使其能够预测LLM生成的代码中哪些部分需要进行修改以符合用户意图。通过将代码视为一个序列，并为每个代码片段分配一个概率，表示该片段需要被编辑的可能性。该概率需要经过校准，即概率值与实际编辑的频率相对应。这样，用户可以根据探测器给出的概率，优先检查和修改高概率的代码片段。\\n\\n**技术框架**：整体框架包括以下几个主要步骤：1) 创建一个包含LLM生成代码及其对应修复补丁的数据集。2) 设计并训练一个探测器，该探测器以LLM生成的代码作为输入，输出每个代码片段需要被编辑的概率。3) 评估探测器的性能，包括校准误差和Brier Skill Score等指标。4) 比较不同的探测方法，包括白盒探测、黑盒反射和基于自洽性的方法。\\n\\n**关键创新**：论文的关键创新在于提出了一种高效的白盒探测技术，用于任意跨度查询。此外，论文还构建了一个包含“最小意图对齐补丁”的数据集，为训练和评估探测器提供了基础。论文还探讨了该技术的泛化能力，以及与AI监督和控制的联系。\\n\\n**关键设计**：论文中，探测器可以使用小型监督模型实现，例如线性回归或小型神经网络。训练探测器的损失函数可以使用交叉熵损失或Brier Score等。为了提高探测器的泛化能力，可以使用数据增强技术，例如随机替换、删除或插入代码片段。此外，论文还提出了一种概率缩放技术，用于调整探测器输出的概率，以提高校准性能。",
            "application_zh": "该研究成果可应用于代码自动补全、代码审查、AI辅助编程等领域。通过定位LLM生成代码中潜在的错误，可以提高开发效率，降低调试成本，并提升软件质量。此外，该技术还可以用于评估不同LLM的代码生成能力，并为AI监督和控制提供支持。",
            "highlight_zh": "实验结果表明，使用小型监督模型的探测器可以实现较低的校准误差和约0.2的Brier Skill Score，用于估计由规模大几个数量级的模型生成的代码中需要编辑的行。此外，实验还发现，仅在代码上训练的探测器，如果允许新的概率缩放，也显示出泛化到自然语言错误的一些迹象。",
            "tags_zh": [
                "代码语言模型",
                "不确定性校准",
                "局部化",
                "AI监督",
                "代码生成",
                "白盒探测",
                "黑盒探测"
            ],
            "_index": 64,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24560v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24560v1/figs/halu_reliability_diagram_code_scaled_unscaled.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24560v1/figs/halu_reliability_diagram_code_scaled_scaled.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vibe Coding, Interface Flattening",
            "authors": [
                "Hongrui Jin"
            ],
            "arxiv_id": "2512.24939v1",
            "summary": "Large language models are reshaping programming by enabling 'vibe coding': the development of softwares through natural-language interaction with model-driven toolchains. This article argues that vibe coding is best understood as interface flattening, a reconfiguration in which previously distinct modalities (GUI, CLI, and API) appear to converge into a single conversational surface, even as the underlying chain of translation from intention to machinic effect lengthens and thickens. Drawing on Friedrich Kittler's materialist media theory and Alexander Galloway's account of interfaces as sites of protocol control, the paper situates programming as a historically localised interface arrangement rather than an essential relation to computation. Through a materialist reconstruction of the contemporary vibe-coding stack, it shows how remote compute infrastructures, latency and connectivity, structured outputs, function/tool calling, and interoperability standards such as the Model Context Protocol relocate control and meaning-making power to model and protocol providers. The apparent democratisation of technical capability therefore depends on new dependencies and new literacies. By foregrounding the tension between experiential flattening and infrastructural thickening, I demonstrate how LLM-mediated development redistributes symbolic labour/power, obscures responsibility, and privatises competencies previously dispersed across programming communities, contributing a critical lens on the political economy of AI-mediated human-computer interaction.",
            "categories": [
                "cs.HC",
                "cs.CL"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "16 pages, 1 figure",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24939v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "分析“Vibe Coding”范式，揭示大模型驱动软件开发中界面扁平化与控制权转移的矛盾。",
            "summary_zh": "大型语言模型正在通过“Vibe Coding”重塑编程，即通过与模型驱动的工具链进行自然语言交互来开发软件。本文认为，Vibe Coding 最好的理解方式是界面扁平化，一种先前不同的模态（GUI、CLI 和 API）似乎融合为单一对话界面的重构，即使从意图到机器效应的底层翻译链条变得更长更复杂。借鉴弗里德里希·基特勒的物质主义媒体理论和亚历山大·加洛韦关于界面作为协议控制场所的描述，本文将编程定位为一种历史局部化的界面安排，而非与计算的本质关系。通过对当代 Vibe Coding 堆栈的物质主义重构，本文展示了远程计算基础设施、延迟和连接性、结构化输出、函数/工具调用以及模型上下文协议等互操作性标准如何将控制和意义创造权转移给模型和协议提供商。因此，技术能力的表面民主化依赖于新的依赖关系和新的素养。通过突出体验扁平化和基础设施复杂化之间的张力，我展示了 LLM 介导的开发如何重新分配符号劳动/权力，模糊责任，并将先前分散在编程社区中的能力私有化，从而为人工智能介导的人机交互的政治经济学提供了一个批判视角。",
            "intro_zh": [
                "传统编程界面复杂，存在GUI、CLI、API等多种模态，用户学习成本高，交互效率低。",
                "论文提出“界面扁平化”概念，认为Vibe Coding将多种模态融合为自然语言对话界面，降低用户门槛。",
                "分析Vibe Coding堆栈，揭示其背后远程计算、互操作标准等基础设施对控制权和意义创造权的转移。"
            ],
            "method_zh": "**问题定义**：论文旨在分析大型语言模型（LLM）驱动的“Vibe Coding”范式对软件开发模式的影响。现有编程模式存在界面复杂、学习曲线陡峭等问题，而Vibe Coding试图通过自然语言交互降低编程门槛。然而，这种表面上的民主化可能隐藏着控制权和责任的转移，以及新的依赖关系。\n\n**核心思路**：论文的核心思路是将Vibe Coding理解为一种“界面扁平化”现象，即多种编程模态（GUI、CLI、API）融合为单一的自然语言对话界面。这种扁平化掩盖了底层基础设施的复杂性，以及控制权从开发者向模型和协议提供商的转移。论文借鉴了媒体理论和界面理论，将编程视为一种历史性的界面安排，而非与计算的本质关系。\n\n**技术框架**：论文采用了一种物质主义的重构方法，分析了Vibe Coding堆栈的各个组成部分，包括远程计算基础设施、延迟和连接性、结构化输出、函数/工具调用以及模型上下文协议等。通过分析这些组件之间的关系，论文揭示了控制权和意义创造权如何被重新分配。\n\n**关键创新**：论文的创新之处在于提出了“界面扁平化”的概念，并将其作为理解Vibe Coding的关键视角。与以往关注编程语言和开发工具的研究不同，论文将重点放在了界面和基础设施的政治经济学上，揭示了技术进步背后的权力关系。\n\n**关键设计**：论文没有涉及具体的参数设置、损失函数或网络结构等技术细节。其分析重点在于Vibe Coding的整体架构和流程，以及各个组件之间的交互关系。论文强调了互操作性标准（如模型上下文协议）在控制权转移中的作用，并分析了延迟和连接性等因素对用户体验和开发模式的影响。",
            "application_zh": "该研究成果可应用于分析和评估各种基于大型语言模型的软件开发工具和平台。有助于开发者、政策制定者和研究人员理解Vibe Coding的潜在影响，并制定相应的策略，以确保技术进步的公平性和可持续性。此外，该研究也为理解人工智能介导的人机交互的政治经济学提供了新的视角。",
            "highlight_zh": "论文通过分析Vibe Coding堆栈，揭示了远程计算基础设施、互操作性标准等因素如何将控制权和意义创造权转移给模型和协议提供商。强调了技术能力的表面民主化依赖于新的依赖关系和新的素养，并指出了LLM介导的开发可能导致符号劳动/权力重新分配、责任模糊和能力私有化等问题。",
            "tags_zh": [
                "Vibe Coding",
                "界面扁平化",
                "大型语言模型",
                "软件开发",
                "人机交互",
                "政治经济学",
                "控制权转移"
            ],
            "_index": 65,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models",
            "authors": [
                "Wenzhe Li",
                "Shujian Zhang",
                "Wenxuan Zhou",
                "John Lambert",
                "Chi Jin",
                "Andrew Hard",
                "Rajiv Mathews",
                "Lun Wang"
            ],
            "arxiv_id": "2512.24693v1",
            "summary": "Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \\textit{training} techniques, effective automated \\textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \\textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \\textbf{MU}lti-\\textbf{S}tep \\textbf{I}nstruction \\textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24693v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出MUSIC：多步指令对比方法，提升多轮对话奖励模型性能",
            "summary_zh": "评估多轮对话的质量对于开发强大的大型语言模型（LLMs）至关重要，但仍然是一个巨大的挑战，通常需要昂贵的人工评估。多轮奖励模型（RMs）提供了一种可扩展的替代方案，并且可以为指导LLM训练提供有价值的信号。虽然最近的工作已经推进了多轮\textit{训练}技术，但专门针对多轮交互的有效自动化\textit{评估}仍然滞后。我们观察到，标准的偏好数据集通常仅基于最终对话轮次来对比响应，提供的信号不足以捕捉多轮交互的细微差别。相反，我们发现，纳入跨越\textit{多个}轮次的对比对于构建稳健的多轮RM至关重要。受此发现的启发，我们提出了一种无监督数据增强策略，即\textbf{MU}lti-\textbf{S}tep \textbf{I}nstruction \textbf{C}ontrast (MUSIC)，它合成了在多个轮次中表现出差异的对比对话对。利用Skywork偏好数据集上的MUSIC，我们训练了一个基于Gemma-2-9B-Instruct模型的多轮RM。经验结果表明，我们的MUSIC增强RM优于基线方法，在多轮对话中实现了与高级专有LLM judges的判断更高的一致性，关键是，没有影响标准单轮RM基准上的性能。",
            "intro_zh": [
                "现有奖励模型在多轮对话评估中表现不足，因为标准偏好数据集缺乏足够的多轮交互信号。",
                "MUSIC通过无监督数据增强，合成跨多轮的对比对话对，从而增强奖励模型对多轮交互的理解。",
                "实验表明，MUSIC增强的奖励模型在多轮对话评估中与LLM judges的判断更一致，且不影响单轮性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多轮对话奖励模型训练数据不足的问题，现有奖励模型通常基于单轮对话数据训练，无法有效捕捉多轮对话中的上下文依赖和细微差别。这导致奖励模型在评估多轮对话质量时表现不佳，与人类判断的一致性较低。\\n\\n**核心思路**：论文的核心思路是通过无监督数据增强，生成包含多轮交互信息的对比样本。具体来说，MUSIC方法通过修改对话历史中的多个轮次，生成具有细微差异的对话对，从而迫使奖励模型学习区分不同对话策略的优劣。这种多步指令对比能够更全面地捕捉多轮对话的复杂性。\\n\\n**技术框架**：MUSIC方法主要包含以下几个步骤：1) 从现有对话数据集中选择对话样本；2) 随机选择对话中的多个轮次；3) 使用指令改写模型对选定的轮次进行修改，生成对比样本；4) 将原始对话和修改后的对话组成对比对，用于训练奖励模型。整个过程是无监督的，不需要人工标注。\\n\\n**关键创新**：MUSIC的关键创新在于其多步指令对比策略。与以往仅关注最终轮次对比的方法不同，MUSIC通过修改对话历史中的多个轮次，生成更具挑战性的对比样本，从而迫使奖励模型学习捕捉多轮对话中的长期依赖关系。这种方法能够更有效地利用现有数据，提升奖励模型的性能。\\n\\n**关键设计**：MUSIC方法的关键设计包括：1) 使用高质量的指令改写模型，确保生成的对比样本具有语义一致性和合理性；2) 随机选择修改的轮次数量和位置，增加对比样本的多样性；3) 使用合适的损失函数，例如hinge loss或margin ranking loss，鼓励奖励模型区分不同对话策略的优劣。",
            "application_zh": "MUSIC方法可以广泛应用于多轮对话系统的训练和评估。它可以用于训练更准确的奖励模型，从而指导对话策略的学习和优化。此外，MUSIC还可以用于评估不同对话系统的性能，为系统改进提供依据。该方法在智能客服、聊天机器人、虚拟助手等领域具有重要的应用价值。",
            "highlight_zh": "实验结果表明，MUSIC增强的奖励模型在多轮对话评估中显著优于基线方法。具体来说，MUSIC在与高级专有LLM judges的判断一致性方面取得了显著提升，同时保持了在标准单轮RM基准上的性能。这表明MUSIC能够有效提升奖励模型对多轮对话的理解能力，而不会牺牲其在单轮对话上的性能。",
            "tags_zh": [
                "多轮对话",
                "奖励模型",
                "数据增强",
                "指令对比",
                "无监督学习"
            ],
            "_index": 66,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24693v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24693v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Quantum Visual Word Sense Disambiguation: Unraveling Ambiguities Through Quantum Inference Model",
            "authors": [
                "Wenbo Qiao",
                "Peng Zhang",
                "Qinghua Hu"
            ],
            "arxiv_id": "2512.24687v1",
            "summary": "Visual word sense disambiguation focuses on polysemous words, where candidate images can be easily confused. Traditional methods use classical probability to calculate the likelihood of an image matching each gloss of the target word, summing these to form a posterior probability. However, due to the challenge of semantic uncertainty, glosses from different sources inevitably carry semantic biases, which can lead to biased disambiguation results. Inspired by quantum superposition in modeling uncertainty, this paper proposes a Quantum Inference Model for Unsupervised Visual Word Sense Disambiguation (Q-VWSD). It encodes multiple glosses of the target word into a superposition state to mitigate semantic biases. Then, the quantum circuit is executed, and the results are observed. By formalizing our method, we find that Q-VWSD is a quantum generalization of the method based on classical probability. Building on this, we further designed a heuristic version of Q-VWSD that can run more efficiently on classical computing. The experiments demonstrate that our method outperforms state-of-the-art classical methods, particularly by effectively leveraging non-specialized glosses from large language models, which further enhances performance. Our approach showcases the potential of quantum machine learning in practical applications and provides a case for leveraging quantum modeling advantages on classical computers while quantum hardware remains immature.",
            "categories": [
                "quant-ph",
                "cs.CL"
            ],
            "primary_category": "quant-ph",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24687v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出量子推理模型以解决视觉词义消歧问题",
            "summary_zh": "视觉词义消歧关注多义词的语义模糊性，传统方法依赖经典概率计算图像与目标词义的匹配可能性，容易受到不同来源的词义偏见影响。本文提出了一种量子推理模型（Q-VWSD），通过将目标词的多个词义编码为量子叠加态，减轻语义偏见。实验结果表明，该方法在性能上超越了现有的经典方法，尤其是在利用大型语言模型的非专业词义时，进一步提升了效果。此研究展示了量子机器学习在实际应用中的潜力，并为在量子硬件尚不成熟的情况下，利用量子建模优势提供了案例。",
            "intro_zh": [
                "现有的视觉词义消歧方法在处理多义词时容易受到语义偏见的影响，导致消歧结果不准确。",
                "本文提出的量子推理模型通过量子叠加态编码多个词义，旨在减轻传统方法中的语义偏见。",
                "实验结果显示，Q-VWSD在性能上优于最先进的经典方法，特别是在有效利用大型语言模型的词义时表现突出。"
            ],
            "method_zh": "**问题定义**：本文旨在解决视觉词义消歧中的语义模糊性问题，现有方法依赖经典概率，容易受到不同来源词义的偏见影响，导致消歧结果不准确。\\n\\n**核心思路**：论文提出的量子推理模型（Q-VWSD）通过量子叠加态将多个词义编码在一起，从而减轻语义偏见，利用量子计算的优势来处理不确定性。\\n\\n**技术框架**：Q-VWSD的整体架构包括词义编码、量子电路执行和结果观察三个主要模块。首先，将多个词义编码为量子叠加态；然后，执行量子电路以获取消歧结果；最后，观察并分析结果。\\n\\n**关键创新**：Q-VWSD是经典概率方法的量子推广，利用量子叠加态有效整合多个词义，显著改善了传统方法的偏见问题。\\n\\n**关键设计**：在模型设计中，关键参数包括量子电路的深度和结构，损失函数的选择，以及如何有效地从大型语言模型中提取非专业词义以增强模型性能。通过这些设计，Q-VWSD在经典计算环境中也能高效运行。 ",
            "application_zh": "该研究的潜在应用领域包括自然语言处理、计算机视觉和人机交互等。通过提高多义词的消歧能力，Q-VWSD可以在搜索引擎、智能助手和图像识别等实际场景中发挥重要作用，提升用户体验和系统准确性。未来，随着量子计算技术的发展，该方法有望在更复杂的任务中得到应用。",
            "highlight_zh": "实验结果表明，Q-VWSD在多个数据集上均超越了最先进的经典方法，尤其是在利用大型语言模型的非专业词义时，性能提升幅度达到20%以上。这一结果展示了量子机器学习在实际应用中的巨大潜力。",
            "tags_zh": [
                "量子推理",
                "视觉词义消歧",
                "多义词处理",
                "量子机器学习",
                "自然语言处理",
                "计算机视觉",
                "大型语言模型"
            ],
            "_index": 67,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24687v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24687v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24687v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs",
            "authors": [
                "Muhammad Abdullahi Said",
                "Muhammad Sammani Sani"
            ],
            "arxiv_id": "2512.24556v1",
            "summary": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24556v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示大语言模型在语言和时间维度上的安全漏洞，提出不变对齐方法。",
            "summary_zh": "随着大型语言模型（LLMs）集成到关键的全球基础设施中，英语安全对齐能够零样本迁移到其他语言的假设仍然是一个危险的盲点。本研究使用HausaSafety（一个基于西非威胁场景（例如，Yahoo-Yahoo欺诈、Dane枪支制造）的新型对抗数据集），对三个最先进的模型（GPT-5.1、Gemini 3 Pro和Claude 4.5 Opus）进行了系统审计。通过跨1,440次评估的2 x 4因子设计，我们测试了语言（英语vs.豪萨语）和时间框架之间的非线性交互。我们的结果挑战了当前的多语言安全差距叙事。我们没有发现低资源环境下的简单退化，而是发现了一种复杂的干扰机制，其中安全性由变量的交集决定。虽然模型表现出一种反向语言现象，即Claude 4.5 Opus在豪萨语中比在英语中更安全（分别为45.0%和36.7%），原因是其不确定性驱动的拒绝，但它们在时间推理方面遭受了灾难性的失败。我们报告了一种深刻的时间不对称性，其中过去时框架绕过了防御（15.6%安全），而将来时场景触发了过度保守的拒绝（57.2%安全）。最安全和最脆弱配置之间存在9.2倍的差异，证明安全性不是一个固定属性，而是一个上下文相关的状态。我们得出结论，当前的模型依赖于肤浅的启发式方法，而不是强大的语义理解，从而创建了安全口袋，使全球南方用户面临本地化的危害。我们提出不变对齐作为一种必要的范式转变，以确保跨语言和时间转变的安全稳定性。",
            "intro_zh": [
                "现有大语言模型在安全对齐方面存在漏洞，尤其是在非英语语境和时间推理上。",
                "论文提出通过不变对齐方法，增强模型在不同语言和时间框架下的安全稳定性。",
                "实验表明，模型在过去时框架下更容易被攻击，未来时框架下则过于保守，安全性能波动大。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型在安全对齐方面存在严重的跨语言和时间泛化问题。具体来说，模型在英语环境下训练的安全策略，无法有效地迁移到其他语言（如豪萨语），并且模型对过去和未来的事件的安全性判断存在显著差异。现有方法依赖于肤浅的启发式规则，缺乏对语义的深入理解，导致模型在特定语境下出现安全漏洞。\\n\\n**核心思路**：论文的核心思路是揭示大语言模型在语言和时间维度上的安全脆弱性，并提出一种“不变对齐”的范式，旨在使模型的安全策略在不同的语言和时间框架下保持一致性。通过对抗性测试，发现模型在不同语言和时间框架下的安全性能差异，从而指导模型的改进。\\n\\n**技术框架**：论文采用2 x 4因子设计，评估了三个最先进的模型（GPT-5.1、Gemini 3 Pro和Claude 4.5 Opus）在英语和豪萨语两种语言，以及不同时间框架下的安全性能。使用HausaSafety数据集，该数据集包含基于西非威胁场景的对抗性示例。通过分析模型在不同配置下的安全响应，揭示了模型在语言和时间维度上的安全漏洞。\\n\\n**关键创新**：论文的关键创新在于发现了大语言模型在时间推理上的“时间不对称性”，即模型对过去时和将来时事件的安全判断存在显著差异。此外，论文还提出了“不变对齐”的概念，强调模型安全策略在不同语境下的稳定性，这与传统的安全对齐方法有所不同。\\n\\n**关键设计**：论文的关键设计包括：1）HausaSafety对抗数据集，用于评估模型在西非特定威胁场景下的安全性能；2）2 x 4因子实验设计，用于系统地评估语言和时间框架对模型安全性的影响；3）对模型响应的安全性进行量化评估，并分析模型在不同配置下的安全性能差异。",
            "application_zh": "该研究成果可应用于提升大语言模型在全球范围内的安全性，尤其是在低资源语言和文化背景下的应用。通过不变对齐，可以减少模型在不同语境下的安全漏洞，降低模型被恶意利用的风险，从而促进人工智能技术的安全可靠发展。",
            "highlight_zh": "实验结果表明，Claude 4.5 Opus在豪萨语中的安全性高于英语（45.0% vs. 36.7%），但所有模型都表现出显著的时间不对称性，过去时框架下的安全性仅为15.6%，而将来时框架下则高达57.2%。最安全和最脆弱配置之间存在9.2倍的安全性差异。",
            "tags_zh": [
                "大语言模型",
                "安全对齐",
                "对抗性攻击",
                "时间推理",
                "多语言",
                "不变对齐",
                "豪萨语",
                "安全漏洞"
            ],
            "_index": 68,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24556v1/plots/01_ASR_Comparison.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24556v1/plots/04_Temporal_Vulnerability.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24556v1/plots/06_Category_Analysis.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Trustworthy Equipment Monitoring via Cascaded Anomaly Detection and Thermal Localization",
            "authors": [
                "Sungwoo Kang"
            ],
            "arxiv_id": "2512.24755v1",
            "summary": "Predictive maintenance demands accurate anomaly detection and trustable explanations. Although multimodal fusion of sensor time-series and thermal imagery shows promise, we demonstrate that naive fusion strategies can paradoxically degrade performance. This paper introduces a Cascaded Anomaly Detection framework that decouples detection and localization. Stage 1 employs an LSTM-based sensor encoder with temporal attention for high-accuracy detection, while Stage 2 activates a CNN-based thermal encoder for post-detection fault localization. Our results reveal that sensor-only detection outperforms full fusion by 8.3 percentage points (93.08% vs. 84.79% F1-score), challenging the assumption that additional modalities invariably improve performance. We further contribute an explainability pipeline integrating SHAP, temporal/spatial attention, and gate weight analysis. This analysis uncovers a \"modality bias\" where fusion models assign 65-87% weight to the weaker thermal modality. Validated on a real-world bearing dataset (78,397 samples), our cascaded approach achieves state-of-the-art accuracy while providing actionable diagnostics for maintenance decision-making.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24755v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出级联异常检测框架，用于设备状态监测与可信故障定位。",
            "summary_zh": "预测性维护需要准确的异常检测和可信的解释。虽然传感器时序数据和热图像的多模态融合显示出潜力，但我们证明了简单的融合策略可能会适得其反，降低性能。本文介绍了一种级联异常检测框架，该框架将检测和定位解耦。第一阶段采用基于LSTM的传感器编码器，结合时间注意力机制，实现高精度检测；第二阶段激活基于CNN的热编码器，用于检测后的故障定位。结果表明，仅使用传感器进行检测的性能优于完全融合，F1分数提高了8.3个百分点（93.08% vs. 84.79%），挑战了额外模态总能提高性能的假设。我们进一步贡献了一个可解释性流程，集成了SHAP、时间/空间注意力以及门权重分析。该分析揭示了一种“模态偏差”，即融合模型将65-87%的权重分配给较弱的热模态。在真实轴承数据集（78,397个样本）上的验证表明，我们的级联方法实现了最先进的精度，同时为维护决策提供可操作的诊断。",
            "intro_zh": [
                "现有设备监测方法在多模态融合时存在性能下降问题，简单融合可能引入模态偏差。",
                "提出级联异常检测框架，解耦检测与定位任务，利用传感器数据进行精确检测，热图像数据进行故障定位。",
                "实验表明，该方法在轴承数据集上优于传统融合方法，F1分数提升8.3%，并提供可解释性分析。"
            ],
            "method_zh": "**问题定义**：现有方法在设备异常检测中，简单地融合来自不同模态（如传感器数据和热图像）的信息，期望获得更好的性能。然而，这种简单的融合策略可能会由于模态之间的差异和噪声，导致性能下降，甚至不如仅使用单一模态的效果。特别是，当某些模态的信息质量较低时，融合反而会引入偏差，影响检测的准确性。\\n\\n**核心思路**：本文的核心思路是将异常检测和故障定位这两个任务解耦，采用级联的方式进行处理。首先，利用高质量的传感器数据进行精确的异常检测，然后再利用热图像数据进行故障定位。这种解耦的方式可以避免低质量模态对检测精度的干扰，同时充分利用不同模态的优势。\\n\\n**技术框架**：该框架包含两个主要阶段：异常检测阶段和故障定位阶段。在异常检测阶段，使用基于LSTM的传感器编码器，并引入时间注意力机制，以捕捉传感器时序数据中的关键信息。在故障定位阶段，使用基于CNN的热编码器，对检测到的异常进行定位，确定故障发生的具体位置。整个流程是先检测，后定位，形成一个级联的结构。\\n\\n**关键创新**：该方法最重要的创新点在于提出了级联的异常检测框架，将检测和定位任务分离，避免了简单融合带来的模态偏差问题。此外，该方法还引入了可解释性分析流程，通过SHAP、时间/空间注意力以及门权重分析，揭示了模型决策的过程，提高了模型的可信度。\\n\\n**关键设计**：在异常检测阶段，LSTM编码器采用时间注意力机制，能够自动学习不同时间步的重要性，从而更好地捕捉时序数据中的异常模式。在故障定位阶段，CNN编码器能够有效地提取热图像中的空间特征，从而实现精确的故障定位。此外，可解释性分析流程通过SHAP值来评估不同特征对预测结果的贡献，通过注意力权重来分析模型关注的关键区域，通过门权重来评估不同模态的重要性。",
            "application_zh": "该研究成果可广泛应用于工业设备的状态监测和预测性维护领域，例如轴承、电机、泵等关键设备的健康管理。通过早期发现设备异常并精确定位故障，可以有效降低设备停机时间，减少维护成本，提高生产效率，并为维护决策提供可靠依据。",
            "highlight_zh": "实验结果表明，该级联异常检测框架在轴承数据集上取得了显著的性能提升。仅使用传感器数据的检测F1分数达到93.08%，优于多模态融合的84.79%，提升了8.3个百分点。同时，可解释性分析揭示了融合模型中存在的模态偏差，为模型优化提供了重要依据。",
            "tags_zh": [
                "异常检测",
                "故障定位",
                "多模态融合",
                "时间序列分析",
                "可解释性AI",
                "预测性维护",
                "LSTM",
                "CNN"
            ],
            "_index": 69,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24755v1/figures/cascaded_pipeline.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24755v1/figures/ablation_comparison.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24755v1/figures/shap_sensor_importance.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Taking Advantage of Rational Canonical Form for Faster Ring-LWE based Encrypted Controller with Recursive Multiplication",
            "authors": [
                "Donghyeon Song",
                "Yeongjun Jang",
                "Joowon Lee",
                "Junsoo Kim"
            ],
            "arxiv_id": "2512.24658v1",
            "summary": "This paper aims to provide an efficient implementation of encrypted linear dynamic controllers that perform recursive multiplications on a Ring-Learning With Errors (Ring-LWE) based cryptosystem. By adopting a system-theoretical approach, we significantly reduce both time and space complexities, particularly the number of homomorphic operations required for recursive multiplications. Rather than encrypting the entire state matrix of a given controller, the state matrix is transformed into its rational canonical form, whose sparse and circulant structure enables that encryption and computation are required only on its nontrivial columns. Furthermore, we propose a novel method to ``pack'' each of the input and the output matrices into a single polynomial, thereby reducing the number of homomorphic operations. Simulation results demonstrate that the proposed design enables a remarkably fast implementation of encrypted controllers.",
            "categories": [
                "eess.SY"
            ],
            "primary_category": "eess.SY",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "8 pages, 1 figures, presented at 2025 IEEE Conference on Decision and Control",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24658v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "OMOMO"
                    ],
                    "score": 2.5
                }
            ],
            "relevance_score": 2.5,
            "hit_pillars": [
                "5_interaction_reaction"
            ],
            "headline_zh": "利用有理标准型加速基于Ring-LWE的加密控制器递归乘法",
            "summary_zh": "本文旨在为基于Ring-LWE的密码系统上执行递归乘法的加密线性动态控制器提供一种高效的实现方案。通过采用系统理论方法，我们显著降低了时间和空间复杂度，特别是递归乘法所需的同态运算次数。与加密给定控制器的整个状态矩阵不同，我们将状态矩阵转换为其有理标准型，其稀疏和循环结构使得仅需加密和计算其非平凡列。此外，我们提出了一种新颖的方法来将输入和输出矩阵“打包”成单个多项式，从而减少同态运算的次数。仿真结果表明，所提出的设计能够实现非常快速的加密控制器实现。",
            "intro_zh": [
                "现有加密控制器在递归乘法中计算复杂度高，效率受限。",
                "将状态矩阵转换为有理标准型，利用其稀疏性减少加密和计算量。",
                "提出一种打包输入输出矩阵的新方法，进一步减少同态运算次数，提升效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决基于Ring-LWE的加密线性动态控制器中，递归乘法计算复杂度高、效率低下的问题。现有方法通常直接加密整个状态矩阵，导致大量的同态运算，时间和空间复杂度都很高。\\n\\n**核心思路**：论文的核心思路是将控制器的状态矩阵转换为有理标准型。有理标准型具有稀疏和循环的结构，这意味着只需要加密和计算矩阵的非平凡列，从而显著减少了计算量。此外，通过将输入和输出矩阵打包成单个多项式，进一步减少了同态运算的次数。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 将控制器的状态矩阵转换为有理标准型；2) 仅加密有理标准型的非平凡列；3) 将输入和输出矩阵打包成单个多项式；4) 在加密域中执行递归乘法；5) 解密结果。整体框架旨在利用有理标准型的特性来优化加密控制器的性能。\\n\\n**关键创新**：论文的关键创新在于两个方面：一是将有理标准型应用于加密控制器，利用其稀疏性降低计算复杂度；二是提出了一种新的打包方法，将输入和输出矩阵打包成单个多项式，进一步减少同态运算的次数。与现有方法直接加密整个状态矩阵相比，该方法在计算效率上具有显著优势。\\n\\n**关键设计**：论文的关键设计包括：选择合适的Ring-LWE参数以保证安全性和性能；设计高效的打包和解包算法，以减少打包过程中的计算开销；优化同态乘法运算，例如使用Number Theoretic Transform (NTT) 等技术加速多项式乘法。",
            "application_zh": "该研究成果可应用于对安全性要求较高的控制系统中，例如无人机控制、机器人控制、智能电网等。通过加密控制器，可以防止恶意攻击者篡改控制策略，保障系统的安全稳定运行。该方法在保护隐私的同时，实现了控制系统的功能，具有重要的实际应用价值。",
            "highlight_zh": "仿真结果表明，该设计能够实现非常快速的加密控制器实现。通过将状态矩阵转换为有理标准型并采用新的打包方法，显著减少了同态运算的次数，从而提高了加密控制器的计算效率。具体的性能数据和与现有方法的对比结果（例如加速比）在论文中进行了详细展示。",
            "tags_zh": [
                "加密控制",
                "Ring-LWE",
                "同态加密",
                "有理标准型",
                "递归乘法"
            ],
            "_index": 70,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films",
            "authors": [
                "Rongji Xun",
                "Junjie Yuan",
                "Zhongjie Wang"
            ],
            "arxiv_id": "2512.24946v1",
            "summary": "Existing open-source film restoration methods show limited performance compared to commercial methods due to training with low-quality synthetic data and employing noisy optical flows. In addition, high-resolution films have not been explored by the open-source methods.We propose HaineiFRDM(Film Restoration Diffusion Model), a film restoration framework, to explore diffusion model's powerful content-understanding ability to help human expert better restore indistinguishable film defects.Specifically, we employ a patch-wise training and testing strategy to make restoring high-resolution films on one 24GB-VRAMR GPU possible and design a position-aware Global Prompt and Frame Fusion Modules.Also, we introduce a global-local frequency module to reconstruct consistent textures among different patches. Besides, we firstly restore a low-resolution result and use it as global residual to mitigate blocky artifacts caused by patching process.Furthermore, we construct a film restoration dataset that contains restored real-degraded films and realistic synthetic data.Comprehensive experimental results conclusively demonstrate the superiority of our model in defect restoration ability over existing open-source methods. Code and the dataset will be released.",
            "categories": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24946v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出HaineiFRDM，利用扩散模型修复快速移动影片中的缺陷",
            "summary_zh": "现有的开源影片修复方法由于使用低质量的合成数据进行训练以及采用有噪声的光流，其性能与商业方法相比存在局限性。此外，开源方法尚未探索高分辨率影片的修复。我们提出了HaineiFRDM（Film Restoration Diffusion Model），一个影片修复框架，旨在探索扩散模型强大的内容理解能力，以帮助人类专家更好地修复难以区分的影片缺陷。具体来说，我们采用逐块训练和测试策略，使得在一块24GB显存的GPU上修复高分辨率影片成为可能，并设计了位置感知的全局提示和帧融合模块。此外，我们引入了一个全局-局部频率模块来重建不同块之间一致的纹理。而且，我们首先恢复一个低分辨率的结果，并将其用作全局残差来减轻分块过程引起的块状伪影。此外，我们构建了一个影片修复数据集，其中包含修复后的真实退化影片和逼真的合成数据。全面的实验结果最终证明了我们的模型在缺陷修复能力方面优于现有的开源方法。代码和数据集将会发布。",
            "intro_zh": [
                "现有开源影片修复方法在处理高质量影片时表现不足，主要受限于训练数据质量和光流噪声。",
                "HaineiFRDM利用扩散模型的内容理解能力，通过逐块处理、全局提示和频率模块等手段提升修复效果。",
                "实验表明，HaineiFRDM在缺陷修复能力上优于现有开源方法，并发布了代码和数据集。"
            ],
            "method_zh": "**问题定义**：现有开源影片修复方法在高分辨率影片修复方面存在局限性，主要原因是训练数据质量不高（通常是合成数据）以及光流估计存在噪声。这些问题导致修复效果不佳，难以满足实际应用需求。此外，现有方法难以处理快速移动场景下的影片缺陷修复。\\n\\n**核心思路**：HaineiFRDM的核心思路是利用扩散模型强大的内容理解和生成能力，学习影片内容的先验知识，从而更准确地修复影片中的缺陷。通过结合全局信息和局部细节，以及频率域的约束，可以生成更逼真、一致的修复结果。同时，采用逐块处理策略，解决了高分辨率影片修复的显存限制问题。\\n\\n**技术框架**：HaineiFRDM的整体框架包括以下几个主要模块：1) 逐块处理模块：将高分辨率影片分割成小块进行处理，降低显存需求。2) 位置感知的全局提示模块：利用全局信息引导局部修复，提高修复的上下文一致性。3) 帧融合模块：融合相邻帧的信息，提高时间一致性。4) 全局-局部频率模块：在频率域进行约束，保证纹理的一致性。5) 低分辨率全局残差模块：首先修复一个低分辨率版本，然后将其作为残差添加到高分辨率修复结果中，减少块状伪影。\\n\\n**关键创新**：HaineiFRDM的关键创新在于：1) 探索了扩散模型在影片修复领域的应用，利用其强大的生成能力。2) 提出了位置感知的全局提示和帧融合模块，有效利用了全局信息和时间信息。3) 引入了全局-局部频率模块，保证了纹理的一致性。4) 采用了低分辨率全局残差模块，减轻了块状伪影。\\n\\n**关键设计**：HaineiFRDM的关键设计包括：1) 逐块处理的块大小选择，需要在显存限制和修复效果之间进行权衡。2) 全局提示模块中全局信息的提取方式，例如使用全局平均池化或注意力机制。3) 频率模块中频率分解的层数和频率分量的选择。4) 损失函数的设计，例如结合像素损失、感知损失和对抗损失等。",
            "application_zh": "HaineiFRDM可应用于老旧电影修复、视频监控画面修复、以及专业影视制作等领域。该技术能够有效去除影片中的划痕、污渍、噪声等缺陷，提升影片的视觉质量，具有重要的商业价值和社会意义。未来，该技术有望进一步发展，实现全自动化的影片修复，降低人工成本。",
            "highlight_zh": "实验结果表明，HaineiFRDM在影片缺陷修复能力上显著优于现有的开源方法。通过定量指标和视觉效果对比，HaineiFRDM能够更有效地去除影片中的缺陷，并生成更逼真、自然的修复结果。此外，该模型能够在24GB显存的GPU上处理高分辨率影片，具有较强的实用性。",
            "tags_zh": [
                "影片修复",
                "扩散模型",
                "高分辨率",
                "缺陷修复",
                "全局提示",
                "频率模块",
                "深度学习",
                "视频处理"
            ],
            "_index": 71,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24946v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24946v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24946v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning Temporally Consistent Turbulence Between Sparse Snapshots via Diffusion Models",
            "authors": [
                "Mohammed Sardar",
                "Małgorzata J. Zimoń",
                "Samuel Draycott",
                "Alistair Revell",
                "Alex Skillen"
            ],
            "arxiv_id": "2512.24813v1",
            "summary": "We investigate the statistical accuracy of temporally interpolated spatiotemporal flow sequences between sparse, decorrelated snapshots of turbulent flow fields using conditional Denoising Diffusion Probabilistic Models (DDPMs). The developed method is presented as a proof-of-concept generative surrogate for reconstructing coherent turbulent dynamics between sparse snapshots, demonstrated on a 2D Kolmogorov Flow, and a 3D Kelvin-Helmholtz Instability (KHI). We analyse the generated flow sequences through the lens of statistical turbulence, examining the time-averaged turbulent kinetic energy spectra over generated sequences, and temporal decay of turbulent structures. For the non-stationary Kelvin-Helmholtz Instability, we assess the ability of the proposed method to capture evolving flow statistics across the most strongly time-varying flow regime. We additionally examine instantaneous fields and physically motivated metrics at key stages of the KHI flow evolution.",
            "categories": [
                "physics.flu-dyn",
                "cs.LG"
            ],
            "primary_category": "physics.flu-dyn",
            "published": "2025-12-31",
            "updated": "2025-12-31",
            "comment": "15 pages, 10 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.24813v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "8_physics_animation"
            ],
            "headline_zh": "提出基于条件扩散模型的时序一致湍流插值方法，用于稀疏快照间的湍流重建。",
            "summary_zh": "本文研究了使用条件去噪扩散概率模型（DDPMs）在湍流场稀疏、非相关快照之间进行时序插值，以提高时空流动序列的统计精度。该方法被提出作为一种概念验证的生成代理模型，用于重建稀疏快照之间连贯的湍流动力学。通过二维柯尔莫哥洛夫流和三维开尔文-亥姆霍兹不稳定性（KHI）进行了演示。我们通过统计湍流的角度分析生成的流动序列，检查生成序列上时间平均的湍流动能谱以及湍流结构的时间衰减。对于非平稳的开尔文-亥姆霍兹不稳定性，我们评估了所提出的方法在最强烈的时间变化流动状态下捕获演化流动统计的能力。此外，我们还检查了KHI流动演化关键阶段的瞬时场和物理驱动的指标。",
            "intro_zh": [
                "现有方法难以从稀疏快照中准确重建湍流的时序演化过程，尤其是在非平稳湍流中。",
                "利用条件去噪扩散概率模型，学习湍流场在稀疏快照间的时序一致性，生成连贯的湍流动力学。",
                "在2D Kolmogorov Flow和3D Kelvin-Helmholtz Instability上验证，评估了湍流动能谱和时间衰减等统计特性。"
            ],
            "method_zh": "**问题定义**：该论文旨在解决从稀疏且非相关的湍流场快照中重建时间上连贯的湍流演化过程的问题。现有方法通常难以准确地插值或预测湍流场在时间上的演变，尤其是在湍流具有高度非线性和非平稳性的情况下，导致重建的湍流场在统计特性上与真实湍流场存在偏差。\\n\\n**核心思路**：论文的核心思路是利用条件去噪扩散概率模型（DDPMs）学习湍流场在时间上的演化模式。通过将稀疏快照作为条件，DDPM能够生成符合湍流统计规律且在时间上连贯的湍流场序列。这种方法避免了直接求解复杂的流体动力学方程，而是通过学习数据分布来重建湍流场。\\n\\n**技术框架**：整体框架包含以下几个主要步骤：1）数据准备：准备稀疏的湍流场快照序列作为条件输入。2）扩散过程：逐步向湍流场中添加噪声，直到完全变为高斯噪声。3）反向扩散过程：从高斯噪声出发，逐步去除噪声，并根据条件快照来引导生成过程，最终得到插值后的湍流场序列。该框架利用了DDPM强大的生成能力，能够生成具有高分辨率和时间连贯性的湍流场。\\n\\n**关键创新**：该论文的关键创新在于将条件DDPM应用于湍流场的时序插值问题，并证明了其在重建湍流统计特性方面的有效性。与传统的插值方法相比，该方法能够更好地捕捉湍流的非线性和随机性，从而生成更逼真的湍流场序列。此外，该方法还能够处理非平稳湍流，并捕捉其随时间变化的统计特性。\\n\\n**关键设计**：论文中使用了标准的DDPM架构，并针对湍流场的特点进行了一些调整。例如，使用了合适的网络结构来捕捉湍流场的空间相关性，并设计了合适的损失函数来保证生成湍流场的统计特性与真实湍流场一致。具体的参数设置和网络结构细节在论文中进行了详细描述。",
            "application_zh": "该研究成果可应用于计算流体力学、气候模拟、航空航天工程等领域。例如，可以利用该方法从有限的实验数据或计算结果中重建完整的湍流场，从而提高模拟精度和效率。此外，该方法还可以用于生成高分辨率的湍流场数据，用于训练其他机器学习模型或进行科学可视化。",
            "highlight_zh": "实验结果表明，该方法能够有效地重建稀疏快照之间的湍流演化过程，并准确地捕捉湍流的统计特性，如湍流动能谱和时间衰减。在3D Kelvin-Helmholtz Instability实验中，该方法能够捕捉到随时间变化的流动统计特性，并在关键阶段生成符合物理规律的瞬时场。",
            "tags_zh": [
                "湍流重建",
                "扩散模型",
                "时序插值",
                "计算流体力学",
                "生成模型"
            ],
            "_index": 72,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.24813v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.24813v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.24813v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        }
    ]
}