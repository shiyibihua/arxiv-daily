{
    "papers": [
        {
            "title": "Is This Tracker On? A Benchmark Protocol for Dynamic Tracking",
            "authors": [
                "Ilona Demler",
                "Saumya Chauhan",
                "Georgia Gkioxari"
            ],
            "arxiv_id": "2510.19819v1",
            "summary": "We introduce ITTO, a challenging new benchmark suite for evaluating and\ndiagnosing the capabilities and limitations of point tracking methods. Our\nvideos are sourced from existing datasets and egocentric real-world recordings,\nwith high-quality human annotations collected through a multi-stage pipeline.\nITTO captures the motion complexity, occlusion patterns, and object diversity\ncharacteristic of real-world scenes -- factors that are largely absent in\ncurrent benchmarks. We conduct a rigorous analysis of state-of-the-art tracking\nmethods on ITTO, breaking down performance along key axes of motion complexity.\nOur findings reveal that existing trackers struggle with these challenges,\nparticularly in re-identifying points after occlusion, highlighting critical\nfailure modes. These results point to the need for new modeling approaches\ntailored to real-world dynamics. We envision ITTO as a foundation testbed for\nadvancing point tracking and guiding the development of more robust tracking\nalgorithms.",
            "headline_zh": "提出ITTO基准套件以评估点跟踪方法在真实世界动态中的性能",
            "intro_zh": [
                "核心问题：现有点跟踪方法在真实世界运动复杂性、遮挡和对象多样性方面存在局限",
                "方法要点：构建基于现有数据集和第一人称视频的基准，采用多阶段流程收集高质量人工标注",
                "实验或效果：分析显示跟踪器在遮挡后重识别方面表现不佳，揭示关键失败模式"
            ],
            "tags_zh": [
                "点跟踪基准",
                "动态跟踪评估",
                "遮挡重识别",
                "真实世界视频",
                "性能诊断"
            ],
            "_index": 0
        },
        {
            "title": "Semantic World Models",
            "authors": [
                "Jacob Berg",
                "Chuning Zhu",
                "Yanda Bao",
                "Ishan Durugkar",
                "Abhishek Gupta"
            ],
            "arxiv_id": "2510.19818v1",
            "summary": "Planning with world models offers a powerful paradigm for robotic control.\nConventional approaches train a model to predict future frames conditioned on\ncurrent frames and actions, which can then be used for planning. However, the\nobjective of predicting future pixels is often at odds with the actual planning\nobjective; strong pixel reconstruction does not always correlate with good\nplanning decisions. This paper posits that instead of reconstructing future\nframes as pixels, world models only need to predict task-relevant semantic\ninformation about the future. For such prediction the paper poses world\nmodeling as a visual question answering problem about semantic information in\nfuture frames. This perspective allows world modeling to be approached with the\nsame tools underlying vision language models. Thus vision language models can\nbe trained as \"semantic\" world models through a supervised finetuning process\non image-action-text data, enabling planning for decision-making while\ninheriting many of the generalization and robustness properties from the\npretrained vision-language models. The paper demonstrates how such a semantic\nworld model can be used for policy improvement on open-ended robotics tasks,\nleading to significant generalization improvements over typical paradigms of\nreconstruction-based action-conditional world modeling. Website available at\nhttps://weirdlabuw.github.io/swm.",
            "headline_zh": "提出语义世界模型以解决机器人控制中像素预测与规划目标不匹配问题",
            "intro_zh": [
                "核心问题：传统世界模型预测未来像素与规划决策目标不一致，导致性能不佳",
                "方法要点：将世界建模视为视觉问答问题，预测任务相关语义信息，利用视觉语言模型进行微调",
                "实验或效果：在开放机器人任务中实现策略改进，显著提升泛化能力"
            ],
            "tags_zh": [
                "语义世界模型",
                "视觉问答",
                "机器人规划",
                "视觉语言模型",
                "泛化改进"
            ],
            "_index": 1
        },
        {
            "title": "olmOCR 2: Unit Test Rewards for Document OCR",
            "authors": [
                "Jake Poznanski",
                "Luca Soldaini",
                "Kyle Lo"
            ],
            "arxiv_id": "2510.19817v1",
            "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
            "headline_zh": "提出olmOCR 2 OCR系统，使用单元测试奖励训练VLM以提升文档转换质量",
            "intro_zh": [
                "核心问题：文档OCR需将PDF等转换为有序纯文本，但复杂布局如数学公式和表格处理困难",
                "方法要点：采用强化学习与可验证奖励，基于合成文档生成多样单元测试进行训练",
                "实验或效果：在olmOCR-Bench基准上实现SOTA，数学公式、表格和多列布局转换改进显著"
            ],
            "tags_zh": [
                "文档OCR",
                "强化学习",
                "单元测试",
                "视觉语言模型",
                "合成数据生成",
                "布局解析"
            ],
            "_index": 2
        },
        {
            "title": "How to Evaluate Monocular Depth Estimation?",
            "authors": [
                "Siyang Wu",
                "Jack Nugent",
                "Willow Yang",
                "Jia Deng"
            ],
            "arxiv_id": "2510.19814v1",
            "summary": "Monocular depth estimation is an important task with rapid progress, but how\nto evaluate it remains an open question, as evidenced by a lack of\nstandardization in existing literature and a large selection of evaluation\nmetrics whose trade-offs and behaviors are not well understood. This paper\ncontributes a novel, quantitative analysis of existing metrics in terms of\ntheir sensitivity to various types of perturbations of ground truth,\nemphasizing comparison to human judgment. Our analysis reveals that existing\nmetrics are severely under-sensitive to curvature perturbation such as making\nflat surfaces wavy. To remedy this, we introduce a new metric based on relative\nsurface normals, along with new depth visualization tools and a principled\nmethod to create composite metrics with better human alignment. Code and data\nare available at: https://github.com/princeton-vl/evalmde.",
            "headline_zh": "提出基于相对表面法线的新指标以改进单目深度估计评估",
            "intro_zh": [
                "核心问题：现有单目深度估计评估指标缺乏标准化，对扰动敏感度不足。",
                "方法要点：引入相对表面法线指标和可视化工具，提升与人类判断对齐。",
                "实验或效果：分析显示现有指标对曲率扰动不敏感，新指标改善评估效果。"
            ],
            "tags_zh": [
                "单目深度估计",
                "评估指标",
                "表面法线",
                "扰动分析",
                "人类对齐"
            ],
            "_index": 3
        },
        {
            "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
            "authors": [
                "Yusu Qian",
                "Eli Bocek-Rivele",
                "Liangchen Song",
                "Jialing Tong",
                "Yinfei Yang",
                "Jiasen Lu",
                "Wenze Hu",
                "Zhe Gan"
            ],
            "arxiv_id": "2510.19808v1",
            "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided\nimage editing capabilities, with systems like GPT-4o and Nano-Banana setting\nnew benchmarks. However, the research community's progress remains constrained\nby the absence of large-scale, high-quality, and openly accessible datasets\nbuilt from real images. We introduce Pico-Banana-400K, a comprehensive\n400K-image dataset for instruction-based image editing. Our dataset is\nconstructed by leveraging Nano-Banana to generate diverse edit pairs from real\nphotographs in the OpenImages collection. What distinguishes Pico-Banana-400K\nfrom previous synthetic datasets is our systematic approach to quality and\ndiversity. We employ a fine-grained image editing taxonomy to ensure\ncomprehensive coverage of edit types while maintaining precise content\npreservation and instruction faithfulness through MLLM-based quality scoring\nand careful curation. Beyond single turn editing, Pico-Banana-400K enables\nresearch into complex editing scenarios. The dataset includes three specialized\nsubsets: (1) a 72K-example multi-turn collection for studying sequential\nediting, reasoning, and planning across consecutive modifications; (2) a\n56K-example preference subset for alignment research and reward model training;\nand (3) paired long-short editing instructions for developing instruction\nrewriting and summarization capabilities. By providing this large-scale,\nhigh-quality, and task-rich resource, Pico-Banana-400K establishes a robust\nfoundation for training and benchmarking the next generation of text-guided\nimage editing models.",
            "headline_zh": "提出Pico-Banana-400K数据集以解决文本引导图像编辑中大规模高质量数据缺失问题",
            "intro_zh": [
                "核心问题：文本引导图像编辑研究因缺乏大规模、高质量、开放的真实图像数据集而受限",
                "方法要点：利用Nano-Banana从OpenImages生成编辑对，通过细粒度分类和多模态模型评分确保质量与多样性",
                "实验或效果：数据集包含40万图像，支持多轮编辑、偏好对齐和指令重写等复杂场景研究"
            ],
            "tags_zh": [
                "文本引导图像编辑",
                "大规模数据集",
                "多模态模型",
                "图像编辑分类",
                "指令忠实度",
                "多轮编辑"
            ],
            "_index": 4
        },
        {
            "title": "Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models",
            "authors": [
                "Xiaozhen Qiao",
                "Jingkai Zhao",
                "Yuqiu Jiang",
                "Xianda Guo",
                "Zhe Sun",
                "Hongyuan Zhang",
                "Xuelong Li"
            ],
            "arxiv_id": "2510.19802v1",
            "summary": "Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization\nthrough large-scale image-text pretraining, yet their performance can drop once\nthe deployment distribution diverges from the training distribution. To address\nthis, Test-Time Adaptation (TTA) methods update models using unlabeled target\ndata. However, existing approaches often ignore two key challenges: prototype\ndegradation in long-tailed distributions and confusion between semantically\nsimilar classes. To tackle these issues, we propose \\textbf{C}lass-Aware\n\\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative\n\\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed\nspecifically for VLMs to enhance generalization under distribution shifts.\nCPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that\ndynamically adjusts per-class capacity based on test-time frequency and\nactivation history, with a rejuvenation mechanism for inactive classes to\nretain rare-category knowledge. Additionally, a \\textit{Negative Contrastive\nLearning} Mechanism identifies and constrains hard visual-textual negatives to\nimprove class separability. The framework employs asymmetric optimization,\nrefining only textual prototypes while anchoring on stable visual features.\nExperiments on 15 benchmarks show that CPL-NC consistently outperforms prior\nTTA methods across both ResNet-50 and ViT-B/16 backbones.",
            "headline_zh": "提出类感知原型学习与负对比方法，以增强视觉语言模型在测试时分布偏移下的泛化能力。",
            "intro_zh": [
                "核心问题：视觉语言模型在分布偏移时性能下降，原型退化和类间混淆是主要挑战。",
                "方法要点：动态调整类原型缓存，结合负对比学习机制提升类可分性。",
                "实验或效果：在15个基准测试中优于现有方法，适用于ResNet-50和ViT-B/16骨干网络。"
            ],
            "tags_zh": [
                "测试时适应",
                "视觉语言模型",
                "原型学习",
                "负对比学习",
                "分布偏移",
                "类感知缓存"
            ],
            "_index": 5
        },
        {
            "title": "OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation",
            "authors": [
                "Guowei Xu",
                "Yuxuan Bian",
                "Ailing Zeng",
                "Mingyi Shi",
                "Shaoli Huang",
                "Wen Li",
                "Lixin Duan",
                "Qiang Xu"
            ],
            "arxiv_id": "2510.19789v1",
            "summary": "This paper introduces OmniMotion-X, a versatile multimodal framework for\nwhole-body human motion generation, leveraging an autoregressive diffusion\ntransformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently\nsupports diverse multimodal tasks, including text-to-motion, music-to-dance,\nspeech-to-gesture, and global spatial-temporal control scenarios (e.g., motion\nprediction, in-betweening, completion, and joint/trajectory-guided synthesis),\nas well as flexible combinations of these tasks. Specifically, we propose the\nuse of reference motion as a novel conditioning signal, substantially enhancing\nthe consistency of generated content, style, and temporal dynamics crucial for\nrealistic animations. To handle multimodal conflicts, we introduce a\nprogressive weak-to-strong mixed-condition training strategy. To enable\nhigh-quality multimodal training, we construct OmniMoCap-X, the largest unified\nmultimodal motion dataset to date, integrating 28 publicly available MoCap\nsources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.\nTo ensure detailed and consistent annotations, we render sequences into videos\nand use GPT-4o to automatically generate structured and hierarchical captions,\ncapturing both low-level actions and high-level semantics. Extensive\nexperimental evaluations confirm that OmniMotion-X significantly surpasses\nexisting methods, demonstrating state-of-the-art performance across multiple\nmultimodal tasks and enabling the interactive generation of realistic,\ncoherent, and controllable long-duration motions.",
            "headline_zh": "提出OmniMotion-X框架以解决多模态全身运动生成问题",
            "intro_zh": [
                "核心问题：多模态任务中运动生成的一致性与可控性不足",
                "方法要点：采用自回归扩散变换器与参考运动条件增强生成质量",
                "实验或效果：在多个任务中超越现有方法，实现长时程可控运动生成"
            ],
            "tags_zh": [
                "多模态运动生成",
                "自回归扩散变换器",
                "参考运动条件",
                "弱到强混合训练",
                "SMPL-X数据集",
                "长时程运动控制"
            ],
            "_index": 6
        },
        {
            "title": "SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas",
            "authors": [
                "Hongyu Ding",
                "Xinyue Liang",
                "Yudong Fang",
                "You Wu",
                "Jieqi Shi",
                "Jing Huo",
                "Wenbin Li",
                "Jing Wu",
                "Yu-Kun Lai",
                "Yang Gao"
            ],
            "arxiv_id": "2510.19766v1",
            "summary": "In this paper, we propose SEA, a novel approach for active robot exploration\nthrough semantic map prediction and a reinforcement learning-based hierarchical\nexploration policy. Unlike existing learning-based methods that rely on\none-step waypoint prediction, our approach enhances the agent's long-term\nenvironmental understanding to facilitate more efficient exploration. We\npropose an iterative prediction-exploration framework that explicitly predicts\nthe missing areas of the map based on current observations. The difference\nbetween the actual accumulated map and the predicted global map is then used to\nguide exploration. Additionally, we design a novel reward mechanism that\nleverages reinforcement learning to update the long-term exploration\nstrategies, enabling us to construct an accurate semantic map within limited\nsteps. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art exploration strategies, achieving superior\ncoverage ares of the global map within the same time constraints.",
            "headline_zh": "提出SEA方法，通过语义地图预测和强化学习策略提升机器人主动探索效率",
            "intro_zh": [
                "核心问题：现有学习型方法依赖单步路径点预测，缺乏长期环境理解，导致探索效率低。",
                "方法要点：采用迭代预测-探索框架，预测缺失地图区域，并基于预测与实际地图差异指导探索。",
                "实验效果：在相同时间限制下，显著优于现有方法，实现更高的全局地图覆盖率。"
            ],
            "tags_zh": [
                "机器人探索",
                "语义地图预测",
                "强化学习",
                "迭代预测框架",
                "主动探索策略"
            ],
            "_index": 7
        },
        {
            "title": "Adaptive Distribution-aware Quantization for Mixed-Precision Neural Networks",
            "authors": [
                "Shaohang Jia",
                "Zhiyong Huang",
                "Zhi Yu",
                "Mingyang Hou",
                "Shuai Miao",
                "Han Yang"
            ],
            "arxiv_id": "2510.19760v1",
            "summary": "Quantization-Aware Training (QAT) is a critical technique for deploying deep\nneural networks on resource-constrained devices. However, existing methods\noften face two major challenges: the highly non-uniform distribution of\nactivations and the static, mismatched codebooks used in weight quantization.\nTo address these challenges, we propose Adaptive Distribution-aware\nQuantization (ADQ), a mixed-precision quantization framework that employs a\ndifferentiated strategy. The core of ADQ is a novel adaptive weight\nquantization scheme comprising three key innovations: (1) a quantile-based\ninitialization method that constructs a codebook closely aligned with the\ninitial weight distribution; (2) an online codebook adaptation mechanism based\non Exponential Moving Average (EMA) to dynamically track distributional shifts;\nand (3) a sensitivity-informed strategy for mixed-precision allocation. For\nactivations, we integrate a hardware-friendly non-uniform-to-uniform mapping\nscheme. Comprehensive experiments validate the effectiveness of our method. On\nImageNet, ADQ enables a ResNet-18 to achieve 71.512% Top-1 accuracy with an\naverage bit-width of only 2.81 bits, outperforming state-of-the-art methods\nunder comparable conditions. Furthermore, detailed ablation studies on CIFAR-10\nsystematically demonstrate the individual contributions of each innovative\ncomponent, validating the rationale and effectiveness of our design.",
            "headline_zh": "提出自适应分布感知量化框架以解决混合精度神经网络量化问题",
            "intro_zh": [
                "核心问题：激活分布高度非均匀和权重量化码本静态不匹配",
                "方法要点：采用分位数初始化、在线码本自适应和敏感度混合精度分配",
                "实验或效果：在ImageNet上ResNet-18达71.512%准确率，平均位宽2.81位"
            ],
            "tags_zh": [
                "混合精度量化",
                "量化感知训练",
                "自适应码本",
                "非均匀分布",
                "硬件友好映射",
                "神经网络压缩"
            ],
            "_index": 8
        },
        {
            "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation",
            "authors": [
                "Jiacheng Liu",
                "Xinyu Wang",
                "Yuqi Lin",
                "Zhikai Wang",
                "Peiru Wang",
                "Peiliang Cai",
                "Qinming Zhou",
                "Zhengan Yan",
                "Zexuan Yan",
                "Zhengyi Shi",
                "Chang Zou",
                "Yue Ma",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2510.19755v1",
            "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
            "headline_zh": "综述扩散缓存方法以提升多模态生成效率",
            "intro_zh": [
                "扩散模型因多步迭代和复杂网络导致高计算开销和延迟",
                "扩散缓存通过特征级跨步重用和层间调度实现无训练加速",
                "分析显示缓存从静态重用演进到动态预测，增强灵活性和集成性"
            ],
            "tags_zh": [
                "扩散模型",
                "缓存方法",
                "多模态生成",
                "高效推理",
                "训练自由加速"
            ],
            "_index": 9
        },
        {
            "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models",
            "authors": [
                "Ameesh Shah",
                "William Chen",
                "Adwait Godbole",
                "Federico Mora",
                "Sanjit A. Seshia",
                "Sergey Levine"
            ],
            "arxiv_id": "2510.19752v1",
            "summary": "Solving complex real-world control tasks often takes multiple tries: if we\nfail at first, we reflect on what went wrong, and change our strategy\naccordingly to avoid making the same mistake. In robotics,\nVision-Language-Action models (VLAs) offer a promising path towards solving\ncomplex control tasks, but lack the ability to contextually and dynamically\nreadjust behavior when they fail to accomplish a task. In this work, we\nintroduce Learning from Inference-Time Execution (LITEN), which connects a VLA\nlow-level policy to a high-level VLM that conditions on past experiences by\nincluding them in-context, allowing it to learn the affordances and\ncapabilities of the low-level VLA. Our approach iterates between a reasoning\nphase that generates and executes plans for the low-level VLA, and an\nassessment phase that reflects on the resulting execution and draws useful\nconclusions to be included in future reasoning contexts. Unlike similar\napproaches to self-refinement in non-robotics domains, LITEN must reflect on\nunstructured real-world robot trajectories (e.g., raw videos), which requires\nstructured guiderails during assessment. Our experimental results demonstrate\nLITEN is able to effectively learn from past experience to generate plans that\nuse high-affordance instructions to accomplish long-horizon tasks.",
            "headline_zh": "提出LITEN方法，通过推理时学习提升视觉-语言-动作模型在机器人任务中的动态调整能力",
            "intro_zh": [
                "核心问题：视觉-语言-动作模型在机器人控制中缺乏失败后的上下文动态行为调整能力",
                "方法要点：结合高低层模型，通过推理与评估阶段迭代学习低层模型的可用性与能力",
                "实验或效果：LITEN能从经验中学习，生成高可用性指令以完成长时程任务"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "推理时学习",
                "机器人控制",
                "长时程任务",
                "自我反思"
            ],
            "_index": 10
        },
        {
            "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
            "authors": [
                "Gunshi Gupta",
                "Karmesh Yadav",
                "Zsolt Kira",
                "Yarin Gal",
                "Rahaf Aljundi"
            ],
            "arxiv_id": "2510.19732v1",
            "summary": "To enable embodied agents to operate effectively over extended timeframes, it\nis crucial to develop models that form and access memories to stay\ncontextualized in their environment. In the current paradigm of training\ntransformer-based policies for embodied sequential decision-making tasks,\nvisual inputs often overwhelm the context limits of transformers, while humans\ncan maintain and utilize a lifetime of experience compressed as memories.\nSignificant compression is possible in principle, as much of the input is\nirrelevant and can be abstracted. However, existing approaches predominantly\nfocus on either recurrent models with fixed-size memory or transformers with\nfull-context reliance. In this work, we propose Memo, a transformer-based\narchitecture and training recipe for reinforcement learning (RL) on\nmemory-intensive, long-horizon tasks. Memo incorporates the creation and\nretrieval of memory by interleaving periodic summarization tokens with the\ninputs of a model during training. We demonstrate Memo's effectiveness on a\ngridworld meta-RL benchmark and a multi-object navigation task in\nphoto-realistic indoor settings. Memo outperforms naive long-context\ntransformer baselines while being more compute and storage efficient.\nAdditionally, Memo generalizes better to longer contexts at inference time and\nremains robust in streaming settings, where historical context must be\ntruncated to fit inference constraints.",
            "headline_zh": "提出Memo架构以解决长时程具身任务中transformer内存效率问题",
            "intro_zh": [
                "核心问题：transformer在具身决策中视觉输入易超上下文限制，需高效记忆机制",
                "方法要点：引入周期性摘要令牌，在训练中集成记忆创建与检索",
                "实验或效果：在网格世界和室内导航任务中优于基线，提升计算效率与泛化能力"
            ],
            "tags_zh": [
                "具身智能",
                "强化学习",
                "transformer架构",
                "内存效率",
                "长时程任务"
            ],
            "_index": 11
        },
        {
            "title": "LyTimeT: Towards Robust and Interpretable State-Variable Discovery",
            "authors": [
                "Kuai Yu",
                "Crystal Su",
                "Xiang Liu",
                "Judah Goldfeder",
                "Mingyuan Shao",
                "Hod Lipson"
            ],
            "arxiv_id": "2510.19716v1",
            "summary": "Extracting the true dynamical variables of a system from high-dimensional\nvideo is challenging due to distracting visual factors such as background\nmotion, occlusions, and texture changes. We propose LyTimeT, a two-phase\nframework for interpretable variable extraction that learns robust and stable\nlatent representations of dynamical systems. In Phase 1, LyTimeT employs a\nspatio-temporal TimeSformer-based autoencoder that uses global attention to\nfocus on dynamically relevant regions while suppressing nuisance variation,\nenabling distraction-robust latent state learning and accurate long-horizon\nvideo prediction. In Phase 2, we probe the learned latent space, select the\nmost physically meaningful dimensions using linear correlation analysis, and\nrefine the transition dynamics with a Lyapunov-based stability regularizer to\nenforce contraction and reduce error accumulation during roll-outs. Experiments\non five synthetic benchmarks and four real-world dynamical systems, including\nchaotic phenomena, show that LyTimeT achieves mutual information and intrinsic\ndimension estimates closest to ground truth, remains invariant under background\nperturbations, and delivers the lowest analytical mean squared error among\nCNN-based (TIDE) and transformer-only baselines. Our results demonstrate that\ncombining spatio-temporal attention with stability constraints yields\npredictive models that are not only accurate but also physically interpretable.",
            "headline_zh": "提出LyTimeT框架以从高维视频中提取鲁棒且可解释的动态系统状态变量",
            "intro_zh": [
                "核心问题：高维视频中动态变量提取受背景运动、遮挡和纹理变化等干扰因素影响",
                "方法要点：采用两阶段框架，结合时空注意力编码和Lyapunov稳定性正则化",
                "实验或效果：在合成和真实系统上验证，实现高精度预测和鲁棒性，优于基线方法"
            ],
            "tags_zh": [
                "动态系统建模",
                "视频预测",
                "可解释AI",
                "时空注意力",
                "稳定性正则化"
            ],
            "_index": 12
        },
        {
            "title": "Explainable Face Presentation Attack Detection via Ensemble-CAM",
            "authors": [
                "Rashik Shadman",
                "M G Sarwar Murshed",
                "Faraz Hussain"
            ],
            "arxiv_id": "2510.19695v1",
            "summary": "Presentation attacks represent a critical security threat where adversaries\nuse fake biometric data, such as face, fingerprint, or iris images, to gain\nunauthorized access to protected systems. Various presentation attack detection\n(PAD) systems have been designed leveraging deep learning (DL) models to\nmitigate this type of threat. Despite their effectiveness, most of the DL\nmodels function as black boxes - their decisions are opaque to their users. The\npurpose of explainability techniques is to provide detailed information about\nthe reason behind the behavior or decision of DL models. In particular, visual\nexplanation is necessary to better understand the decisions or predictions of\nDL-based PAD systems and determine the key regions due to which a biometric\nimage is considered real or fake by the system. In this work, a novel\ntechnique, Ensemble-CAM, is proposed for providing visual explanations for the\ndecisions made by deep learning-based face PAD systems. Our goal is to improve\nDL-based face PAD systems by providing a better understanding of their\nbehavior. Our provided visual explanations will enhance the transparency and\ntrustworthiness of DL-based face PAD systems.",
            "headline_zh": "提出Ensemble-CAM以增强基于深度学习的面部呈现攻击检测系统的可解释性",
            "intro_zh": [
                "核心问题：深度学习模型在面部呈现攻击检测中决策不透明，影响系统可信度。",
                "方法要点：引入Ensemble-CAM技术，为检测决策提供视觉解释，突出关键区域。",
                "实验或效果：未知具体实验细节，但旨在提升系统透明度和用户信任。"
            ],
            "tags_zh": [
                "面部呈现攻击检测",
                "可解释人工智能",
                "视觉解释",
                "深度学习模型",
                "Ensemble-CAM"
            ],
            "_index": 13
        },
        {
            "title": "Curvilinear Structure-preserving Unpaired Cross-domain Medical Image Translation",
            "authors": [
                "Zihao Chen",
                "Yi Zhou",
                "Xudong Jiang",
                "Li Chen",
                "Leopold Schmetterer",
                "Bingyao Tan",
                "Jun Cheng"
            ],
            "arxiv_id": "2510.19679v1",
            "summary": "Unpaired image-to-image translation has emerged as a crucial technique in\nmedical imaging, enabling cross-modality synthesis, domain adaptation, and data\naugmentation without costly paired datasets. Yet, existing approaches often\ndistort fine curvilinear structures, such as microvasculature, undermining both\ndiagnostic reliability and quantitative analysis. This limitation is\nconsequential in ophthalmic and vascular imaging, where subtle morphological\nchanges carry significant clinical meaning. We propose Curvilinear\nStructure-preserving Translation (CST), a general framework that explicitly\npreserves fine curvilinear structures during unpaired translation by\nintegrating structure consistency into the training. Specifically, CST augments\nbaseline models with a curvilinear extraction module for topological\nsupervision. It can be seamlessly incorporated into existing methods. We\nintegrate it into CycleGAN and UNSB as two representative backbones.\nComprehensive evaluation across three imaging modalities: optical coherence\ntomography angiography, color fundus and X-ray coronary angiography\ndemonstrates that CST improves translation fidelity and achieves\nstate-of-the-art performance. By reinforcing geometric integrity in learned\nmappings, CST establishes a principled pathway toward curvilinear\nstructure-aware cross-domain translation in medical imaging.",
            "headline_zh": "提出CST框架以解决医学图像无配对翻译中细曲线结构失真问题",
            "intro_zh": [
                "现有方法在医学图像无配对翻译中易扭曲细曲线结构，影响诊断可靠性",
                "CST通过集成曲线提取模块和拓扑监督，增强结构一致性",
                "在多种成像模态实验中，CST提升翻译保真度并达到先进性能"
            ],
            "tags_zh": [
                "医学图像翻译",
                "曲线结构保留",
                "无配对学习",
                "拓扑监督",
                "跨域合成"
            ],
            "_index": 14
        },
        {
            "title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
            "authors": [
                "John Burden",
                "Jonathan Prunty",
                "Ben Slater",
                "Matthieu Tehenan",
                "Greg Davis",
                "Lucy Cheke"
            ],
            "arxiv_id": "2510.19678v1",
            "summary": "Multimodal large language models (MLLMs) achieve strong performance on\nvision-language tasks, yet their visual processing is opaque. Most black-box\nevaluations measure task accuracy, but reveal little about underlying\nmechanisms. Drawing on cognitive psychology, we adapt classic visual search\nparadigms -- originally developed to study human perception -- to test whether\nMLLMs exhibit the ``pop-out'' effect, where salient visual features are\ndetected independently of distractor set size. Using controlled experiments\ntargeting colour, size and lighting features, we find that advanced MLLMs\nexhibit human-like pop-out effects in colour or size-based disjunctive (single\nfeature) search, as well as capacity limits for conjunctive (multiple feature)\nsearch. We also find evidence to suggest that MLLMs, like humans, incorporate\nnatural scene priors such as lighting direction into object representations. We\nreinforce our findings using targeted fine-tuning and mechanistic\ninterpretability analyses. Our work shows how visual search can serve as a\ncognitively grounded diagnostic tool for evaluating perceptual capabilities in\nMLLMs.",
            "headline_zh": "提出视觉搜索作为行为测试，评估多模态大语言模型的感知机制。",
            "intro_zh": [
                "核心问题：多模态大语言模型的视觉处理机制不透明，现有评估难以揭示底层机制。",
                "方法要点：借鉴认知心理学，采用经典视觉搜索范式测试模型是否表现出弹出效应。",
                "实验或效果：模型在颜色或大小搜索中表现出人类类似弹出效应，并受场景先验影响。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视觉搜索",
                "弹出效应",
                "认知心理学",
                "机制评估",
                "场景先验"
            ],
            "_index": 15
        },
        {
            "title": "Fast Marker Detection for UV-Based Visual Relative Localisation in Agile UAV Swarms",
            "authors": [
                "Vojtěch Vrba",
                "Viktor Walter",
                "Petr Štěpán",
                "Martin Saska"
            ],
            "arxiv_id": "2510.19663v1",
            "summary": "A novel approach for the fast onboard detection of isolated markers for\nvisual relative localisation of multiple teammates in agile UAV swarms is\nintroduced in this paper. As the detection forms a key component of real-time\nlocalisation systems, a three-fold innovation is presented, consisting of an\noptimised procedure for CPUs, a GPU shader program, and a functionally\nequivalent FPGA streaming architecture. For the proposed CPU and GPU solutions,\nthe mean processing time per pixel of input camera frames was accelerated by\ntwo to three orders of magnitude compared to the state of the art. For the\nlocalisation task, the proposed FPGA architecture offered the most significant\noverall acceleration by minimising the total delay from camera exposure to\ndetection results. Additionally, the proposed solutions were evaluated on\nvarious 32-bit and 64-bit embedded platforms to demonstrate their efficiency,\nas well as their feasibility for applications using low-end UAVs and MAVs.\nThus, it has become a crucial enabling technology for agile UAV swarming.",
            "headline_zh": "提出快速标记检测方法以支持敏捷无人机群的视觉相对定位",
            "intro_zh": [
                "核心问题：敏捷无人机群中实时视觉相对定位需要快速检测孤立标记。",
                "方法要点：优化CPU程序、GPU着色器和FPGA流架构，加速处理速度。",
                "实验或效果：CPU和GPU方案处理速度提升2-3个数量级，FPGA最小化延迟。"
            ],
            "tags_zh": [
                "标记检测",
                "视觉相对定位",
                "无人机群",
                "实时系统",
                "硬件加速"
            ],
            "_index": 16
        },
        {
            "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments",
            "authors": [
                "Hongyu Ding",
                "Ziming Xu",
                "Yudong Fang",
                "You Wu",
                "Zixuan Chen",
                "Jieqi Shi",
                "Jing Huo",
                "Yifan Zhang",
                "Yang Gao"
            ],
            "arxiv_id": "2510.19655v1",
            "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment.",
            "headline_zh": "提出LaViRA框架以解决零样本视觉语言导航中的泛化与推理权衡问题",
            "intro_zh": [
                "核心问题：零样本视觉语言导航在连续环境中面临泛化与推理能力的权衡",
                "方法要点：采用语言-视觉-机器人动作的粗到细层次分解，利用多模态大模型优势",
                "实验或效果：在VLN-CE基准上显著超越现有方法，展示优越泛化能力"
            ],
            "tags_zh": [
                "零样本导航",
                "视觉语言导航",
                "多模态大模型",
                "动作分解",
                "连续环境"
            ],
            "_index": 17
        },
        {
            "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction",
            "authors": [
                "Zhida Zhao",
                "Talas Fu",
                "Yifan Wang",
                "Lijun Wang",
                "Huchuan Lu"
            ],
            "arxiv_id": "2510.19654v1",
            "summary": "Despite remarkable progress in driving world models, their potential for\nautonomous systems remains largely untapped: the world models are mostly\nlearned for world simulation and decoupled from trajectory planning. While\nrecent efforts aim to unify world modeling and planning in a single framework,\nthe synergistic facilitation mechanism of world modeling for planning still\nrequires further exploration. In this work, we introduce a new driving paradigm\nnamed Policy World Model (PWM), which not only integrates world modeling and\ntrajectory planning within a unified architecture, but is also able to benefit\nplanning using the learned world knowledge through the proposed action-free\nfuture state forecasting scheme. Through collaborative state-action prediction,\nPWM can mimic the human-like anticipatory perception, yielding more reliable\nplanning performance. To facilitate the efficiency of video forecasting, we\nfurther introduce a dynamically enhanced parallel token generation mechanism,\nequipped with a context-guided tokenizer and an adaptive dynamic focal loss.\nDespite utilizing only front camera input, our method matches or exceeds\nstate-of-the-art approaches that rely on multi-view and multi-modal inputs.\nCode and model weights will be released at\nhttps://github.com/6550Zhao/Policy-World-Model.",
            "headline_zh": "提出策略世界模型以统一世界建模与轨迹规划，提升自动驾驶性能",
            "intro_zh": [
                "核心问题：现有驾驶世界模型多用于仿真，与轨迹规划脱节，协同机制未充分探索。",
                "方法要点：集成世界建模与规划，通过无动作未来状态预测和协作状态-动作预测。",
                "实验或效果：仅用前摄像头输入，性能匹配或超越依赖多视图多模态的先进方法。"
            ],
            "tags_zh": [
                "自动驾驶世界模型",
                "轨迹规划",
                "状态-动作预测",
                "视频预测",
                "动态增强机制",
                "协作预测"
            ],
            "_index": 18
        },
        {
            "title": "MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement Learning and Local Zoom",
            "authors": [
                "Yifan Li",
                "Fenghe Tang",
                "Yingtai Li",
                "Shaohua Kevin Zhou"
            ],
            "arxiv_id": "2510.19626v1",
            "summary": "General-purpose large Vision-Language Models (VLMs) demonstrate strong\ncapabilities in generating detailed descriptions for natural images. However,\ntheir performance in the medical domain remains suboptimal, even for relatively\nstraightforward tasks, primarily due to the lack of large-scale, high-quality,\nspecialized medical imaging datasets and the neglect of the diagnostic process\nthat progresses from coarse to fine-grained. To address the first issue, we\nconstruct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second\nissue, we propose MedReason-R1, a medical VLM with explicit reasoning process\nfor disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds\nzoom-in disease region-of-interest areas into the image, highlighting the\ncrucial role of both global localization and disease-specific details in\nenhancing the model's diagnostic performance. Furthermore, we introduce the\nGRPO reinforcement learning framework to MedReason-R1, which enables effective\nreasoning without relying on costly manual annotations. Compared to recent\ngeneral-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art\nperformance in CT disease diagnosis while retaining generalization. The code,\ncheckpoints, and dataset are available at:\nhttps://github.com/Leevan001/MedReason-R1",
            "headline_zh": "提出MedReason-R1模型，结合强化学习与局部放大，提升CT疾病诊断性能。",
            "intro_zh": [
                "通用视觉语言模型在医学图像诊断中表现不佳，因缺乏高质量数据集和忽略从粗到细诊断过程。",
                "构建CT-RATE-VQA数据集，并设计MedReason-R1模型，嵌入局部放大区域以增强诊断细节。",
                "引入GRPO强化学习框架，无需昂贵人工标注，在CT诊断中达到先进性能并保持泛化能力。"
            ],
            "tags_zh": [
                "医学视觉语言模型",
                "CT疾病诊断",
                "强化学习",
                "局部放大",
                "数据集构建",
                "诊断推理"
            ],
            "_index": 19
        },
        {
            "title": "Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning",
            "authors": [
                "Zhengxuan Wei",
                "Jiajin Tang",
                "Sibei Yang"
            ],
            "arxiv_id": "2510.19622v1",
            "summary": "Existing Moment Retrieval methods face three critical bottlenecks: (1) data\nscarcity forces models into shallow keyword-feature associations; (2) boundary\nambiguity in transition regions between adjacent events; (3) insufficient\ndiscrimination of fine-grained semantics (e.g., distinguishing ``kicking\" vs.\n``throwing\" a ball). In this paper, we propose a zero-external-dependency\nAugmented Moment Retrieval framework, AMR, designed to overcome local optima\ncaused by insufficient data annotations and the lack of robust boundary and\nsemantic discrimination capabilities. AMR is built upon two key insights: (1)\nit resolves ambiguous boundary information and semantic confusion in existing\nannotations without additional data (avoiding costly manual labeling), and (2)\nit preserves boundary and semantic discriminative capabilities enhanced by\ntraining while generalizing to real-world scenarios, significantly improving\nperformance. Furthermore, we propose a two-stage training framework with\ncold-start and distillation adaptation. The cold-start stage employs curriculum\nlearning on augmented data to build foundational boundary/semantic awareness.\nThe distillation stage introduces dual query sets: Original Queries maintain\nDETR-based localization using frozen Base Queries from the cold-start model,\nwhile Active Queries dynamically adapt to real-data distributions. A\ncross-stage distillation loss enforces consistency between Original and Base\nQueries, preventing knowledge forgetting while enabling real-world\ngeneralization. Experiments on multiple benchmarks show that AMR achieves\nimproved performance over prior state-of-the-art approaches.",
            "headline_zh": "提出零依赖两阶段学习框架AMR，以解决时刻检索中的数据稀缺、边界模糊和语义区分不足问题。",
            "intro_zh": [
                "核心问题：现有方法面临数据稀缺、边界模糊和细粒度语义区分不足的瓶颈。",
                "方法要点：采用两阶段训练，冷启动阶段增强边界和语义感知，蒸馏阶段通过双查询集实现泛化。",
                "实验或效果：在多个基准测试中，AMR性能优于先前最先进方法。"
            ],
            "tags_zh": [
                "时刻检索",
                "两阶段学习",
                "数据增强",
                "蒸馏训练",
                "边界检测",
                "语义区分"
            ],
            "_index": 20
        },
        {
            "title": "Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism",
            "authors": [
                "Junfei Zhou",
                "Penglin Dai",
                "Quanmin Wei",
                "Bingyi Liu",
                "Xiao Wu",
                "Jianping Wang"
            ],
            "arxiv_id": "2510.19618v1",
            "summary": "Multi-agent collaboration enhances the perception capabilities of individual\nagents through information sharing. However, in real-world applications,\ndifferences in sensors and models across heterogeneous agents inevitably lead\nto domain gaps during collaboration. Existing approaches based on adaptation\nand reconstruction fail to support pragmatic heterogeneous collaboration due to\ntwo key limitations: (1) Intrusive retraining of the encoder or core modules\ndisrupts the established semantic consistency among agents; and (2)\naccommodating new agents incurs high computational costs, limiting scalability.\nTo address these challenges, we present a novel Generative Communication\nmechanism (GenComm) that facilitates seamless perception across heterogeneous\nmulti-agent systems through feature generation, without altering the original\nnetwork, and employs lightweight numerical alignment of spatial information to\nefficiently integrate new agents at minimal cost. Specifically, a tailored\nDeformable Message Extractor is designed to extract spatial message for each\ncollaborator, which is then transmitted in place of intermediate features. The\nSpatial-Aware Feature Generator, utilizing a conditional diffusion model,\ngenerates features aligned with the ego agent's semantic space while preserving\nthe spatial information of the collaborators. These generated features are\nfurther refined by a Channel Enhancer before fusion. Experiments conducted on\nthe OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm\noutperforms existing state-of-the-art methods, achieving an 81\\% reduction in\nboth computational cost and parameter count when incorporating new agents. Our\ncode is available at https://github.com/jeffreychou777/GenComm.",
            "headline_zh": "提出生成通信机制以解决异构多智能体协作感知中的领域差距问题",
            "intro_zh": [
                "核心问题：异构智能体因传感器和模型差异导致协作时出现领域差距，现有方法破坏语义一致性且扩展成本高",
                "方法要点：使用生成通信机制，通过特征生成和空间信息对齐，无需修改原网络，实现轻量级新智能体集成",
                "实验或效果：在多个数据集上优于现有方法，新智能体集成时计算成本和参数数量减少81%"
            ],
            "tags_zh": [
                "异构多智能体协作",
                "生成通信机制",
                "特征生成",
                "空间信息对齐",
                "轻量级集成",
                "条件扩散模型"
            ],
            "_index": 21
        },
        {
            "title": "Beyond sparse denoising in frames: minimax estimation with a scattering transform",
            "authors": [
                "Nathanaël Cuvelle--Magar",
                "Stéphane Mallat"
            ],
            "arxiv_id": "2510.19612v1",
            "summary": "A considerable amount of research in harmonic analysis has been devoted to\nnon-linear estimators of signals contaminated by additive Gaussian noise. They\nare implemented by thresholding coefficients in a frame, which provide a sparse\nsignal representation, or by minimising their $\\ell^1$ norm. However, sparse\nestimators in frames are not sufficiently rich to adapt to complex signal\nregularities. For cartoon images whose edges are piecewise $\\bf C^\\alpha$\ncurves, wavelet, curvelet and Xlet frames are suboptimal if the Lipschitz\nexponent $\\alpha \\leq 2$ is an unknown parameter. Deep convolutional neural\nnetworks have recently obtained much better numerical results, which reach the\nminimax asymptotic bounds for all $\\alpha$. Wavelet scattering coefficients\nhave been introduced as simplified convolutional neural network models. They\nare computed by transforming the modulus of wavelet coefficients with a second\nwavelet transform. We introduce a denoising estimator by jointly minimising and\nmaximising the $\\ell^1$ norms of different subsets of scattering coefficients.\nWe prove that these $\\ell^1$ norms capture different types of geometric image\nregularity. Numerical experiments show that this denoising estimator reaches\nthe minimax asymptotic bound for cartoon images for all Lipschitz exponents\n$\\alpha \\leq 2$. We state this numerical result as a mathematical conjecture.\nIt provides a different harmonic analysis approach to suppress noise from\nsignals, and to specify the geometric regularity of functions. It also opens a\nmathematical bridge between harmonic analysis and denoising estimators with\ndeep convolutional network.",
            "headline_zh": "提出基于散射变换的极小极大估计方法，用于卡通图像去噪。",
            "intro_zh": [
                "核心问题：传统稀疏估计器在帧中无法适应复杂信号规律，如未知Lipschitz指数的卡通图像。",
                "方法要点：通过联合最小化和最大化散射系数子集的ℓ¹范数，捕捉几何规律性。",
                "实验或效果：数值实验显示该方法达到极小极大渐近界，支持数学猜想。"
            ],
            "tags_zh": [
                "散射变换",
                "极小极大估计",
                "图像去噪",
                "几何规律性",
                "谐波分析"
            ],
            "_index": 22
        },
        {
            "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography",
            "authors": [
                "Haozhe Luo",
                "Shelley Zixin Shu",
                "Ziyu Zhou",
                "Sebastian Otalora",
                "Mauricio Reyes"
            ],
            "arxiv_id": "2510.19599v1",
            "summary": "Vision-language models (VLMs) have recently shown remarkable zero-shot\nperformance in medical image understanding, yet their grounding ability, the\nextent to which textual concepts align with visual evidence, remains\nunderexplored. In the medical domain, however, reliable grounding is essential\nfor interpretability and clinical adoption. In this work, we present the first\nsystematic benchmark for evaluating cross-modal interpretability in chest\nX-rays across seven CLIP-style VLM variants. We generate visual explanations\nusing cross-attention and similarity-based localization maps, and\nquantitatively assess their alignment with radiologist-annotated regions across\nmultiple pathologies. Our analysis reveals that: (1) while all VLM variants\ndemonstrate reasonable localization for large and well-defined pathologies,\ntheir performance substantially degrades for small or diffuse lesions; (2)\nmodels that are pretrained on chest X-ray-specific datasets exhibit improved\nalignment compared to those trained on general-domain data. (3) The overall\nrecognition ability and grounding ability of the model are strongly correlated.\nThese findings underscore that current VLMs, despite their strong recognition\nability, still fall short in clinically reliable grounding, highlighting the\nneed for targeted interpretability benchmarks before deployment in medical\npractice. XBench code is available at\nhttps://github.com/Roypic/Benchmarkingattention",
            "headline_zh": "提出XBench基准以评估胸片视觉语言模型的跨模态可解释性",
            "intro_zh": [
                "核心问题：视觉语言模型在医学图像中的视觉证据对齐能力不足，影响临床可靠性。",
                "方法要点：使用交叉注意力和相似性定位图生成视觉解释，并与放射科医生标注区域对齐评估。",
                "实验或效果：发现模型对小病灶定位性能下降，特定数据集预训练可改善对齐，识别与定位能力相关。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "医学图像解释",
                "胸片基准",
                "跨模态定位",
                "可解释性评估"
            ],
            "_index": 23
        },
        {
            "title": "CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization",
            "authors": [
                "Zhou Lei",
                "Pan Gang",
                "Wang Jiahao",
                "Sun Di"
            ],
            "arxiv_id": "2510.19597v1",
            "summary": "Image Forgery Localization (IFL) is a crucial task in image forensics, aimed\nat accurately identifying manipulated or tampered regions within an image at\nthe pixel level. Existing methods typically generate a single deterministic\nlocalization map, which often lacks the precision and reliability required for\nhigh-stakes applications such as forensic analysis and security surveillance.\nTo enhance the credibility of predictions and mitigate the risk of errors, we\nintroduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a\nforged image, CBDiff generates multiple diverse and plausible localization\nmaps, thereby offering a richer and more comprehensive representation of the\nforgery distribution. This approach addresses the uncertainty and variability\ninherent in tampered regions. Furthermore, CBDiff innovatively incorporates\nBernoulli noise into the diffusion process to more faithfully reflect the\ninherent binary and sparse properties of forgery masks. Additionally, CBDiff\nintroduces a Time-Step Cross-Attention (TSCAttention), which is specifically\ndesigned to leverage semantic feature guidance with temporal steps to improve\nmanipulation detection. Extensive experiments on eight publicly benchmark\ndatasets demonstrate that CBDiff significantly outperforms existing\nstate-of-the-art methods, highlighting its strong potential for real-world\ndeployment.",
            "headline_zh": "提出条件伯努利扩散模型以解决图像伪造定位中的不确定性问题",
            "intro_zh": [
                "核心问题：现有方法生成单一确定性定位图，缺乏精度和可靠性，难以满足高要求应用。",
                "方法要点：引入条件伯努利扩散模型，生成多样伪造定位图，并融入伯努利噪声和时间步交叉注意力。",
                "实验或效果：在八个公开数据集上实验，性能显著优于现有先进方法，具有实际部署潜力。"
            ],
            "tags_zh": [
                "图像伪造定位",
                "扩散模型",
                "伯努利噪声",
                "时间步交叉注意力",
                "像素级检测",
                "不确定性建模"
            ],
            "_index": 24
        },
        {
            "title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation",
            "authors": [
                "Su Ho Han",
                "Jeongseok Hyun",
                "Pilhyeon Lee",
                "Minho Shim",
                "Dongyoon Wee",
                "Seon Joo Kim"
            ],
            "arxiv_id": "2510.19592v1",
            "summary": "Multimodal large language models (MLLMs) demonstrate strong video\nunderstanding by attending to visual tokens relevant to textual queries. To\ndirectly adapt this for localization in a training-free manner, we cast video\nreasoning segmentation as a video QA task and extract attention maps via\nrollout mechanism. However, raw attention maps are noisy and poorly aligned\nwith object regions. We propose Decomposed Attention Fusion (DecAF), which\nrefines these maps through two mechanisms: (1) contrastive object-background\nfusion and (2) complementary video-frame fusion. This method suppresses\nirrelevant activations and enhances object-focused cues, enabling direct\nconversion of attention maps into coarse segmentation masks. In addition, we\nintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.\nUnlike existing methods that jointly train MLLMs with SAM, our method operates\nentirely without retraining. DecAF outperforms training-free methods and\nachieves performance comparable to training-based methods on both referring and\nreasoning VOS benchmarks. The code will be available at\nhttps://github.com/HYUNJS/DecAF.",
            "headline_zh": "提出Decomposed Attention Fusion方法，以训练免费方式实现视频推理分割。",
            "intro_zh": [
                "核心问题：MLLM注意力图噪声大，与对象区域对齐差。",
                "方法要点：通过对比对象-背景融合和互补视频帧融合精炼注意力图。",
                "实验效果：在VOS基准上优于训练免费方法，性能接近训练方法。"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "视频推理分割",
                "注意力机制",
                "训练免费方法",
                "对象定位"
            ],
            "_index": 25
        },
        {
            "title": "Digitizing Paper ECGs at Scale: An Open-Source Algorithm for Clinical Research",
            "authors": [
                "Elias Stenhede",
                "Agnar Martin Bjørnstad",
                "Arian Ranjbar"
            ],
            "arxiv_id": "2510.19590v1",
            "summary": "Millions of clinical ECGs exist only as paper scans, making them unusable for\nmodern automated diagnostics. We introduce a fully automated, modular framework\nthat converts scanned or photographed ECGs into digital signals, suitable for\nboth clinical and research applications. The framework is validated on 37,191\nECG images with 1,596 collected at Akershus University Hospital, where the\nalgorithm obtains a mean signal-to-noise ratio of 19.65 dB on scanned papers\nwith common artifacts. It is further evaluated on the Emory Paper Digitization\nECG Dataset, comprising 35,595 images, including images with perspective\ndistortion, wrinkles, and stains. The model improves on the state-of-the-art in\nall subcategories. The full software is released as open-source, promoting\nreproducibility and further development. We hope the software will contribute\nto unlocking retrospective ECG archives and democratize access to AI-driven\ndiagnostics.",
            "headline_zh": "提出开源算法框架，将纸质心电图数字化以支持临床研究。",
            "intro_zh": [
                "核心问题：大量心电图仅存为纸质扫描，无法用于自动化诊断。",
                "方法要点：开发全自动模块化框架，将扫描或拍摄心电图转换为数字信号。",
                "实验或效果：在37,191张图像上验证，信噪比达19.65 dB，优于现有技术。"
            ],
            "tags_zh": [
                "心电图数字化",
                "开源算法",
                "临床研究",
                "信号处理",
                "自动化诊断"
            ],
            "_index": 26
        },
        {
            "title": "Uncertainty evaluation of segmentation models for Earth observation",
            "authors": [
                "Melanie Rey",
                "Andriy Mnih",
                "Maxim Neumann",
                "Matt Overlan",
                "Drew Purves"
            ],
            "arxiv_id": "2510.19586v1",
            "summary": "This paper investigates methods for estimating uncertainty in semantic\nsegmentation predictions derived from satellite imagery. Estimating uncertainty\nfor segmentation presents unique challenges compared to standard image\nclassification, requiring scalable methods producing per-pixel estimates. While\nmost research on this topic has focused on scene understanding or medical\nimaging, this work benchmarks existing methods specifically for remote sensing\nand Earth observation applications. Our evaluation focuses on the practical\nutility of uncertainty measures, testing their ability to identify prediction\nerrors and noise-corrupted input image regions. Experiments are conducted on\ntwo remote sensing datasets, PASTIS and ForTy, selected for their differences\nin scale, geographic coverage, and label confidence. We perform an extensive\nevaluation featuring several models, such as Stochastic Segmentation Networks\nand ensembles, in combination with a number of neural architectures and\nuncertainty metrics. We make a number of practical recommendations based on our\nfindings.",
            "headline_zh": "评估遥感图像分割模型的不确定性方法，以识别预测错误和噪声区域",
            "intro_zh": [
                "核心问题：语义分割不确定性估计在遥感应用中面临可扩展性和逐像素估计挑战",
                "方法要点：基准测试多种模型如随机分割网络和集成方法，结合不同架构和不确定性指标",
                "实验或效果：在PASTIS和ForTy数据集上评估不确定性度量识别错误和噪声的能力"
            ],
            "tags_zh": [
                "语义分割",
                "不确定性估计",
                "遥感图像",
                "基准测试",
                "预测错误识别"
            ],
            "_index": 27
        },
        {
            "title": "Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark",
            "authors": [
                "Yu Wu",
                "Ke Shu",
                "Jonas Fischer",
                "Lidia Pivovarova",
                "David Rosson",
                "Eetu Mäkelä",
                "Mikko Tolonen"
            ],
            "arxiv_id": "2510.19585v1",
            "summary": "This paper presents a novel task of extracting Latin fragments from\nmixed-language historical documents with varied layouts. We benchmark and\nevaluate the performance of large foundation models against a multimodal\ndataset of 724 annotated pages. The results demonstrate that reliable Latin\ndetection with contemporary models is achievable. Our study provides the first\ncomprehensive analysis of these models' capabilities and limits for this task.",
            "headline_zh": "提出基于大语言模型的拉丁语检测方法，以解决历史书籍中混合语言片段提取问题。",
            "intro_zh": [
                "核心问题：从布局多样的混合语言历史文档中提取拉丁语片段。",
                "方法要点：使用大基础模型对724页多模态数据集进行基准测试。",
                "实验或效果：当代模型可实现可靠的拉丁语检测，并分析其能力与局限。"
            ],
            "tags_zh": [
                "拉丁语检测",
                "大语言模型",
                "多模态基准",
                "历史文档分析",
                "混合语言处理"
            ],
            "_index": 28
        },
        {
            "title": "Addressing the Depth-of-Field Constraint: A New Paradigm for High Resolution Multi-Focus Image Fusion",
            "authors": [
                "Luca Piano",
                "Peng Huanwen",
                "Radu Ciprian Bilcu"
            ],
            "arxiv_id": "2510.19581v1",
            "summary": "Multi-focus image fusion (MFIF) addresses the depth-of-field (DOF)\nlimitations of optical lenses, where only objects within a specific range\nappear sharp. Although traditional and deep learning methods have advanced the\nfield, challenges persist, including limited training data, domain gaps from\nsynthetic datasets, and difficulties with regions lacking information. We\npropose VAEEDOF, a novel MFIF method that uses a distilled variational\nautoencoder for high-fidelity, efficient image reconstruction. Our fusion\nmodule processes up to seven images simultaneously, enabling robust fusion\nacross diverse focus points. To address data scarcity, we introduce\nMattingMFIF, a new syntetic 4K dataset, simulating realistic DOF effects from\nreal photographs. Our method achieves state-of-the-art results, generating\nseamless artifact-free fused images and bridging the gap between synthetic and\nreal-world scenarios, offering a significant step forward in addressing complex\nMFIF challenges. The code, and weights are available here:",
            "headline_zh": "提出VAEEDOF方法以解决多焦点图像融合中的深度限制问题",
            "intro_zh": [
                "核心问题：光学镜头景深限制导致图像部分区域模糊，传统方法存在数据不足和域差距问题。",
                "方法要点：使用蒸馏变分自编码器进行高效图像重建，融合模块可同时处理多张图像。",
                "实验或效果：引入MattingMFIF数据集，实现无伪影融合，在合成与真实场景中表现优异。"
            ],
            "tags_zh": [
                "多焦点图像融合",
                "变分自编码器",
                "景深限制",
                "合成数据集",
                "图像重建",
                "蒸馏训练"
            ],
            "_index": 29
        },
        {
            "title": "Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration",
            "authors": [
                "Francisco Mena",
                "Dino Ienco",
                "Cassio F. Dantas",
                "Roberto Interdonato",
                "Andreas Dengel"
            ],
            "arxiv_id": "2510.19579v1",
            "summary": "Multi-modal co-learning is emerging as an effective paradigm in machine\nlearning, enabling models to collaboratively learn from different modalities to\nenhance single-modality predictions. Earth Observation (EO) represents a\nquintessential domain for multi-modal data analysis, wherein diverse remote\nsensors collect data to sense our planet. This unprecedented volume of data\nintroduces novel challenges. Specifically, the access to the same sensor\nmodalities at both training and inference stages becomes increasingly complex\nbased on real-world constraints affecting remote sensing platforms. In this\ncontext, multi-modal co-learning presents a promising strategy to leverage the\nvast amount of sensor-derived data available at the training stage to improve\nsingle-modality models for inference-time deployment. Most current research\nefforts focus on designing customized solutions for either particular\ndownstream tasks or specific modalities available at the inference stage. To\naddress this, we propose a novel multi-modal co-learning framework capable of\ngeneralizing across various tasks without targeting a specific modality for\ninference. Our approach combines contrastive and modality discriminative\nlearning together to guide single-modality models to structure the internal\nmodel manifold into modality-shared and modality-specific information. We\nevaluate our framework on four EO benchmarks spanning classification and\nregression tasks across different sensor modalities, where only one of the\nmodalities available during training is accessible at inference time. Our\nresults demonstrate consistent predictive improvements over state-of-the-art\napproaches from the recent machine learning and computer vision literature, as\nwell as EO-specific methods. The obtained findings validate our framework in\nthe single-modality inference scenarios across a diverse range of EO\napplications.",
            "headline_zh": "提出多模态协同学习框架，以提升地球观测中单模态模型的预测性能",
            "intro_zh": [
                "核心问题：地球观测中训练与推理阶段模态访问不一致，影响单模态模型部署",
                "方法要点：结合对比学习和模态判别学习，分离模态共享与特定信息",
                "实验或效果：在四个基准测试中优于现有方法，验证单模态推理场景有效性"
            ],
            "tags_zh": [
                "多模态协同学习",
                "地球观测",
                "单模态推理",
                "对比学习",
                "模态判别学习",
                "遥感数据分析"
            ],
            "_index": 30
        },
        {
            "title": "VGD: Visual Geometry Gaussian Splatting for Feed-Forward Surround-view Driving Reconstruction",
            "authors": [
                "Junhong Lin",
                "Kangli Wang",
                "Shunzhou Wang",
                "Songlin Fan",
                "Ge Li",
                "Wei Gao"
            ],
            "arxiv_id": "2510.19578v1",
            "summary": "Feed-forward surround-view autonomous driving scene reconstruction offers\nfast, generalizable inference ability, which faces the core challenge of\nensuring generalization while elevating novel view quality. Due to the\nsurround-view with minimal overlap regions, existing methods typically fail to\nensure geometric consistency and reconstruction quality for novel views. To\ntackle this tension, we claim that geometric information must be learned\nexplicitly, and the resulting features should be leveraged to guide the\nelevating of semantic quality in novel views. In this paper, we introduce\n\\textbf{Visual Gaussian Driving (VGD)}, a novel feed-forward end-to-end\nlearning framework designed to address this challenge. To achieve generalizable\ngeometric estimation, we design a lightweight variant of the VGGT architecture\nto efficiently distill its geometric priors from the pre-trained VGGT to the\ngeometry branch. Furthermore, we design a Gaussian Head that fuses multi-scale\ngeometry tokens to predict Gaussian parameters for novel view rendering, which\nshares the same patch backbone as the geometry branch. Finally, we integrate\nmulti-scale features from both geometry and Gaussian head branches to jointly\nsupervise a semantic refinement model, optimizing rendering quality through\nfeature-consistent learning. Experiments on nuScenes demonstrate that our\napproach significantly outperforms state-of-the-art methods in both objective\nmetrics and subjective quality under various settings, which validates VGD's\nscalability and high-fidelity surround-view reconstruction.",
            "headline_zh": "提出VGD框架以解决环视驾驶场景重建中的几何一致性与新视图质量问题",
            "intro_zh": [
                "环视驾驶场景重建中，视图重叠少导致几何一致性与新视图质量难以保证",
                "采用轻量VGGT变体提取几何先验，并设计高斯头融合多尺度几何特征预测渲染参数",
                "在nuScenes数据集上验证，VGD在客观指标和主观质量上优于现有方法"
            ],
            "tags_zh": [
                "环视驾驶重建",
                "几何一致性",
                "高斯渲染",
                "多尺度特征融合",
                "前馈学习"
            ],
            "_index": 31
        },
        {
            "title": "Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection",
            "authors": [
                "Ariana Yi",
                "Ce Zhou",
                "Liyang Xiao",
                "Qiben Yan"
            ],
            "arxiv_id": "2510.19574v1",
            "summary": "As object detection models are increasingly deployed in cyber-physical\nsystems such as autonomous vehicles (AVs) and surveillance platforms, ensuring\ntheir security against adversarial threats is essential. While prior work has\nexplored adversarial attacks in the image domain, those attacks in the video\ndomain remain largely unexamined, especially in the no-box setting. In this\npaper, we present {\\alpha}-Cloak, the first no-box adversarial attack on object\ndetectors that operates entirely through the alpha channel of RGBA videos.\n{\\alpha}-Cloak exploits the alpha channel to fuse a malicious target video with\na benign video, resulting in a fused video that appears innocuous to human\nviewers but consistently fools object detectors. Our attack requires no access\nto model architecture, parameters, or outputs, and introduces no perceptible\nartifacts. We systematically study the support for alpha channels across common\nvideo formats and playback applications, and design a fusion algorithm that\nensures visual stealth and compatibility. We evaluate {\\alpha}-Cloak on five\nstate-of-the-art object detectors, a vision-language model, and a multi-modal\nlarge language model (Gemini-2.0-Flash), demonstrating a 100% attack success\nrate across all scenarios. Our findings reveal a previously unexplored\nvulnerability in video-based perception systems, highlighting the urgent need\nfor defenses that account for the alpha channel in adversarial settings.",
            "headline_zh": "提出Alpha-Cloak无盒攻击，利用RGBA视频alpha通道欺骗视频目标检测器",
            "intro_zh": [
                "核心问题：视频目标检测器在无盒设置下易受对抗攻击，alpha通道漏洞未受关注",
                "方法要点：通过alpha通道融合恶意与良性视频，实现视觉隐形且无需模型信息",
                "实验或效果：在多种检测器和模型中攻击成功率100%，揭示新安全威胁"
            ],
            "tags_zh": [
                "无盒对抗攻击",
                "视频目标检测",
                "alpha通道",
                "视觉隐形",
                "多模态模型安全"
            ],
            "_index": 32
        },
        {
            "title": "HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking",
            "authors": [
                "Yao Deng",
                "Xian Zhong",
                "Wenxuan Liu",
                "Zhaofei Yu",
                "Jingling Yuan",
                "Tiejun Huang"
            ],
            "arxiv_id": "2510.19560v1",
            "summary": "RGB cameras excel at capturing rich texture details with high spatial\nresolution, whereas event cameras offer exceptional temporal resolution and a\nhigh dynamic range (HDR). Leveraging their complementary strengths can\nsubstantially enhance object tracking under challenging conditions, such as\nhigh-speed motion, HDR environments, and dynamic background interference.\nHowever, a significant spatio-temporal asymmetry exists between these two\nmodalities due to their fundamentally different imaging mechanisms, hindering\neffective multi-modal integration. To address this issue, we propose\n{Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge\ndistillation framework that explicitly models and mitigates spatio-temporal\nasymmetries. Specifically, HAD proposes a hierarchical alignment strategy that\nminimizes information loss while maintaining the student network's\ncomputational efficiency and parameter compactness. Extensive experiments\ndemonstrate that HAD consistently outperforms state-of-the-art methods, and\ncomprehensive ablation studies further validate the effectiveness and necessity\nof each designed component. The code will be released soon.",
            "headline_zh": "提出分层非对称蒸馏以解决事件相机与RGB相机时空不对称问题",
            "intro_zh": [
                "核心问题：事件相机与RGB相机成像机制不同导致时空不对称，阻碍多模态融合。",
                "方法要点：设计分层对齐策略，减少信息损失，保持学生网络高效与紧凑。",
                "实验或效果：在高速运动、HDR等场景下优于现有方法，消融实验验证组件有效性。"
            ],
            "tags_zh": [
                "事件相机跟踪",
                "多模态知识蒸馏",
                "时空不对称",
                "分层对齐",
                "目标跟踪"
            ],
            "_index": 33
        },
        {
            "title": "A Matter of Time: Revealing the Structure of Time in Vision-Language Models",
            "authors": [
                "Nidham Tekaya",
                "Manuela Waldner",
                "Matthias Zeppelzauer"
            ],
            "arxiv_id": "2510.19559v1",
            "summary": "Large-scale vision-language models (VLMs) such as CLIP have gained popularity\nfor their generalizable and expressive multimodal representations. By\nleveraging large-scale training data with diverse textual metadata, VLMs\nacquire open-vocabulary capabilities, solving tasks beyond their training\nscope. This paper investigates the temporal awareness of VLMs, assessing their\nability to position visual content in time. We introduce TIME10k, a benchmark\ndataset of over 10,000 images with temporal ground truth, and evaluate the\ntime-awareness of 37 VLMs by a novel methodology. Our investigation reveals\nthat temporal information is structured along a low-dimensional, non-linear\nmanifold in the VLM embedding space. Based on this insight, we propose methods\nto derive an explicit ``timeline'' representation from the embedding space.\nThese representations model time and its chronological progression and thereby\nfacilitate temporal reasoning tasks. Our timeline approaches achieve\ncompetitive to superior accuracy compared to a prompt-based baseline while\nbeing computationally efficient. All code and data are available at\nhttps://tekayanidham.github.io/timeline-page/.",
            "headline_zh": "提出时间线表示方法以增强视觉语言模型的时间推理能力",
            "intro_zh": [
                "研究视觉语言模型对视觉内容的时间定位能力",
                "发现时间信息在嵌入空间中呈低维非线性流形结构",
                "基于此提出时间线表示方法，在基准测试中表现优于基线"
            ],
            "tags_zh": [
                "视觉语言模型",
                "时间感知",
                "嵌入空间",
                "时间线表示",
                "基准数据集",
                "时间推理"
            ],
            "_index": 34
        },
        {
            "title": "The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models",
            "authors": [
                "Xiaofeng Zhang",
                "Aaron Courville",
                "Michal Drozdzal",
                "Adriana Romero-Soriano"
            ],
            "arxiv_id": "2510.19557v1",
            "summary": "Text-to-image (T2I) models offer great potential for creating virtually\nlimitless synthetic data, a valuable resource compared to fixed and finite real\ndatasets. Previous works evaluate the utility of synthetic data from T2I models\non three key desiderata: quality, diversity, and consistency. While prompt\nengineering is the primary means of interacting with T2I models, the systematic\nimpact of prompt complexity on these critical utility axes remains\nunderexplored. In this paper, we first conduct synthetic experiments to\nmotivate the difficulty of generalization w.r.t. prompt complexity and explain\nthe observed difficulty with theoretical derivations. Then, we introduce a new\nevaluation framework that can compare the utility of real data and synthetic\ndata, and present a comprehensive analysis of how prompt complexity influences\nthe utility of synthetic data generated by commonly used T2I models. We conduct\nour study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and\nevaluate different inference-time intervention methods. Our synthetic\nexperiments show that generalizing to more general conditions is harder than\nthe other way round, since the former needs an estimated likelihood that is not\nlearned by diffusion models. Our large-scale empirical experiments reveal that\nincreasing prompt complexity results in lower conditional diversity and prompt\nconsistency, while reducing the synthetic-to-real distribution shift, which\naligns with the synthetic experiments. Moreover, current inference-time\ninterventions can augment the diversity of the generations at the expense of\nmoving outside the support of real data. Among those interventions, prompt\nexpansion, by deliberately using a pre-trained language model as a likelihood\nestimator, consistently achieves the highest performance in both image\ndiversity and aesthetics, even higher than that of real data.",
            "headline_zh": "分析提示复杂度对T2I模型合成数据质量、多样性和一致性的影响",
            "intro_zh": [
                "核心问题：提示复杂度对T2I模型合成数据效用（质量、多样性、一致性）的系统影响未充分研究。",
                "方法要点：引入新评估框架，比较真实与合成数据效用，并进行大规模实验分析。",
                "实验或效果：增加提示复杂度降低条件多样性和提示一致性，但减少分布偏移；提示扩展方法表现最佳。"
            ],
            "tags_zh": [
                "文本到图像模型",
                "合成数据评估",
                "提示复杂度",
                "分布偏移",
                "推理时干预",
                "提示扩展"
            ],
            "_index": 35
        },
        {
            "title": "[De|Re]constructing VLMs' Reasoning in Counting",
            "authors": [
                "Simone Alghisi",
                "Gabriel Roccabruna",
                "Massimo Rizzoli",
                "Seyed Mahed Mousavi",
                "Giuseppe Riccardi"
            ],
            "arxiv_id": "2510.19555v1",
            "summary": "Vision-Language Models (VLMs) have recently gained attention due to their\ncompetitive performance on multiple downstream tasks, achieved by following\nuser-input instructions. However, VLMs still exhibit several limitations in\nvisual reasoning, such as difficulties in identifying relations (e.g., spatial,\ntemporal, and among objects), understanding temporal sequences (e.g., frames),\nand counting objects. In this work, we go beyond score-level benchmark\nevaluations of VLMs by investigating the underlying causes of their failures\nand proposing a targeted approach to improve their reasoning capabilities. We\nstudy the reasoning skills of seven state-of-the-art VLMs in the counting task\nunder controlled experimental conditions. Our experiments show that VLMs are\nhighly sensitive to the number and type of objects, their spatial arrangement,\nand the co-occurrence of distractors. A layer-wise analysis reveals that errors\nare due to incorrect mapping of the last-layer representation into the output\nspace. Our targeted training shows that fine-tuning just the output layer\nimproves accuracy by up to 21%. We corroborate these findings by achieving\nconsistent improvements on real-world datasets.",
            "headline_zh": "通过输出层微调提升视觉语言模型在计数任务中的推理能力",
            "intro_zh": [
                "核心问题：视觉语言模型在计数任务中易受对象数量、空间排列和干扰物影响",
                "方法要点：分析模型层表示，发现错误源于最后一层映射问题",
                "实验或效果：仅微调输出层可使准确率提升高达21%，并在真实数据集验证"
            ],
            "tags_zh": [
                "视觉语言模型",
                "计数任务",
                "推理能力",
                "层分析",
                "输出层微调",
                "对象检测"
            ],
            "_index": 36
        },
        {
            "title": "Optimizing Prosthetic Wrist Movement: A Model Predictive Control Approach",
            "authors": [
                "Francesco Schetter",
                "Shifa Sulaiman",
                "Shoby George",
                "Paolino De Risi",
                "Fanny Ficuciello"
            ],
            "arxiv_id": "2510.19541v1",
            "summary": "The integration of advanced control strategies into prosthetic hands is\nessential to improve their adaptability and performance. In this study, we\npresent an implementation of a Model Predictive Control (MPC) strategy to\nregulate the motions of a soft continuum wrist section attached to a\ntendon-driven prosthetic hand with less computational effort. MPC plays a\ncrucial role in enhancing the functionality and responsiveness of prosthetic\nhands. By leveraging predictive modeling, this approach enables precise\nmovement adjustments while accounting for dynamic user interactions. This\nadvanced control strategy allows for the anticipation of future movements and\nadjustments based on the current state of the prosthetic device and the\nintentions of the user. Kinematic and dynamic modelings are performed using\nEuler-Bernoulli beam and Lagrange methods respectively. Through simulation and\nexperimental validations, we demonstrate the effectiveness of MPC in optimizing\nwrist articulation and user control. Our findings suggest that this technique\nsignificantly improves the prosthetic hand dexterity, making movements more\nnatural and intuitive. This research contributes to the field of robotics and\nbiomedical engineering by offering a promising direction for intelligent\nprosthetic systems.",
            "headline_zh": "提出模型预测控制方法以优化假肢手腕运动，提升适应性和性能。",
            "intro_zh": [
                "核心问题：假肢手适应性和性能不足，需先进控制策略。",
                "方法要点：采用模型预测控制，结合预测建模和动态交互，减少计算负担。",
                "实验或效果：通过仿真和实验验证，显著提高手腕灵活性和用户控制自然度。"
            ],
            "tags_zh": [
                "模型预测控制",
                "假肢手腕优化",
                "软连续体手腕",
                "动态建模",
                "仿真验证"
            ],
            "_index": 37
        },
        {
            "title": "PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis",
            "authors": [
                "Qing Mao",
                "Tianxin Huang",
                "Yu Zhu",
                "Jinqiu Sun",
                "Yanning Zhang",
                "Gim Hee Lee"
            ],
            "arxiv_id": "2510.19527v1",
            "summary": "Pairwise camera pose estimation from sparsely overlapping image pairs remains\na critical and unsolved challenge in 3D vision. Most existing methods struggle\nwith image pairs that have small or no overlap. Recent approaches attempt to\naddress this by synthesizing intermediate frames using video interpolation and\nselecting key frames via a self-consistency score. However, the generated\nframes are often blurry due to small overlap inputs, and the selection\nstrategies are slow and not explicitly aligned with pose estimation. To solve\nthese cases, we propose Hybrid Video Generation (HVG) to synthesize clearer\nintermediate frames by coupling a video interpolation model with a\npose-conditioned novel view synthesis model, where we also propose a Feature\nMatching Selector (FMS) based on feature correspondence to select intermediate\nframes appropriate for pose estimation from the synthesized results. Extensive\nexperiments on Cambridge Landmarks, ScanNet, DL3DV-10K, and NAVI demonstrate\nthat, compared to existing SOTA methods, PoseCrafter can obviously enhance the\npose estimation performances, especially on examples with small or no overlap.",
            "headline_zh": "提出PoseCrafter通过混合视频生成解决稀疏重叠图像对的姿态估计问题",
            "intro_zh": [
                "核心问题：稀疏重叠图像对的相机姿态估计困难，现有方法在小或无重叠时性能差",
                "方法要点：结合视频插值和姿态条件新视图合成，生成清晰中间帧并基于特征匹配选择",
                "实验或效果：在多个数据集上显著提升姿态估计性能，尤其小或无重叠场景"
            ],
            "tags_zh": [
                "姿态估计",
                "视频合成",
                "特征匹配",
                "3D视觉",
                "稀疏重叠"
            ],
            "_index": 38
        },
        {
            "title": "CARES: Context-Aware Resolution Selector for VLMs",
            "authors": [
                "Moshe Kimhi",
                "Nimrod Shabtay",
                "Raja Giryes",
                "Chaim Baskin",
                "Eli Schwartz"
            ],
            "arxiv_id": "2510.19496v1",
            "summary": "Large vision-language models (VLMs) commonly process images at native or high\nresolution to remain effective across tasks. This inflates visual tokens ofter\nto 97-99% of total tokens, resulting in high compute and latency, even when\nlow-resolution images would suffice. We introduce \\emph{CARES}-a\n\\textbf{C}ontext-\\textbf{A}ware \\textbf{R}esolution \\textbf{S}elector, a\nlightweight preprocessing module that, given an image-query pair, predicts the\n\\emph{minimal} sufficient input resolution. CARES uses a compact VLM (350M) to\nextract features and predict when a target pretrained VLM's response converges\nto its peak ability to answer correctly. Though trained as a discrete\nclassifier over a set of optional resolutions, CARES interpolates continuous\nresolutions at inference for fine-grained control. Across five multimodal\nbenchmarks spanning documents and natural images, as well as diverse target\nVLMs, CARES preserves task performance while reducing compute by up to 80%.",
            "headline_zh": "提出CARES模块以动态选择图像分辨率，降低视觉语言模型计算开销。",
            "intro_zh": [
                "视觉语言模型常处理高分辨率图像，导致视觉令牌占比高，增加计算和延迟。",
                "CARES使用轻量VLM预测最小足够分辨率，训练为离散分类器，推理时支持连续插值。",
                "在多个基准测试中，CARES保持任务性能，计算量减少高达80%。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "分辨率选择",
                "计算优化",
                "轻量预处理",
                "多模态基准"
            ],
            "_index": 39
        },
        {
            "title": "Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning",
            "authors": [
                "Kevin Huang",
                "Rosario Scalise",
                "Cleah Winston",
                "Ayush Agrawal",
                "Yunchu Zhang",
                "Rohan Baijal",
                "Markus Grotz",
                "Byron Boots",
                "Benjamin Burchfiel",
                "Hongkai Dai",
                "Masha Itkina",
                "Paarth Shah",
                "Abhishek Gupta"
            ],
            "arxiv_id": "2510.19495v1",
            "summary": "Imitation learning has proven effective for training robots to perform\ncomplex tasks from expert human demonstrations. However, it remains limited by\nits reliance on high-quality, task-specific data, restricting adaptability to\nthe diverse range of real-world object configurations and scenarios. In\ncontrast, non-expert data -- such as play data, suboptimal demonstrations,\npartial task completions, or rollouts from suboptimal policies -- can offer\nbroader coverage and lower collection costs. However, conventional imitation\nlearning approaches fail to utilize this data effectively. To address these\nchallenges, we posit that with right design decisions, offline reinforcement\nlearning can be used as a tool to harness non-expert data to enhance the\nperformance of imitation learning policies. We show that while standard offline\nRL approaches can be ineffective at actually leveraging non-expert data under\nthe sparse data coverage settings typically encountered in the real world,\nsimple algorithmic modifications can allow for the utilization of this data,\nwithout significant additional assumptions. Our approach shows that broadening\nthe support of the policy distribution can allow imitation algorithms augmented\nby offline RL to solve tasks robustly, showing considerably enhanced recovery\nand generalization behavior. In manipulation tasks, these innovations\nsignificantly increase the range of initial conditions where learned policies\nare successful when non-expert data is incorporated. Moreover, we show that\nthese methods are able to leverage all collected data, including partial or\nsuboptimal demonstrations, to bolster task-directed policy performance. This\nunderscores the importance of algorithmic techniques for using non-expert data\nfor robust policy learning in robotics.",
            "headline_zh": "提出离线强化学习方法以利用非专家数据增强模仿学习的鲁棒性",
            "intro_zh": [
                "模仿学习依赖高质量专家数据，难以适应真实世界多样场景",
                "通过离线强化学习算法改进，利用非专家数据扩展策略分布支持",
                "在机器人操作任务中，显著提升策略的恢复能力和泛化性能"
            ],
            "tags_zh": [
                "模仿学习",
                "离线强化学习",
                "非专家数据",
                "机器人操作",
                "策略鲁棒性"
            ],
            "_index": 40
        },
        {
            "title": "Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts",
            "authors": [
                "Chen Li",
                "Huiying Xu",
                "Changxin Gao",
                "Zeyu Wang",
                "Yun Liu",
                "Xinzhong Zhu"
            ],
            "arxiv_id": "2510.19487v1",
            "summary": "Single-source Domain Generalized Object Detection (SDGOD), as a cutting-edge\nresearch topic in computer vision, aims to enhance model generalization\ncapability in unseen target domains through single-source domain training.\nCurrent mainstream approaches attempt to mitigate domain discrepancies via data\naugmentation techniques. However, due to domain shift and limited\ndomain-specific knowledge, models tend to fall into the pitfall of spurious\ncorrelations. This manifests as the model's over-reliance on simplistic\nclassification features (e.g., color) rather than essential domain-invariant\nrepresentations like object contours. To address this critical challenge, we\npropose the Cauvis (Causal Visual Prompts) method. First, we introduce a\nCross-Attention Prompts module that mitigates bias from spurious features by\nintegrating visual prompts with cross-attention. To address the inadequate\ndomain knowledge coverage and spurious feature entanglement in visual prompts\nfor single-domain generalization, we propose a dual-branch adapter that\ndisentangles causal-spurious features while achieving domain adaptation via\nhigh-frequency feature extraction. Cauvis achieves state-of-the-art performance\nwith 15.9-31.4% gains over existing domain generalization methods on SDGOD\ndatasets, while exhibiting significant robustness advantages in complex\ninterference environments.",
            "headline_zh": "提出Cauvis方法以解决单源域泛化目标检测中的伪相关性问题",
            "intro_zh": [
                "核心问题：模型在单源域训练中易陷入伪相关，过度依赖颜色等浅层特征而非轮廓等不变表示。",
                "方法要点：引入跨注意力提示模块和双分支适配器，解耦因果-伪特征并提取高频特征实现域适应。",
                "实验或效果：在SDGOD数据集上性能提升15.9-31.4%，并在复杂干扰环境中展现强鲁棒性。"
            ],
            "tags_zh": [
                "单源域泛化目标检测",
                "因果视觉提示",
                "跨注意力机制",
                "特征解耦",
                "高频特征提取",
                "鲁棒性增强"
            ],
            "_index": 41
        },
        {
            "title": "Mitigating representation bias caused by missing pixels in methane plume detection",
            "authors": [
                "Julia Wąsala",
                "Joannes D. Maasakkers",
                "Ilse Aben",
                "Rochelle Schneider",
                "Holger Hoos",
                "Mitra Baratchi"
            ],
            "arxiv_id": "2510.19478v1",
            "summary": "Most satellite images have systematically missing pixels (i.e., missing data\nnot at random (MNAR)) due to factors such as clouds. If not addressed, these\nmissing pixels can lead to representation bias in automated feature extraction\nmodels. In this work, we show that spurious association between the label and\nthe number of missing values in methane plume detection can cause the model to\nassociate the coverage (i.e., the percentage of valid pixels in an image) with\nthe label, subsequently under-detecting plumes in low-coverage images. We\nevaluate multiple imputation approaches to remove the dependence between the\ncoverage and a label. Additionally, we propose a weighted resampling scheme\nduring training that removes the association between the label and the coverage\nby enforcing class balance in each coverage bin. Our results show that both\nresampling and imputation can significantly reduce the representation bias\nwithout hurting balanced accuracy, precision, or recall. Finally, we evaluate\nthe capability of the debiased models using these techniques in an operational\nscenario and demonstrate that the debiased models have a higher chance of\ndetecting plumes in low-coverage images.",
            "headline_zh": "提出加权重采样和插补方法以缓解甲烷羽流检测中的表示偏差",
            "intro_zh": [
                "卫星图像中系统缺失像素导致表示偏差，模型错误关联覆盖度与标签",
                "采用插补和加权重采样方法，强制覆盖度分箱中的类别平衡",
                "实验显示方法显著减少偏差，提升低覆盖图像中的羽流检测能力"
            ],
            "tags_zh": [
                "甲烷羽流检测",
                "表示偏差缓解",
                "缺失数据插补",
                "加权重采样",
                "卫星图像分析"
            ],
            "_index": 42
        },
        {
            "title": "PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation",
            "authors": [
                "Zhuoyang Xie",
                "Yibo Zhao",
                "Hui Huang",
                "Riwei Wang",
                "Zan Gao"
            ],
            "arxiv_id": "2510.19475v1",
            "summary": "Monocular 3D human pose estimation remains a fundamentally ill-posed inverse\nproblem due to the inherent depth ambiguity in 2D-to-3D lifting. While\ncontemporary video-based methods leverage temporal context to enhance spatial\nreasoning, they operate under a critical paradigm limitation: processing each\nsequence in isolation, thereby failing to exploit the strong structural\nregularities and repetitive motion patterns that pervade human movement across\nsequences. This work introduces the Pattern Reuse Graph Convolutional Network\n(PRGCN), a novel framework that formalizes pose estimation as a problem of\npattern retrieval and adaptation. At its core, PRGCN features a graph memory\nbank that learns and stores a compact set of pose prototypes, encoded as\nrelational graphs, which are dynamically retrieved via an attention mechanism\nto provide structured priors. These priors are adaptively fused with hard-coded\nanatomical constraints through a memory-driven graph convolution, ensuring\ngeometrical plausibility. To underpin this retrieval process with robust\nspatiotemporal features, we design a dual-stream hybrid architecture that\nsynergistically combines the linear-complexity, local temporal modeling of\nMamba-based state-space models with the global relational capacity of\nself-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarks\ndemonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPE\nof 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domain\ngeneralization capability. Our work posits that the long-overlooked mechanism\nof cross-sequence pattern reuse is pivotal to advancing the field, shifting the\nparadigm from per-sequence optimization towards cumulative knowledge learning.",
            "headline_zh": "提出PRGCN以解决单目3D人体姿态估计中的跨序列模式重用问题",
            "intro_zh": [
                "核心问题：单目3D姿态估计因2D到3D提升的深度模糊性而病态，现有方法孤立处理序列，忽略跨序列运动模式。",
                "方法要点：设计图记忆网络，通过注意力机制检索和融合姿态原型，结合Mamba与自注意力进行时空特征提取。",
                "实验或效果：在Human3.6M和MPI-INF-3DHP基准上达到SOTA，MPJPE分别为37.1mm和13.4mm，提升跨域泛化能力。"
            ],
            "tags_zh": [
                "3D人体姿态估计",
                "图卷积网络",
                "模式重用",
                "记忆网络",
                "时空建模",
                "跨序列学习"
            ],
            "_index": 43
        },
        {
            "title": "Predicting before Reconstruction: A generative prior framework for MRI acceleration",
            "authors": [
                "Juhyung Park",
                "Rokgi Hong",
                "Roh-Eul Yoo",
                "Jaehyeon Koo",
                "Se Young Chun",
                "Seung Hong Choi",
                "Jongho Lee"
            ],
            "arxiv_id": "2510.19472v1",
            "summary": "Recent advancements in artificial intelligence have created transformative\ncapabilities in image synthesis and generation, enabling diverse research\nfields to innovate at revolutionary speed and spectrum. In this study, we\nleverage this generative power to introduce a new paradigm for accelerating\nMagnetic Resonance Imaging (MRI), introducing a shift from image reconstruction\nto proactive predictive imaging. Despite being a cornerstone of modern patient\ncare, MRI's lengthy acquisition times limit clinical throughput. Our novel\nframework addresses this challenge by first predicting a target contrast image,\nwhich then serves as a data-driven prior for reconstructing highly\nunder-sampled data. This informative prior is predicted by a generative model\nconditioned on diverse data sources, such as other contrast images, previously\nscanned images, acquisition parameters, patient information. We demonstrate\nthis approach with two key applications: (1) reconstructing FLAIR images using\npredictions from T1w and/or T2w scans, and (2) reconstructing T1w images using\npredictions from previously acquired T1w scans. The framework was evaluated on\ninternal and multiple public datasets (total 14,921 scans; 1,051,904 slices),\nincluding multi-channel k-space data, for a range of high acceleration factors\n(x4, x8 and x12). The results demonstrate that our prediction-prior\nreconstruction method significantly outperforms other approaches, including\nthose with alternative or no prior information. Through this framework we\nintroduce a fundamental shift from image reconstruction towards a new paradigm\nof predictive imaging.",
            "headline_zh": "提出生成先验框架以加速MRI，通过预测目标对比图像重构欠采样数据",
            "intro_zh": [
                "MRI采集时间长，限制临床吞吐量，需加速成像过程",
                "使用生成模型预测目标对比图像，作为数据驱动先验重构欠采样数据",
                "在多个数据集上评估，高加速因子下性能优于其他方法"
            ],
            "tags_zh": [
                "MRI加速",
                "生成先验",
                "图像重构",
                "预测成像",
                "欠采样数据"
            ],
            "_index": 44
        },
        {
            "title": "PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks",
            "authors": [
                "Ali Sadeghkhani",
                "Brandon Bennett",
                "Masoud Babaei",
                "Arash Rabbani"
            ],
            "arxiv_id": "2510.19465v1",
            "summary": "Obtaining truly representative pore-scale images that match bulk formation\nproperties remains a fundamental challenge in subsurface characterization, as\nnatural spatial heterogeneity causes extracted sub-images to deviate\nsignificantly from core-measured values. This challenge is compounded by data\nscarcity, where physical samples are only available at sparse well locations.\nThis study presents a multi-conditional Generative Adversarial Network (cGAN)\nframework that generates representative pore-scale images with precisely\ncontrolled properties, addressing both the representativeness challenge and\ndata availability constraints. The framework was trained on thin section\nsamples from four depths (1879.50-1943.50 m) of a carbonate formation,\nsimultaneously conditioning on porosity values and depth parameters within a\nsingle unified model. This approach captures both universal pore network\nprinciples and depth-specific geological characteristics, from grainstone\nfabrics with interparticle-intercrystalline porosity to crystalline textures\nwith anhydrite inclusions. The model achieved exceptional porosity control\n(R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197.\nMorphological validation confirmed preservation of critical pore network\ncharacteristics including average pore radius, specific surface area, and\ntortuosity, with statistical differences remaining within acceptable geological\ntolerances. Most significantly, generated images demonstrated superior\nrepresentativeness with dual-constraint errors of 1.9-11.3% compared to\n36.4-578% for randomly extracted real sub-images. This capability provides\ntransformative tools for subsurface characterization, particularly valuable for\ncarbon storage, geothermal energy, and groundwater management applications\nwhere knowing the representative morphology of the pore space is critical for\nimplementing digital rock physics.",
            "headline_zh": "提出PCP-GAN多条件生成对抗网络，以生成具有精确控制属性的代表性孔隙尺度图像，解决地下表征中的代表性和数据稀缺问题。",
            "intro_zh": [
                "核心问题：地下孔隙图像代表性不足，自然异质性导致子图像偏离核心测量值，且物理样本稀缺。",
                "方法要点：使用多条件GAN，同时约束孔隙率和深度参数，生成统一模型捕获通用孔隙网络和深度特定地质特征。",
                "实验或效果：模型实现高精度孔隙控制（R²=0.95），形态验证保留关键孔隙特征，生成图像代表性强，误差远低于随机提取图像。"
            ],
            "tags_zh": [
                "生成对抗网络",
                "孔隙尺度图像重建",
                "多条件生成",
                "地下表征",
                "数字岩石物理",
                "孔隙网络控制"
            ],
            "_index": 45
        },
        {
            "title": "Exploring \"Many in Few\" and \"Few in Many\" Properties in Long-Tailed, Highly-Imbalanced IC Defect Classification",
            "authors": [
                "Hao-Chiang Shao",
                "Chun-Hao Chang",
                "Yu-Hsien Lin",
                "Chia-Wen Lin",
                "Shao-Yun Fang",
                "Yan-Hsiu Liu"
            ],
            "arxiv_id": "2510.19463v1",
            "summary": "Despite significant advancements in deep classification techniques and in-lab\nautomatic optical inspection models for long-tailed or highly imbalanced data,\napplying these approaches to real-world IC defect classification tasks remains\nchallenging. This difficulty stems from two primary factors. First, real-world\nconditions, such as the high yield-rate requirements in the IC industry, result\nin data distributions that are far more skewed than those found in general\npublic imbalanced datasets. Consequently, classifiers designed for open\nimbalanced datasets often fail to perform effectively in real-world scenarios.\nSecond, real-world samples exhibit a mix of class-specific attributes and\nclass-agnostic, domain-related features. This complexity adds significant\ndifficulty to the classification process, particularly for highly imbalanced\ndatasets. To address these challenges, this paper introduces the IC-Defect-14\ndataset, a large, highly imbalanced IC defect image dataset sourced from AOI\nsystems deployed in real-world IC production lines. This dataset is\ncharacterized by its unique \"intra-class clusters\" property, which presents two\nmajor challenges: large intra-class diversity and high inter-class similarity.\nThese characteristics, rarely found simultaneously in existing public datasets,\nsignificantly degrade the performance of current state-of-the-art classifiers\nfor highly imbalanced data. To tackle this challenge, we propose ReCAME-Net,\nwhich follows a multi-expert classifier framework and integrates a regional\nchannel attention module, metric learning losses, a hard category mining\nstrategy, and a knowledge distillation procedure. Extensive experimental\nevaluations demonstrate that ReCAME-Net outperforms previous state-of-the-art\nmodels on the IC-Defect-14 dataset while maintaining comparable performance and\ncompetitiveness on general public datasets.",
            "headline_zh": "提出ReCAME-Net以解决IC缺陷分类中高度不平衡数据的挑战",
            "intro_zh": [
                "核心问题：真实IC缺陷数据高度不平衡，类内多样性和类间相似性导致分类困难",
                "方法要点：采用多专家分类器框架，集成区域通道注意力、度量学习损失和知识蒸馏",
                "实验或效果：在IC-Defect-14数据集上优于现有方法，并在公共数据集上保持竞争力"
            ],
            "tags_zh": [
                "IC缺陷分类",
                "高度不平衡数据",
                "多专家分类器",
                "区域通道注意力",
                "度量学习",
                "知识蒸馏"
            ],
            "_index": 46
        },
        {
            "title": "Automated Morphological Analysis of Neurons in Fluorescence Microscopy Using YOLOv8",
            "authors": [
                "Banan Alnemri",
                "Arwa Basbrain"
            ],
            "arxiv_id": "2510.19455v1",
            "summary": "Accurate segmentation and precise morphological analysis of neuronal cells in\nfluorescence microscopy images are crucial steps in neuroscience and biomedical\nimaging applications. However, this process is labor-intensive and\ntime-consuming, requiring significant manual effort and expertise to ensure\nreliable outcomes. This work presents a pipeline for neuron instance\nsegmentation and measurement based on a high-resolution dataset of\nstem-cell-derived neurons. The proposed method uses YOLOv8, trained on manually\nannotated microscopy images. The model achieved high segmentation accuracy,\nexceeding 97%. In addition, the pipeline utilized both ground truth and\npredicted masks to extract biologically significant features, including cell\nlength, width, area, and grayscale intensity values. The overall accuracy of\nthe extracted morphological measurements reached 75.32%, further supporting the\neffectiveness of the proposed approach. This integrated framework offers a\nvaluable tool for automated analysis in cell imaging and neuroscience research,\nreducing the need for manual annotation and enabling scalable, precise\nquantification of neuron morphology.",
            "headline_zh": "提出基于YOLOv8的神经元形态自动分析管道，用于荧光显微镜图像。",
            "intro_zh": [
                "核心问题：荧光显微镜图像中神经元细胞分割与形态分析耗时且依赖人工。",
                "方法要点：使用YOLOv8模型在手动标注数据集上进行训练，实现实例分割。",
                "实验或效果：分割准确率超97%，形态测量准确率达75.32%。"
            ],
            "tags_zh": [
                "神经元分割",
                "YOLOv8",
                "荧光显微镜",
                "形态分析",
                "实例分割"
            ],
            "_index": 47
        },
        {
            "title": "Reasoning Like Experts: Leveraging Multimodal Large Language Models for Drawing-based Psychoanalysis",
            "authors": [
                "Xueqi Ma",
                "Yanbei Jiang",
                "Sarah Erfani",
                "James Bailey",
                "Weifeng Liu",
                "Krista A. Ehinger",
                "Jey Han Lau"
            ],
            "arxiv_id": "2510.19451v1",
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance across various objective multimodal perception tasks, yet their\napplication to subjective, emotionally nuanced domains, such as psychological\nanalysis, remains largely unexplored. In this paper, we introduce PICK, a\nmulti-step framework designed for Psychoanalytical Image Comprehension through\nhierarchical analysis and Knowledge injection with MLLMs, specifically focusing\non the House-Tree-Person (HTP) Test, a widely used psychological assessment in\nclinical practice. First, we decompose drawings containing multiple instances\ninto semantically meaningful sub-drawings, constructing a hierarchical\nrepresentation that captures spatial structure and content across three levels:\nsingle-object level, multi-object level, and whole level. Next, we analyze\nthese sub-drawings at each level with a targeted focus, extracting\npsychological or emotional insights from their visual cues. We also introduce\nan HTP knowledge base and design a feature extraction module, trained with\nreinforcement learning, to generate a psychological profile for single-object\nlevel analysis. This profile captures both holistic stylistic features and\ndynamic object-specific features (such as those of the house, tree, or person),\ncorrelating them with psychological states. Finally, we integrate these\nmulti-faceted information to produce a well-informed assessment that aligns\nwith expert-level reasoning. Our approach bridges the gap between MLLMs and\nspecialized expert domains, offering a structured and interpretable framework\nfor understanding human mental states through visual expression. Experimental\nresults demonstrate that the proposed PICK significantly enhances the\ncapability of MLLMs in psychological analysis. It is further validated as a\ngeneral framework through extensions to emotion understanding tasks.",
            "headline_zh": "提出PICK框架，利用多模态大语言模型进行基于绘画的心理分析",
            "intro_zh": [
                "核心问题：多模态大语言模型在主观情感领域如心理分析的应用尚少探索",
                "方法要点：通过层次分解、知识注入和强化学习提取心理特征，构建多级分析",
                "实验或效果：PICK显著提升心理分析能力，并验证为通用框架扩展至情感理解任务"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "心理分析",
                "绘画测试",
                "层次分析",
                "知识注入",
                "强化学习"
            ],
            "_index": 48
        },
        {
            "title": "Multi-Camera Worker Tracking in Logistics Warehouse Considering Wide-Angle Distortion",
            "authors": [
                "Yuki Mori",
                "Kazuma Kano",
                "Yusuke Asai",
                "Shin Katayama",
                "Kenta Urano",
                "Takuro Yonezawa",
                "Nobuo Kawaguchi"
            ],
            "arxiv_id": "2510.19432v1",
            "summary": "With the spread of e-commerce, the logistics market is growing around the\nworld. Therefore, improving the efficiency of warehouse operations is\nessential. To achieve this, various approaches have been explored, and among\nthem, the use of digital twins is gaining attention. To make this approach\npossible, it is necessary to accurately collect the positions of workers in a\nwarehouse and reflect them in a virtual space. However, a single camera has\nlimitations in its field of view, therefore sensing with multiple cameras is\nnecessary. In this study, we explored a method to track workers using 19\nwide-angle cameras installed on the ceiling, looking down at the floor of the\nlogistics warehouse. To understand the relationship between the camera\ncoordinates and the actual positions in the warehouse, we performed alignment\nbased on the floor surface. However, due to the characteristics of wide-angle\ncameras, significant distortion occurs at the edges of the image, particularly\nin the vertical direction. To address this, the detected worker positions from\neach camera were aligned based on foot positions, reducing the effects of image\ndistortion, and enabling accurate position alignment across cameras. As a\nresult, we confirmed an improvement of over 20% in tracking accuracy.\nFurthermore, we compared multiple methods for utilizing appearance features and\nvalidated the effectiveness of the proposed approach.",
            "headline_zh": "提出基于多相机和脚部对齐的方法，以解决物流仓库中广角畸变下的工人跟踪问题。",
            "intro_zh": [
                "核心问题：单相机视野有限，广角相机边缘畸变影响工人位置准确采集。",
                "方法要点：使用19个广角相机，基于脚部位置对齐，减少图像畸变影响。",
                "实验或效果：跟踪精度提升超过20%，验证了外观特征利用方法的有效性。"
            ],
            "tags_zh": [
                "多相机跟踪",
                "广角畸变校正",
                "物流仓库监控",
                "工人位置对齐",
                "数字孪生"
            ],
            "_index": 49
        },
        {
            "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model",
            "authors": [
                "GigaBrain Team",
                "Angen Ye",
                "Boyuan Wang",
                "Chaojun Ni",
                "Guan Huang",
                "Guosheng Zhao",
                "Haoyun Li",
                "Jie Li",
                "Jiagang Zhu",
                "Lv Feng",
                "Peng Li",
                "Qiuping Deng",
                "Runqi Ouyang",
                "Wenkang Qin",
                "Xinze Chen",
                "Xiaofeng Wang",
                "Yang Wang",
                "Yifan Li",
                "Yilong Li",
                "Yiran Ding",
                "Yuan Xu",
                "Yun Ye",
                "Yukun Zhou",
                "Zhehao Dong",
                "Zhenan Wang",
                "Zhichao Liu",
                "Zheng Zhu"
            ],
            "arxiv_id": "2510.19430v1",
            "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.",
            "headline_zh": "提出GigaBrain-0，利用世界模型生成数据以解决机器人视觉-语言-动作模型数据收集难题",
            "intro_zh": [
                "核心问题：大规模真实机器人数据收集成本高，限制视觉-语言-动作模型的泛化能力。",
                "方法要点：通过世界模型生成多样化数据，减少对真实数据的依赖，并引入RGBD输入和具身链式思维监督。",
                "实验或效果：在灵巧、长视界和移动操作任务中实现优越泛化，包括外观、物体放置和视角变化。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "世界模型数据生成",
                "RGBD输入建模",
                "具身链式思维",
                "机器人泛化",
                "轻量级变体"
            ],
            "_index": 50
        },
        {
            "title": "From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data",
            "authors": [
                "Mete Harun Akcay",
                "Buse Gul Atli",
                "Siddharth Prakash Rao",
                "Alexandros Bakas"
            ],
            "arxiv_id": "2510.19418v1",
            "summary": "As the volume of stored data continues to grow, identifying and protecting\nsensitive information within large repositories becomes increasingly\nchallenging, especially when shared with multiple users with different roles\nand permissions. This work presents a system architecture for trusted data\nsharing with policy-driven access control, enabling selective protection of\nsensitive regions while maintaining scalability. The proposed architecture\nintegrates four core modules that combine automated detection of sensitive\nregions, post-correction, key management, and access control. Sensitive regions\nare secured using a hybrid scheme that employs symmetric encryption for\nefficiency and Attribute-Based Encryption for policy enforcement. The system\nsupports efficient key distribution and isolates key storage to strengthen\noverall security. To demonstrate its applicability, we evaluate the system on\nvisual datasets, where Privacy-Sensitive Objects in images are automatically\ndetected, reassessed, and selectively encrypted prior to sharing in a data\nrepository. Experimental results show that our system provides effective PSO\ndetection, increases macro-averaged F1 score (5%) and mean Average Precision\n(10%), and maintains an average policy-enforced decryption time of less than 1\nsecond per image. These results demonstrate the effectiveness, efficiency and\nscalability of our proposed solution for fine-grained access control.",
            "headline_zh": "提出基于机器学习的细粒度访问控制系统，以保护视觉数据中的敏感区域。",
            "intro_zh": [
                "核心问题：大规模数据共享中敏感信息识别与保护困难，需支持多角色权限管理。",
                "方法要点：集成敏感区域自动检测、后校正、密钥管理和访问控制模块，采用混合加密方案。",
                "实验或效果：在视觉数据集上验证，提升检测性能，平均解密时间低于1秒，确保高效可扩展。"
            ],
            "tags_zh": [
                "细粒度访问控制",
                "敏感区域检测",
                "混合加密",
                "视觉数据保护",
                "密钥管理",
                "策略执行"
            ],
            "_index": 51
        },
        {
            "title": "Risk Assessment of an Autonomous Underwater Snake Robot in Confined Operations",
            "authors": [
                "Abdelrahman Sayed Sayed"
            ],
            "arxiv_id": "2510.19415v1",
            "summary": "The growing interest in ocean discovery imposes a need for inspection and\nintervention in confined and demanding environments. Eely's slender shape, in\naddition to its ability to change its body configurations, makes articulated\nunderwater robots an adequate option for such environments. However, operation\nof Eely in such environments imposes demanding requirements on the system, as\nit must deal with uncertain and unstructured environments, extreme\nenvironmental conditions, and reduced navigational capabilities. This paper\nproposes a Bayesian approach to assess the risks of losing Eely during two\nmission scenarios. The goal of this work is to improve Eely's performance and\nthe likelihood of mission success. Sensitivity analysis results are presented\nin order to demonstrate the causes having the highest impact on losing Eely.",
            "headline_zh": "提出贝叶斯方法评估自主水下蛇形机器人在受限操作中的风险",
            "intro_zh": [
                "核心问题：水下蛇形机器人在受限环境中面临不确定性和导航能力下降的风险",
                "方法要点：采用贝叶斯方法评估机器人在两种任务场景中丢失的概率",
                "实验或效果：通过敏感性分析识别对丢失风险影响最大的因素"
            ],
            "tags_zh": [
                "自主水下机器人",
                "风险评估",
                "贝叶斯方法",
                "敏感性分析",
                "受限环境操作"
            ],
            "_index": 52
        },
        {
            "title": "Spatio-temporal Sign Language Representation and Translation",
            "authors": [
                "Yasser Hamidullah",
                "Josef van Genabith",
                "Cristina España-Bonet"
            ],
            "arxiv_id": "2510.19413v1",
            "summary": "This paper describes the DFKI-MLT submission to the WMT-SLT 2022 sign\nlanguage translation (SLT) task from Swiss German Sign Language (video) into\nGerman (text). State-of-the-art techniques for SLT use a generic seq2seq\narchitecture with customized input embeddings. Instead of word embeddings as\nused in textual machine translation, SLT systems use features extracted from\nvideo frames. Standard approaches often do not benefit from temporal features.\nIn our participation, we present a system that learns spatio-temporal feature\nrepresentations and translation in a single model, resulting in a real\nend-to-end architecture expected to better generalize to new data sets. Our\nbest system achieved $5\\pm1$ BLEU points on the development set, but the\nperformance on the test dropped to $0.11\\pm0.06$ BLEU points.",
            "headline_zh": "提出端到端时空特征学习模型以改进手语翻译性能",
            "intro_zh": [
                "标准手语翻译方法常忽略视频的时序特征，影响翻译准确性。",
                "采用单一模型学习时空特征表示和翻译，实现端到端架构。",
                "在开发集BLEU达5±1，测试集性能下降至0.11±0.06。"
            ],
            "tags_zh": [
                "手语翻译",
                "时空特征学习",
                "端到端架构",
                "视频特征提取",
                "BLEU评估"
            ],
            "_index": 53
        },
        {
            "title": "A Radius of Robust Feasibility Approach to Directional Sensors in Uncertain Terrain",
            "authors": [
                "Vanshika Datta",
                "C. Nahak"
            ],
            "arxiv_id": "2510.19407v1",
            "summary": "A sensor has the ability to probe its surroundings. However, uncertainties in\nits exact location can significantly compromise its sensing performance. The\nradius of robust feasibility defines the maximum range within which robust\nfeasibility is ensured. This work introduces a novel approach integrating it\nwith the directional sensor networks to enhance coverage using a distributed\ngreedy algorithm. In particular, we provide an exact formula for the radius of\nrobust feasibility of sensors in a directional sensor network. The proposed\nmodel strategically orients the sensors in regions with high coverage\npotential, accounting for robustness in the face of uncertainty. We analyze the\nalgorithm's adaptability in dynamic environments, demonstrating its ability to\nenhance efficiency and robustness. Experimental results validate its efficacy\nin maximizing coverage and optimizing sensor orientations, highlighting its\npractical advantages for real-world scenarios.",
            "headline_zh": "提出半径稳健可行性方法以增强不确定地形中定向传感器网络的覆盖",
            "intro_zh": [
                "传感器位置不确定性影响感知性能，需确保稳健覆盖。",
                "引入半径稳健可行性公式，结合分布式贪婪算法优化传感器方向。",
                "实验验证算法在动态环境中提升覆盖效率和稳健性。"
            ],
            "tags_zh": [
                "定向传感器网络",
                "半径稳健可行性",
                "分布式贪婪算法",
                "传感器覆盖优化",
                "不确定性处理"
            ],
            "_index": 54
        },
        {
            "title": "Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes",
            "authors": [
                "Zhiyuan Feng",
                "Zhaolu Kang",
                "Qijie Wang",
                "Zhiying Du",
                "Jiongrui Yan",
                "Shubin Shi",
                "Chengbo Yuan",
                "Huizhi Liang",
                "Yu Deng",
                "Qixiu Li",
                "Rushuai Yang",
                "Arctanx An",
                "Leqi Zheng",
                "Weijie Wang",
                "Shawn Chen",
                "Sicheng Xu",
                "Yaobo Liang",
                "Jiaolong Yang",
                "Baining Guo"
            ],
            "arxiv_id": "2510.19400v1",
            "summary": "Vision-language models (VLMs) are essential to Embodied AI, enabling robots\nto perceive, reason, and act in complex environments. They also serve as the\nfoundation for the recent Vision-Language-Action (VLA) models. Yet most\nevaluations of VLMs focus on single-view settings, leaving their ability to\nintegrate multi-view information underexplored. At the same time, multi-camera\nsetups are increasingly standard in robotic platforms, as they provide\ncomplementary perspectives to mitigate occlusion and depth ambiguity. Whether\nVLMs can effectively leverage such multi-view inputs for robotic reasoning\ntherefore remains an open question. To bridge this gap, we introduce\nMV-RoboBench, a benchmark specifically designed to evaluate the multi-view\nspatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBench\nconsists of 1.7k manually curated QA items across eight subtasks, divided into\ntwo primary categories: spatial understanding and robotic execution. We\nevaluate a diverse set of existing VLMs, including both open-source and\nclosed-source models, along with enhanced versions incorporating CoT-inspired\ntechniques. The results show that state-of-the-art models remain far below\nhuman performance, underscoring the substantial challenges VLMs face in\nmulti-view robotic perception. Additionally, our analysis uncovers two key\nfindings: (i) spatial intelligence and robotic task execution are positively\ncorrelated in multi-view robotic scenarios; and (ii) strong performance on\nexisting general-purpose single-view spatial understanding benchmarks does not\nreliably translate to success in the robotic spatial tasks assessed by our\nbenchmark. We release MV-RoboBench as an open resource to foster progress in\nspatially grounded VLMs and VLAs, providing not only data but also a\nstandardized evaluation protocol for multi-view embodied reasoning.",
            "headline_zh": "提出MV-RoboBench基准以评估视觉语言模型在机器人场景中的多视角空间推理能力",
            "intro_zh": [
                "核心问题：视觉语言模型在多视角机器人感知中的空间推理能力未被充分探索",
                "方法要点：构建包含1.7k问答项的基准，涵盖空间理解和机器人执行任务",
                "实验或效果：评估显示现有模型性能远低于人类，揭示多视角推理挑战"
            ],
            "tags_zh": [
                "视觉语言模型",
                "多视角空间推理",
                "机器人基准",
                "空间理解",
                "机器人执行",
                "评估协议"
            ],
            "_index": 55
        },
        {
            "title": "Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets",
            "authors": [
                "Basavasagar Patil",
                "Sydney Belt",
                "Jayjun Lee",
                "Nima Fazeli",
                "Bernadette Bucher"
            ],
            "arxiv_id": "2510.19373v1",
            "summary": "Increasingly large datasets of robot actions and sensory observations are\nbeing collected to train ever-larger neural networks. These datasets are\ncollected based on tasks and while these tasks may be distinct in their\ndescriptions, many involve very similar physical action sequences (e.g., 'pick\nup an apple' versus 'pick up an orange'). As a result, many datasets of robotic\ntasks are substantially imbalanced in terms of the physical robotic actions\nthey represent. In this work, we propose a simple sampling strategy for policy\ntraining that mitigates this imbalance. Our method requires only a few lines of\ncode to integrate into existing codebases and improves generalization. We\nevaluate our method in both pre-training small models and fine-tuning large\nfoundational models. Our results show substantial improvements on low-resource\ntasks compared to prior state-of-the-art methods, without degrading performance\non high-resource tasks. This enables more effective use of model capacity for\nmulti-task policies. We also further validate our approach in a real-world\nsetup on a Franka Panda robot arm across a diverse set of tasks.",
            "headline_zh": "提出温度采样方法以解决机器人学习策略在数据集不平衡时的泛化问题",
            "intro_zh": [
                "机器人数据集因任务相似导致动作序列不平衡，影响模型训练",
                "采用简单采样策略，易于集成代码库，提升泛化能力",
                "实验显示在低资源任务中性能显著提升，高资源任务无退化"
            ],
            "tags_zh": [
                "机器人学习",
                "数据集不平衡",
                "采样策略",
                "策略训练",
                "泛化提升"
            ],
            "_index": 56
        },
        {
            "title": "AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields",
            "authors": [
                "Woo Jae Kim",
                "Kyu Beom Han",
                "Yoonki Cho",
                "Youngju Na",
                "Junsik Jung",
                "Sooel Son",
                "Sung-eui Yoon"
            ],
            "arxiv_id": "2510.19371v1",
            "summary": "As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D\nscene representation and novel view synthesis, protecting their intellectual\nproperty (IP) from unauthorized use is becoming increasingly crucial. In this\nwork, we aim to protect the IP of NeRFs by injecting adversarial perturbations\nthat disrupt their unauthorized applications. However, perturbing the 3D\ngeometry of NeRFs can easily deform the underlying scene structure and thus\nsubstantially degrade the rendering quality, which has led existing attempts to\navoid geometric perturbations or restrict them to explicit spaces like meshes.\nTo overcome this limitation, we introduce a learnable sensitivity to quantify\nthe spatially varying impact of geometric perturbations on rendering quality.\nBuilding upon this, we propose AegisRF, a novel framework that consists of a\nPerturbation Field, which injects adversarial perturbations into the\npre-rendering outputs (color and volume density) of NeRF models to fool an\nunauthorized downstream target model, and a Sensitivity Field, which learns the\nsensitivity to adaptively constrain geometric perturbations, preserving\nrendering quality while disrupting unauthorized use. Our experimental\nevaluations demonstrate the generalized applicability of AegisRF across diverse\ndownstream tasks and modalities, including multi-view image classification and\nvoxel-based 3D localization, while maintaining high visual fidelity. Codes are\navailable at https://github.com/wkim97/AegisRF.",
            "headline_zh": "提出AegisRF框架以保护神经辐射场知识产权，通过对抗扰动破坏未授权使用",
            "intro_zh": [
                "核心问题：神经辐射场易受未授权使用，现有方法避免几何扰动以防渲染质量下降",
                "方法要点：引入可学习敏感度场，自适应约束几何扰动，保持渲染质量同时注入对抗扰动",
                "实验或效果：在多种下游任务中验证通用性，如多视图图像分类，保持高视觉保真度"
            ],
            "tags_zh": [
                "神经辐射场保护",
                "对抗扰动",
                "知识产权安全",
                "几何扰动约束",
                "多任务评估"
            ],
            "_index": 57
        },
        {
            "title": "ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling",
            "authors": [
                "Golnaz Raja",
                "Ruslan Agishev",
                "Miloš Prágr",
                "Joni Pajarinen",
                "Karel Zimmermann",
                "Arun Kumar Singh",
                "Reza Ghabcheloo"
            ],
            "arxiv_id": "2510.19364v1",
            "summary": "Uncertainty-aware robot motion prediction is crucial for downstream\ntraversability estimation and safe autonomous navigation in unstructured,\noff-road environments, where terrain is heterogeneous and perceptual\nuncertainty is high. Most existing methods assume deterministic or spatially\nindependent terrain uncertainties, ignoring the inherent local correlations of\n3D spatial data and often producing unreliable predictions. In this work, we\nintroduce an efficient probabilistic framework that explicitly models spatially\ncorrelated aleatoric uncertainty over terrain parameters as a probabilistic\nworld model and propagates this uncertainty through a differentiable physics\nengine for probabilistic trajectory forecasting. By leveraging structured\nconvolutional operators, our approach provides high-resolution multivariate\npredictions at manageable computational cost. Experimental evaluation on a\npublicly available dataset shows significantly improved uncertainty estimation\nand trajectory prediction accuracy over aleatoric uncertainty estimation\nbaselines.",
            "headline_zh": "提出概率物理信息粗糙地形世界建模框架，以提升非结构化环境中的机器人运动预测准确性。",
            "intro_zh": [
                "核心问题：现有方法忽略3D空间数据的局部相关性，导致不确定地形预测不可靠。",
                "方法要点：建模空间相关随机不确定性，并通过可微分物理引擎传播以预测轨迹。",
                "实验或效果：在公开数据集上显著改进不确定性估计和轨迹预测精度。"
            ],
            "tags_zh": [
                "概率世界建模",
                "空间不确定性",
                "可微分物理引擎",
                "轨迹预测",
                "粗糙地形导航"
            ],
            "_index": 58
        },
        {
            "title": "Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model",
            "authors": [
                "Yu Fang",
                "Xinyu Wang",
                "Xuehe Zhang",
                "Wanli Xue",
                "Mingwei Zhang",
                "Shengyong Chen",
                "Jie Zhao"
            ],
            "arxiv_id": "2510.19356v1",
            "summary": "The wide application of flow-matching methods has greatly promoted the\ndevelopment of robot imitation learning. However, these methods all face the\nproblem of high inference time. To address this issue, researchers have\nproposed distillation methods and consistency methods, but the performance of\nthese methods still struggles to compete with that of the original diffusion\nmodels and flow-matching models. In this article, we propose a one-step\nshortcut method with multi-step integration for robot imitation learning. To\nbalance the inference speed and performance, we extend the multi-step\nconsistency loss on the basis of the shortcut model, split the one-step loss\ninto multi-step losses, and improve the performance of one-step inference.\nSecondly, to solve the problem of unstable optimization of the multi-step loss\nand the original flow-matching loss, we propose an adaptive gradient allocation\nmethod to enhance the stability of the learning process. Finally, we evaluate\nthe proposed method in two simulation benchmarks and five real-world\nenvironment tasks. The experimental results verify the effectiveness of the\nproposed algorithm.",
            "headline_zh": "提出多步一致集成捷径模型以平衡机器人模仿学习的推理速度与性能",
            "intro_zh": [
                "核心问题：流匹配方法在机器人模仿学习中推理时间高，现有蒸馏和一致性方法性能不足。",
                "方法要点：扩展多步一致性损失，分割一步损失为多步，并采用自适应梯度分配稳定优化。",
                "实验或效果：在模拟基准和真实环境任务中验证算法有效性，提升一步推理性能。"
            ],
            "tags_zh": [
                "机器人模仿学习",
                "流匹配方法",
                "多步一致性损失",
                "自适应梯度分配",
                "一步推理优化"
            ],
            "_index": 59
        },
        {
            "title": "DARE: A Deformable Adaptive Regularization Estimator for Learning-Based Medical Image Registration",
            "authors": [
                "Ahsan Raza Siyal",
                "Markus Haltmeier",
                "Ruth Steiger",
                "Malik Galijasevic",
                "Elke Ruth Gizewski",
                "Astrid Ellen Grams"
            ],
            "arxiv_id": "2510.19353v1",
            "summary": "Deformable medical image registration is a fundamental task in medical image\nanalysis. While deep learning-based methods have demonstrated superior accuracy\nand computational efficiency compared to traditional techniques, they often\noverlook the critical role of regularization in ensuring robustness and\nanatomical plausibility. We propose DARE (Deformable Adaptive Regularization\nEstimator), a novel registration framework that dynamically adjusts elastic\nregularization based on the gradient norm of the deformation field. Our\napproach integrates strain and shear energy terms, which are adaptively\nmodulated to balance stability and flexibility. To ensure physically realistic\ntransformations, DARE includes a folding-prevention mechanism that penalizes\nregions with negative deformation Jacobian. This strategy mitigates\nnon-physical artifacts such as folding, avoids over-smoothing, and improves\nboth registration accuracy and anatomical plausibility",
            "headline_zh": "提出DARE框架以解决医学图像配准中正则化不足问题",
            "intro_zh": [
                "核心问题：基于深度学习的医学图像配准方法常忽略正则化，影响鲁棒性和解剖合理性",
                "方法要点：动态调整弹性正则化，整合应变和剪切能量项，并包含折叠预防机制",
                "实验或效果：提高配准精度和解剖合理性，减少非物理伪影如折叠"
            ],
            "tags_zh": [
                "医学图像配准",
                "自适应正则化",
                "变形场优化",
                "折叠预防",
                "深度学习框架"
            ],
            "_index": 60
        },
        {
            "title": "ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation",
            "authors": [
                "Omer Tariq",
                "Muhammad Bilal",
                "Muneeb Ul Hassan",
                "Dongsoo Han",
                "Jon Crowcroft"
            ],
            "arxiv_id": "2510.19352v1",
            "summary": "Data-driven inertial sequence learning has revolutionized navigation in\nGPS-denied environments, offering superior odometric resolution compared to\ntraditional Bayesian methods. However, deep learning-based inertial tracking\nsystems remain vulnerable to privacy breaches that can expose sensitive\ntraining data. \\hl{Existing differential privacy solutions often compromise\nmodel performance by introducing excessive noise, particularly in\nhigh-frequency inertial measurements.} In this article, we propose ConvXformer,\na hybrid architecture that fuses ConvNeXt blocks with Transformer encoders in a\nhierarchical structure for robust inertial navigation. We propose an efficient\ndifferential privacy mechanism incorporating adaptive gradient clipping and\ngradient-aligned noise injection (GANI) to protect sensitive information while\nensuring model performance. Our framework leverages truncated singular value\ndecomposition for gradient processing, enabling precise control over the\nprivacy-utility trade-off. Comprehensive performance evaluations on benchmark\ndatasets (OxIOD, RIDI, RoNIN) demonstrate that ConvXformer surpasses\nstate-of-the-art methods, achieving more than 40% improvement in positioning\naccuracy while ensuring $(\\epsilon,\\delta)$-differential privacy guarantees. To\nvalidate real-world performance, we introduce the Mech-IO dataset, collected\nfrom the mechanical engineering building at KAIST, where intense magnetic\nfields from industrial equipment induce significant sensor perturbations. This\ndemonstrated robustness under severe environmental distortions makes our\nframework well-suited for secure and intelligent navigation in cyber-physical\nsystems.",
            "headline_zh": "提出ConvXformer混合架构，结合差分隐私机制，提升GPS缺失环境下的惯性导航精度与隐私保护。",
            "intro_zh": [
                "深度学习惯性导航易泄露敏感数据，现有差分隐私方法因噪声过多损害模型性能。",
                "融合ConvNeXt块与Transformer编码器，采用自适应梯度裁剪和GANI机制保护隐私。",
                "在多个数据集上验证，定位精度提升超40%，并在强磁场环境中展示鲁棒性。"
            ],
            "tags_zh": [
                "惯性导航",
                "差分隐私",
                "混合架构",
                "Transformer",
                "ConvNeXt",
                "传感器扰动"
            ],
            "_index": 61
        },
        {
            "title": "Learning To Defer To A Population With Limited Demonstrations",
            "authors": [
                "Nilesh Ramgolam",
                "Gustavo Carneiro",
                "Hsiang-Ting",
                "Chen"
            ],
            "arxiv_id": "2510.19351v1",
            "summary": "This paper addresses the critical data scarcity that hinders the practical\ndeployment of learning to defer (L2D) systems to the population. We introduce a\ncontext-aware, semi-supervised framework that uses meta-learning to generate\nexpert-specific embeddings from only a few demonstrations. We demonstrate the\nefficacy of a dual-purpose mechanism, where these embeddings are used first to\ngenerate a large corpus of pseudo-labels for training, and subsequently to\nenable on-the-fly adaptation to new experts at test-time. The experiment\nresults on three different datasets confirm that a model trained on these\nsynthetic labels rapidly approaches oracle-level performance, validating the\ndata efficiency of our approach. By resolving a key training bottleneck, this\nwork makes adaptive L2D systems more practical and scalable, paving the way for\nhuman-AI collaboration in real-world environments. To facilitate\nreproducibility and address implementation details not covered in the main\ntext, we provide our source code and training configurations at\nhttps://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations.",
            "headline_zh": "提出基于元学习的上下文感知半监督框架，以解决学习延迟系统中数据稀缺问题。",
            "intro_zh": [
                "核心问题：学习延迟系统在部署中面临数据稀缺，限制其适应人群。",
                "方法要点：使用元学习从少量演示生成专家特定嵌入，并用于伪标签生成和测试时适应。",
                "实验或效果：在三个数据集上验证，模型通过合成标签训练快速接近Oracle性能。"
            ],
            "tags_zh": [
                "学习延迟",
                "元学习",
                "半监督学习",
                "专家嵌入",
                "伪标签生成",
                "自适应系统"
            ],
            "_index": 62
        },
        {
            "title": "DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile Phone Agents",
            "authors": [
                "Kai Shi",
                "Jun Yang",
                "Ni Yang",
                "Binqiang Pan",
                "Qingsong Xie",
                "Chao Zhang",
                "Zhenyu Yang",
                "Tianhuang Su",
                "Haonan Lu"
            ],
            "arxiv_id": "2510.19336v1",
            "summary": "Mobile Phone Agents (MPAs) have emerged as a promising research direction due\nto their broad applicability across diverse scenarios. While Multimodal Large\nLanguage Models (MLLMs) serve as the foundation for MPAs, their effectiveness\nin handling multiple mobile phone tasks simultaneously remains limited.\nAlthough multitask supervised fine-tuning (SFT) is widely adopted for multitask\nlearning, existing approaches struggle to determine optimal training data\ncompositions for peak performance. To address this challenge, we propose DaMo\n(Data Mixture Optimizer) - a novel solution employing a trainable network that\npredicts optimal data mixtures by forecasting downstream task performance for\nany given dataset ratio. To support comprehensive evaluation, we introduce\nPhoneAgentBench, the first specialized benchmark to evaluate MLLMs on\nmultimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse\nreal-world industrial mobile application scenarios. Demonstrating strong\npredictive capability (R^2=0.81) in small-scale pilot experiments, DaMo\nefficiently extrapolates optimal data mixing configurations. Our results show\nDaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to\nalternative methods. Furthermore, extensive experiments across established\nbenchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench\nreveal DaMo's superior generalization, outperforming other approaches by 2.57%\nin terms of average score. When used solely for MLLM optimization on the\nBFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably,\nDaMo maintains robust scalability, preserving its effectiveness when applied to\nother model architectures. The code and dataset are available at\nhttps://github.com/OPPO-Mente-Lab/DaMo.git",
            "headline_zh": "提出DaMo数据混合优化器以提升多模态大模型在移动手机代理中的多任务性能",
            "intro_zh": [
                "核心问题：多模态大模型在移动手机代理中处理多任务时，现有方法难以确定最优训练数据组合。",
                "方法要点：DaMo使用可训练网络预测下游任务性能，以优化数据混合配置。",
                "实验或效果：在PhoneAgentBench上性能提升3.38%，并在多个基准测试中展现优越泛化能力。"
            ],
            "tags_zh": [
                "数据混合优化",
                "多模态大模型",
                "移动手机代理",
                "多任务学习",
                "基准测试"
            ],
            "_index": 63
        },
        {
            "title": "A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP",
            "authors": [
                "Ying Dai",
                "Wei Yu Chen"
            ],
            "arxiv_id": "2510.19333v1",
            "summary": "This paper presents a novel training-free framework for open-vocabulary image\nsegmentation and object recognition (OVSR), which leverages EfficientNetB0, a\nconvolutional neural network, for unsupervised segmentation and CLIP, a\nvision-language model, for open-vocabulary object recognition. The proposed\nframework adopts a two stage pipeline: unsupervised image segmentation followed\nby segment-level recognition via vision-language alignment. In the first stage,\npixel-wise features extracted from EfficientNetB0 are decomposed using singular\nvalue decomposition to obtain latent representations, which are then clustered\nusing hierarchical clustering to segment semantically meaningful regions. The\nnumber of clusters is adaptively determined by the distribution of singular\nvalues. In the second stage, the segmented regions are localized and encoded\ninto image embeddings using the Vision Transformer backbone of CLIP. Text\nembeddings are precomputed using CLIP's text encoder from category-specific\nprompts, including a generic something else prompt to support open set\nrecognition. The image and text embeddings are concatenated and projected into\na shared latent feature space via SVD to enhance cross-modal alignment.\nRecognition is performed by computing the softmax over the similarities between\nthe projected image and text embeddings. The proposed method is evaluated on\nstandard benchmarks, including COCO, ADE20K, and PASCAL VOC, achieving\nstate-of-the-art performance in terms of Hungarian mIoU, precision, recall, and\nF1-score. These results demonstrate the effectiveness, flexibility, and\ngeneralizability of the proposed framework.",
            "headline_zh": "提出无需训练框架，结合EfficientNet和CLIP实现开放词汇图像分割与识别",
            "intro_zh": [
                "核心问题：开放词汇图像分割与识别，无需额外训练数据。",
                "方法要点：两阶段流程，先无监督分割，后跨模态对齐识别。",
                "实验效果：在COCO等基准上，实现SOTA性能，验证泛化能力。"
            ],
            "tags_zh": [
                "开放词汇图像分割",
                "无监督分割",
                "跨模态对齐",
                "EfficientNet",
                "CLIP模型",
                "图像识别"
            ],
            "_index": 64
        },
        {
            "title": "BrainMCLIP: Brain Image Decoding with Multi-Layer feature Fusion of CLIP",
            "authors": [
                "Tian Xia",
                "Zihan Ma",
                "Xinlong Wang",
                "Qing Liu",
                "Xiaowei He",
                "Tianming Liu",
                "Yudan Ren"
            ],
            "arxiv_id": "2510.19332v1",
            "summary": "Decoding images from fMRI often involves mapping brain activity to CLIP's\nfinal semantic layer. To capture finer visual details, many approaches add a\nparameter-intensive VAE-based pipeline. However, these approaches overlook rich\nobject information within CLIP's intermediate layers and contradicts the\nbrain's functionally hierarchical. We introduce BrainMCLIP, which pioneers a\nparameter-efficient, multi-layer fusion approach guided by human visual\nsystem's functional hierarchy, eliminating the need for such a separate VAE\npathway. BrainMCLIP aligns fMRI signals from functionally distinct visual areas\n(low-/high-level) to corresponding intermediate and final CLIP layers,\nrespecting functional hierarchy. We further introduce a Cross-Reconstruction\nstrategy and a novel multi-granularity loss. Results show BrainMCLIP achieves\nhighly competitive performance, particularly excelling on high-level semantic\nmetrics where it matches or surpasses SOTA(state-of-the-art) methods, including\nthose using VAE pipelines. Crucially, it achieves this with substantially fewer\nparameters, demonstrating a reduction of\n71.7\\%(Table.\\ref{tab:compare_clip_vae}) compared to top VAE-based SOTA\nmethods, by avoiding the VAE pathway. By leveraging intermediate CLIP features,\nit effectively captures visual details often missed by CLIP-only approaches,\nstriking a compelling balance between semantic accuracy and detail fidelity\nwithout requiring a separate VAE pipeline.",
            "headline_zh": "提出BrainMCLIP以高效解码fMRI脑图像，融合CLIP多层特征并避免VAE路径。",
            "intro_zh": [
                "核心问题：现有方法忽略CLIP中间层信息，且与大脑功能层次不符，依赖参数密集的VAE。",
                "方法要点：基于视觉系统功能层次，对齐fMRI信号与CLIP多层特征，引入跨重建策略和多粒度损失。",
                "实验或效果：在高级语义指标上媲美或超越SOTA，参数减少71.7%，无需VAE路径。"
            ],
            "tags_zh": [
                "脑图像解码",
                "CLIP特征融合",
                "功能层次对齐",
                "参数高效模型",
                "fMRI信号处理"
            ],
            "_index": 65
        },
        {
            "title": "Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization",
            "authors": [
                "Juncheng Wang",
                "Lei Shang",
                "Ziqi Liu",
                "Wang Lu",
                "Xixu Hu",
                "Zhe Hu",
                "Jindong Wang",
                "Shujun Wang"
            ],
            "arxiv_id": "2510.19330v1",
            "summary": "Crowd localization plays a crucial role in visual scene understanding towards\npredicting each pedestrian location in a crowd, thus being applicable to\nvarious downstream tasks. However, existing approaches suffer from significant\nperformance degradation due to discrepancies in head scale distributions (scale\nshift) between training and testing data, a challenge known as domain\ngeneralization (DG). This paper aims to comprehend the nature of scale shift\nwithin the context of domain generalization for crowd localization models. To\nthis end, we address four critical questions: (i) How does scale shift\ninfluence crowd localization in a DG scenario? (ii) How can we quantify this\ninfluence? (iii) What causes this influence? (iv) How to mitigate the\ninfluence? Initially, we conduct a systematic examination of how crowd\nlocalization performance varies with different levels of scale shift. Then, we\nestablish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms to\nquantify the influence. Through extensive experiments, we demonstrate the\nlimitations of existing algorithms and underscore the importance and complexity\nof scale shift, a topic that remains insufficiently explored. To deepen our\nunderstanding, we provide a rigorous theoretical analysis on scale shift.\nBuilding on these insights, we further propose an effective algorithm called\nCausal Feature Decomposition and Anisotropic Processing (Catto) to mitigate the\ninfluence of scale shift in DG settings. Later, we also provide extensive\nanalytical experiments, revealing four significant insights for future\nresearch. Our results emphasize the importance of this novel and applicable\nresearch direction, which we term Scale Shift Domain Generalization.",
            "headline_zh": "提出Catto算法以缓解人群定位中尺度偏移对领域泛化的影响",
            "intro_zh": [
                "核心问题：训练与测试数据中头部尺度分布差异导致人群定位性能下降",
                "方法要点：通过因果特征分解与各向异性处理来减轻尺度偏移影响",
                "实验或效果：建立ScaleBench基准，验证算法有效性并揭示现有方法局限"
            ],
            "tags_zh": [
                "人群定位",
                "领域泛化",
                "尺度偏移",
                "因果特征分解",
                "基准测试"
            ],
            "_index": 66
        },
        {
            "title": "Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters",
            "authors": [
                "Panagiotis Agrafiotis",
                "Begüm Demir"
            ],
            "arxiv_id": "2510.19329v1",
            "summary": "Accurate, detailed, and regularly updated bathymetry, coupled with complex\nsemantic content, is essential for under-mapped shallow-water environments\nfacing increasing climatological and anthropogenic pressures. However, existing\napproaches that derive either depth or seabed classes from remote sensing\nimagery treat these tasks in isolation, forfeiting the mutual benefits of their\ninteraction and hindering the broader adoption of deep learning methods. To\naddress these limitations, we introduce Seabed-Net, a unified multi-task\nframework that simultaneously predicts bathymetry and pixel-based seabed\nclassification from remote sensing imagery of various resolutions. Seabed-Net\nemploys dual-branch encoders for bathymetry estimation and pixel-based seabed\nclassification, integrates cross-task features via an Attention Feature Fusion\nmodule and a windowed Swin-Transformer fusion block, and balances objectives\nthrough dynamic task uncertainty weighting. In extensive evaluations at two\nheterogeneous coastal sites, it consistently outperforms traditional empirical\nmodels and traditional machine learning regression methods, achieving up to\n75\\% lower RMSE. It also reduces bathymetric RMSE by 10-30\\% compared to\nstate-of-the-art single-task and multi-task baselines and improves seabed\nclassification accuracy up to 8\\%. Qualitative analyses further demonstrate\nenhanced spatial consistency, sharper habitat boundaries, and corrected depth\nbiases in low-contrast regions. These results confirm that jointly modeling\ndepth with both substrate and seabed habitats yields synergistic gains,\noffering a robust, open solution for integrated shallow-water mapping. Code and\npretrained weights are available at https://github.com/pagraf/Seabed-Net.",
            "headline_zh": "提出Seabed-Net多任务网络，联合估计浅水区水深和海底分类。",
            "intro_zh": [
                "现有方法孤立处理水深估计和海底分类，无法利用任务间交互优势。",
                "采用双分支编码器、注意力特征融合和动态任务权重，实现多任务学习。",
                "在异质海岸评估中，显著降低RMSE并提高分类精度，增强空间一致性。"
            ],
            "tags_zh": [
                "多任务学习",
                "水深估计",
                "海底分类",
                "遥感图像",
                "浅水区映射",
                "注意力融合"
            ],
            "_index": 67
        },
        {
            "title": "Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer",
            "authors": [
                "Hai-jie Yuan",
                "Heng Zhang",
                "Fei Yin"
            ],
            "arxiv_id": "2510.19321v1",
            "summary": "Handwritten signature verification is a crucial aspect of identity\nauthentication, with applications in various domains such as finance and\ne-commerce. However, achieving high accuracy in signature verification remains\nchallenging due to intra-user variability and the risk of forgery. This paper\nintroduces a novel approach for dynamic signature verification: the\nTemporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the\nGraph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both\nspatial and temporal dependencies in signature data. TS-GATR enhances\nverification performance by representing signatures as graphs, where each node\ncaptures dynamic features (e.g. position, velocity, pressure), and by using\nattention mechanisms to model their complex relationships. The proposed method\nfurther employs a Dual-Graph Attention Transformer (DGATR) module, which\nutilizes k-step and k-nearest neighbor adjacency graphs to model local and\nglobal spatial features, respectively. To capture long-term temporal\ndependencies, the model integrates GRU, thereby enhancing its ability to learn\ndynamic features during signature verification. Comprehensive experiments\nconducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR\nsurpasses current state-of-the-art approaches, consistently achieving lower\nEqual Error Rates (EER) across various scenarios.",
            "headline_zh": "提出时空图注意力Transformer以提升在线手写签名验证准确性",
            "intro_zh": [
                "核心问题：手写签名验证因用户内变异性与伪造风险而准确率低",
                "方法要点：结合图注意力网络与门控循环单元建模时空依赖关系",
                "实验或效果：在MSDS和DeepSignDB数据集上EER低于现有方法"
            ],
            "tags_zh": [
                "手写签名验证",
                "图注意力网络",
                "门控循环单元",
                "时空建模",
                "动态特征",
                "身份认证"
            ],
            "_index": 68
        },
        {
            "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
            "authors": [
                "Byung-Kwan Lee",
                "Ryo Hachiuma",
                "Yong Man Ro",
                "Yu-Chiang Frank Wang",
                "Yueh-Hua Wu"
            ],
            "arxiv_id": "2510.19307v1",
            "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.",
            "headline_zh": "提出统一强化与模仿学习算法，以高效训练轻量级视觉语言模型。",
            "intro_zh": [
                "视觉语言模型规模大，在资源受限环境中不实用。",
                "结合强化学习和对抗模仿学习，提升学生模型生成能力。",
                "实验显示，RIL在多个基准上缩小与先进模型的性能差距。"
            ],
            "tags_zh": [
                "视觉语言模型",
                "强化学习",
                "模仿学习",
                "模型蒸馏",
                "轻量级模型",
                "对抗训练"
            ],
            "_index": 69
        },
        {
            "title": "FrogDeepSDM: Improving Frog Counting and Occurrence Prediction Using Multimodal Data and Pseudo-Absence Imputation",
            "authors": [
                "Chirag Padubidri",
                "Pranesh Velmurugan",
                "Andreas Lanitis",
                "Andreas Kamilaris"
            ],
            "arxiv_id": "2510.19305v1",
            "summary": "Monitoring species distribution is vital for conservation efforts, enabling\nthe assessment of environmental impacts and the development of effective\npreservation strategies. Traditional data collection methods, including citizen\nscience, offer valuable insights but remain limited in coverage and\ncompleteness. Species Distribution Modelling (SDM) helps address these gaps by\nusing occurrence data and environmental variables to predict species presence\nacross large regions. In this study, we enhance SDM accuracy for frogs (Anura)\nby applying deep learning and data imputation techniques using data from the\n\"EY - 2022 Biodiversity Challenge.\" Our experiments show that data balancing\nsignificantly improved model performance, reducing the Mean Absolute Error\n(MAE) from 189 to 29 in frog counting tasks. Feature selection identified key\nenvironmental factors influencing occurrence, optimizing inputs while\nmaintaining predictive accuracy. The multimodal ensemble model, integrating\nland cover, NDVI, and other environmental inputs, outperformed individual\nmodels and showed robust generalization across unseen regions. The fusion of\nimage and tabular data improved both frog counting and habitat classification,\nachieving 84.9% accuracy with an AUC of 0.90. This study highlights the\npotential of multimodal learning and data preprocessing techniques such as\nbalancing and imputation to improve predictive ecological modeling when data\nare sparse or incomplete, contributing to more precise and scalable\nbiodiversity monitoring.",
            "headline_zh": "提出FrogDeepSDM方法，通过多模态数据和伪缺失插补改进青蛙计数与分布预测",
            "intro_zh": [
                "核心问题：物种分布数据稀疏或不完整，限制生态监测准确性",
                "方法要点：应用深度学习和数据平衡技术，融合图像与表格数据",
                "实验或效果：模型显著降低计数误差，准确率达84.9%，AUC为0.90"
            ],
            "tags_zh": [
                "物种分布建模",
                "深度学习",
                "多模态学习",
                "数据插补",
                "生态监测",
                "青蛙计数"
            ],
            "_index": 70
        },
        {
            "title": "Vision-Based Mistake Analysis in Procedural Activities: A Review of Advances and Challenges",
            "authors": [
                "Konstantinos Bacharidis",
                "Antonis A. Argyros"
            ],
            "arxiv_id": "2510.19292v1",
            "summary": "Mistake analysis in procedural activities is a critical area of research with\napplications spanning industrial automation, physical rehabilitation, education\nand human-robot collaboration. This paper reviews vision-based methods for\ndetecting and predicting mistakes in structured tasks, focusing on procedural\nand executional errors. By leveraging advancements in computer vision,\nincluding action recognition, anticipation and activity understanding,\nvision-based systems can identify deviations in task execution, such as\nincorrect sequencing, use of improper techniques, or timing errors. We explore\nthe challenges posed by intra-class variability, viewpoint differences and\ncompositional activity structures, which complicate mistake detection.\nAdditionally, we provide a comprehensive overview of existing datasets,\nevaluation metrics and state-of-the-art methods, categorizing approaches based\non their use of procedural structure, supervision levels and learning\nstrategies. Open challenges, such as distinguishing permissible variations from\ntrue mistakes and modeling error propagation are discussed alongside future\ndirections, including neuro-symbolic reasoning and counterfactual state\nmodeling. This work aims to establish a unified perspective on vision-based\nmistake analysis in procedural activities, highlighting its potential to\nenhance safety, efficiency and task performance across diverse domains.",
            "headline_zh": "综述基于视觉的程序活动错误分析方法，应用于工业自动化和人机协作等领域",
            "intro_zh": [
                "核心问题：程序活动中错误检测受类内变异和视角差异等挑战影响",
                "方法要点：利用动作识别和活动理解技术识别执行偏差",
                "实验或效果：未知，本文为综述未报告具体实验效果"
            ],
            "tags_zh": [
                "错误检测",
                "程序活动理解",
                "动作识别",
                "计算机视觉",
                "综述方法"
            ],
            "_index": 71
        },
        {
            "title": "TARMAC: A Taxonomy for Robot Manipulation in Chemistry",
            "authors": [
                "Kefeng Huang",
                "Jonathon Pipe",
                "Alice E. Martin",
                "Tianyuan Wang",
                "Barnabas A. Franklin",
                "Andy M. Tyrrell",
                "Ian J. S. Fairlamb",
                "Jihong Zhu"
            ],
            "arxiv_id": "2510.19289v1",
            "summary": "Chemistry laboratory automation aims to increase throughput, reproducibility,\nand safety, yet many existing systems still depend on frequent human\nintervention. Advances in robotics have reduced this dependency, but without a\nstructured representation of the required skills, autonomy remains limited to\nbespoke, task-specific solutions with little capacity to transfer beyond their\ninitial design. Current experiment abstractions typically describe\nprotocol-level steps without specifying the robotic actions needed to execute\nthem. This highlights the lack of a systematic account of the manipulation\nskills required for robots in chemistry laboratories. To address this gap, we\nintroduce TARMAC - a Taxonomy for Robot Manipulation in Chemistry - a\ndomain-specific framework that defines and organizes the core manipulations\nneeded in laboratory practice. Based on annotated teaching-lab demonstrations\nand supported by experimental validation, TARMAC categorizes actions according\nto their functional role and physical execution requirements. Beyond serving as\na descriptive vocabulary, TARMAC can be instantiated as robot-executable\nprimitives and composed into higher-level macros, enabling skill reuse and\nsupporting scalable integration into long-horizon workflows. These\ncontributions provide a structured foundation for more flexible and autonomous\nlaboratory automation. More information is available at\nhttps://tarmac-paper.github.io/",
            "headline_zh": "提出TARMAC分类法以解决化学实验室机器人操作技能缺乏结构化表示的问题",
            "intro_zh": [
                "核心问题：化学实验室自动化依赖人工干预，缺乏机器人操作技能的系统化描述。",
                "方法要点：基于教学演示定义功能角色和物理执行要求，构建领域特定分类法。",
                "实验或效果：通过实验验证，支持技能重用和长时程工作流集成。"
            ],
            "tags_zh": [
                "机器人操作分类法",
                "化学实验室自动化",
                "技能重用",
                "长时程工作流",
                "领域特定框架"
            ],
            "_index": 72
        },
        {
            "title": "Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning",
            "authors": [
                "Safa Ben Atitallah",
                "Maha Driss",
                "Wadii Boulila",
                "Anis Koubaa"
            ],
            "arxiv_id": "2510.19282v1",
            "summary": "Alzheimer disease is a severe brain disorder that causes harm in various\nbrain areas and leads to memory damage. The limited availability of labeled\nmedical data poses a significant challenge for accurate Alzheimer disease\ndetection. There is a critical need for effective methods to improve the\naccuracy of Alzheimer disease detection, considering the scarcity of labeled\ndata, the complexity of the disease, and the constraints related to data\nprivacy. To address this challenge, our study leverages the power of big data\nin the form of pre-trained Convolutional Neural Networks (CNNs) within the\nframework of Few-Shot Learning (FSL) and ensemble learning. We propose an\nensemble approach based on a Prototypical Network (ProtoNet), a powerful method\nin FSL, integrating various pre-trained CNNs as encoders. This integration\nenhances the richness of features extracted from medical images. Our approach\nalso includes a combination of class-aware loss and entropy loss to ensure a\nmore precise classification of Alzheimer disease progression levels. The\neffectiveness of our method was evaluated using two datasets, the Kaggle\nAlzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and\n99.86%, respectively. The comparison of our results with relevant\nstate-of-the-art studies demonstrated that our approach achieved superior\naccuracy and highlighted its validity and potential for real-world applications\nin early Alzheimer disease detection.",
            "headline_zh": "提出基于原型网络和集成学习的少样本学习方法以增强早期阿尔茨海默病检测",
            "intro_zh": [
                "核心问题：标记医学数据稀缺，影响阿尔茨海默病检测准确性。",
                "方法要点：集成预训练CNN作为编码器，结合类感知损失和熵损失。",
                "实验或效果：在Kaggle和ADNI数据集上准确率达99.72%和99.86%。"
            ],
            "tags_zh": [
                "阿尔茨海默病检测",
                "少样本学习",
                "集成学习",
                "原型网络",
                "卷积神经网络",
                "医学图像分析"
            ],
            "_index": 73
        },
        {
            "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation",
            "authors": [
                "Nobline Yoo",
                "Olga Russakovsky",
                "Ye Zhu"
            ],
            "arxiv_id": "2510.19278v1",
            "summary": "Text-to-image (T2I) diffusion models have achieved strong performance in\nsemantic alignment, yet they still struggle with generating the correct number\nof objects specified in prompts. Existing approaches typically incorporate\nauxiliary counting networks as external critics to enhance numeracy. However,\nsince these critics must provide gradient guidance during generation, they are\nrestricted to regression-based models that are inherently differentiable, thus\nexcluding detector-based models with superior counting ability, whose\ncount-via-enumeration nature is non-differentiable. To overcome this\nlimitation, we propose Detector-to-Differentiable (D2D), a novel framework that\ntransforms non-differentiable detection models into differentiable critics,\nthereby leveraging their superior counting ability to guide numeracy\ngeneration. Specifically, we design custom activation functions to convert\ndetector logits into soft binary indicators, which are then used to optimize\nthe noise prior at inference time with pre-trained T2I models. Our extensive\nexperiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of\nvarying complexity (low-density, high-density, and multi-object scenarios)\ndemonstrate consistent and substantial improvements in object counting accuracy\n(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),\nwith minimal degradation in overall image quality and computational overhead.",
            "headline_zh": "提出D2D框架将非可微检测器转化为可微批评器，以提升文本到图像生成中的对象计数准确性",
            "intro_zh": [
                "核心问题：文本到图像扩散模型在生成指定对象数量时存在困难，现有方法受限于可微性而无法使用高性能检测器",
                "方法要点：设计自定义激活函数将检测器输出转换为软二进制指示器，用于在推理时优化噪声先验",
                "实验或效果：在多个基准测试中显著提升计数准确率，最高达13.7%，且图像质量和计算开销影响小"
            ],
            "tags_zh": [
                "文本到图像生成",
                "对象计数",
                "可微批评器",
                "检测器转换",
                "扩散模型优化"
            ],
            "_index": 74
        },
        {
            "title": "MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation",
            "authors": [
                "Zhang Nengbo",
                "Ho Hann Woei"
            ],
            "arxiv_id": "2510.19273v1",
            "summary": "Accurate and efficient recognition of Micro Air Vehicle (MAV) motion is\nessential for enabling real-time perception and coordination in autonomous\naerial swarm. However, most existing approaches rely on large, computationally\nintensive models that are unsuitable for resource-limited MAV platforms, which\nresults in a trade-off between recognition accuracy and inference speed. To\naddress these challenges, this paper proposes a lightweight MAV action\nrecognition framework, MobiAct, designed to achieve high accuracy with low\ncomputational cost. Specifically, MobiAct adopts MobileNetV4 as the backbone\nnetwork and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD)\nstrategy to effectively transfer MAV motion features from a teacher network\n(ResNet18) to a student network, thereby enhancing knowledge transfer\nefficiency. Furthermore, a parameter-free attention mechanism is integrated\ninto the architecture to improve recognition accuracy without increasing model\ncomplexity. In addition, a hybrid loss training strategy is developed to\ncombine multiple loss objectives, which ensures stable and robust optimization\nduring training. Experimental results demonstrate that the proposed MobiAct\nachieves low-energy and low-computation MAV action recognition, while\nmaintaining the fastest action decoding speed among compared methods. Across\nall three self-collected datasets, MobiAct achieves an average recognition\naccuracy of 92.12%, while consuming only 136.16 pJ of energy and processing\nrecognition at a rate of 8.84 actions per second. Notably, MobiAct decodes\nactions up to 2 times faster than the leading method, with highly comparable\nrecognition accuracy, highlighting its superior efficiency in MAV action\nrecognition.",
            "headline_zh": "提出MobiAct框架，使用MobileNetV4与知识蒸馏，实现高效MAV动作识别。",
            "intro_zh": [
                "核心问题：现有MAV动作识别模型计算量大，不适合资源受限平台。",
                "方法要点：采用MobileNetV4骨干，结合阶段正交知识蒸馏和无参数注意力机制。",
                "实验效果：在自收集数据集上平均准确率92.12%，能耗低且解码速度快。"
            ],
            "tags_zh": [
                "MAV动作识别",
                "知识蒸馏",
                "MobileNetV4",
                "轻量级模型",
                "对比学习",
                "注意力机制"
            ],
            "_index": 75
        },
        {
            "title": "SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution",
            "authors": [
                "Yun Kai Zhuang"
            ],
            "arxiv_id": "2510.19272v1",
            "summary": "Real-world image super-resolution (Real-ISR) must handle complex degradations\nand inherent reconstruction ambiguities. While generative models have improved\nperceptual quality, a key trade-off remains with computational cost. One-step\ndiffusion models offer speed but often produce structural inaccuracies due to\ndistillation artifacts. To address this, we propose a novel SR framework that\nenhances a one-step diffusion model using a ControlNet mechanism for semantic\nedge guidance. This integrates edge information to provide dynamic structural\ncontrol during single-pass inference. We also introduce a hybrid loss combining\nL2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy,\nperceptual quality, and geometric precision. Experiments show our method\neffectively improves structural integrity and realism while maintaining the\nefficiency of one-step generation, achieving a superior balance between output\nquality and inference speed. The results of test datasets will be published at\nhttps://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_link\nand the related code will be published at\nhttps://github.com/ARBEZ-ZEBRA/SCEESR.",
            "headline_zh": "提出语义控制边缘增强框架以提升一步扩散超分辨率的几何精度与效率",
            "intro_zh": [
                "真实图像超分辨率需处理复杂退化与重建模糊，生成模型在感知质量与计算成本间存在权衡",
                "使用ControlNet机制集成边缘信息，在单步推理中提供动态结构控制，结合混合损失优化",
                "实验表明方法有效提升结构完整性和真实感，保持一步生成效率，平衡输出质量与速度"
            ],
            "tags_zh": [
                "图像超分辨率",
                "一步扩散模型",
                "ControlNet",
                "边缘增强",
                "混合损失函数",
                "结构控制"
            ],
            "_index": 76
        },
        {
            "title": "Hierarchical DLO Routing with Reinforcement Learning and In-Context Vision-language Models",
            "authors": [
                "Mingen Li",
                "Houjian Yu",
                "Yixuan Huang",
                "Youngjin Hong",
                "Changhyun Choi"
            ],
            "arxiv_id": "2510.19268v1",
            "summary": "Long-horizon routing tasks of deformable linear objects (DLOs), such as\ncables and ropes, are common in industrial assembly lines and everyday life.\nThese tasks are particularly challenging because they require robots to\nmanipulate DLO with long-horizon planning and reliable skill execution.\nSuccessfully completing such tasks demands adapting to their nonlinear\ndynamics, decomposing abstract routing goals, and generating multi-step plans\ncomposed of multiple skills, all of which require accurate high-level reasoning\nduring execution. In this paper, we propose a fully autonomous hierarchical\nframework for solving challenging DLO routing tasks. Given an implicit or\nexplicit routing goal expressed in language, our framework leverages\nvision-language models~(VLMs) for in-context high-level reasoning to synthesize\nfeasible plans, which are then executed by low-level skills trained via\nreinforcement learning. To improve robustness in long horizons, we further\nintroduce a failure recovery mechanism that reorients the DLO into\ninsertion-feasible states. Our approach generalizes to diverse scenes involving\nobject attributes, spatial descriptions, as well as implicit language commands.\nIt outperforms the next best baseline method by nearly 50% and achieves an\noverall success rate of 92.5% across long-horizon routing scenarios.",
            "headline_zh": "提出分层框架结合强化学习和视觉语言模型，以解决可变形线性物体的长时程路由任务。",
            "intro_zh": [
                "核心问题：可变形线性物体在工业装配中需长时程规划和可靠技能执行，适应非线性动态。",
                "方法要点：利用视觉语言模型进行上下文推理生成计划，强化学习训练低层技能执行。",
                "实验或效果：在长时程路由场景中成功率92.5%，优于基线方法近50%。"
            ],
            "tags_zh": [
                "可变形线性物体路由",
                "分层框架",
                "强化学习",
                "视觉语言模型",
                "长时程规划",
                "失败恢复机制"
            ],
            "_index": 77
        },
        {
            "title": "Advances in 4D Representation: Geometry, Motion, and Interaction",
            "authors": [
                "Mingrui Zhao",
                "Sauradip Nag",
                "Kai Wang",
                "Aditya Vora",
                "Guangda Ji",
                "Peter Chun",
                "Ali Mahdavi-Amiri",
                "Hao Zhang"
            ],
            "arxiv_id": "2510.19255v1",
            "summary": "We present a survey on 4D generation and reconstruction, a fast-evolving\nsubfield of computer graphics whose developments have been propelled by recent\nadvances in neural fields, geometric and motion deep learning, as well 3D\ngenerative artificial intelligence (GenAI). While our survey is not the first\nof its kind, we build our coverage of the domain from a unique and distinctive\nperspective of 4D representations\\/}, to model 3D geometry evolving over time\nwhile exhibiting motion and interaction. Specifically, instead of offering an\nexhaustive enumeration of many works, we take a more selective approach by\nfocusing on representative works to highlight both the desirable properties and\nensuing challenges of each representation under different computation,\napplication, and data scenarios. The main take-away message we aim to convey to\nthe readers is on how to select and then customize the appropriate 4D\nrepresentations for their tasks. Organizationally, we separate the 4D\nrepresentations based on three key pillars: geometry, motion, and interaction.\nOur discourse will not only encompass the most popular representations of\ntoday, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),\nbut also bring attention to relatively under-explored representations in the 4D\ncontext, such as structured models and long-range motions. Throughout our\nsurvey, we will reprise the role of large language models (LLMs) and video\nfoundational models (VFMs) in a variety of 4D applications, while steering our\ndiscussion towards their current limitations and how they can be addressed. We\nalso provide a dedicated coverage on what 4D datasets are currently available,\nas well as what is lacking, in driving the subfield forward. Project\npage:https://mingrui-zhao.github.io/4DRep-GMI/",
            "headline_zh": "综述4D表示方法，指导选择与定制以处理几何、运动和交互问题",
            "intro_zh": [
                "核心问题：如何表示3D几何随时间演化，涉及运动与交互，以推动4D生成与重建。",
                "方法要点：基于几何、运动和交互三大支柱，分析代表性方法如NeRF和3DGS。",
                "实验或效果：讨论LLM和VFM在4D应用中的角色，并指出当前局限与改进方向。"
            ],
            "tags_zh": [
                "4D表示",
                "神经场",
                "几何建模",
                "运动分析",
                "交互建模",
                "生成式AI"
            ],
            "_index": 78
        },
        {
            "title": "Background Fades, Foreground Leads: Curriculum-Guided Background Pruning for Efficient Foreground-Centric Collaborative Perception",
            "authors": [
                "Yuheng Wu",
                "Xiangbo Gao",
                "Quang Tau",
                "Zhengzhong Tu",
                "Dongman Lee"
            ],
            "arxiv_id": "2510.19250v1",
            "summary": "Collaborative perception enhances the reliability and spatial coverage of\nautonomous vehicles by sharing complementary information across vehicles,\noffering a promising solution to long-tail scenarios that challenge\nsingle-vehicle perception. However, the bandwidth constraints of vehicular\nnetworks make transmitting the entire feature map impractical. Recent methods,\ntherefore, adopt a foreground-centric paradigm, transmitting only predicted\nforeground-region features while discarding the background, which encodes\nessential context. We propose FadeLead, a foreground-centric framework that\novercomes this limitation by learning to encapsulate background context into\ncompact foreground features during training. At the core of our design is a\ncurricular learning strategy that leverages background cues early on but\nprogressively prunes them away, forcing the model to internalize context into\nforeground representations without transmitting background itself. Extensive\nexperiments on both simulated and real-world benchmarks show that FadeLead\noutperforms prior methods under different bandwidth settings, underscoring the\neffectiveness of context-enriched foreground sharing.",
            "headline_zh": "提出FadeLead框架以解决协作感知中带宽限制下背景上下文丢失问题",
            "intro_zh": [
                "核心问题：车辆网络带宽限制下，仅传输前景特征导致背景上下文丢失，影响感知可靠性。",
                "方法要点：采用课程学习策略，逐步修剪背景，将背景上下文封装到紧凑前景特征中。",
                "实验或效果：在模拟和真实基准测试中，优于现有方法，适应不同带宽设置。"
            ],
            "tags_zh": [
                "协作感知",
                "前景中心化",
                "课程学习",
                "背景修剪",
                "带宽优化",
                "上下文封装"
            ],
            "_index": 79
        },
        {
            "title": "Space Object Detection using Multi-frame Temporal Trajectory Completion Method",
            "authors": [
                "Xiaoqing Lan",
                "Biqiao Xin",
                "Bingshu Wang",
                "Han Zhang",
                "Laixian Zhang"
            ],
            "arxiv_id": "2510.19220v1",
            "summary": "Space objects in Geostationary Earth Orbit (GEO) present significant\ndetection challenges in optical imaging due to weak signals, complex stellar\nbackgrounds, and environmental interference. In this paper, we enhance\nhigh-frequency features of GEO targets while suppressing background noise at\nthe single-frame level through wavelet transform. Building on this, we propose\na multi-frame temporal trajectory completion scheme centered on the Hungarian\nalgorithm for globally optimal cross-frame matching. To effectively mitigate\nmissing and false detections, a series of key steps including temporal matching\nand interpolation completion, temporal-consistency-based noise filtering, and\nprogressive trajectory refinement are designed in the post-processing pipeline.\nExperimental results on the public SpotGEO dataset demonstrate the\neffectiveness of the proposed method, achieving an F_1 score of 90.14%.",
            "headline_zh": "提出多帧时序轨迹补全方法以解决GEO空间目标检测中的弱信号和背景干扰问题",
            "intro_zh": [
                "核心问题：GEO空间目标在光学成像中信号弱、背景复杂，检测困难。",
                "方法要点：使用小波变换增强目标特征，匈牙利算法进行跨帧匹配，后处理优化轨迹。",
                "实验或效果：在SpotGEO数据集上F1分数达90.14%，验证方法有效性。"
            ],
            "tags_zh": [
                "空间目标检测",
                "多帧时序分析",
                "轨迹补全",
                "小波变换",
                "匈牙利算法",
                "后处理优化"
            ],
            "_index": 80
        },
        {
            "title": "SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion",
            "authors": [
                "Xiaozhi Li",
                "Huijun Di",
                "Jian Li",
                "Feng Liu",
                "Wei Liang"
            ],
            "arxiv_id": "2510.19215v1",
            "summary": "3D object detection is essential for autonomous driving. As an emerging\nsensor, 4D imaging radar offers advantages as low cost, long-range detection,\nand accurate velocity measurement, making it highly suitable for object\ndetection. However, its sparse point clouds and low resolution limit object\ngeometric representation and hinder multi-modal fusion. In this study, we\nintroduce SFGFusion, a novel camera-4D imaging radar detection network guided\nby surface fitting. By estimating quadratic surface parameters of objects from\nimage and radar data, the explicit surface fitting model enhances spatial\nrepresentation and cross-modal interaction, enabling more reliable prediction\nof fine-grained dense depth. The predicted depth serves two purposes: 1) in an\nimage branch to guide the transformation of image features from perspective\nview (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improving\nspatial mapping accuracy; and 2) in a surface pseudo-point branch to generate\ndense pseudo-point cloud, mitigating the radar point sparsity. The original\nradar point cloud is also encoded in a separate radar branch. These two point\ncloud branches adopt a pillar-based method and subsequently transform the\nfeatures into the BEV space. Finally, a standard 2D backbone and detection head\nare used to predict object labels and bounding boxes from BEV features.\nExperimental results show that SFGFusion effectively fuses camera and 4D radar\nfeatures, achieving superior performance on the TJ4DRadSet and view-of-delft\n(VoD) object detection benchmarks.",
            "headline_zh": "提出SFGFusion通过表面拟合引导相机与4D雷达融合，提升自动驾驶3D物体检测性能。",
            "intro_zh": [
                "核心问题：4D雷达点云稀疏且分辨率低，限制物体几何表示和多模态融合。",
                "方法要点：使用表面拟合模型估计物体参数，生成密集伪点云和统一BEV特征。",
                "实验效果：在TJ4DRadSet和VoD基准上实现优越检测性能。"
            ],
            "tags_zh": [
                "3D物体检测",
                "多模态融合",
                "表面拟合",
                "4D雷达",
                "鸟瞰图",
                "自动驾驶"
            ],
            "_index": 81
        },
        {
            "title": "MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting",
            "authors": [
                "In-Hwan Jin",
                "Hyeongju Mun",
                "Joonsoo Kim",
                "Kugjin Yun",
                "Kyeongbo Kong"
            ],
            "arxiv_id": "2510.19210v1",
            "summary": "Recent advances in dynamic scene reconstruction have significantly benefited\nfrom 3D Gaussian Splatting, yet existing methods show inconsistent performance\nacross diverse scenes, indicating no single approach effectively handles all\ndynamic challenges. To overcome these limitations, we propose Mixture of\nExperts for Dynamic Gaussian Splatting (MoE-GS), a unified framework\nintegrating multiple specialized experts via a novel Volume-aware Pixel Router.\nOur router adaptively blends expert outputs by projecting volumetric\nGaussian-level weights into pixel space through differentiable weight\nsplatting, ensuring spatially and temporally coherent results. Although MoE-GS\nimproves rendering quality, the increased model capacity and reduced FPS are\ninherent to the MoE architecture. To mitigate this, we explore two\ncomplementary directions: (1) single-pass multi-expert rendering and gate-aware\nGaussian pruning, which improve efficiency within the MoE framework, and (2) a\ndistillation strategy that transfers MoE performance to individual experts,\nenabling lightweight deployment without architectural changes. To the best of\nour knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts\ntechniques into dynamic Gaussian splatting. Extensive experiments on the N3V\nand Technicolor datasets demonstrate that MoE-GS consistently outperforms\nstate-of-the-art methods with improved efficiency. Video demonstrations are\navailable at https://anonymous.4open.science/w/MoE-GS-68BA/.",
            "headline_zh": "提出MoE-GS框架以解决动态场景重建中性能不一致问题",
            "intro_zh": [
                "动态场景重建中现有方法性能不一致，缺乏统一处理动态挑战的方案",
                "集成多个专家模型，通过体积感知像素路由器自适应混合输出",
                "实验显示在N3V和Technicolor数据集上优于现有方法，效率提升"
            ],
            "tags_zh": [
                "动态场景重建",
                "高斯泼溅",
                "专家混合",
                "体积感知路由",
                "模型蒸馏",
                "渲染效率"
            ],
            "_index": 82
        },
        {
            "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis",
            "authors": [
                "Matteo Bortolon",
                "Nuno Ferreira Duarte",
                "Plinio Moreno",
                "Fabio Poiesi",
                "José Santos-Victor",
                "Alessio Del Bue"
            ],
            "arxiv_id": "2510.19200v1",
            "summary": "Achieving dexterous robotic grasping with multi-fingered hands remains a\nsignificant challenge. While existing methods rely on complete 3D scans to\npredict grasp poses, these approaches face limitations due to the difficulty of\nacquiring high-quality 3D data in real-world scenarios. In this paper, we\nintroduce GRASPLAT, a novel grasping framework that leverages consistent 3D\ninformation while being trained solely on RGB images. Our key insight is that\nby synthesizing physically plausible images of a hand grasping an object, we\ncan regress the corresponding hand joints for a successful grasp. To achieve\nthis, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of\nreal hand-object interactions, enabling end-to-end training with RGB data.\nUnlike prior methods, our approach incorporates a photometric loss that refines\ngrasp predictions by minimizing discrepancies between rendered and real images.\nWe conduct extensive experiments on both synthetic and real-world grasping\ndatasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9%\nover existing image-based methods. Project page:\nhttps://mbortolon97.github.io/grasplat/",
            "headline_zh": "提出GRASPLAT框架，通过新视角合成实现灵巧抓取",
            "intro_zh": [
                "核心问题：多指手灵巧抓取依赖完整3D扫描，但真实场景中获取高质量3D数据困难。",
                "方法要点：利用3D高斯泼溅合成手-物体交互图像，通过光度损失优化抓取预测。",
                "实验或效果：在合成和真实数据集上，抓取成功率比现有图像方法提升高达36.9%。"
            ],
            "tags_zh": [
                "灵巧抓取",
                "新视角合成",
                "3D高斯泼溅",
                "光度损失",
                "机器人抓取",
                "RGB图像训练"
            ],
            "_index": 83
        },
        {
            "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks",
            "authors": [
                "Kai Zeng",
                "Zhanqian Wu",
                "Kaixin Xiong",
                "Xiaobao Wei",
                "Xiangyu Guo",
                "Zhenxin Zhu",
                "Kalok Ho",
                "Lijun Zhou",
                "Bohan Zeng",
                "Ming Lu",
                "Haiyang Sun",
                "Bing Wang",
                "Guang Chen",
                "Hangjun Ye",
                "Wentao Zhang"
            ],
            "arxiv_id": "2510.19195v1",
            "summary": "Recent advancements in driving world models enable controllable generation of\nhigh-quality RGB videos or multimodal videos. Existing methods primarily focus\non metrics related to generation quality and controllability. However, they\noften overlook the evaluation of downstream perception tasks, which are\n$\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing\nmethods usually leverage a training strategy that first pretrains on synthetic\ndata and finetunes on real data, resulting in twice the epochs compared to the\nbaseline (real data only). When we double the epochs in the baseline, the\nbenefit of synthetic data becomes negligible. To thoroughly demonstrate the\nbenefit of synthetic data, we introduce Dream4Drive, a novel synthetic data\ngeneration framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance\nmaps and subsequently renders the 3D assets onto these guidance maps. Finally,\nthe driving world model is fine-tuned to produce the edited, multi-view\nphotorealistic videos, which can be used to train the downstream perception\nmodels. Dream4Drive enables unprecedented flexibility in generating multi-view\ncorner cases at scale, significantly boosting corner case perception in\nautonomous driving. To facilitate future research, we also contribute a\nlarge-scale 3D asset dataset named DriveObj3D, covering the typical categories\nin driving scenarios and enabling diverse 3D-aware video editing. We conduct\ncomprehensive experiments to show that Dream4Drive can effectively boost the\nperformance of downstream perception models under various training epochs.\nProject: $\\href{https://wm-research.github.io/Dream4Drive/}{this\\ https\\ URL}$",
            "headline_zh": "提出Dream4Drive框架作为合成数据生成器，以增强自动驾驶感知任务性能",
            "intro_zh": [
                "现有驾驶世界模型忽视下游感知任务评估，导致合成数据益处不明显",
                "Dream4Drive分解视频为3D感知引导图，渲染3D资产并微调模型生成多视角视频",
                "实验显示Dream4Drive在各种训练轮次下有效提升下游感知模型性能"
            ],
            "tags_zh": [
                "驾驶世界模型",
                "合成数据生成",
                "3D感知引导",
                "多视角视频编辑",
                "自动驾驶感知",
                "角落案例增强"
            ],
            "_index": 84
        },
        {
            "title": "Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning",
            "authors": [
                "Takehiro Aoshima",
                "Yusuke Shinohara",
                "Park Byeongseon"
            ],
            "arxiv_id": "2510.19193v1",
            "summary": "Reward-based fine-tuning of video diffusion models is an effective approach\nto improve the quality of generated videos, as it can fine-tune models without\nrequiring real-world video datasets. However, it can sometimes be limited to\nspecific performances because conventional reward functions are mainly aimed at\nenhancing the quality across the whole generated video sequence, such as\naesthetic appeal and overall consistency. Notably, the temporal consistency of\nthe generated video often suffers when applying previous approaches to\nimage-to-video (I2V) generation tasks. To address this limitation, we propose\nVideo Consistency Distance (VCD), a novel metric designed to enhance temporal\nconsistency, and fine-tune a model with the reward-based fine-tuning framework.\nTo achieve coherent temporal consistency relative to a conditioning image, VCD\nis defined in the frequency space of video frame features to capture frame\ninformation effectively through frequency-domain analysis. Experimental results\nacross multiple I2V datasets demonstrate that fine-tuning a video generation\nmodel with VCD significantly enhances temporal consistency without degrading\nother performance compared to the previous method.",
            "headline_zh": "提出视频一致性距离以增强图像到视频生成中的时序一致性",
            "intro_zh": [
                "核心问题：基于奖励的微调在图像到视频生成中常导致时序一致性不足",
                "方法要点：定义视频一致性距离，在频域分析视频帧特征以提升时序一致性",
                "实验或效果：多数据集实验显示，微调后时序一致性显著增强，其他性能未下降"
            ],
            "tags_zh": [
                "图像到视频生成",
                "奖励微调",
                "时序一致性",
                "频域分析",
                "视频扩散模型"
            ],
            "_index": 85
        },
        {
            "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning",
            "authors": [
                "Fengyuan Sun",
                "Hui Chen",
                "Xinhao Xu",
                "Dandan Zheng",
                "Jingdong Chen",
                "Jun Zhou",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "arxiv_id": "2510.19183v1",
            "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
            "headline_zh": "提出PruneHal方法，通过自适应KV缓存剪枝减少多模态大语言模型中的幻觉问题",
            "intro_zh": [
                "核心问题：多模态大语言模型幻觉与视觉令牌注意力不足相关，冗余令牌分散注意力",
                "方法要点：采用训练无关的自适应KV缓存剪枝，增强对关键视觉信息的关注",
                "实验或效果：在多个基准测试中验证，无需额外训练且推理成本几乎为零"
            ],
            "tags_zh": [
                "多模态大语言模型",
                "幻觉缓解",
                "KV缓存剪枝",
                "注意力机制",
                "训练无关方法"
            ],
            "_index": 86
        },
        {
            "title": "Malaria Detection from Blood Cell Images Using XceptionNet",
            "authors": [
                "Warisa Nusrat",
                "Mostafijur Rahman",
                "Ayatullah Faruk Mollah"
            ],
            "arxiv_id": "2510.19182v1",
            "summary": "Malaria, which primarily spreads with the bite of female anopheles mosquitos,\noften leads to death of people - specifically children in the age-group of 0-5\nyears. Clinical experts identify malaria by observing RBCs in blood smeared\nimages with a microscope. Lack of adequate professional knowledge and skills,\nand most importantly manual involvement may cause incorrect diagnosis.\nTherefore, computer aided automatic diagnosis stands as a preferred substitute.\nIn this paper, well-demonstrated deep networks have been applied to extract\ndeep intrinsic features from blood cell images and thereafter classify them as\nmalaria infected or healthy cells. Among the six deep convolutional networks\nemployed in this work viz. AlexNet, XceptionNet, VGG-19, Residual Attention\nNetwork, DenseNet-121 and Custom-CNN. Residual Attention Network and\nXceptionNet perform relatively better than the rest on a publicly available\nmalaria cell image dataset. They yield an average accuracy of 97.28% and 97.55%\nrespectively, that surpasses other related methods on the same dataset. These\nfindings highly encourage the reality of deep learning driven method for\nautomatic and reliable detection of malaria while minimizing direct manual\ninvolvement.",
            "headline_zh": "应用XceptionNet等深度网络实现疟疾血细胞图像自动检测，准确率达97.55%。",
            "intro_zh": [
                "核心问题：手动显微镜检查疟疾血细胞易误诊，需自动化诊断方法。",
                "方法要点：比较六种深度卷积网络，XceptionNet和残差注意力网络表现最佳。",
                "实验效果：在公开数据集上，XceptionNet准确率97.55%，优于其他方法。"
            ],
            "tags_zh": [
                "疟疾检测",
                "血细胞图像",
                "XceptionNet",
                "残差注意力网络",
                "深度学习",
                "自动诊断"
            ],
            "_index": 87
        },
        {
            "title": "FootFormer: Estimating Stability from Visual Input",
            "authors": [
                "Keaton Kraiger",
                "Jingjing Li",
                "Skanda Bharadwaj",
                "Jesse Scott",
                "Robert T. Collins",
                "Yanxi Liu"
            ],
            "arxiv_id": "2510.19170v1",
            "summary": "We propose FootFormer, a cross-modality approach for jointly predicting human\nmotion dynamics directly from visual input. On multiple datasets, FootFormer\nachieves statistically significantly better or equivalent estimates of foot\npressure distributions, foot contact maps, and center of mass (CoM), as\ncompared with existing methods that generate one or two of those measures.\nFurthermore, FootFormer achieves SOTA performance in estimating\nstability-predictive components (CoP, CoM, BoS) used in classic kinesiology\nmetrics. Code and data are available at\nhttps://github.com/keatonkraiger/Vision-to-Stability.git.",
            "headline_zh": "提出FootFormer从视觉输入联合预测人体运动动态以估计稳定性",
            "intro_zh": [
                "核心问题：从视觉输入估计人体稳定性相关指标，如足压分布和质心",
                "方法要点：采用跨模态方法联合预测多个运动动态指标",
                "实验或效果：在多个数据集上优于或等效于现有方法，达到SOTA性能"
            ],
            "tags_zh": [
                "人体运动分析",
                "视觉输入估计",
                "稳定性预测",
                "跨模态方法",
                "足部压力分布",
                "质心估计"
            ],
            "_index": 88
        },
        {
            "title": "X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning",
            "authors": [
                "Yunzhe Wang",
                "Soham Hans",
                "Volkan Ustun"
            ],
            "arxiv_id": "2510.19150v1",
            "summary": "Human team tactics emerge from each player's individual perspective and their\nability to anticipate, interpret, and adapt to teammates' intentions. While\nadvances in video understanding have improved the modeling of team interactions\nin sports, most existing work relies on third-person broadcast views and\noverlooks the synchronous, egocentric nature of multi-agent learning. We\nintroduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay\nfootage from 45 professional-level matches of the popular e-sports game\nCounter-Strike 2, designed to facilitate research on multi-agent\ndecision-making in complex 3D environments. X-Ego-CS provides cross-egocentric\nvideo streams that synchronously capture all players' first-person perspectives\nalong with state-action trajectories. Building on this resource, we propose\nCross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric\nvisual streams to foster team-level tactical situational awareness from an\nindividual's perspective. We evaluate CECL on a teammate-opponent location\nprediction task, demonstrating its effectiveness in enhancing an agent's\nability to infer both teammate and opponent positions from a single\nfirst-person view using state-of-the-art video encoders. Together, X-Ego-CS and\nCECL establish a foundation for cross-egocentric multi-agent benchmarking in\nesports. More broadly, our work positions gameplay understanding as a testbed\nfor multi-agent modeling and tactical learning, with implications for\nspatiotemporal reasoning and human-AI teaming in both virtual and real-world\ndomains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.",
            "headline_zh": "提出跨自我中心对比学习以提升团队战术感知，应用于电子竞技视频分析",
            "intro_zh": [
                "核心问题：现有视频理解依赖第三人称视角，忽略多智能体同步自我中心学习。",
                "方法要点：引入跨自我中心对比学习，对齐队友自我中心视觉流以增强战术感知。",
                "实验或效果：在队友-对手位置预测任务中，验证方法提升单视角位置推断能力。"
            ],
            "tags_zh": [
                "跨自我中心学习",
                "多智能体视频理解",
                "团队战术感知",
                "电子竞技基准",
                "对比学习",
                "位置预测"
            ],
            "_index": 89
        }
    ]
}