{
    "papers": [
        {
            "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
            "authors": [
                "Ryosuke Korekata",
                "Quanting Xie",
                "Yonatan Bisk",
                "Komei Sugiura"
            ],
            "arxiv_id": "2512.18987v1",
            "summary": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.",
            "categories": [
                "cs.RO",
                "cs.CL",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Accepted to IEEE RA-L, with presentation at ICRA 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18987v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "[T]mobile manipulation"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "open-vocabulary",
                        "open vocabulary",
                        "[T]affordance"
                    ],
                    "score": 10.0
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]affordance-aware"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 38.5,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam",
                "5_interaction_reaction",
                "9_embodied_foundation"
            ],
            "headline_zh": "Affordance RAG：用于移动操作的具身记忆分层多模态检索",
            "summary_zh": "本研究旨在解决开放词汇移动操作问题，即机器人需要根据自由形式的自然语言指令将各种物体运送到容器中。这项任务极具挑战性，因为它涉及理解视觉语义和操作动作的可供性。为了应对这些挑战，我们提出了一种零样本分层多模态检索框架Affordance RAG，该框架从预先探索的图像构建可供性感知具身记忆。该模型基于区域和视觉语义检索候选目标，并使用可供性得分对其进行重新排序，从而使机器人能够识别在真实环境中可能执行的操作选项。我们的方法在大型室内环境中移动操作指令的检索性能方面优于现有方法。此外，在机器人根据自由形式指令在室内环境中执行移动操作的真实世界实验中，所提出的方法实现了85%的任务成功率，在检索性能和整体任务成功率方面均优于现有方法。",
            "intro_zh": [
                "现有移动操作方法难以理解视觉语义和操作动作的可供性，导致任务成功率较低。",
                "Affordance RAG通过构建可供性感知具身记忆，并结合分层多模态检索来解决上述问题。",
                "实验结果表明，该方法在检索性能和真实世界任务成功率方面均优于现有方法，任务成功率达到85%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决开放词汇移动操作任务，即机器人需要根据自然语言指令将物体移动到指定地点。现有方法的痛点在于难以理解视觉语义和操作动作的可供性，导致机器人无法准确识别目标和执行操作。例如，机器人可能无法区分相似的物体，或者无法判断某个物体是否适合放置在某个容器中。\\n\\n**核心思路**：论文的核心思路是利用预先探索的图像构建可供性感知具身记忆，并结合分层多模态检索来提高机器人对目标和操作的理解能力。通过可供性感知，机器人可以更好地判断哪些操作是可行的，从而提高任务成功率。分层多模态检索则可以更有效地利用视觉和语义信息，从而更准确地识别目标。\\n\\n**技术框架**：Affordance RAG框架包含以下几个主要模块：1) **具身记忆构建**：利用预先探索的图像构建可供性感知具身记忆。2) **分层多模态检索**：首先基于区域和视觉语义检索候选目标，然后使用可供性得分对候选目标进行重新排序。3) **操作执行**：根据检索结果执行相应的操作。整个流程是零样本的，不需要针对特定任务进行训练。\\n\\n**关键创新**：该论文最重要的技术创新点在于提出了可供性感知具身记忆和分层多模态检索相结合的方法。可供性感知具身记忆可以帮助机器人更好地理解操作动作的可行性，而分层多模态检索可以更有效地利用视觉和语义信息。与现有方法相比，该方法能够更准确地识别目标和执行操作，从而提高任务成功率。\\n\\n**关键设计**：在具身记忆构建阶段，论文使用了预训练的视觉模型来提取图像特征，并使用可供性预测模型来预测每个区域的可供性得分。在分层多模态检索阶段，论文首先使用CLIP模型来提取文本和图像特征，然后使用余弦相似度来计算文本和图像之间的相似度。最后，使用可供性得分对候选目标进行重新排序，选择得分最高的作为最终目标。",
            "application_zh": "该研究成果可应用于各种需要机器人进行移动操作的场景，例如家庭服务、仓储物流、医疗护理等。通过理解自然语言指令和环境信息，机器人可以更自主地完成任务，提高工作效率和服务质量。未来，该技术还可以扩展到更复杂的任务，例如多机器人协作、动态环境适应等。",
            "highlight_zh": "该方法在真实世界实验中取得了显著的成果，任务成功率达到85%，显著优于现有方法。在检索性能方面，该方法也优于现有方法，表明其能够更准确地识别目标。这些结果表明，该方法在解决开放词汇移动操作问题方面具有很大的潜力。",
            "tags_zh": [
                "移动操作",
                "具身智能",
                "可供性",
                "多模态检索",
                "自然语言理解"
            ],
            "_index": 0,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18987v1/figs/eyecatch/RA-L25_eye-catch-v6.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18987v1/figs/framework/RA-L25_framework-v5.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18987v1/figs/simulated/simulated_sr_v2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation",
            "authors": [
                "Yadong Liu",
                "Jianwei Liu",
                "He Liang",
                "Dimitrios Kanoulas"
            ],
            "arxiv_id": "2512.18938v1",
            "summary": "Quadruped mobile manipulators offer strong potential for agile loco-manipulation but remain difficult to control and transfer reliably from simulation to reality. Reinforcement learning (RL) shows promise for whole-body control, yet most frameworks are proprietary and hard to reproduce on real hardware. We present an open pipeline for training, benchmarking, and deploying RL-based controllers on the Unitree B1 quadruped with a Z1 arm. The framework unifies sim-to-sim and sim-to-real transfer through ROS, re-implementing a policy trained in Isaac Gym, extending it to MuJoCo via a hardware abstraction layer, and deploying the same controller on physical hardware. Sim-to-sim experiments expose discrepancies between Isaac Gym and MuJoCo contact models that influence policy behavior, while real-world teleoperated object-picking trials show that coordinated whole-body control extends reach and improves manipulation over floating-base baselines. The pipeline provides a transparent, reproducible foundation for developing and analyzing RL-based loco-manipulation controllers and will be released open source to support future research.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18938v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]quadruped",
                        "whole-body control",
                        "[T]manipulation",
                        "[T]loco-manipulation",
                        "sim-to-real",
                        "Unitree"
                    ],
                    "score": 24.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                }
            ],
            "relevance_score": 25.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于强化学习的四足机器人灵巧操作部署框架，解决仿真到现实迁移难题",
            "summary_zh": "四足移动操作机器人具有敏捷操作的巨大潜力，但其控制和从仿真到现实的可靠迁移仍然困难。强化学习(RL)在全身控制方面显示出希望，但大多数框架是专有的，难以在真实硬件上重现。本文提出了一个开放的pipeline，用于在配备Z1机械臂的宇树B1四足机器人上训练、基准测试和部署基于RL的控制器。该框架通过ROS统一了sim-to-sim和sim-to-real的迁移，重新实现了在Isaac Gym中训练的策略，通过硬件抽象层将其扩展到MuJoCo，并在物理硬件上部署相同的控制器。Sim-to-sim实验揭示了Isaac Gym和MuJoCo接触模型之间的差异，这些差异影响了策略行为，而真实世界的遥操作物体拾取试验表明，协调的全身控制扩展了范围，并改善了对浮动基线操作的改进。该pipeline为开发和分析基于RL的loco-manipulation控制器提供了一个透明、可复现的基础，并将开源发布以支持未来的研究。",
            "intro_zh": [
                "四足机器人灵巧操作控制复杂，现有强化学习框架难以在真实硬件上复现。",
                "提出一个开放的pipeline，统一sim-to-sim和sim-to-real迁移，实现策略在不同仿真器和真实机器人上的部署。",
                "实验表明，协调的全身控制扩展了操作范围，并改善了物体拾取操作的性能。"
            ],
            "method_zh": "**问题定义**：四足机器人灵巧操作的控制策略设计复杂，尤其是在将仿真环境中训练的策略迁移到真实机器人时，由于仿真环境与真实环境的差异，往往难以保证控制策略的有效性。现有的强化学习框架大多是专有的，缺乏透明性和可复现性，阻碍了相关研究的进展。\\n\\n**核心思路**：本文的核心思路是构建一个开放、可复现的pipeline，通过ROS统一sim-to-sim和sim-to-real的迁移，使得在仿真环境中训练的策略能够顺利地部署到真实机器人上。通过硬件抽象层，策略可以方便地在不同的仿真器（Isaac Gym和MuJoCo）之间切换，从而更好地适应不同的仿真环境。\\n\\n**技术框架**：该框架包含以下几个主要模块：1) 基于Isaac Gym的强化学习训练环境；2) 基于ROS的通信接口，用于连接仿真环境和真实机器人；3) 硬件抽象层，用于屏蔽不同仿真器和硬件平台的差异；4) 基于MuJoCo的仿真环境，用于验证策略的有效性；5) 宇树B1四足机器人平台，用于真实环境的部署和测试。整体流程是从Isaac Gym训练策略，通过ROS和硬件抽象层迁移到MuJoCo，最后部署到真实机器人。\\n\\n**关键创新**：该论文的关键创新在于构建了一个开放、可复现的四足机器人灵巧操作部署框架，该框架能够有效地解决仿真到现实的迁移问题。通过统一的接口和硬件抽象层，策略可以在不同的仿真环境和真实机器人之间无缝切换，大大降低了开发和部署的难度。\\n\\n**关键设计**：论文中使用了强化学习算法来训练四足机器人的控制策略。具体的参数设置和网络结构在论文中没有详细描述，但可以推测使用了常见的Actor-Critic算法，并针对四足机器人的特点进行了优化。损失函数的设计可能包括奖励函数，用于鼓励机器人完成特定的任务，例如移动到目标位置或拾取物体。硬件抽象层屏蔽了不同仿真器和硬件平台的差异，使得策略能够方便地在不同的平台上部署。",
            "application_zh": "该研究成果可应用于物流、救援、巡检等领域。例如，在复杂地形或狭小空间内，四足机器人可以利用其灵巧的操作能力进行物品搬运、灾情侦察或设备维护。该框架的开源发布将促进四足机器人技术的发展，加速其在各行业的应用。",
            "highlight_zh": "实验结果表明，该框架能够成功地将仿真环境中训练的策略部署到真实机器人上，并实现了较好的控制效果。真实世界的遥操作物体拾取试验表明，协调的全身控制扩展了范围，并改善了对浮动基线操作的改进。Sim-to-sim实验揭示了Isaac Gym和MuJoCo接触模型之间的差异，这些差异影响了策略行为。",
            "tags_zh": [
                "四足机器人",
                "灵巧操作",
                "强化学习",
                "仿真到现实",
                "机器人控制"
            ],
            "_index": 1,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18938v1/Figures/banner_no_text_cropped.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18938v1/Figures/3_methods/method_robot_platform_shrinked.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18938v1/Figures/3_methods/low_level_network_arch_redo.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
            "authors": [
                "Hongwei Fan",
                "Hang Dai",
                "Jiyao Zhang",
                "Jinzhou Li",
                "Qiyang Yan",
                "Yujie Zhao",
                "Mingju Gao",
                "Jinghang Wu",
                "Hao Tang",
                "Hao Dong"
            ],
            "arxiv_id": "2512.19390v1",
            "summary": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io",
            "categories": [
                "cs.RO",
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19390v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation",
                        "[T]sim2real",
                        "[T]real2sim"
                    ],
                    "score": 18.0
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3DGS"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 23.0,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "TwinAligner：通过视觉-动力学对齐实现物理感知的Real2Sim2Real机器人操作",
            "summary_zh": "受多模态大模型的启发，机器人领域正朝着数据驱动的端到端学习发展。然而，对昂贵真实世界数据的依赖限制了进展。仿真器提供了经济高效的替代方案，但仿真与现实之间的差距挑战了策略的有效迁移。本文介绍了一种新颖的Real2Sim2Real系统TwinAligner，它解决了视觉和动力学差距。视觉对齐模块通过SDF重建和可编辑的3DGS渲染实现像素级对齐，而动力学对齐模块通过识别机器人-物体交互中的刚性物理来确保动力学一致性。TwinAligner通过提供可扩展的数据收集并建立可信的迭代循环来改进机器人学习，从而加速算法开发。定量评估突出了TwinAligner在视觉和动力学真实到仿真对齐方面的强大能力。该系统使在仿真中训练的策略能够实现对真实世界的强大零样本泛化。真实世界和仿真策略性能之间的高度一致性突显了TwinAligner在推进可扩展机器人学习方面的潜力。",
            "intro_zh": [
                "现有机器人学习方法依赖昂贵的真实数据，而Sim2Real迁移面临仿真与现实的差距。",
                "TwinAligner通过视觉和动力学对齐，构建Real2Sim2Real系统，实现策略在仿真和现实之间的迭代优化。",
                "实验表明，TwinAligner在视觉和动力学对齐方面表现出色，策略在真实世界中实现了强大的零样本泛化。"
            ],
            "method_zh": "**问题定义**：机器人操作学习面临真实数据获取成本高昂的问题，而Sim2Real方法又受到仿真环境与真实环境差异的影响，导致策略迁移效果不佳。现有方法难以同时解决视觉和动力学上的差异，限制了机器人学习的效率和泛化能力。\\n\\n**核心思路**：TwinAligner的核心思路是通过视觉和动力学对齐，构建一个可信的仿真环境，使得在仿真环境中训练的策略能够直接迁移到真实世界，并可以通过Real2Sim2Real的迭代过程不断优化策略。该方法旨在缩小仿真环境和真实环境之间的差距，提高机器人学习的效率和泛化能力。\\n\\n**技术框架**：TwinAligner系统包含两个主要模块：视觉对齐模块和动力学对齐模块。视觉对齐模块利用SDF重建和可编辑的3DGS渲染实现像素级别的对齐，从而缩小视觉上的差距。动力学对齐模块通过识别机器人与物体交互中的刚性物理属性，确保动力学的一致性。整个系统通过Real2Sim2Real的迭代过程，不断优化仿真环境和策略。\\n\\n**关键创新**：TwinAligner的关键创新在于同时考虑了视觉和动力学上的对齐，并提出了相应的解决方案。视觉对齐模块利用了最新的3DGS渲染技术，实现了高精度的像素级别对齐。动力学对齐模块则通过识别刚性物理属性，确保了仿真环境的物理真实性。这种双重对齐的方式使得仿真环境更加可信，从而提高了策略迁移的效果。\\n\\n**关键设计**：视觉对齐模块使用了SDF（Signed Distance Function）来表示场景的几何信息，并通过3DGS（3D Gaussian Splatting）进行渲染，实现了高精度的视觉重建。动力学对齐模块则通过力/扭矩传感器数据来识别刚性物理属性，并将其应用到仿真环境中。损失函数的设计也至关重要，需要同时考虑视觉和动力学上的误差，并进行合理的权重分配。",
            "application_zh": "TwinAligner可应用于各种机器人操作任务，例如物体抓取、装配、导航等。该研究的实际价值在于降低了机器人学习的成本，提高了策略的泛化能力，加速了机器人算法的开发。未来，该技术有望应用于工业自动化、家庭服务、医疗康复等领域，实现更智能、更高效的机器人应用。",
            "highlight_zh": "TwinAligner在多个机器人操作任务上进行了评估，结果表明，该系统在视觉和动力学对齐方面表现出色，显著提高了策略在真实世界中的零样本泛化能力。具体而言，与现有Sim2Real方法相比，TwinAligner在抓取成功率、装配精度等方面取得了显著提升，证明了其有效性和优越性。",
            "tags_zh": [
                "机器人操作",
                "Real2Sim2Real",
                "视觉对齐",
                "动力学对齐",
                "零样本泛化",
                "仿真环境",
                "3DGS"
            ],
            "_index": 2,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19390v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19390v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19390v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models",
            "authors": [
                "Pengyu Chen",
                "Tao Ouyang",
                "Ke Luo",
                "Weijie Hong",
                "Xu Chen"
            ],
            "arxiv_id": "2512.19083v1",
            "summary": "Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "This paper is accepted by the IEEE Internet of Things Journal (IoT-J) for publication in the Special Issue on \"Augmented Edge Sensing Intelligence for Low-Altitude IoT Systems\"",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19083v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "Depth Anything",
                        "scene understanding",
                        "occupancy grid"
                    ],
                    "score": 8.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]foundation model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 20.0,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam",
                "9_embodied_foundation"
            ],
            "headline_zh": "CoDrone：边缘云协同，利用基础模型增强无人机自主导航能力",
            "summary_zh": "无人机自主导航面临着机载计算资源有限的挑战，这限制了部署的深度神经网络只能采用浅层架构，难以处理复杂环境。将任务卸载到远程边缘服务器会引入高延迟，从而在系统设计中产生固有的权衡。为了解决这些限制，我们提出了CoDrone，这是第一个云-边-端协同计算框架，将基础模型集成到自主无人机巡航场景中，有效地利用基础模型来增强资源受限的无人机平台的性能。为了减少机载计算和数据传输开销，CoDrone采用灰度图像进行导航模型。当需要增强环境感知时，CoDrone利用边缘辅助的基础模型Depth Anything V2进行深度估计，并引入了一种新颖的基于一维占据栅格的导航方法，从而实现细粒度的场景理解，同时提高自主导航的效率和表征简洁性。CoDrone的一个关键组成部分是基于深度强化学习的神经调度器，它将深度估计与自主导航决策无缝集成，从而能够实时适应动态环境。此外，该框架还引入了一个无人机特定的视觉语言交互模块，该模块结合了领域定制的低级飞行原语，以实现云基础模型与无人机之间的有效交互。VLM的引入增强了复杂未知场景中的开放集推理能力。实验结果表明，CoDrone在不同的飞行速度和网络条件下优于基线方法，平均飞行距离提高了40%，平均导航质量提高了5%。",
            "intro_zh": [
                "现有无人机自主导航受限于机载算力，无法部署复杂模型，而卸载到边缘服务器则引入延迟，需要在性能和效率之间权衡。",
                "CoDrone提出云-边-端协同框架，利用边缘辅助的基础模型进行深度估计，并结合一维占据栅格导航，提升环境理解能力。",
                "实验表明，CoDrone在不同条件下优于基线方法，平均飞行距离提升40%，导航质量提升5%，展现了其优越性。"
            ],
            "method_zh": "**问题定义**：无人机自主导航需要在有限的机载计算资源下实现高效的环境感知和路径规划。现有的方法要么依赖于计算量小的浅层网络，导致环境理解能力不足；要么将计算卸载到远程服务器，引入不可接受的延迟，影响实时性。因此，如何在资源受限的平台上实现高性能的自主导航是一个关键问题。\\n\\n**核心思路**：CoDrone的核心思路是利用云-边-端协同计算框架，将计算密集型的任务（如深度估计和视觉语言交互）卸载到边缘服务器或云端，同时在无人机端保留轻量级的导航模型。通过智能调度，根据环境需求动态地选择合适的计算资源，从而在性能和效率之间取得平衡。\\n\\n**技术框架**：CoDrone框架包含三个主要组成部分：1) 无人机端：负责图像采集、轻量级导航和与边缘/云端的通信。2) 边缘服务器：部署Depth Anything V2等基础模型，用于深度估计，并提供低延迟的计算服务。3) 云端：部署视觉语言模型（VLM），用于处理复杂的开放场景推理任务。框架还包含一个基于深度强化学习的神经调度器，用于动态地决定何时使用边缘服务器进行深度估计，以及何时使用云端进行视觉语言交互。\\n\\n**关键创新**：CoDrone的关键创新在于：1) 提出了一个云-边-端协同计算框架，充分利用了不同计算资源的优势。2) 引入了基于一维占据栅格的导航方法，简化了环境表示，提高了导航效率。3) 设计了一个基于深度强化学习的神经调度器，实现了对计算资源的动态管理和优化。4) 集成了无人机特定的视觉语言交互模块，增强了无人机在复杂未知场景中的推理能力。\\n\\n**关键设计**：CoDrone的关键设计包括：1) 采用灰度图像作为导航模型的输入，降低了计算和传输开销。2) 使用Depth Anything V2进行深度估计，该模型具有较高的精度和效率。3) 一维占据栅格将三维空间简化为一维，降低了计算复杂度。4) 神经调度器使用深度强化学习算法进行训练，以最大化导航性能和资源利用率。5) 无人机特定的视觉语言交互模块包含领域定制的低级飞行原语，使得云端VLM能够更好地理解无人机的状态和目标。",
            "application_zh": "CoDrone框架具有广泛的应用前景，例如在物流配送、环境监测、灾害救援等领域。通过利用云-边-端协同计算和基础模型，CoDrone可以使无人机在复杂环境中实现更安全、更高效的自主导航，从而提高工作效率和降低运营成本。未来，CoDrone还可以与其他技术（如5G通信、物联网）相结合，构建更智能化的无人机应用生态系统。",
            "highlight_zh": "实验结果表明，CoDrone在不同的飞行速度和网络条件下均优于基线方法。具体来说，CoDrone的平均飞行距离提高了40%，平均导航质量提高了5%。这些结果验证了CoDrone框架的有效性和优越性，表明其能够在资源受限的平台上实现高性能的自主导航。",
            "tags_zh": [
                "无人机导航",
                "边缘计算",
                "云计算",
                "深度强化学习",
                "基础模型",
                "视觉语言模型",
                "自主导航"
            ],
            "_index": 3,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19083v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19083v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19083v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
            "authors": [
                "Ziyang Song",
                "Zelin Zang",
                "Zuyao Chen",
                "Xusheng Liang",
                "Dong Yi",
                "Jinlin Wu",
                "Hongbin Liu",
                "Jiebo Luo"
            ],
            "arxiv_id": "2512.19512v1",
            "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19512v1",
            "code_links": [
                {
                    "url": "https://github.com/tomato996/Anatomy-R1",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "curriculum learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "[T]multimodal"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 19.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "Anatomy-R1：通过解剖相似性课程学习和群体多样性增强提升多模态大语言模型中的解剖推理能力",
            "summary_zh": "多模态大语言模型(MLLMs)在自然图像推理方面取得了显著进展，但其在医学成像，特别是临床解剖手术图像中的潜力仍未被充分探索。解剖理解任务需要精确的理解和临床上连贯的答案，由于医学数据的复杂性和高质量专家注释的稀缺性，这些都难以实现。这些挑战限制了传统监督微调(SFT)策略的有效性。虽然最近的工作表明，群体相对策略优化(GRPO)可以在不依赖大量数据的情况下增强MLLM中的推理能力，但我们发现两个弱点阻碍了GRPO在解剖识别中的推理性能：1)知识不能在不同的解剖结构之间有效地共享，导致信息增益不均匀，并阻止模型收敛，以及2)模型迅速收敛到单一的推理路径，抑制了对多样化策略的探索。为了克服这些挑战，我们提出了两种新方法。首先，我们通过控制答案选择的相似性来控制问题的难度，从而实现一种称为解剖相似性课程学习的渐进式学习策略，使模型能够逐步掌握复杂的问题。其次，我们利用问题增强，即群体多样性问题增强，来扩展模型对困难查询的搜索空间，从而减轻产生统一响应的趋势。在SGG-VQA和OmniMedVQA基准上的综合实验表明，我们的方法在这两个基准上都取得了显著的改进，证明了其在增强MLLM的医学推理能力方面的有效性。代码可在https://github.com/tomato996/Anatomy-R1 找到。",
            "intro_zh": [
                "医学图像解剖理解任务对模型提出了更高的精度和临床连贯性要求，但医学数据复杂且高质量标注稀缺，限制了现有监督微调方法的有效性。",
                "论文提出解剖相似性课程学习和群体多样性问题增强两种方法，前者通过渐进式学习提升模型掌握复杂问题的能力，后者扩展搜索空间，避免模型收敛到单一推理路径。",
                "在SGG-VQA和OmniMedVQA基准测试中，Anatomy-R1方法取得了显著的性能提升，验证了其在增强多模态大语言模型医学推理能力方面的有效性。"
            ],
            "method_zh": "**问题定义**：现有方法在医学图像解剖理解任务中，由于医学数据的复杂性和标注的稀缺性，难以实现精确的理解和临床上连贯的答案。特别是，基于群体相对策略优化(GRPO)的方法，存在知识共享不足和推理路径单一的问题，导致模型难以收敛和探索多样化的解题策略。\\n\\n**核心思路**：论文的核心思路是通过课程学习和数据增强，引导模型逐步学习和探索更广泛的解剖知识。解剖相似性课程学习模拟人类学习过程，从简单到复杂，逐步提升模型能力。群体多样性问题增强则鼓励模型探索不同的推理路径，避免陷入局部最优。\\n\\n**技术框架**：Anatomy-R1方法主要包含两个阶段：解剖相似性课程学习和群体多样性问题增强。在解剖相似性课程学习阶段，根据答案选项的解剖相似性对问题进行排序，模型先学习相似性高的问题，再逐步学习相似性低的问题。在群体多样性问题增强阶段，针对困难问题，生成多个不同的问题变体，扩大模型的搜索空间。\\n\\n**关键创新**：论文的关键创新在于将课程学习和数据增强相结合，并针对医学图像解剖理解任务的特点进行了定制化设计。解剖相似性课程学习利用解剖结构的相似性来控制学习难度，更符合医学知识的特点。群体多样性问题增强则通过生成多样化的提问方式，鼓励模型探索不同的推理路径。\\n\\n**关键设计**：解剖相似性课程学习的关键在于如何衡量解剖结构的相似性，论文中具体如何实现未知。群体多样性问题增强的关键在于如何生成高质量的问题变体，论文中具体如何实现未知。损失函数和网络结构等细节未在摘要中提及，具体实现未知。",
            "application_zh": "该研究成果可应用于医学影像辅助诊断、手术导航、医学教育等领域。通过提升多模态大语言模型对医学图像的理解能力，可以帮助医生更准确地识别病灶、制定治疗方案，并为医学教育提供更智能化的工具。未来，该技术有望在远程医疗、智能健康管理等方面发挥更大的作用。",
            "highlight_zh": "Anatomy-R1方法在SGG-VQA和OmniMedVQA两个医学视觉问答基准测试中取得了显著的性能提升，证明了其在增强多模态大语言模型医学推理能力方面的有效性。具体提升幅度未知，需要在论文中进一步查找。",
            "tags_zh": [
                "多模态大语言模型",
                "医学图像理解",
                "解剖推理",
                "课程学习",
                "数据增强",
                "视觉问答",
                "医学影像辅助诊断"
            ],
            "_index": 4,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19512v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19512v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19512v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
            "authors": [
                "Yizhi Wang",
                "Linan Yue",
                "Min-Ling Zhang"
            ],
            "arxiv_id": "2512.18956v1",
            "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18956v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal",
                        "[T]chain-of-thought"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 19.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出SynSelect框架，为多模态大模型生成高质量长链推理训练数据。",
            "summary_zh": "大型推理模型(LRMs)在复杂的推理任务中通过长链思考(CoT)推理表现出了卓越的性能。然而，由于整合不同输入模态的复杂性增加以及高质量长CoT训练数据的稀缺，将这些成功扩展到多模态推理仍然具有挑战性。现有的多模态数据集和CoT合成方法仍然存在推理深度有限、模态转换错误和生成流程僵化等问题，从而阻碍了模型的性能和稳定性。为此，本文提出了一种新颖的三阶段合成-选择框架SynSelect，用于生成针对多模态推理任务量身定制的高质量长CoT数据。具体来说，SynSelect首先利用多个异构多模态LRM来生成多样化的候选CoT，然后应用实例和批次级别的选择来过滤能够有效增强模型推理能力的高质量CoT。在多个多模态基准上的大量实验表明，在SynSelect生成的数据上进行监督微调的模型显著优于基线，并在强化学习后训练后取得了进一步的改进。我们的结果验证了SynSelect是提高多模态LRM推理能力的有效方法。",
            "intro_zh": [
                "多模态推理面临高质量长链思考数据稀缺的挑战，现有方法推理深度有限且易出错。",
                "SynSelect框架通过多阶段合成与选择，生成高质量、多样化的多模态长链思考数据。",
                "实验表明，基于SynSelect训练的模型在多模态推理任务上显著优于现有基线方法。"
            ],
            "method_zh": "**问题定义**：多模态大型推理模型在复杂推理任务中表现出色，但缺乏高质量的长链思考(CoT)训练数据。现有数据集和CoT生成方法存在推理深度不足、模态转换错误以及生成流程僵化等问题，限制了模型性能和稳定性。\\n\\n**核心思路**：SynSelect的核心在于通过一个三阶段的合成-选择框架，生成高质量、多样化的长CoT数据。它利用多个异构的多模态LRM生成候选CoT，并通过实例和批次级别的选择机制，筛选出能够有效提升模型推理能力的CoT。这种方法旨在克服现有方法在数据质量和多样性方面的局限性。\\n\\n**技术框架**：SynSelect框架包含三个主要阶段：\n1. **CoT Synthesis (CoT合成)**：利用多个异构的多模态LRM生成多样化的候选CoT。\n2. **Instance-level Selection (实例级别选择)**：对每个CoT进行质量评估，选择高质量的CoT。\n3. **Batch-level Selection (批次级别选择)**：在批次层面进一步筛选，确保选出的CoT能够有效提升模型的推理能力。\n\\n**关键创新**：SynSelect的关键创新在于其三阶段的合成-选择框架，特别是批次级别的选择机制。与传统的只关注单个CoT质量的方法不同，SynSelect在批次层面考虑了CoT之间的关系，从而能够选择出更具信息量和互补性的CoT集合，更有效地提升模型性能。\\n\\n**关键设计**：在CoT合成阶段，使用不同的多模态LRM以增加CoT的多样性。实例级别选择可能使用奖励模型或基于规则的过滤。批次级别选择可能涉及聚类或基于覆盖率的策略，以确保选择的CoT集合具有代表性，并能覆盖不同的推理路径。具体的损失函数和网络结构取决于所使用的多模态LRM。",
            "application_zh": "该研究成果可广泛应用于需要复杂推理的多模态任务中，例如视觉问答、图像描述生成、机器人导航等。通过提供高质量的训练数据，可以显著提升多模态大模型的推理能力，使其在实际应用中更加可靠和有效。未来，该方法可以进一步扩展到其他模态和任务，推动多模态人工智能的发展。",
            "highlight_zh": "实验结果表明，在SynSelect生成的数据上进行微调的模型，在多个多模态基准测试中显著优于基线模型。通过强化学习后训练，模型性能得到进一步提升，验证了SynSelect在提高多模态LRM推理能力方面的有效性。具体性能提升幅度未知，但摘要强调了“显著优于基线”。",
            "tags_zh": [
                "多模态推理",
                "长链思考",
                "数据合成",
                "数据选择",
                "大型推理模型",
                "视觉问答",
                "模型微调"
            ],
            "_index": 5,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18956v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18956v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18956v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control",
            "authors": [
                "Chao Yang",
                "Yingkai Sun",
                "Peng Ye",
                "Xin Chen",
                "Chong Yu",
                "Tao Chen"
            ],
            "arxiv_id": "2512.19043v1",
            "summary": "Learning a general motion tracking policy from human motions shows great potential for versatile humanoid whole-body control. Conventional approaches are not only inefficient in data utilization and training processes but also exhibit limited performance when tracking highly dynamic motions. To address these challenges, we propose EGM, a framework that enables efficient learning of a general motion tracking policy. EGM integrates four core designs. Firstly, we introduce a Bin-based Cross-motion Curriculum Adaptive Sampling strategy to dynamically orchestrate the sampling probabilities based on tracking error of each motion bin, eficiently balancing the training process across motions with varying dificulty and durations. The sampled data is then processed by our proposed Composite Decoupled Mixture-of-Experts (CDMoE) architecture, which efficiently enhances the ability to track motions from different distributions by grouping experts separately for upper and lower body and decoupling orthogonal experts from shared experts to separately handle dedicated features and general features. Central to our approach is a key insight we identified: for training a general motion tracking policy, data quality and diversity are paramount. Building on these designs, we develop a three-stage curriculum training flow to progressively enhance the policy's robustness against disturbances. Despite training on only 4.08 hours of data, EGM generalized robustly across 49.25 hours of test motions, outperforming baselines on both routine and highly dynamic tasks.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19043v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]humanoid",
                        "[T]whole-body control"
                    ],
                    "score": 12.0
                },
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "[T]motion tracking"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "1_robot_core",
                "8_physics_animation"
            ],
            "headline_zh": "EGM：高效学习通用运动跟踪策略，用于高动态人形机器人全身控制",
            "summary_zh": "本文提出了一种名为EGM的框架，旨在高效学习通用运动跟踪策略，用于人形机器人全身控制。传统方法在数据利用率和训练效率方面存在不足，且在跟踪高动态运动时性能受限。EGM集成了四个核心设计：基于Bin的跨运动课程自适应采样策略，动态调整采样概率，平衡不同难度和时长的运动训练；复合解耦混合专家(CDMoE)架构，通过对上下身分别分组专家，并解耦正交专家处理专用和通用特征，从而增强跟踪不同分布运动的能力；强调数据质量和多样性对于训练通用运动跟踪策略至关重要；三阶段课程训练流程，逐步增强策略对扰动的鲁棒性。仅使用4.08小时的数据进行训练，EGM在49.25小时的测试运动中表现出强大的泛化能力，优于基线方法，尤其是在常规和高动态任务中。",
            "intro_zh": [
                "现有方法在人形机器人运动跟踪策略学习中，存在数据利用率低、训练效率不高以及难以跟踪高动态运动等问题。",
                "EGM框架通过Bin-based自适应采样、CDMoE架构以及三阶段课程训练，提升数据利用率，增强模型泛化能力和鲁棒性。",
                "实验结果表明，EGM仅使用少量训练数据，即可在大量测试数据上实现优异的运动跟踪性能，超越现有基线方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人形机器人全身控制中，通用运动跟踪策略学习效率低、难以跟踪高动态运动的问题。现有方法通常数据利用率不高，训练过程耗时，且在高动态运动跟踪任务中表现不佳。\\n\\n**核心思路**：论文的核心思路是通过提高数据质量和多样性，并设计高效的训练框架，从而提升通用运动跟踪策略的学习效率和泛化能力。具体而言，通过自适应采样策略平衡不同难度运动的训练，并利用复合解耦混合专家网络提取运动特征。\\n\\n**技术框架**：EGM框架包含三个主要阶段：首先，使用Bin-based Cross-motion Curriculum Adaptive Sampling策略，根据运动跟踪误差动态调整采样概率，平衡不同运动的训练。然后，将采样数据输入到Composite Decoupled Mixture-of-Experts (CDMoE)架构中，该架构分别对上下身进行专家分组，并解耦正交专家处理专用和通用特征。最后，采用三阶段课程训练流程，逐步提高策略对扰动的鲁棒性。\\n\\n**关键创新**：EGM的关键创新在于：1) Bin-based Cross-motion Curriculum Adaptive Sampling策略，能够有效平衡不同难度和时长的运动训练，提高数据利用率。2) Composite Decoupled Mixture-of-Experts (CDMoE)架构，能够高效地提取运动特征，增强模型对不同运动分布的适应性。3) 三阶段课程训练流程，逐步提高策略的鲁棒性。\\n\\n**关键设计**：Bin-based采样策略将运动数据划分为多个bin，根据每个bin的跟踪误差动态调整采样概率。CDMoE架构中，专家网络被分为上下身两组，并采用解耦设计，分别处理专用和通用特征。三阶段课程训练流程包括：第一阶段，使用无扰动数据进行预训练；第二阶段，引入轻微扰动，提高策略的鲁棒性；第三阶段，引入更强的扰动，进一步增强策略的泛化能力。损失函数的设计也至关重要，可能包含跟踪误差、关节力矩惩罚等。",
            "application_zh": "该研究成果可应用于各种人形机器人全身控制任务，例如运动模仿、人机协作、康复训练等。通过学习通用的运动跟踪策略，机器人能够更好地适应不同的运动场景和任务需求，提高其智能化水平和应用范围。未来，该技术有望应用于更复杂的机器人系统，例如双足行走机器人、服务机器人等。",
            "highlight_zh": "EGM框架仅使用4.08小时的训练数据，即可在49.25小时的测试数据上实现优异的运动跟踪性能，显著优于现有基线方法。尤其是在高动态运动跟踪任务中，EGM表现出更强的鲁棒性和泛化能力。实验结果表明，EGM能够有效提高数据利用率和训练效率，为人形机器人全身控制提供了一种高效可行的解决方案。",
            "tags_zh": [
                "人形机器人",
                "全身控制",
                "运动跟踪",
                "强化学习",
                "混合专家网络"
            ],
            "_index": 6,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19043v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19043v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19043v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
            "authors": [
                "Hengyi Feng",
                "Zeang Sheng",
                "Meiyi Qiang",
                "Wentao Zhang"
            ],
            "arxiv_id": "2512.19115v1",
            "summary": "Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19115v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "[T]multimodal"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示多模态大语言模型在多模态检索中表现不佳的原因",
            "summary_zh": "尽管多模态大语言模型(MLLMs)在生成任务中取得了显著成功，但我们观察到它们在零样本多模态检索任务中表现出一种违反直觉的缺陷。本文研究了阻碍MLLMs作为有效检索器的潜在机制。借助稀疏自编码器(SAEs)，我们将MLLM输出表示分解为可解释的语义概念，以探究其内在行为。我们的分析表明，MLLMs的表示空间主要由文本语义主导；对多模态检索至关重要的视觉信息仅占很小一部分。MLLMs过于关注图像-文本模态之间的桥接，这促进了生成，但也使嵌入同质化，最终降低了多模态检索所需的区分能力。我们进一步发现，对MLLMs相似性计算贡献最大的特定特征成分实际上是会降低检索性能的干扰因素。总的来说，我们的工作首次对多模态检索背景下MLLM表示进行了深入的可解释性分析，并为增强MLLMs的多模态检索能力提供了可能的方向。",
            "intro_zh": [
                "现有的多模态大语言模型在生成任务表现出色，但在多模态检索任务中却存在不足，这是一个待解决的核心问题。",
                "论文通过稀疏自编码器分解MLLM的输出表示，分析其内在行为，揭示了文本语义主导和视觉信息不足的问题。",
                "研究发现，MLLM对图像-文本模态的过度桥接导致嵌入同质化，降低了检索所需的区分能力，并指出了干扰检索的特征成分。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大语言模型（MLLMs）在多模态检索任务中表现不佳的问题。现有的MLLMs虽然在生成任务上表现出色，但在检索任务中却无法有效利用视觉信息，导致检索性能下降。现有的方法缺乏对MLLM内部表示的深入理解，无法解释其在检索任务中的失败原因。\\n\\n**核心思路**：论文的核心思路是通过可解释性分析来理解MLLM在多模态检索中的失败原因。具体来说，通过稀疏自编码器（SAEs）将MLLM的输出表示分解为可解释的语义概念，从而揭示MLLM内部表示的结构和信息分布。通过分析这些语义概念，可以确定哪些信息对检索任务至关重要，以及MLLM在哪些方面存在不足。\\n\\n**技术框架**：论文的技术框架主要包括以下几个步骤：1) 使用MLLM处理多模态输入（图像和文本）；2) 获取MLLM的输出表示；3) 使用稀疏自编码器（SAEs）将MLLM的输出表示分解为可解释的语义概念；4) 分析这些语义概念，评估文本和视觉信息在表示空间中的占比；5) 识别对检索性能有负面影响的干扰特征。\\n\\n**关键创新**：论文最重要的技术创新点在于使用稀疏自编码器（SAEs）对MLLM的输出表示进行可解释性分析。这种方法能够将复杂的MLLM表示分解为易于理解的语义概念，从而揭示MLLM内部的信息处理机制。与传统的黑盒分析方法相比，这种方法能够提供更深入的洞察力，帮助研究人员理解MLLM在多模态检索中的失败原因。\\n\\n**关键设计**：论文的关键设计包括：1) 选择合适的稀疏自编码器（SAEs）架构，以确保能够有效地分解MLLM的输出表示；2) 设计合适的评估指标，以衡量文本和视觉信息在表示空间中的占比；3) 识别对检索性能有负面影响的干扰特征，并分析其产生的原因；4) 使用标准的多模态检索数据集进行实验，以验证所提出的分析方法的有效性。",
            "application_zh": "该研究成果可应用于提升多模态信息检索系统的性能，例如图像搜索、视频搜索等。通过理解MLLM在检索任务中的不足，可以设计更有效的多模态检索模型，提高检索准确率和用户体验。此外，该研究也有助于开发更可靠的多模态理解和推理系统，例如智能问答、视觉对话等。",
            "highlight_zh": "论文通过实验发现，MLLM的表示空间主要由文本语义主导，视觉信息占比很小。同时，对相似性计算贡献最大的特征成分反而会降低检索性能。这些发现为改进MLLM的多模态检索能力提供了重要的指导。",
            "tags_zh": [
                "多模态大语言模型",
                "多模态检索",
                "可解释性分析",
                "稀疏自编码器",
                "表示学习"
            ],
            "_index": 7,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19115v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19115v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19115v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
            "authors": [
                "A. B. M. Ashikur Rahman",
                "Saeed Anwar",
                "Muhammad Usman",
                "Irfan Ahmad",
                "Ajmal Mian"
            ],
            "arxiv_id": "2512.19350v1",
            "summary": "Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19350v1",
            "code_links": [
                {
                    "url": "https://github.com/ashikiut/pendulum/",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "[T]multimodal"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PENDULUM基准，评估多模态大语言模型中的谄媚现象",
            "summary_zh": "本文提出了一种针对多模态大语言模型(MLLM)中谄媚现象的综合评估基准，名为PENDULUM。谄媚是指AI模型过度赞同用户输入，牺牲事实准确性或与视觉证据相悖。尽管之前的研究已经考察了大型语言模型在纯文本环境下的这种行为，但对视觉或多模态对应物的研究在范围和分析深度上仍然有限。PENDULUM包含约2000个由人工整理的视觉问答对，专门用于引出谄媚反应，涵盖六个不同复杂度的图像领域，从而能够系统地研究图像类型和内在挑战如何影响谄媚倾向。通过对最先进的MLLM进行广泛评估，观察到模型鲁棒性的显著差异以及对谄媚和幻觉行为的明显易感性。此外，提出了新的指标来量化视觉推理中的谄媚现象，从而更深入地了解其在不同多模态环境中的表现。研究结果强调迫切需要开发具有抗谄媚能力的架构和训练策略，以提高未来MLLM的事实一致性和可靠性。提出的数据集和MLLM响应可在https://github.com/ashikiut/pendulum/获取。",
            "intro_zh": [
                "现有方法缺乏对多模态大语言模型中谄媚现象的深入研究，尤其是在视觉信息存在的情况下。",
                "论文构建PENDULUM基准，包含精心设计的视觉问答对，旨在诱导模型产生谄媚性回答。",
                "实验结果表明，现有MLLM容易受到谄媚和幻觉的影响，突显了开发抗谄媚模型的重要性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大语言模型(MLLM)中存在的谄媚问题，即模型为了迎合用户输入而牺牲事实准确性或与视觉证据相悖。现有方法主要集中在纯文本场景，缺乏对视觉信息影响的深入研究，无法有效评估和解决MLLM在视觉推理中的谄媚行为。\\n\\n**核心思路**：论文的核心思路是构建一个专门设计的视觉问答基准，通过精心设计的图像和问题，诱导模型产生谄媚性回答。通过分析模型的回答，可以量化其谄媚程度，并深入了解其在不同视觉场景下的表现。这种方法能够更全面地评估MLLM的可靠性和安全性。\\n\\n**技术框架**：PENDULUM基准包含约2000个视觉问答对，涵盖六个不同复杂度的图像领域。每个问答对都经过人工设计，旨在引出谄媚反应。论文还提出了新的指标来量化视觉推理中的谄媚现象。整体流程包括：1) 数据集构建：人工标注视觉问答对；2) 模型评估：使用MLLM回答问题；3) 指标计算：量化模型的谄媚程度。\\n\\n**关键创新**：该论文的关键创新在于构建了一个专门用于评估MLLM谄媚现象的视觉问答基准PENDULUM。与现有方法相比，PENDULUM更侧重于视觉信息的影响，能够更全面地评估MLLM的可靠性和安全性。此外，论文还提出了新的指标来量化视觉推理中的谄媚现象，为后续研究提供了参考。\\n\\n**关键设计**：PENDULUM基准的关键设计包括：1) 图像选择：选择具有不同复杂度的图像，以评估模型在不同视觉场景下的表现；2) 问题设计：设计能够诱导模型产生谄媚性回答的问题；3) 负样本构建：构建与视觉证据相悖的负样本，以评估模型的辨别能力。此外，论文还提出了基于准确率和一致性的指标来量化模型的谄媚程度。",
            "application_zh": "该研究成果可应用于开发更可靠、更安全的MLLM，尤其是在需要高度事实准确性的场景中，例如医疗诊断、自动驾驶和金融分析。通过提高MLLM的抗谄媚能力，可以减少模型产生错误或误导性信息的风险，从而提高其在实际应用中的价值。",
            "highlight_zh": "实验结果表明，现有最先进的MLLM在PENDULUM基准上表现出显著的谄媚倾向和幻觉行为。不同模型在鲁棒性方面存在显著差异，表明模型架构和训练策略对谄媚现象有重要影响。该研究为开发抗谄媚的MLLM提供了重要的实验依据。",
            "tags_zh": [
                "多模态大语言模型",
                "谄媚现象",
                "视觉问答",
                "评估基准",
                "视觉推理",
                "事实一致性",
                "幻觉"
            ],
            "_index": 8,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
            "authors": [
                "Chenghao Li",
                "Chaoning Zhang",
                "Yi Lu",
                "Shuxu Chen",
                "Xudong Wang",
                "Jiaquan Zhang",
                "Zhicheng Wang",
                "Zhengxun Jin",
                "Kuien Liu",
                "Sung-Ho Bae",
                "Guoqing Wang",
                "Yang Yang",
                "Hen Tao Shen"
            ],
            "arxiv_id": "2512.19135v1",
            "summary": "With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19135v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "[T]chain-of-thought"
                    ],
                    "score": 18.0
                }
            ],
            "relevance_score": 18.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用拓扑数据分析理解大语言模型中的思维链",
            "summary_zh": "随着大型语言模型（LLMs）的发展，特别是长推理链技术的引入，LLMs在复杂问题解决中的推理能力得到了显著增强。虽然长推理链的能力很强，但我们不禁要问：为什么不同的推理链在推理中的表现不同？推理链的哪些组成部分起着关键作用？现有的研究主要从功能角度评估推理链，而很少关注其结构机制。为了弥补这一差距，本研究首次从结构角度分析和评估推理链的质量。我们应用拓扑数据分析（TDA）中的持久同调，将推理步骤映射到语义空间，提取拓扑特征，并分析结构变化。这些变化揭示了语义连贯性、逻辑冗余，并识别逻辑中断和差距。通过计算同调群，我们评估了不同尺度的连通性和冗余性，使用条形码和持久性图来量化稳定性和一致性。我们的结果表明，推理链的拓扑结构复杂性与准确性呈正相关。更复杂的链能更快地识别出正确的答案，而成功的推理表现出更简单的拓扑结构，减少冗余和循环，从而提高效率和可解释性。这项工作为推理链质量评估提供了一个新的视角，并为未来的优化提供了指导。",
            "intro_zh": [
                "现有研究主要从功能角度评估LLM推理链，缺乏对其内在结构机制的深入理解。",
                "该论文利用拓扑数据分析（TDA）中的持久同调，将推理步骤映射到语义空间，并提取拓扑特征。",
                "实验结果表明，推理链的拓扑结构复杂性与准确性正相关，成功的推理链具有更简单的拓扑结构。"
            ],
            "method_zh": "**问题定义**：现有方法主要关注大型语言模型推理链的功能性评估，缺乏对其结构性机制的理解。不同的推理链表现差异很大，但我们并不清楚哪些组成部分起关键作用，以及如何从结构上评估推理链的质量。\\n\\n**核心思路**：该论文的核心思路是利用拓扑数据分析（TDA）中的持久同调，将推理链中的每个步骤映射到语义空间，然后分析这些步骤在语义空间中的拓扑结构。通过分析拓扑结构，可以揭示推理链的语义连贯性、逻辑冗余以及逻辑断裂点。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1. 将推理链中的每个步骤嵌入到语义空间中（例如，使用预训练的词嵌入或句子嵌入模型）。2. 使用持久同调分析语义空间中点集的拓扑结构，计算同调群，并生成条形码和持久性图。3. 分析条形码和持久性图，提取拓扑特征，例如循环的数量、持久性等。4. 将拓扑特征与推理链的准确性进行关联分析，评估推理链的质量。\\n\\n**关键创新**：该论文的关键创新在于将拓扑数据分析应用于大型语言模型的推理链分析。这是首次从结构角度评估推理链的质量，并提供了一种新的视角来理解LLM的推理过程。与现有方法相比，该方法不仅关注推理链的功能，还关注其内在的结构特征。\\n\\n**关键设计**：论文中关键的设计包括：1. 选择合适的语义嵌入模型，将推理步骤映射到语义空间。2. 选择合适的距离度量，用于计算语义空间中点之间的距离。3. 选择合适的持久同调算法，计算同调群和生成条形码/持久性图。4. 设计合适的拓扑特征，用于评估推理链的质量，例如循环的数量、持久性等。具体的参数设置和网络结构等技术细节在论文中可能没有详细描述，需要进一步查阅论文原文。",
            "application_zh": "该研究成果可应用于提升大型语言模型的推理能力和可解释性。通过分析推理链的拓扑结构，可以识别和优化推理过程中的逻辑错误和冗余步骤，从而提高推理的准确性和效率。此外，该方法还可以用于评估不同推理策略的优劣，指导LLM的训练和优化。",
            "highlight_zh": "实验结果表明，推理链的拓扑结构复杂性与准确性呈正相关。更复杂的链能更快地识别出正确的答案，而成功的推理表现出更简单的拓扑结构，减少冗余和循环。这表明拓扑结构分析可以有效地评估推理链的质量，并为优化推理过程提供指导。",
            "tags_zh": [
                "大型语言模型",
                "思维链",
                "拓扑数据分析",
                "持久同调",
                "推理能力"
            ],
            "_index": 9,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19135v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19135v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19135v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
            "authors": [
                "Wendong Bu",
                "Kaihang Pan",
                "Yuze Lin",
                "Jiacheng Li",
                "Kai Shen",
                "Wenqiao Zhang",
                "Juncheng Li",
                "Jun Xiao",
                "Siliang Tang"
            ],
            "arxiv_id": "2512.19159v1",
            "summary": "Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19159v1",
            "code_links": [
                {
                    "url": "https://OmniMoGen.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "text-to-motion",
                        "[T]motion generation",
                        "VQ-VAE"
                    ],
                    "score": 12.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 15.5,
            "hit_pillars": [
                "4_motion_diffusion",
                "9_embodied_foundation"
            ],
            "headline_zh": "OmniMoGen：通过学习交错的文本-动作指令，统一了人体运动生成任务。",
            "summary_zh": "大型语言模型（LLMs）已在单个框架内统一了各种语言任务，但这种统一性在人体运动生成中仍未被探索。现有方法仅限于孤立的任务，限制了自由形式和全目标生成的灵活性。为了解决这个问题，我们提出了OmniMoGen，一个统一的框架，它通过交错的文本-动作指令实现多功能的运动生成。OmniMoGen建立在一个简洁的RVQ-VAE和Transformer架构之上，支持端到端的指令驱动运动生成。我们构建了一个大规模的X2Mo数据集，包含超过137K的交错文本-动作指令，并引入了AnyContext，一个用于评估交错运动生成的基准。实验表明，OmniMoGen在文本到运动、运动编辑和AnyContext上实现了最先进的性能，展现了诸如组合编辑、自我反思生成和知识驱动生成等新兴能力。这些结果标志着朝着下一代智能运动生成迈出了一步。",
            "intro_zh": [
                "现有运动生成方法局限于孤立任务，缺乏自由形式和全目标生成的灵活性。",
                "OmniMoGen通过学习交错的文本-动作指令，统一了多种运动生成任务，实现通用运动生成。",
                "OmniMoGen在文本到运动、运动编辑和AnyContext基准上取得了SOTA性能，并展现了新兴能力。"
            ],
            "method_zh": "**问题定义**：现有的人体运动生成方法通常针对特定任务设计，例如文本到运动、运动编辑等，缺乏通用性和灵活性。用户难以通过自由组合文本和动作指令来控制运动生成过程，限制了应用场景。现有方法难以处理复杂的、多目标的运动生成任务。\\n\\n**核心思路**：论文的核心思路是将各种运动生成任务统一到一个框架下，通过学习交错的文本-动作指令，使模型能够理解和生成复杂的运动序列。通过将文本和动作视为统一的输入模态，模型可以根据上下文信息生成更自然、更符合用户意图的运动。\\n\\n**技术框架**：OmniMoGen框架基于RVQ-VAE（Residual Vector Quantized Variational Autoencoder）和Transformer架构。RVQ-VAE用于将连续的运动数据离散化为码本，Transformer则用于学习文本和离散化运动码之间的关系。整个框架支持端到端的训练，可以直接根据交错的文本-动作指令生成运动序列。框架包含编码器、解码器和量化模块。\\n\\n**关键创新**：OmniMoGen的关键创新在于其统一的框架设计，能够处理多种运动生成任务。通过学习交错的文本-动作指令，模型能够理解复杂的上下文信息，并生成更符合用户意图的运动。此外，X2Mo数据集的构建和AnyContext基准的提出，为交错运动生成的研究提供了数据和评估标准。\\n\\n**关键设计**：RVQ-VAE的码本大小、Transformer的网络结构（层数、注意力头数等）、损失函数的设计（包括重构损失、量化损失等）是关键的设计细节。论文还可能采用了特定的数据增强方法来提高模型的泛化能力。具体的参数设置和网络结构需要在论文中查找。",
            "application_zh": "OmniMoGen具有广泛的应用前景，例如虚拟现实、游戏开发、动画制作、机器人控制等。它可以用于生成各种自然的人体运动，例如行走、跑步、跳跃、舞蹈等。通过结合文本指令，用户可以精确地控制运动的风格、速度、方向等。此外，OmniMoGen还可以用于运动编辑，例如修改运动的姿势、节奏等。未来，该技术有望应用于智能康复、人机交互等领域。",
            "highlight_zh": "OmniMoGen在文本到运动、运动编辑和AnyContext基准上取得了SOTA性能。在AnyContext基准上，OmniMoGen显著优于现有方法，展现了强大的交错运动生成能力。此外，OmniMoGen还展现了组合编辑、自我反思生成和知识驱动生成等新兴能力，证明了其在复杂运动生成任务上的潜力。",
            "tags_zh": [
                "人体运动生成",
                "文本到运动",
                "运动编辑",
                "交错指令学习",
                "统一框架"
            ],
            "_index": 10,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19159v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19159v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19159v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Decoupled Generative Modeling for Human-Object Interaction Synthesis",
            "authors": [
                "Hwanhee Jung",
                "Seunggwan Lee",
                "Jeongyoon Yoon",
                "SeungHyeon Kim",
                "Giljoo Nam",
                "Qixing Huang",
                "Sangpil Kim"
            ],
            "arxiv_id": "2512.19049v1",
            "summary": "Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19049v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "penetration"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]human-object interaction",
                        "HOI"
                    ],
                    "score": 10.0
                }
            ],
            "relevance_score": 14.5,
            "hit_pillars": [
                "1_robot_core",
                "4_motion_diffusion",
                "5_interaction_reaction"
            ],
            "headline_zh": "提出DecHOI，解耦路径规划与动作生成，实现逼真的人-物交互合成",
            "summary_zh": "本文提出了一种用于人-物交互（HOI）合成的解耦生成模型DecHOI，旨在解决现有方法中存在的复杂性高、灵活性差以及易出错等问题。DecHOI将路径规划和动作合成分离，首先由轨迹生成器生成人和物体的轨迹，无需预先指定中间路点；然后，动作生成器以这些轨迹为条件，合成详细的动作。为了进一步提高接触的真实感，采用了对抗训练，判别器专注于远端关节的动态。该框架还支持对移动对象的建模，并能在动态场景中进行响应式、长序列的规划，同时保持规划的一致性。在FullBodyManipulation和3D-FUTURE两个基准测试中，DecHOI在大多数定量指标和定性评估上都优于现有方法，感知研究也更倾向于我们的结果。",
            "intro_zh": [
                "现有HOI合成方法依赖手动指定的中间路点，并将所有优化目标置于单一网络，导致复杂性增加和灵活性降低。",
                "DecHOI通过解耦路径规划和动作合成，先生成轨迹再合成动作，有效解决了人和物体运动不同步或穿透等问题。",
                "通过对抗训练增强接触真实感，并支持动态场景中的长序列规划，DecHOI在多个基准测试中超越现有方法。"
            ],
            "method_zh": "**问题定义**：现有的人-物交互（HOI）合成方法通常需要手动指定中间路点，并且将所有优化目标放在一个单一的网络中进行优化。这导致了几个问题：一是增加了模型的复杂性，二是降低了模型的灵活性，三是容易产生错误，例如人和物体的运动不同步，或者发生穿透现象。这些问题限制了HOI合成的真实性和可控性。\\n\\n**核心思路**：DecHOI的核心思路是将HOI合成过程解耦为两个阶段：路径规划和动作合成。首先，使用轨迹生成器生成人和物体的运动轨迹，而无需预先指定中间路点。然后，使用动作生成器以这些轨迹为条件，生成详细的动作。这种解耦的设计使得模型更加灵活，并且可以更好地控制人和物体的运动。\\n\\n**技术框架**：DecHOI的整体框架包含两个主要模块：轨迹生成器和动作生成器。轨迹生成器负责生成人和物体的运动轨迹，它接收场景信息作为输入，并输出人和物体的三维位置和姿态序列。动作生成器以轨迹生成器的输出作为条件，生成详细的动作，包括关节角度和接触力等。为了提高接触的真实感，DecHOI还引入了一个判别器，用于区分真实的人-物交互和生成的交互。\\n\\n**关键创新**：DecHOI的关键创新在于解耦了路径规划和动作合成。这种解耦的设计使得模型更加灵活，并且可以更好地控制人和物体的运动。此外，DecHOI还引入了一个判别器，用于提高接触的真实感。这种对抗训练的方式可以有效地学习到真实的人-物交互的动态特征。\\n\\n**关键设计**：DecHOI的关键设计包括：1) 使用变分自编码器（VAE）作为轨迹生成器，以学习运动轨迹的潜在空间表示；2) 使用循环神经网络（RNN）作为动作生成器，以生成时序相关的动作序列；3) 使用对抗训练，判别器专注于远端关节的动态，以提高接触的真实感；4) 损失函数包括轨迹损失、动作损失和对抗损失，用于优化轨迹生成器、动作生成器和判别器。",
            "application_zh": "DecHOI在3D计算机视觉和机器人领域具有广泛的应用前景，例如动画制作、具身控制、虚拟现实和增强现实等。它可以用于生成逼真的人-物交互动画，提高机器人的操作能力，以及创建更加沉浸式的虚拟现实体验。此外，DecHOI还可以用于数据增强，生成更多的人-物交互数据，以训练更加鲁棒的计算机视觉模型。",
            "highlight_zh": "DecHOI在FullBodyManipulation和3D-FUTURE两个基准测试中取得了显著的性能提升。在定量指标方面，DecHOI在运动真实性、接触真实性和同步性等方面均优于现有方法。在定性评估方面，用户更倾向于DecHOI生成的人-物交互动画。感知研究表明，DecHOI能够生成更加逼真和自然的HOI。",
            "tags_zh": [
                "人-物交互合成",
                "解耦生成模型",
                "轨迹生成",
                "动作合成",
                "对抗训练"
            ],
            "_index": 11,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19049v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19049v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19049v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
            "authors": [
                "Dixuan Lin",
                "Tianyou Wang",
                "Zhuoyang Pan",
                "Yufu Wang",
                "Lingjie Liu",
                "Kostas Daniilidis"
            ],
            "arxiv_id": "2512.19684v1",
            "summary": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19684v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "scene reconstruction"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "physically plausible"
                    ],
                    "score": 2.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 13.5,
            "hit_pillars": [
                "1_robot_core",
                "3_perception_slam",
                "4_motion_diffusion",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出首个系统，从单目视频零样本重建场景内物体操作过程。",
            "summary_zh": "本文构建了首个系统，旨在解决从单目RGB视频中重建场景内物体操作的问题。由于场景重建的不适定性、手-物深度歧义以及对物理上合理交互的需求，这项任务极具挑战性。现有方法通常在以手为中心的坐标系中操作，忽略了场景信息，从而限制了度量精度和实际应用。我们的方法首先利用数据驱动的基础模型来初始化核心组件，包括物体网格和姿态、场景点云以及手部姿态。然后，我们应用一个两阶段优化，从抓取到交互，恢复完整的手-物运动，并使其与输入视频中观察到的场景信息保持一致。",
            "intro_zh": [
                "现有方法在重建场景内物体操作时，忽略场景信息，导致度量精度不足，限制了实际应用。",
                "该方法利用数据驱动的基础模型初始化物体、场景和手部姿态，并通过两阶段优化恢复手-物交互运动。",
                "该系统是首个能够从单目RGB视频中零样本重建场景内物体操作的系统，具有重要的研究意义。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从单目RGB视频中重建场景内物体操作的问题。现有方法主要以手为中心，忽略了场景信息，导致重建精度不高，难以应用于实际场景。手-物深度歧义和物理合理性约束也增加了重建的难度。\\n\\n**核心思路**：论文的核心思路是利用数据驱动的基础模型来初始化场景、物体和手部的姿态，然后通过优化方法来恢复手-物交互的完整运动轨迹。通过将手-物交互与场景信息对齐，可以提高重建的精度和物理合理性。\\n\\n**技术框架**：该方法主要包含两个阶段：初始化阶段和优化阶段。在初始化阶段，利用数据驱动的基础模型（如预训练的3D物体检测器、场景重建模型和手部姿态估计器）来初始化物体网格和姿态、场景点云以及手部姿态。在优化阶段，采用两阶段优化策略，首先优化手部姿态和物体姿态，然后优化手-物交互的运动轨迹，同时考虑场景约束和物理合理性约束。\\n\\n**关键创新**：该方法最重要的创新点在于将数据驱动的基础模型与优化方法相结合，实现了从单目视频中零样本重建场景内物体操作。与现有方法相比，该方法能够更好地利用场景信息，提高重建的精度和物理合理性。\\n\\n**关键设计**：在初始化阶段，使用了预训练的3D物体检测器来检测物体并估计其初始姿态。在优化阶段，设计了专门的损失函数来约束手部姿态、物体姿态和手-物交互的运动轨迹，同时考虑了场景约束和物理合理性约束。具体的损失函数可能包括：手部姿态的先验损失、物体姿态的先验损失、手-物交互的接触损失、场景一致性损失和物理合理性损失。",
            "application_zh": "该研究成果可应用于机器人操作、人机交互、虚拟现实/增强现实等领域。例如，机器人可以利用该技术理解人类的物体操作行为，从而更好地完成任务。在人机交互中，该技术可以用于捕捉用户的手部动作，实现更自然的人机交互。在VR/AR中，可以用于重建虚拟场景中的物体操作过程，增强用户的沉浸感。",
            "highlight_zh": "该论文提出了首个能够从单目RGB视频中零样本重建场景内物体操作的系统。通过结合数据驱动的基础模型和优化方法，该方法能够有效地利用场景信息，提高重建的精度和物理合理性。虽然论文中没有给出具体的性能数据，但其创新性在于解决了现有方法忽略场景信息的问题，为相关领域的研究提供了新的思路。",
            "tags_zh": [
                "物体操作重建",
                "单目视频",
                "零样本学习",
                "场景理解",
                "手部姿态估计"
            ],
            "_index": 12,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19684v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19684v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19684v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
            "authors": [
                "Martin Sedlacek",
                "Pavlo Yefanov",
                "Georgy Ponimatkin",
                "Jai Bardhan",
                "Simon Pilc",
                "Mederic Fourmy",
                "Evangelos Kazakos",
                "Cees G. M. Snoek",
                "Josef Sivic",
                "Vladimir Petrik"
            ],
            "arxiv_id": "2512.19562v1",
            "summary": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "9 pages, 10 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19562v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "vision-language-action",
                        "VLA"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "REALM：用于机器人操作泛化能力的真实-模拟验证基准",
            "summary_zh": "视觉-语言-动作（VLA）模型使机器人能够理解和执行自然语言指令描述的任务。然而，一个关键挑战在于它们泛化到训练环境之外的能力，这在现实世界中评估既困难又昂贵。为了解决这个问题，我们提出了REALM，一个新的模拟环境和基准，旨在评估VLA模型的泛化能力，特别强调通过高保真视觉效果和对齐的机器人控制，在模拟和真实世界性能之间建立强大的相关性。我们的环境提供了15个扰动因子、7个操作技能和超过3500个对象。最后，我们建立了两个任务集作为基准，并评估了π_{0}、π_{0}-FAST和GR00T N1.5 VLA模型，表明泛化和鲁棒性仍然是一个开放的挑战。更广泛地说，我们还表明，模拟为我们提供了现实世界的宝贵代理，并允许我们系统地探测和量化VLA的弱点和失败模式。",
            "intro_zh": [
                "现有VLA模型在真实机器人操作任务中泛化能力不足，且真实环境评估成本高昂。",
                "REALM通过高保真模拟环境和对齐的机器人控制，建立模拟与真实世界性能的强相关性。",
                "实验评估了多个VLA模型在REALM基准上的泛化能力，揭示了现有模型的局限性。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言-动作（VLA）模型在机器人操作任务中，难以泛化到未见过的环境和物体。在真实环境中评估这些模型的泛化能力成本高昂，且难以系统性地控制各种扰动因素。因此，需要一个能够有效评估VLA模型泛化能力的基准，并且该基准需要与真实世界具有较强的相关性。\\n\\n**核心思路**：REALM的核心思路是构建一个高保真度的模拟环境，该环境能够尽可能地模拟真实世界的视觉效果和机器人控制。通过精心设计的扰动因子和多样化的物体，来模拟真实世界中可能遇到的各种情况，从而评估VLA模型的泛化能力。同时，通过实验验证模拟环境与真实世界性能的相关性，确保评估结果的有效性。\\n\\n**技术框架**：REALM包含以下几个主要组成部分：1）一个基于模拟器的环境，提供高保真度的视觉效果和物理模拟；2）一个包含15个扰动因子的集合，用于模拟真实世界中的各种干扰；3）一个包含7个操作技能的集合，用于评估VLA模型在不同任务上的表现；4）一个包含超过3500个物体的集合，用于增加环境的多样性；5）两个任务集，用于构成基准，评估VLA模型的泛化能力。\\n\\n**关键创新**：REALM的关键创新在于其高保真度的模拟环境和与真实世界性能的强相关性。通过精心设计的扰动因子和多样化的物体，REALM能够有效地模拟真实世界中的各种情况，从而评估VLA模型的泛化能力。此外，REALM还提供了一个标准化的基准，方便研究人员比较不同VLA模型的性能。\\n\\n**关键设计**：REALM的关键设计包括：1）使用高分辨率的纹理和光照模型，以提高视觉效果的逼真度；2）使用精确的物理引擎，以模拟真实的机器人控制；3）精心选择15个扰动因子，包括光照变化、物体位置变化、噪声等；4）使用多样化的物体集合，包括不同形状、大小和材质的物体；5）设计两个任务集，分别评估VLA模型在不同难度级别上的泛化能力。",
            "application_zh": "REALM可用于评估和改进视觉-语言-动作模型的泛化能力，推动机器人技术在复杂和动态环境中的应用。例如，在家庭服务机器人、工业自动化、自动驾驶等领域，REALM可以帮助开发更鲁棒和可靠的机器人系统。该基准还有助于研究人员更好地理解VLA模型的弱点，并开发更有效的训练方法。",
            "highlight_zh": "论文评估了π_{0}、π_{0}-FAST和GR00T N1.5等VLA模型在REALM基准上的性能，结果表明现有模型在泛化能力和鲁棒性方面仍有提升空间。实验还验证了模拟环境与真实世界性能之间的相关性，表明REALM可以作为评估VLA模型泛化能力的有效代理。",
            "tags_zh": [
                "机器人操作",
                "视觉-语言-动作模型",
                "泛化能力",
                "模拟环境",
                "基准测试"
            ],
            "_index": 13,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19562v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19562v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19562v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors",
            "authors": [
                "Jose Gustavo Buenaventura Carreon",
                "Floris Erich",
                "Roman Mykhailyshyn",
                "Tomohiro Motoda",
                "Ryo Hanai",
                "Yukiyasu Domae"
            ],
            "arxiv_id": "2512.19148v1",
            "summary": "We present a cross robot visuomotor learning framework that integrates diffusion policy based control with 3D semantic scene representations from D3Fields to enable category level generalization in manipulation. Its modular design supports diverse robot camera configurations including UR5 arms with Microsoft Azure Kinect arrays and bimanual manipulators with Intel RealSense sensors through a low latency control stack and intuitive teleoperation. A unified configuration layer enables seamless switching between setups for flexible data collection training and evaluation. In a grasp and lift block task the framework achieved an 80 percent success rate after only 100 demonstration episodes demonstrating robust skill transfer between platforms and sensing modalities. This design paves the way for scalable real world studies in cross robotic generalization.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "6 pages, 7 figures, conference: SII 2026. Cancun, Mexico",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19148v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "bi-manual",
                        "teleoperation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]policy learning",
                        "diffusion policy"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出基于场信息的柔性策略学习框架，实现跨机器人和传感器的操作技能泛化",
            "summary_zh": "本文提出了一种跨机器人视觉运动学习框架，该框架集成了基于扩散策略的控制和来自D3Fields的3D语义场景表示，以实现操作中的类别级别泛化。其模块化设计支持多种机器人相机配置，包括带有Microsoft Azure Kinect阵列的UR5机械臂和带有Intel RealSense传感器的双臂机械手，通过低延迟控制堆栈和直观的遥操作实现。统一的配置层实现了设置之间的无缝切换，从而实现灵活的数据收集、训练和评估。在抓取和抬起块的任务中，该框架在仅100次演示后就达到了80%的成功率，证明了平台和传感模式之间强大的技能迁移能力。这种设计为跨机器人泛化中的可扩展现实世界研究铺平了道路。",
            "intro_zh": [
                "现有机器人视觉运动学习方法难以在不同机器人平台和传感器配置之间泛化，限制了其在实际场景中的应用。",
                "该论文提出了一种基于场信息的柔性策略学习框架，利用扩散策略和3D语义场景表示实现跨平台和传感器的技能迁移。",
                "实验表明，该框架在抓取和抬起块的任务中，仅需少量演示即可达到较高的成功率，验证了其泛化能力。"
            ],
            "method_zh": "**问题定义**：现有机器人视觉运动学习方法通常针对特定机器人平台和传感器设计，难以适应不同硬件配置，导致模型泛化能力差，需要大量特定平台的数据进行训练。这限制了机器人技术在复杂和多样化环境中的应用。\\n\\n**核心思路**：该论文的核心思路是将扩散策略控制与3D语义场景表示相结合，利用D3Fields提取场景的语义信息，并将其作为扩散策略的条件，从而实现对不同机器人和传感器配置的解耦。通过这种方式，模型可以学习到与具体平台无关的操作技能，从而实现跨平台泛化。\\n\\n**技术框架**：该框架包含以下主要模块：1) 3D语义场景表示模块（D3Fields），用于提取场景的语义信息；2) 扩散策略控制模块，用于生成机器人的控制指令；3) 低延迟控制堆栈，用于实现机器人运动的实时控制；4) 统一配置层，用于实现不同机器人和传感器配置之间的无缝切换。整个流程包括数据收集、模型训练和策略执行三个阶段。\\n\\n**关键创新**：该论文的关键创新在于将扩散策略控制与3D语义场景表示相结合，实现了一种柔性的策略学习框架，能够有效地解决跨机器人和传感器配置的泛化问题。与传统的基于图像或点云的视觉运动学习方法相比，该方法能够更好地利用场景的语义信息，从而提高模型的泛化能力。\\n\\n**关键设计**：D3Fields使用体素网格来表示3D场景，并为每个体素分配一个语义标签。扩散策略控制模块使用Transformer网络来学习从场景表示到机器人控制指令的映射。损失函数包括重构损失和策略损失，用于优化场景表示和控制策略。低延迟控制堆栈采用PID控制算法，确保机器人运动的实时性和稳定性。",
            "application_zh": "该研究成果可应用于各种机器人操作任务，例如工业自动化、家庭服务机器人、医疗机器人等。通过该框架，可以快速地将操作技能迁移到不同的机器人平台和传感器配置上，从而降低开发成本，提高机器人的适应性和智能化水平。未来，该技术有望推动机器人技术在更广泛领域的应用。",
            "highlight_zh": "在抓取和抬起块的任务中，该框架在仅100次演示后就达到了80%的成功率。这表明该框架具有强大的技能迁移能力，能够在不同平台和传感模式之间实现有效的知识共享。与传统的视觉运动学习方法相比，该框架能够显著减少所需的训练数据量，并提高模型的泛化能力。",
            "tags_zh": [
                "机器人操作",
                "视觉运动学习",
                "扩散策略",
                "3D语义场景表示",
                "跨机器人泛化"
            ],
            "_index": 14,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19148v1/figures/Picture1.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19148v1/figures/Picture2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19148v1/figures/Diagram2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
            "authors": [
                "Xu Liu",
                "Yu Liu",
                "Hanshuo Qiu",
                "Yang Qirong",
                "Zhouhui Lian"
            ],
            "arxiv_id": "2512.19024v1",
            "summary": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19024v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "embodied AI",
                        "VLA",
                        "VLN",
                        "multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "IndoorUAV：提出室内无人机视觉-语言导航基准与方法，填补相关研究空白。",
            "summary_zh": "视觉-语言导航（VLN）使智能体能够通过遵循自然语言指令并在视觉观察的基础上在复杂环境中导航。虽然现有工作主要集中在地面机器人或室外无人机（UAV）上，但基于室内无人机的VLN仍未得到充分探索，尽管它与现实世界的应用（如密闭空间内的检查、交付和搜索救援）相关。为了弥合这一差距，我们引入了\textbf{IndoorUAV}，这是一个专门为室内无人机VLN量身定制的新基准和方法。我们首先从Habitat模拟器中整理了1000多个多样且结构丰富的3D室内场景。在这些环境中，我们模拟了真实的无人机飞行动态，以手动收集各种3D导航轨迹，并通过数据增强技术进一步丰富。此外，我们设计了一个自动注释流水线，为每个轨迹生成不同粒度的自然语言指令。这个过程产生了超过16,000条高质量轨迹，构成了专注于长时程VLN的\textbf{IndoorUAV-VLN}子集。为了支持短时程规划，我们通过选择语义上显著的关键帧并重新生成简洁的指令，将长轨迹分割成子轨迹，形成了\textbf{IndoorUAV-VLA}子集。最后，我们引入了\textbf{IndoorUAV-Agent}，这是一个专为我们的基准设计的导航模型，利用了任务分解和多模态推理。我们希望IndoorUAV能够成为一个宝贵的资源，以推进室内空中导航领域中视觉-语言具身人工智能的研究。",
            "intro_zh": [
                "现有VLN研究主要集中于地面机器人或室外无人机，缺乏对室内无人机VLN的深入探索，限制了其在实际场景中的应用。",
                "论文提出IndoorUAV基准，包含多样化的3D室内场景和高质量的视觉-语言导航轨迹，并设计了IndoorUAV-Agent导航模型。",
                "IndoorUAV基准包含长时程VLN和短时程VLA两个子集，并利用任务分解和多模态推理提升导航性能。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言导航（VLN）研究主要集中在地面机器人和室外无人机，忽略了室内无人机VLN这一重要方向。室内无人机在狭小空间内的导航具有独特的挑战，例如复杂的环境结构、有限的视野和精确的控制需求。缺乏专门的基准数据集和针对性模型阻碍了该领域的发展。\\n\\n**核心思路**：论文的核心思路是构建一个高质量的室内无人机VLN基准数据集，并设计一个能够有效利用视觉和语言信息进行导航的智能体。通过模拟真实的无人机飞行动态和自动生成自然语言指令，IndoorUAV数据集能够反映室内无人机导航的实际挑战。IndoorUAV-Agent模型则通过任务分解和多模态推理来提升导航性能。\\n\\n**技术框架**：IndoorUAV包含两个子集：IndoorUAV-VLN和IndoorUAV-VLA。IndoorUAV-VLN专注于长时程VLN，包含超过16,000条高质量轨迹。IndoorUAV-VLA则通过分割长轨迹并重新生成指令，支持短时程规划。IndoorUAV-Agent模型采用任务分解策略，将导航任务分解为多个子任务，例如路径规划、姿态控制和避障。模型利用多模态信息融合技术，将视觉信息和语言指令进行有效整合，从而实现精确的导航。\\n\\n**关键创新**：论文的关键创新在于构建了首个专门针对室内无人机VLN的基准数据集IndoorUAV。该数据集包含多样化的3D室内场景、真实的无人机飞行动态和高质量的自然语言指令。此外，论文提出的IndoorUAV-Agent模型采用任务分解和多模态推理策略，能够有效应对室内无人机导航的挑战。\\n\\n**关键设计**：IndoorUAV数据集的构建过程中，采用了数据增强技术来增加数据的多样性。自动注释流水线能够生成不同粒度的自然语言指令。IndoorUAV-Agent模型采用了深度强化学习算法进行训练，并设计了专门的奖励函数来鼓励智能体完成导航任务。具体的网络结构和参数设置在论文中有详细描述（未知）。",
            "application_zh": "该研究成果可应用于室内无人机的多种实际场景，例如建筑物内部的巡检、仓库内的货物盘点、灾难现场的搜索救援等。通过结合视觉和语言信息，无人机能够更智能地理解人类指令，并在复杂环境中自主导航，从而提高工作效率和安全性。未来，该技术有望在智慧城市、智能家居等领域发挥重要作用。",
            "highlight_zh": "论文构建了包含超过16,000条轨迹的IndoorUAV-VLN数据集，并提出了IndoorUAV-Agent导航模型。实验结果表明，IndoorUAV-Agent在IndoorUAV基准上取得了显著的性能提升（具体数值未知），验证了任务分解和多模态推理策略的有效性。该基准的发布将促进室内无人机视觉-语言导航领域的研究。",
            "tags_zh": [
                "室内无人机",
                "视觉-语言导航",
                "基准数据集",
                "任务分解",
                "多模态融合"
            ],
            "_index": 15,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19024v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19024v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19024v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
            "authors": [
                "Niclas Griesshaber",
                "Jochen Streb"
            ],
            "arxiv_id": "2512.19675v1",
            "summary": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",
            "categories": [
                "econ.GN",
                "cs.CV",
                "cs.DL"
            ],
            "primary_category": "econ.GN",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19675v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用多模态LLM从档案图像扫描件构建德国专利历史数据集",
            "summary_zh": "本文利用多模态大型语言模型（LLM），具体而言是Gemini-2.5-Pro和Gemini-2.5-Flash-Lite，构建了一个包含306,070项德国专利（1877-1918）的数据集，数据来源于9,562份档案图像扫描件。基准测试初步表明，多模态LLM能够创建比研究助理更高质量的数据集，同时在从图像语料库构建专利数据集时，速度提高了795倍以上，成本降低了205倍。每页嵌入约20到50个专利条目，以双栏格式排列，并以哥特体和罗马字体印刷。主要来源材料的字体和布局复杂性表明，多模态LLM是经济史中数据集构建方式的范式转变。我们开源了基准测试和专利数据集以及基于LLM的数据管道，该管道可以使用LLM辅助编码工具轻松地适应其他图像语料库，从而降低了技术水平较低的研究人员的门槛。最后，我们解释了部署LLM进行历史数据集构建的经济学原理，并推测了对经济史领域的潜在影响。",
            "intro_zh": [
                "经济史研究中，历史档案图像数据蕴含丰富信息，但人工标注成本高、效率低，阻碍了大规模数据集的构建。",
                "论文提出了一种基于多模态LLM的自动化数据管道，利用LLM强大的图像理解和文本处理能力，高效提取专利信息。",
                "实验表明，该方法在数据集构建速度和成本上远超人工，且数据质量更高，为经济史研究提供了一种新的数据获取途径。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从大量历史档案图像扫描件中高效、低成本地提取结构化专利数据的问题。现有方法依赖人工标注，耗时耗力，难以处理大规模数据集。此外，历史文档的字体复杂、排版不规范，进一步增加了人工标注的难度。\\n\\n**核心思路**：论文的核心思路是利用多模态LLM的强大能力，将图像理解和文本处理相结合，自动化地从图像中提取专利信息。通过训练LLM识别历史文档的布局、字体和内容，实现高效的数据抽取和结构化。\\n\\n**技术框架**：论文构建了一个基于LLM的数据管道，主要包含以下几个阶段：1) 图像预处理：对档案图像进行清洗、校正等预处理操作，提高图像质量。2) LLM推理：使用多模态LLM（Gemini-2.5-Pro和Gemini-2.5-Flash-Lite）对图像进行分析，识别专利条目，提取关键信息。3) 数据结构化：将提取的信息进行结构化处理，构建专利数据集。4) 数据验证：对提取的数据进行验证和校对，确保数据质量。\\n\\n**关键创新**：论文的关键创新在于将多模态LLM应用于历史档案图像数据的处理，实现了自动化、高效的数据集构建。与传统OCR方法相比，LLM具有更强的图像理解和文本处理能力，能够更好地处理复杂字体和排版。此外，论文还开源了数据管道和数据集，降低了其他研究人员的使用门槛。\\n\\n**关键设计**：论文使用了Gemini-2.5-Pro和Gemini-2.5-Flash-Lite两种LLM模型，并针对历史文档的特点进行了微调。具体的技术细节，例如损失函数、网络结构等，论文中没有详细描述，属于未知信息。",
            "application_zh": "该研究成果可广泛应用于历史文献数字化、档案管理、知识图谱构建等领域。通过自动化提取历史文献中的信息，可以加速历史研究进程，为经济史、社会史等领域提供更丰富的数据支持。此外，该方法还可以应用于其他类型的图像数据处理，例如医学影像分析、遥感图像解译等。",
            "highlight_zh": "实验结果表明，基于多模态LLM的数据管道在构建德国专利数据集时，速度比人工提高了795倍以上，成本降低了205倍，并且数据质量更高。这些数据充分证明了多模态LLM在历史数据集构建方面的巨大潜力。",
            "tags_zh": [
                "多模态学习",
                "大型语言模型",
                "历史文档",
                "数据集构建",
                "图像识别",
                "经济史",
                "专利数据"
            ],
            "_index": 16,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19675v1/figure-representative-page.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19675v1/figure-patent-entry.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19675v1/figure-RA-interface.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "4D Gaussian Splatting as a Learned Dynamical System",
            "authors": [
                "Arnold Caleb Asiimwe",
                "Carl Vondrick"
            ],
            "arxiv_id": "2512.19648v1",
            "summary": "We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19648v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting",
                        "[T]splatting"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "EvoGS：将4D高斯溅射重构为可学习的动态系统，实现时序一致性动态场景建模",
            "summary_zh": "本文将4D高斯溅射重新解释为一个连续时间的动态系统，其中场景运动源于对学习到的神经动态场的积分，而不是应用逐帧形变。这种被称为EvoGS的公式将高斯表示视为一个演化的物理系统，其状态在学习到的运动规律下连续演变。这解锁了基于形变的方法所不具备的能力：（1）通过对底层运动规律建模，实现从稀疏时间监督中进行样本高效学习；（2）时间外推，能够进行超出观察时间范围的前向和后向预测；（3）组合动力学，允许局部动力学注入以实现可控的场景合成。在动态场景基准上的实验表明，EvoGS相比于形变场基线，实现了更好的运动连贯性和时间一致性，同时保持了实时渲染。",
            "intro_zh": [
                "现有基于形变的4D高斯溅射方法在处理复杂动态场景时，存在运动不连贯和时间一致性差的问题。",
                "EvoGS将4D高斯溅射视为连续时间动态系统，通过学习神经动态场来模拟场景运动，实现更自然的运动建模。",
                "实验表明，EvoGS在动态场景建模中，相比形变场基线，显著提升了运动连贯性和时间一致性，并支持时间外推和可控场景合成。"
            ],
            "method_zh": "**问题定义**：现有4D高斯溅射方法通常采用逐帧形变来模拟动态场景，这种方法在处理复杂运动时容易出现运动不连贯和时间一致性问题。尤其是在稀疏时间监督下，形变场的泛化能力有限，难以准确预测未观测时间点的场景状态。此外，基于形变的方法难以实现对场景运动的精细控制和时间外推。\n\n**核心思路**：EvoGS的核心思想是将4D高斯溅射视为一个连续时间动态系统，场景中的高斯粒子不再是简单地逐帧形变，而是根据学习到的神经动态场进行连续演化。通过对底层运动规律进行建模，EvoGS能够更好地捕捉场景的动态特性，从而提高运动连贯性和时间一致性。\n\n**技术框架**：EvoGS的整体框架包括以下几个主要模块：1) 高斯粒子初始化：在初始时刻，使用一组高斯粒子来表示场景。2) 神经动态场学习：使用神经网络学习一个神经动态场，该场描述了高斯粒子在每个位置的速度和加速度。3) 动态系统积分：使用数值积分方法，根据学习到的神经动态场，对高斯粒子的状态进行连续时间演化。4) 渲染：在每个时间点，使用演化后的高斯粒子进行渲染，生成场景图像。\n\n**关键创新**：EvoGS最重要的技术创新在于将4D高斯溅射与连续时间动态系统相结合。与传统的基于形变的方法相比，EvoGS能够直接学习场景的运动规律，从而更好地捕捉场景的动态特性。此外，EvoGS还支持时间外推和可控场景合成，这为动态场景建模提供了更大的灵活性。\n\n**关键设计**：EvoGS的关键设计包括：1) 神经动态场的网络结构：可以使用MLP或其他神经网络结构来表示神经动态场。2) 损失函数：可以使用图像重建损失、运动平滑损失等来训练神经动态场。3) 数值积分方法：可以使用欧拉法、龙格-库塔法等数值积分方法来对高斯粒子的状态进行演化。4) 局部动力学注入：通过修改特定区域的神经动态场，可以实现对场景运动的局部控制。",
            "application_zh": "EvoGS具有广泛的应用前景，例如：视频编辑、虚拟现实、增强现实、机器人导航等。它可以用于生成高质量的动态场景，实现逼真的虚拟体验。此外，EvoGS还可以用于预测未来场景状态，为机器人导航和决策提供支持。通过对场景运动进行精细控制，EvoGS还可以用于创建各种特效和动画。",
            "highlight_zh": "EvoGS在动态场景基准测试中表现出色，相比于基于形变场的基线方法，在运动连贯性和时间一致性方面取得了显著提升。实验结果表明，EvoGS能够更好地捕捉场景的动态特性，生成更逼真的动态场景。此外，EvoGS还支持时间外推和可控场景合成，这为动态场景建模提供了更大的灵活性。",
            "tags_zh": [
                "4D高斯溅射",
                "动态场景建模",
                "神经动态场",
                "连续时间动态系统",
                "时间外推",
                "可控场景合成",
                "运动连贯性",
                "时间一致性"
            ],
            "_index": 17,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19648v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19648v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19648v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
            "authors": [
                "Zhenyang Huang",
                "Xiao Yu",
                "Yi Zhang",
                "Decheng Wang",
                "Hang Ruan"
            ],
            "arxiv_id": "2512.19354v1",
            "summary": "Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19354v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "ReasonCD：提出多模态推理大模型，用于遥感图像隐式兴趣变更语义挖掘",
            "summary_zh": "遥感图像变化检测是遥感智能解译中的一项基础任务，其核心目标是识别感兴趣的变化区域（CRoI）内的变化。目前的多模态大模型编码了丰富的人类语义知识，可用于指导遥感变化检测等任务。然而，现有方法在利用语义指导检测用户的CRoI时，过度依赖于CRoI的显式文本描述，导致在面对隐式CRoI文本描述时，性能几乎完全失效。本文提出了一种名为ReasonCD的多模态推理变化检测模型，该模型能够挖掘用户的隐式任务意图。该模型利用预训练大语言模型的强大推理能力来挖掘用户的隐式任务意图，并根据这些意图获得不同的变化检测结果。在公共数据集上的实验表明，该模型实现了出色的变化检测性能，在BCDD数据集上的F1得分为92.1%。此外，为了验证其卓越的推理功能，本文基于SECOND数据集标注了一个推理数据子集。实验结果表明，该模型不仅擅长基于基本推理的变化检测任务，还可以解释推理过程，以辅助人类决策。",
            "intro_zh": [
                "现有遥感变化检测方法依赖显式文本描述，无法处理隐式兴趣变更语义，导致性能下降。",
                "ReasonCD模型利用预训练大语言模型的推理能力，挖掘用户隐式意图，指导变化检测。",
                "实验表明，ReasonCD在BCDD数据集上F1值达92.1%，并能解释推理过程辅助决策。"
            ],
            "method_zh": "**问题定义**：遥感图像变化检测旨在识别图像中发生变化的区域。现有方法依赖于对感兴趣变化区域（CRoI）的显式文本描述，当用户提供的是隐式或模糊的描述时，这些方法无法准确理解用户的意图，导致检测性能显著下降。这种对显式信息的过度依赖是现有方法的痛点。\\n\\n**核心思路**：ReasonCD的核心思路是利用预训练大语言模型（LLM）强大的推理能力，从用户的隐式文本描述中挖掘出其真实的意图。通过理解用户意图，模型可以更准确地识别出用户真正感兴趣的变化区域，从而提高变化检测的准确性。这种方法避免了对显式文本描述的过度依赖。\\n\\n**技术框架**：ReasonCD模型主要包含以下几个关键模块：1) 多模态输入编码器：用于提取遥感图像和文本描述的特征。2) 大语言模型推理模块：利用预训练的LLM，根据图像特征和文本描述，推理出用户的隐式意图。3) 变化检测模块：根据LLM推理出的用户意图，对图像进行变化检测，生成变化区域的掩码。整体流程是先进行多模态特征提取，然后利用LLM进行意图推理，最后根据推理结果进行变化检测。\\n\\n**关键创新**：ReasonCD的关键创新在于将预训练大语言模型的推理能力引入到遥感图像变化检测任务中。与现有方法不同，ReasonCD能够处理隐式文本描述，挖掘用户的真实意图，从而实现更准确的变化检测。这种基于推理的方法是与现有方法最本质的区别。\\n\\n**关键设计**：ReasonCD的关键设计包括：1) 使用预训练的LLM作为推理模块，充分利用其强大的语义理解和推理能力。2) 设计了合适的提示工程（Prompt Engineering），引导LLM进行意图推理。3) 针对遥感图像变化检测任务，对LLM进行了微调，以提高其在该任务上的性能。损失函数方面，可能采用了交叉熵损失或Dice损失等，以优化变化检测的准确性。具体的网络结构细节和参数设置需要在论文中进一步查找。",
            "application_zh": "ReasonCD可应用于灾害评估、城市规划、环境监测等领域。通过理解用户的隐式需求，该模型能够更准确地识别出感兴趣的变化区域，为决策者提供更可靠的信息支持。未来，该模型有望应用于更广泛的遥感图像分析任务，例如目标检测、图像分割等，提升遥感智能解译的自动化水平。",
            "highlight_zh": "ReasonCD在BCDD数据集上取得了92.1%的F1分数，证明了其出色的变化检测性能。更重要的是，该模型在基于SECOND数据集构建的推理数据集上表现出色，不仅能完成推理任务，还能解释推理过程，为人类决策提供依据。这表明ReasonCD具有强大的推理能力和实用价值。",
            "tags_zh": [
                "遥感图像变化检测",
                "多模态推理",
                "大语言模型",
                "隐式语义挖掘",
                "预训练模型"
            ],
            "_index": 18,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19354v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19354v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19354v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
            "authors": [
                "Marios Thoma",
                "Zenonas Theodosiou",
                "Harris Partaourides",
                "Vassilis Vassiliades",
                "Loizos Michael",
                "Andreas Lanitis"
            ],
            "arxiv_id": "2512.19190v1",
            "summary": "Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "24 pages, 7 figures, 9 tables, Dataset: https://doi.org/10.5281/zenodo.10907945, Code: https://github.com/CYENS/PEDESTRIAN",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19190v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric",
                        "[T]egocentric vision"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "6_video_extraction"
            ],
            "headline_zh": "提出行人视角障碍物检测数据集PEDESTRIAN，用于提升城市人行道安全。",
            "summary_zh": "步行是主要的交通方式，对保持健康至关重要。然而，城市人行道经常被各种障碍物阻碍，影响行人自由通行，构成安全隐患。普适计算和第一人称视角技术的发展为设计能够自动实时检测这些障碍物的系统提供了可能，从而提高行人安全。有效识别算法的开发依赖于全面且均衡的第一人称视角数据集。本文介绍了PEDESTRIAN数据集，其中包含29种城市人行道上常见的障碍物的第一人称视角数据。使用手机摄像头收集了总共340个视频，捕捉了行人的视角。此外，我们还展示了一系列实验的结果，这些实验使用提出的数据集训练了几种最先进的深度学习算法，这些算法可以用作障碍物检测和识别任务的基准。",
            "intro_zh": [
                "城市人行道障碍物阻碍行人通行，存在安全隐患，但缺乏有效的第一人称视角障碍物检测方法。",
                "论文构建了包含29种常见障碍物的PEDESTRIAN数据集，利用行人视角视频数据进行障碍物检测。",
                "实验结果表明，该数据集可用于训练深度学习算法，为行人安全提供基准。"
            ],
            "method_zh": "**问题定义**：论文旨在解决城市人行道上行人面临的障碍物检测问题。现有方法缺乏针对行人第一人称视角的有效数据集，难以训练出鲁棒的障碍物检测模型。这些障碍物可能对行人安全构成威胁，尤其对于弱势群体，如老年人和残疾人。\\n\\n**核心思路**：论文的核心思路是构建一个大规模、多样化的行人第一人称视角数据集，用于训练和评估障碍物检测算法。通过提供真实场景下的行人视角数据，可以使模型更好地学习到障碍物的特征，从而提高检测的准确性和鲁棒性。\\n\\n**技术框架**：该研究主要围绕数据集的构建和实验展开。首先，使用手机摄像头采集了340个视频，覆盖29种常见的城市人行道障碍物。然后，使用这些数据训练了多种最先进的深度学习算法，并评估了它们的性能。整个流程包括数据采集、数据标注、模型训练和性能评估四个主要阶段。\\n\\n**关键创新**：该论文的关键创新在于构建了PEDESTRIAN数据集，这是一个专门针对行人第一人称视角障碍物检测的数据集。与现有的通用目标检测数据集不同，PEDESTRIAN数据集更加关注行人视角下的障碍物，并且包含了多种常见的城市人行道障碍物，更贴合实际应用场景。\\n\\n**关键设计**：数据集包含了340个视频，覆盖了29种不同的障碍物。视频数据使用手机摄像头采集，模拟了真实的行人视角。论文没有详细说明具体的模型参数设置、损失函数或网络结构，而是将数据集作为基准，供研究者使用不同的模型进行训练和评估。未来的研究可以探索不同的模型结构和训练策略，以进一步提高障碍物检测的性能。",
            "application_zh": "该研究成果可应用于开发智能辅助系统，例如为视力障碍人士提供导航辅助，或为自动驾驶轮椅提供障碍物规避功能。通过实时检测人行道上的障碍物，可以显著提高行人的安全性，尤其是在城市环境中。未来，结合可穿戴设备和边缘计算技术，可以实现更加便捷和高效的行人安全保障系统。",
            "highlight_zh": "论文通过实验验证了PEDESTRIAN数据集的有效性，使用该数据集训练的深度学习算法可以实现较好的障碍物检测性能。虽然论文没有给出具体的性能指标，但强调了该数据集可以作为障碍物检测和识别任务的基准。未来的研究可以基于该数据集，进一步优化模型结构和训练策略，以提高检测精度和鲁棒性。",
            "tags_zh": [
                "行人安全",
                "障碍物检测",
                "第一人称视角",
                "数据集",
                "深度学习"
            ],
            "_index": 19,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19190v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19190v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19190v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting",
            "authors": [
                "Tiantian Li",
                "Xinjie Zhang",
                "Xingtong Ge",
                "Tongda Xu",
                "Dailan He",
                "Jun Zhang",
                "Yan Wang"
            ],
            "arxiv_id": "2512.19108v1",
            "summary": "Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Accepted to AAAI 2026.Code URL:https://github.com/Sweethyh/GaussianImage_plus.git",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19108v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]gaussian splatting",
                        "[T]splatting"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "GaussianImage++：利用2D高斯溅射增强图像表示与压缩性能",
            "summary_zh": "隐式神经表示(INRs)在图像表示和压缩方面取得了显著成功，但它们需要大量的训练时间和内存。同时，最近的2D高斯溅射(GS)方法(例如，GaussianImage)通过高效的基于图元的渲染提供了有希望的替代方案。然而，这些方法需要过多的高斯图元来维持高视觉保真度。为了挖掘基于GS的方法的潜力，我们提出了GaussianImage++，它利用有限的高斯图元来实现令人印象深刻的表示和压缩性能。首先，我们引入了一种失真驱动的密集化机制，它根据信号强度逐步分配高斯图元。其次，我们为每个图元采用上下文感知高斯滤波器，这有助于密集化，以基于不同的图像内容优化高斯图元。第三，我们集成了属性分离的可学习标量量化器和量化感知训练，从而能够高效地压缩图元属性。实验结果表明了我们方法的有效性。特别是，GaussianImage++在表示和压缩性能方面优于GaussianImage和基于INRs的COIN，同时保持了实时解码和低内存使用。",
            "intro_zh": [
                "现有基于高斯溅射的图像表示方法需要大量高斯图元以保证视觉质量，计算和存储成本高昂。",
                "GaussianImage++通过失真驱动的密集化、上下文感知高斯滤波器和属性分离量化等技术，使用更少图元实现高效表示。",
                "实验表明，GaussianImage++在图像表示和压缩方面优于GaussianImage和COIN，同时保持实时解码和低内存占用。"
            ],
            "method_zh": "**问题定义**：现有基于2D高斯溅射的图像表示方法，如GaussianImage，为了达到较高的视觉保真度，需要使用大量的Gaussian primitives。这导致了计算和存储成本的显著增加，限制了其在资源受限环境中的应用。因此，如何在保证图像质量的前提下，减少Gaussian primitives的数量，是本论文要解决的核心问题。\\n\\n**核心思路**：GaussianImage++的核心思路是通过智能地分配和优化Gaussian primitives，以及高效地压缩其属性，从而在有限数量的primitives下实现高质量的图像表示和压缩。具体来说，该方法通过失真驱动的密集化机制，将更多的primitives分配到图像中信号强度较高的区域，同时利用上下文感知的高斯滤波器来优化primitives的形状和位置。此外，该方法还采用了属性分离的可学习标量量化器和量化感知训练，以实现对primitives属性的高效压缩。\\n\\n**技术框架**：GaussianImage++的整体框架主要包含三个阶段：1) **失真驱动的密集化**：根据图像信号强度自适应地增加Gaussian primitives的数量。2) **上下文感知高斯滤波**：利用上下文信息优化每个primitive的形状和位置。3) **属性分离量化与量化感知训练**：对primitive的属性进行高效压缩，并在训练过程中考虑量化的影响。\\n\\n**关键创新**：GaussianImage++的关键创新在于以下几个方面：1) **失真驱动的密集化机制**：不同于传统的均匀分配primitives的方法，该方法能够根据图像内容自适应地分配primitives，从而在保证图像质量的同时，减少primitives的总数。2) **上下文感知高斯滤波器**：利用图像的上下文信息来优化primitives的形状和位置，从而提高图像的表示能力。3) **属性分离量化与量化感知训练**：通过对primitive的属性进行高效压缩，降低了存储成本，同时通过量化感知训练保证了图像质量。\\n\\n**关键设计**：在失真驱动的密集化机制中，论文定义了一个失真度量，用于衡量图像的局部失真程度，并根据该失真度量来决定是否需要增加primitives。在上下文感知高斯滤波器中，论文使用了一个卷积神经网络来预测每个primitive的滤波器参数，该网络以primitive周围的图像区域作为输入。在属性分离量化中，论文将primitive的属性分为多个组，并对每个组使用不同的量化器。此外，论文还采用了量化感知训练，即在训练过程中模拟量化的过程，从而使模型能够更好地适应量化的影响。",
            "application_zh": "GaussianImage++在图像压缩、图像传输、图像编辑等领域具有广泛的应用前景。它可以用于开发更高效的图像压缩算法，降低图像存储和传输的成本。此外，它还可以用于图像编辑，例如图像修复、图像增强等。未来，该技术有望应用于移动设备、云计算、虚拟现实等领域，为用户提供更好的图像体验。",
            "highlight_zh": "GaussianImage++在图像表示和压缩方面取得了显著的性能提升。实验结果表明，GaussianImage++在保持实时解码和低内存占用的前提下，优于GaussianImage和基于INRs的COIN等方法。具体来说，GaussianImage++在图像质量方面取得了显著提升，同时减少了所需的Gaussian primitives的数量，从而降低了计算和存储成本。",
            "tags_zh": [
                "图像表示",
                "图像压缩",
                "高斯溅射",
                "隐式神经表示",
                "量化感知训练"
            ],
            "_index": 20,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19108v1/fig/teaser_kodak_psnr_v3.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19108v1/fig/framework_v5.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19108v1/fig/figure3_v2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation",
            "authors": [
                "Khanh Nguyen",
                "Dasith de Silva Edirimuni",
                "Ghulam Mubashar Hassan",
                "Ajmal Mian"
            ],
            "arxiv_id": "2512.19088v1",
            "summary": "Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Accepted to AAAI 2026 Workshop on New Frontiers in Information Retrieval",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19088v1",
            "code_links": [
                {
                    "url": "https://github.com/ndkhanh360/BoxOVIS",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "[T]open-vocabulary",
                        "[T]open vocabulary"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "提出基于2D框引导的开放词汇实例分割方法，用于从3D场景中检索目标",
            "summary_zh": "从场景级点云中定位和检索目标是一个具有广泛应用前景的挑战性问题，常见方法是开放词汇3D实例分割。虽然现有方法表现出色，但它们严重依赖SAM和CLIP从伴随点云的图像中生成和分类3D实例掩码，导致计算开销大、处理速度慢，限制了其在实际场景中的部署。Open-YOLO 3D通过使用实时2D检测器来分类由预训练3D分割器直接从点云生成的类别无关掩码，从而缓解了这个问题，无需SAM和CLIP，显著减少了推理时间。然而，Open-YOLO 3D通常无法泛化到3D训练数据中不常出现的目标类别。本文提出了一种方法，该方法从RGB图像中生成3D实例掩码，并由2D开放词汇检测器引导，用于识别新颖目标。我们的方法继承了2D检测器识别新颖目标的能力，同时保持了高效的分类，从而能够快速准确地从开放式文本查询中检索稀有实例。代码将在https://github.com/ndkhanh360/BoxOVIS上提供。",
            "intro_zh": [
                "现有3D实例分割方法依赖SAM和CLIP，计算开销大，推理速度慢，难以在实际场景中部署。",
                "该论文提出一种基于2D框引导的开放词汇实例分割方法，利用2D检测器识别新颖目标，并生成3D实例掩码。",
                "该方法继承了2D检测器识别新颖目标的能力，同时保持了高效的分类，能够快速准确地检索稀有实例。"
            ],
            "method_zh": "**问题定义**：现有基于点云的3D实例分割方法，特别是开放词汇场景下的目标检索，面临着泛化能力和计算效率的挑战。现有方法依赖于SAM和CLIP等模型，需要大量的计算资源，且对于训练数据中不常见的类别泛化能力较弱。Open-YOLO 3D虽然提升了效率，但对罕见物体的识别能力不足。\\n\\n**核心思路**：该论文的核心思路是利用2D开放词汇检测器的强大目标识别能力，特别是对于新颖物体的识别能力，来引导3D实例掩码的生成。通过将2D检测结果与3D点云数据相结合，可以有效地提高3D实例分割的准确性和效率，同时增强对罕见物体的识别能力。\\n\\n**技术框架**：该方法的技术框架主要包括以下几个阶段：1) 使用2D开放词汇检测器在RGB图像上检测目标，获得目标的2D bounding box；2) 将2D bounding box投影到3D点云空间，得到3D bounding box；3) 利用3D bounding box引导3D实例掩码的生成，例如通过点云分割算法，将3D bounding box内的点云分割出来，作为目标的实例掩码；4) 使用文本查询对分割出的3D实例进行检索和分类。\\n\\n**关键创新**：该方法最重要的技术创新点在于利用2D开放词汇检测器来引导3D实例掩码的生成。与直接从3D点云生成实例掩码的方法相比，该方法可以更好地利用2D图像中的语义信息，提高分割的准确性和对新颖物体的识别能力。同时，避免了对SAM和CLIP的依赖，降低了计算开销。\\n\\n**关键设计**：关键设计包括：1) 如何将2D bounding box准确地投影到3D点云空间；2) 如何利用3D bounding box有效地引导3D实例掩码的生成，例如使用基于点云的分割算法，或者基于深度学习的分割模型；3) 如何设计损失函数，使得生成的3D实例掩码与2D检测结果保持一致性。",
            "application_zh": "该研究成果可广泛应用于机器人导航、增强现实、三维场景理解等领域。例如，在机器人导航中，机器人可以利用该方法快速准确地识别和定位场景中的目标物体，从而更好地完成导航任务。在增强现实中，用户可以通过文本查询快速检索和定位场景中的目标物体，从而获得更丰富的增强现实体验。该方法还有助于提升三维场景理解的智能化水平。",
            "highlight_zh": "该论文提出了一种新颖的基于2D框引导的开放词汇3D实例分割方法，显著提升了对罕见物体的识别能力，同时保持了较高的计算效率。实验结果表明，该方法在开放词汇3D实例分割任务上取得了优异的性能，相较于现有方法，在准确率和效率上均有显著提升。具体的性能数据和对比基线将在论文中详细展示。",
            "tags_zh": [
                "3D实例分割",
                "开放词汇",
                "目标检索",
                "点云处理",
                "2D检测",
                "3D场景理解",
                "机器人视觉"
            ],
            "_index": 21,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19088v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19088v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
            "authors": [
                "Kun Zhao",
                "Siyuan Dai",
                "Yingying Zhang",
                "Guodong Liu",
                "Pengfei Gu",
                "Chenghua Lin",
                "Paul M. Thompson",
                "Alex Leow",
                "Heng Huang",
                "Lifang He",
                "Liang Zhan",
                "Haoteng Tang"
            ],
            "arxiv_id": "2512.18986v1",
            "summary": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18986v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "R-GenIMA：融合神经影像与遗传信息的Alzheimer病进展可解释多模态AI模型",
            "summary_zh": "本研究提出R-GenIMA，一种可解释的多模态大语言模型，旨在整合宏观神经解剖学改变与微观遗传易感性，从而实现阿尔茨海默病（AD）的早期检测。R-GenIMA将ROI-wise视觉Transformer与遗传提示相结合，联合建模结构MRI和单核苷酸多态性（SNP）变异。该框架将每个解剖学分割的大脑区域表示为视觉token，并将SNP谱编码为结构化文本，从而实现跨模态注意力，将区域萎缩模式与潜在的遗传因素联系起来。在ADNI队列上的实验表明，R-GenIMA在正常认知（NC）、主观记忆障碍（SMC）、轻度认知障碍（MCI）和AD的四分类任务中取得了最先进的性能。此外，该模型通过识别特定阶段的大脑区域和基因特征，以及整个疾病过程中的连贯的ROI-基因关联模式，提供了生物学上有意义的解释。基于注意力的归因揭示了与已建立的GWAS支持的AD风险基因座（包括APOE、BIN1、CLU和RBFOX1）一致富集的基因。阶段解析的神经解剖学特征识别了跨疾病阶段的共享脆弱性中心，以及阶段特异性模式：主观下降中的纹状体参与，前驱期损害中的额颞叶参与，以及AD中整合的多模态网络破坏。这些结果表明，可解释的多模态AI可以综合影像和遗传学信息，揭示机制见解，为临床可部署的工具奠定基础，从而实现更早的风险分层，并为阿尔茨海默病的精准治疗策略提供信息。",
            "intro_zh": [
                "现有方法难以对齐神经影像和遗传异构信号，限制了阿尔茨海默病（AD）的早期检测。",
                "R-GenIMA利用ROI-wise视觉Transformer和遗传提示，将大脑区域表示为视觉token，SNP谱编码为文本，实现跨模态关联。",
                "R-GenIMA在ADNI数据集上实现了四分类任务的SOTA性能，并揭示了疾病阶段特异性的大脑区域和基因特征。"
            ],
            "method_zh": "**问题定义**：阿尔茨海默病（AD）的早期检测需要整合宏观的神经解剖学变化和微观的遗传易感性。然而，现有的多模态方法难以有效地对齐和融合这些异构信号，导致预测精度和可解释性不足。因此，需要一种能够桥接神经影像和遗传信息的模型，以揭示AD的潜在机制。\n\n**核心思路**：R-GenIMA的核心思路是将神经影像数据（结构MRI）和遗传数据（SNP）进行联合建模，通过跨模态注意力机制学习它们之间的关联。具体来说，将大脑区域作为视觉token，SNP谱作为结构化文本，利用Transformer架构学习区域萎缩模式与潜在遗传因素之间的关系。这种设计能够捕捉到AD进展过程中大脑结构和基因表达的复杂相互作用。\n\n**技术框架**：R-GenIMA的整体框架包括以下几个主要模块：1) ROI-wise视觉Transformer：将结构MRI图像分割成多个感兴趣区域（ROI），并将每个ROI表示为一个视觉token。2) 遗传提示：将SNP数据编码为结构化文本，作为模型的输入。3) 跨模态注意力机制：利用Transformer架构中的自注意力机制，学习视觉token和文本提示之间的关联。4) 分类器：基于学习到的跨模态表示，对AD的不同阶段进行分类（NC, SMC, MCI, AD）。\n\n**关键创新**：R-GenIMA的关键创新在于其可解释的多模态建模方法。通过将大脑区域表示为视觉token，SNP谱表示为文本提示，并利用跨模态注意力机制学习它们之间的关联，该模型能够提供生物学上有意义的解释。与现有方法相比，R-GenIMA不仅提高了预测精度，还能够揭示AD进展过程中大脑结构和基因表达的复杂相互作用。\n\n**关键设计**：R-GenIMA的关键设计包括：1) ROI分割策略：选择合适的ROI分割方案，以确保每个ROI都具有生物学意义。2) 遗传提示编码：设计有效的SNP数据编码方法，以捕捉遗传变异与AD风险之间的关系。3) 注意力机制：使用多头注意力机制，以捕捉不同ROI和SNP之间的复杂关联。4) 损失函数：使用交叉熵损失函数，优化模型的分类性能。",
            "application_zh": "R-GenIMA具有广泛的应用前景，可用于阿尔茨海默病（AD）的早期风险预测、疾病分期和精准治疗。通过整合神经影像和遗传信息，该模型可以识别高风险个体，并为他们提供个性化的干预措施。此外，R-GenIMA还可以用于药物研发，通过识别与AD相关的关键基因和脑区，为新药开发提供靶点。未来，该模型有望成为临床医生辅助诊断和治疗AD的重要工具。",
            "highlight_zh": "R-GenIMA在ADNI数据集上实现了四分类任务的SOTA性能，显著优于现有方法。该模型能够识别与AD相关的关键基因（如APOE、BIN1、CLU和RBFOX1）和脑区，并揭示疾病阶段特异性的神经解剖学特征。例如，纹状体参与主观下降，额颞叶参与前驱期损害，多模态网络破坏则发生在AD阶段。",
            "tags_zh": [
                "阿尔茨海默病",
                "多模态学习",
                "神经影像",
                "遗传信息",
                "可解释AI",
                "Transformer",
                "ROI分析"
            ],
            "_index": 22,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18986v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18986v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18986v1/images/main_fig.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
            "authors": [
                "Zhe Yang",
                "Xiaoshuang Sheng",
                "Zhengnan Zhang",
                "Jidong Wu",
                "Zexing Wang",
                "Xin He",
                "Shenghua Xu",
                "Guanjing Xiong"
            ],
            "arxiv_id": "2512.19107v1",
            "summary": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19107v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出FC-MIR框架，通过帧压缩多模态轨迹推理实现意图感知的移动屏幕推荐。",
            "summary_zh": "本文提出FC-MIR框架，旨在解决移动UI操作轨迹中用户意图识别的问题，从而促进UI理解和任务自动化。该框架利用关键帧采样和自适应连接，减少视觉冗余，提高推理效率。同时，集成先进的闭源多模态大语言模型（MLLM）或微调模型（如Qwen3-VL），用于轨迹总结和意图预测。研究进一步扩展到预测后操作和搜索建议生成，并引入细粒度指标评估总结、预测和建议的实用性。通过UI-Agents和真实用户交互构建的UI轨迹数据集进行评估，结果表明压缩方法在50%-60%压缩率下保持性能；闭源和微调MLLM均表现出强大的意图总结能力，支持轻量级设备部署。然而，MLLM在生成有用和“令人惊讶”的建议方面仍有改进空间。最后，该框架已部署在实际环境中，集成了UI感知和UI-Agent代理，为该领域的未来发展奠定基础。",
            "intro_zh": [
                "现有方法在移动设备上部署多模态大语言模型（MLLM）时，面临计算成本高昂和冗余帧处理效率低下的挑战。",
                "FC-MIR框架通过关键帧采样和自适应连接，有效降低视觉冗余，提升MLLM在移动设备上的推理效率，同时进行意图预测。",
                "实验结果表明，该框架在保持性能的同时实现了较高的压缩率，并验证了MLLM在意图总结方面的能力，为轻量级部署奠定基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从移动UI操作轨迹中识别用户意图的问题。现有方法，特别是直接应用MLLM的方法，在移动设备上部署时面临计算量大、效率低下的问题，主要是因为视频帧存在大量冗余信息，导致不必要的计算开销。\\n\\n**核心思路**：论文的核心思路是通过减少视觉冗余来提高推理效率，同时保持意图识别的准确性。具体而言，采用关键帧采样和自适应连接的方法，从UI操作轨迹中提取最具代表性的帧，并将其输入到MLLM中进行处理。这样可以在显著减少计算量的同时，保留足够的信息用于意图理解。\\n\\n**技术框架**：FC-MIR框架主要包含以下几个阶段：1) **UI轨迹数据采集**：收集UI-Agent和真实用户的UI操作轨迹数据。2) **关键帧采样**：使用算法从UI轨迹中选择关键帧，减少视觉冗余。3) **自适应连接**：将关键帧进行连接，形成MLLM的输入。4) **MLLM推理**：使用闭源MLLM或微调的开源MLLM（如Qwen3-VL）进行轨迹总结、意图预测、后操作生成和搜索建议生成。5) **评估**：使用细粒度指标评估总结、预测和建议的实用性。\\n\\n**关键创新**：该论文的关键创新在于提出了帧压缩方法，通过关键帧采样和自适应连接，显著降低了MLLM处理移动UI操作轨迹时的计算量，使其更适合在移动设备上部署。此外，还提出了细粒度的评估指标，用于评估轨迹总结、意图预测和建议的实用性。\\n\\n**关键设计**：关键帧采样算法的具体实现细节未知，但其目标是选择最具代表性的帧，以最大程度地保留信息并减少冗余。自适应连接方法的具体实现细节也未知，但其目标是有效地将关键帧连接起来，以便MLLM能够理解UI操作的上下文。论文中使用了闭源MLLM和微调的Qwen3-VL模型，具体微调策略未知。评估指标的设计考虑了总结、预测和建议的实用性和“令人惊讶”程度。",
            "application_zh": "该研究成果可应用于智能助手、自动化测试、用户行为分析等领域。例如，可以利用该框架构建更智能的UI自动化测试工具，或者为用户提供更个性化的应用推荐和操作建议。通过理解用户在移动设备上的操作意图，可以实现更高效、更便捷的人机交互。",
            "highlight_zh": "实验结果表明，FC-MIR框架在50%-60%的压缩率下仍能保持意图识别的性能。闭源和微调的MLLM在轨迹总结方面表现出色，验证了其在轻量级设备上部署的潜力。然而，MLLM在生成有用和“令人惊讶”的建议方面仍有提升空间，表明未来研究方向。",
            "tags_zh": [
                "移动UI理解",
                "意图识别",
                "多模态大语言模型",
                "关键帧采样",
                "帧压缩",
                "UI自动化",
                "人机交互"
            ],
            "_index": 23,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19107v1/lc_e.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19107v1/l.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19107v1/guanjian.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
            "authors": [
                "Filippos Ventirozos",
                "Peter Appleby",
                "Matthew Shardlow"
            ],
            "arxiv_id": "2512.19651v1",
            "summary": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "9 pages, 3 figures, 3 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19651v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "[T]chain-of-thought"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于UMR的CoT提示方法，探索零样本ACSA任务",
            "summary_zh": "方面-类别情感分析(ACSA)通过识别评论中的特定主题及其相关情感来提供细粒度的洞察。虽然监督学习方法在该领域占据主导地位，但新领域带标注数据的稀缺性和高成本带来了重大障碍。我们认为，在数据标注资源有限的情况下，利用大型语言模型(LLM)在零样本设置中是一种实用的替代方案。在这项工作中，我们提出了一种新颖的思维链(CoT)提示技术，该技术利用中间的统一意义表示(UMR)来构建ACSA任务的推理过程。我们针对三个模型(Qwen3-4B、Qwen3-8B和Gemini-2.5-Pro)和四个不同的数据集，评估了这种基于UMR的方法与标准CoT基线相比的性能。我们的研究结果表明，UMR的有效性可能取决于模型。虽然初步结果表明，对于像Qwen3-8B这样的中型模型，性能具有可比性，但这些观察结果值得进一步研究，特别是关于其对较小模型架构的潜在适用性。需要进一步研究以确定这些发现在不同模型规模上的普遍适用性。",
            "intro_zh": [
                "ACSA任务依赖标注数据，但数据标注成本高昂且稀缺，限制了其在新领域的应用。",
                "论文提出一种基于统一意义表示(UMR)的CoT提示方法，旨在提升零样本ACSA性能。",
                "实验结果表明，UMR的有效性可能与模型相关，中等规模模型表现出可比性，但需进一步研究。"
            ],
            "method_zh": "**问题定义**：论文旨在解决在零样本场景下，如何利用大型语言模型(LLM)进行方面-类别情感分析(ACSA)的问题。现有方法依赖于大量标注数据进行监督学习，但在新领域中，标注数据的获取成本高昂且数据稀缺，限制了ACSA的应用。因此，如何在没有标注数据的情况下，充分利用LLM的知识和推理能力，成为一个重要的挑战。\\n\\n**核心思路**：论文的核心思路是利用Chain-of-Thought (CoT) 提示技术，并引入统一意义表示(UMR)作为中间步骤，来引导LLM进行更结构化的推理。通过将ACSA任务分解为更小的、更易于理解的步骤，并利用UMR来表示这些步骤之间的关系，可以提高LLM的推理能力和准确性。\\n\\n**技术框架**：整体框架包含以下几个主要阶段：1) 输入评论文本；2) 使用CoT提示，引导LLM生成中间的UMR表示；3) 基于UMR表示，LLM推断出方面、类别和情感极性；4) 输出最终的ACSA结果。UMR作为中间表示，连接了输入文本和最终的ACSA结果，起到了桥梁的作用。\\n\\n**关键创新**：论文的关键创新在于引入了UMR作为CoT提示的中间步骤。UMR提供了一种结构化的方式来表示评论文本的语义信息，并帮助LLM更好地理解文本的含义。与传统的CoT提示相比，基于UMR的CoT提示可以更有效地引导LLM进行推理，并提高ACSA的准确性。\\n\\n**关键设计**：论文中UMR的具体形式和生成方式未详细说明，这部分信息未知。CoT提示的设计也未给出具体细节，例如使用的提示语模板、超参数设置等。实验中使用了Qwen3-4B、Qwen3-8B和Gemini-2.5-Pro等LLM，但没有详细说明这些模型的具体配置和训练方式。",
            "application_zh": "该研究成果可应用于在线评论分析、舆情监控、产品改进等领域。通过零样本ACSA，企业可以快速了解用户对产品或服务的评价，无需大量标注数据，降低了成本。未来，该方法可以扩展到其他自然语言处理任务，例如文本摘要、机器翻译等，具有广阔的应用前景。",
            "highlight_zh": "实验结果表明，基于UMR的CoT提示方法在某些模型上表现出与标准CoT基线相当的性能，尤其是在中等规模模型（如Qwen3-8B）上。然而，研究结果也表明，UMR的有效性可能与模型相关，需要进一步研究以确定其在不同模型规模上的泛化能力。具体的性能提升幅度未知。",
            "tags_zh": [
                "零样本学习",
                "方面情感分析",
                "思维链",
                "统一意义表示",
                "大型语言模型"
            ],
            "_index": 24,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Event Extraction in Large Language Model",
            "authors": [
                "Bobo Li",
                "Xudong Han",
                "Jiang Liu",
                "Yuzhe Ding",
                "Liqiang Jing",
                "Zhaoqi Zhang",
                "Jinheng Li",
                "Xinya Du",
                "Fei Li",
                "Meishan Zhang",
                "Min Zhang",
                "Aixin Sun",
                "Philip S. Yu",
                "Hao Fei"
            ],
            "arxiv_id": "2512.19537v1",
            "summary": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "38 pages, 9 Figures, 5 Tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19537v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "multimodal"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "综述性论文：探讨大语言模型在事件抽取中的应用，并展望未来发展方向。",
            "summary_zh": "大型语言模型（LLMs）和多模态LLMs正在改变事件抽取（EE）领域：提示和生成通常可以在零样本或少样本设置中产生结构化输出。然而，基于LLM的流程面临部署差距，包括弱约束下的幻觉、长上下文和跨文档中脆弱的时间和因果链接，以及有限的上下文窗口内的长期知识管理。我们认为，EE应该被视为一个系统组件，为以LLM为中心的解决方案提供认知支架。事件模式和槽约束为 grounding 和验证创建接口；以事件为中心的结构充当逐步推理的可控中间表示；事件链接支持基于图的RAG的关系感知检索；事件存储提供超出上下文窗口的可更新的情节和代理记忆。本综述涵盖了文本和多模态环境中的EE，组织了任务和分类，追溯了从基于规则和神经模型到指令驱动和生成框架的方法演变，并总结了公式、解码策略、架构、表示、数据集和评估。我们还回顾了跨语言、低资源和特定领域的设置，并强调了可靠的以事件为中心的系统的开放挑战和未来方向。最后，我们概述了LLM时代的核心开放挑战和未来方向，旨在将EE从静态抽取发展为开放世界系统的结构可靠、代理就绪的感知和记忆层。",
            "intro_zh": [
                "现有基于大语言模型的事件抽取方法在弱约束下容易产生幻觉，且难以处理长上下文和跨文档的时间因果关系。",
                "论文提出将事件抽取视为LLM解决方案的认知支架，利用事件模式、结构、链接和存储来增强LLM的可靠性和长期记忆能力。",
                "该综述全面回顾了事件抽取领域的发展历程，并对未来基于LLM的事件抽取系统提出了挑战和发展方向。"
            ],
            "method_zh": "**问题定义**：事件抽取旨在从文本或多模态数据中识别和提取事件信息，包括事件类型、参与者、时间和地点等。现有方法，特别是基于大型语言模型的方法，虽然在零样本或少样本设置下表现出潜力，但仍存在幻觉问题，难以处理长上下文和跨文档的复杂关系，并且上下文窗口限制了长期知识的管理。\\n\\n**核心思路**：论文的核心思路是将事件抽取视为一个认知支架，为大型语言模型提供结构化的信息和约束，从而提高其可靠性和推理能力。通过将事件模式和槽约束作为 grounding 和验证的接口，将事件结构作为中间表示，将事件链接用于关系感知检索，以及将事件存储用于长期记忆，可以有效地解决现有方法的不足。\\n\\n**技术框架**：该综述性论文没有提出具体的模型架构，而是从宏观层面分析了事件抽取在LLM时代的角色。它涵盖了事件抽取的任务和分类，回顾了从基于规则和神经模型到指令驱动和生成框架的方法演变，并总结了公式、解码策略、架构、表示、数据集和评估。此外，还讨论了跨语言、低资源和特定领域的设置。\\n\\n**关键创新**：该论文的关键创新在于将事件抽取重新定位为LLM的认知支架，强调了事件模式、结构、链接和存储在提高LLM可靠性和长期记忆能力方面的作用。这种观点为未来的事件抽取研究提供了新的方向，即如何将事件抽取与LLM更好地结合，构建更强大的事件感知系统。\\n\\n**关键设计**：由于是综述性论文，没有具体的参数设置、损失函数或网络结构等技术细节。但是，论文强调了事件模式和槽约束的重要性，可以将其视为一种软约束，用于指导LLM的生成过程，减少幻觉的产生。此外，事件链接和事件存储的设计可以有效地扩展LLM的上下文窗口，使其能够处理更长的文本和更复杂的事件关系。",
            "application_zh": "该研究成果对信息抽取、知识图谱构建、问答系统、智能助手等领域具有重要应用价值。通过构建可靠的事件感知和记忆层，可以提升LLM在开放世界系统中的表现，例如在新闻分析、金融风险评估、医疗诊断等领域。",
            "highlight_zh": "该论文是一篇全面的综述，总结了事件抽取领域的发展历程，并深入分析了LLM在事件抽取中的应用。它强调了将事件抽取作为LLM认知支架的重要性，并为未来的研究方向提供了有价值的见解。虽然没有提供具体的性能数据，但其对现有方法的不足和未来发展方向的分析具有重要的指导意义。",
            "tags_zh": [
                "事件抽取",
                "大型语言模型",
                "认知支架",
                "知识图谱",
                "信息抽取",
                "自然语言处理",
                "多模态学习"
            ],
            "_index": 25,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19537v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19537v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19537v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs",
            "authors": [
                "Ziyan Zhang",
                "Chao Wang",
                "Zhuo Chen",
                "Lei Chen",
                "Chiyi Li",
                "Kai Song"
            ],
            "arxiv_id": "2512.19092v1",
            "summary": "Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19092v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model",
                        "chain-of-thought"
                    ],
                    "score": 12.0
                }
            ],
            "relevance_score": 12.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出ROG框架，结合知识图谱检索与大语言模型推理，解决复杂逻辑推理难题。",
            "summary_zh": "针对现实世界知识图谱（KG）的不完整性和逻辑查询结构的复杂性，论文提出了一种基于大语言模型（LLM）的ROG（Reasoning Over knowledge Graphs with large language models）框架，用于处理一阶逻辑（FOL）查询的KG推理。ROG是一种集成式框架，它结合了查询感知的KG邻域检索与基于LLM的思维链推理。该方法将复杂的FOL查询分解为一系列更简单的子查询，检索紧凑的、与查询相关的子图作为上下文证据，并使用LLM执行逐步逻辑推理，避免了对特定任务的嵌入优化的需求。在标准KG推理基准上的实验表明，ROG在平均倒数排名（MRR）方面始终优于强大的基于嵌入的基线，尤其是在高复杂度查询类型上，获得了显著的提升。这些结果表明，将结构化KG检索与LLM驱动的逻辑推理相结合，为复杂的KG推理任务提供了一种鲁棒而有效的替代方案。",
            "intro_zh": [
                "现有知识图谱推理方法难以处理复杂查询，尤其是在涉及多算子、深层推理链或异构KG模式时。",
                "ROG框架结合查询感知的KG邻域检索与大语言模型推理，将复杂查询分解为子查询，利用LLM进行逐步推理。",
                "实验表明，ROG在标准KG推理基准上优于基于嵌入的基线，尤其在高复杂度查询上提升显著。"
            ],
            "method_zh": "**问题定义**：论文旨在解决知识图谱上复杂逻辑推理的问题，特别是处理包含多重逻辑运算符和深层推理链的复杂查询。现有方法，如基于嵌入的方法，在处理简单查询时表现良好，但在面对复杂查询时，由于难以捕捉复杂的逻辑关系和KG的异构性，泛化能力不足。这些方法通常需要针对特定任务进行嵌入优化，缺乏通用性。\\n\\n**核心思路**：ROG的核心思路是将复杂查询分解为一系列简单的子查询，并利用大语言模型（LLM）的强大推理能力逐步解决这些子查询。通过检索与查询相关的知识图谱子图，为LLM提供上下文信息，引导LLM进行逻辑推理。这种方法避免了直接在整个知识图谱上进行推理，降低了计算复杂度，并利用LLM的知识和推理能力来弥补知识图谱的不完整性。\\n\\n**技术框架**：ROG框架主要包含以下几个阶段：1) **查询分解**：将复杂的FOL查询分解为一系列更简单的子查询。2) **子图检索**：根据每个子查询，从知识图谱中检索相关的子图作为上下文证据。3) **LLM推理**：利用LLM对检索到的子图进行推理，逐步解决子查询，最终得到最终答案。4) **结果聚合**：将各个子查询的结果进行聚合，得到最终的推理结果。\\n\\n**关键创新**：ROG的关键创新在于将结构化的知识图谱检索与非结构化的大语言模型推理相结合。与传统的基于嵌入的方法不同，ROG不需要对知识图谱进行嵌入学习，而是直接利用LLM的知识和推理能力。此外，ROG通过查询分解和子图检索，有效地降低了推理的复杂性，提高了推理的效率和准确性。\\n\\n**关键设计**：ROG的关键设计包括：1) **查询分解策略**：如何将复杂的FOL查询分解为一系列有效的子查询。2) **子图检索算法**：如何高效地检索与查询相关的子图，保证检索到的子图包含足够的上下文信息。3) **LLM提示工程**：如何设计合适的提示（prompt），引导LLM进行逻辑推理，并获得准确的答案。论文中可能使用了特定的提示模板或微调策略来优化LLM的推理性能。具体参数设置和损失函数等细节未知。",
            "application_zh": "该研究成果可应用于智能问答系统、知识图谱补全、推荐系统等领域。通过结合知识图谱的结构化信息和LLM的推理能力，可以构建更智能、更可靠的AI系统。未来，该方法有望在医疗诊断、金融风控等需要复杂推理的领域发挥重要作用。",
            "highlight_zh": "实验结果表明，ROG在标准KG推理基准上显著优于基于嵌入的基线方法，尤其是在处理高复杂度查询时，MRR指标提升明显。这表明ROG在处理复杂逻辑推理任务方面具有更强的能力和更好的泛化性能。具体的性能提升幅度未知。",
            "tags_zh": [
                "知识图谱推理",
                "大语言模型",
                "逻辑推理",
                "查询分解",
                "子图检索"
            ],
            "_index": 26,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19092v1/fig1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19092v1/fig2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
            "authors": [
                "Valentin Schmidberger",
                "Manuel Eberhardinger",
                "Setareh Maghsudi",
                "Johannes Maucher"
            ],
            "arxiv_id": "2512.19228v1",
            "summary": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Accepted at ICMLA 2025, the first two authors contributed equally",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19228v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 11.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "利用大语言模型生成程序化规则，用于文档伪造检测",
            "summary_zh": "文档伪造对法律、经济和政府流程构成日益严重的威胁，需要越来越复杂的验证机制。一种方法是使用合理性检查，即基于规则的程序，评估数据的正确性和内部一致性，以检测异常或篡改迹象。虽然这些验证程序对于确保数据完整性至关重要，但现有的合理性检查是由软件工程师手动实现的，这非常耗时。大语言模型（LLM）在代码生成方面的最新进展为自动化和扩展这些检查的生成提供了新的潜力。然而，使LLM适应未知领域的特定需求仍然是一个重大挑战。本研究探讨了通过不同的微调策略，在领域特定代码和数据上进行调整的LLM，在受限的硬件资源上生成用于伪造检测的基于规则的合理性检查的能力。我们对开源LLM，Llama 3.1 8B和OpenCoder 8B，在从真实应用场景中提取的结构化数据集上进行微调，并评估生成的合理性检查在以前未见过的伪造模式上的效果。结果表明，这些模型能够生成可执行且有效的验证程序。这也突出了LLM作为可扩展工具的潜力，以支持安全敏感环境中需要可理解性的人工决策。",
            "intro_zh": [
                "现有文档伪造检测的合理性检查依赖人工实现，效率低下且难以扩展。",
                "论文提出利用大语言模型自动生成规则，通过领域数据微调使其适应伪造检测任务。",
                "实验表明，微调后的LLM能够生成有效且可执行的验证程序，提升检测效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决文档伪造检测中，人工编写合理性检查规则耗时且难以扩展的问题。现有的方法依赖于软件工程师手动实现，不仅效率低下，而且难以应对不断涌现的新型伪造手段。因此，迫切需要一种能够自动生成和维护这些规则的方法。\\n\\n**核心思路**：论文的核心思路是利用大语言模型（LLM）的代码生成能力，通过在领域特定数据上进行微调，使LLM能够自动生成用于文档伪造检测的程序化规则。这种方法旨在将人工编写规则的过程自动化，从而提高效率和可扩展性。\\n\\n**技术框架**：整体框架包括以下几个主要阶段：1) 构建领域特定数据集，该数据集包含从真实应用场景中提取的结构化数据，用于训练和评估LLM。2) 选择合适的开源LLM，例如Llama 3.1 8B和OpenCoder 8B。3) 使用领域特定数据集对LLM进行微调，采用不同的微调策略以优化模型性能。4) 使用微调后的LLM生成程序化规则，用于文档伪造检测。5) 在未见过的伪造模式上评估生成的规则的有效性。\\n\\n**关键创新**：论文的关键创新在于将大语言模型应用于文档伪造检测领域，并探索了利用LLM自动生成程序化规则的可能性。与传统的手动编写规则的方法相比，该方法具有更高的效率和可扩展性。此外，论文还研究了不同的微调策略对模型性能的影响，为实际应用提供了指导。\\n\\n**关键设计**：论文的关键设计包括：1) 领域特定数据集的构建，需要仔细选择和处理数据，以确保其能够反映真实应用场景中的伪造模式。2) 微调策略的选择，需要根据具体的任务和数据集进行调整，以获得最佳性能。3) 生成规则的评估，需要设计合适的评估指标，以衡量规则的有效性和可靠性。",
            "application_zh": "该研究成果可应用于金融、法律、政府等多个领域，用于自动检测和预防文档伪造行为。通过自动生成合理性检查规则，可以显著提高文档验证的效率和准确性，降低人工成本，并有效应对不断演变的伪造技术。未来，该技术有望集成到各种文档管理和验证系统中，提升整体安全性。",
            "highlight_zh": "实验结果表明，经过领域数据微调的Llama 3.1 8B和OpenCoder 8B模型，能够生成可执行且有效的文档伪造检测规则。这些规则在未见过的伪造模式上表现良好，验证了LLM在自动化生成安全敏感领域规则的潜力。具体性能数据和对比基线在论文中进行了详细展示。",
            "tags_zh": [
                "文档伪造检测",
                "大语言模型",
                "代码生成",
                "规则生成",
                "领域微调"
            ],
            "_index": 27,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19228v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
            "authors": [
                "Pengxuan Yang",
                "Ben Lu",
                "Zhongpu Xia",
                "Chao Han",
                "Yinfeng Gao",
                "Teng Zhang",
                "Kun Zhan",
                "XianPeng Lang",
                "Yupeng Zheng",
                "Qichao Zhang"
            ],
            "arxiv_id": "2512.19133v1",
            "summary": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "AAAI 2026, first version",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19133v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]world model",
                        "representation learning"
                    ],
                    "score": 7.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "WorldRFT：通过强化微调的潜在世界模型规划，提升自动驾驶安全性与规划能力",
            "summary_zh": "本文提出WorldRFT，一个面向规划的潜在世界模型框架，旨在通过分层规划分解和局部感知交互细化机制，以及强化学习微调(RFT)，将场景表征学习与规划对齐，从而提升自动驾驶中安全关键策略的性能。WorldRFT集成了视觉-几何基础模型以增强3D空间感知，采用分层规划任务分解来指导表征优化，并利用局部感知迭代细化来推导面向规划的驾驶策略。此外，引入了组相对策略优化(GRPO)，通过轨迹高斯化和碰撞感知奖励来微调驾驶策略，从而系统地提高安全性。WorldRFT在nuScenes和NavSim基准测试中均达到了SOTA性能。在nuScenes上，碰撞率降低了83%（0.30% -> 0.05%）。在NavSim上，仅使用摄像头传感器输入，就达到了与基于激光雷达的SOTA方法DiffusionDrive相当的性能（87.8 vs. 88.1 PDMS）。",
            "intro_zh": [
                "现有基于重构的潜在世界模型在自动驾驶中存在感知与规划任务耦合，导致规划优化次优。",
                "WorldRFT通过分层规划分解和局部感知交互细化，对齐场景表征学习与规划，并使用强化学习微调策略。",
                "实验表明，WorldRFT在nuScenes和NavSim上均达到SOTA，显著降低碰撞率并提升驾驶性能。"
            ],
            "method_zh": "**问题定义**：现有基于潜在世界模型的自动驾驶方法，通常以重构为导向进行表征学习，这使得感知和规划任务相互纠缠，导致规划性能无法达到最优。尤其是在安全关键场景下，这种耦合会带来潜在的安全风险。现有方法难以有效利用几何信息，并且缺乏针对规划任务的优化。\n\\n**核心思路**：WorldRFT的核心思路是将场景表征学习与规划任务对齐。通过引入分层规划任务分解，引导表征学习过程，使其更加关注与规划相关的特征。同时，利用局部感知交互细化机制，迭代优化驾驶策略，使其更好地适应局部环境。此外，通过强化学习微调，进一步提升策略的安全性和性能。\n\\n**技术框架**：WorldRFT框架主要包含以下几个模块：1) 视觉-几何基础模型：用于提取场景的3D空间信息。2) 分层规划任务分解：将复杂的驾驶任务分解为多个子任务，例如路径规划、速度控制等。3) 局部感知交互细化：通过迭代的方式，根据局部环境信息调整驾驶策略。4) 强化学习微调：使用组相对策略优化(GRPO)算法，对驾驶策略进行微调，提升安全性。\n\\n**关键创新**：WorldRFT的关键创新在于：1) 面向规划的表征学习：通过分层规划任务分解，引导表征学习过程，使其更加关注与规划相关的特征。2) 局部感知交互细化：通过迭代的方式，根据局部环境信息调整驾驶策略，提高策略的适应性。3) 组相对策略优化(GRPO)：通过轨迹高斯化和碰撞感知奖励，对驾驶策略进行微调，提升安全性。\n\\n**关键设计**：视觉-几何基础模型使用了预训练的视觉模型和几何先验知识，用于提取场景的3D空间信息。分层规划任务分解将驾驶任务分解为多个子任务，每个子任务对应一个独立的策略。局部感知交互细化模块使用循环神经网络(RNN)来建模驾驶策略的动态变化。组相对策略优化(GRPO)算法使用高斯分布来建模轨迹，并引入碰撞感知奖励来约束策略的学习过程。具体参数设置和损失函数细节未知。",
            "application_zh": "WorldRFT的研究成果可应用于各种自动驾驶场景，包括城市道路、高速公路和停车场等。该方法能够提升自动驾驶系统的安全性、可靠性和规划能力，降低事故风险，提高交通效率。此外，该方法还可以应用于机器人导航、虚拟现实等领域，具有广泛的应用前景。",
            "highlight_zh": "WorldRFT在nuScenes数据集上将碰撞率降低了83%（0.30% -> 0.05%），显著提升了安全性。在NavSim数据集上，仅使用摄像头传感器输入，就达到了与基于激光雷达的SOTA方法DiffusionDrive相当的性能（87.8 vs. 88.1 PDMS），表明该方法具有很强的竞争力。",
            "tags_zh": [
                "自动驾驶",
                "潜在世界模型",
                "强化学习",
                "分层规划",
                "表征学习"
            ],
            "_index": 28,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19133v1/figure/fig1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19133v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19133v1/figure/subfig2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
            "authors": [
                "Apoorv Vyas",
                "Heng-Jui Chang",
                "Cheng-Fu Yang",
                "Po-Yao Huang",
                "Luya Gao",
                "Julius Richter",
                "Sanyuan Chen",
                "Matt Le",
                "Piotr Dollár",
                "Christoph Feichtenhofer",
                "Ann Lee",
                "Wei-Ning Hsu"
            ],
            "arxiv_id": "2512.19687v1",
            "summary": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",
            "categories": [
                "cs.SD",
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.SD",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19687v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "contrastive learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出PE-AV：基于大规模对比学习的音视频感知统一编码器，实现跨模态对齐与检索。",
            "summary_zh": "本文提出了一种新的音视频编码器家族，名为感知编码器音视频（PE-AV），它通过大规模对比学习进行训练，用于音频和视频理解。PE-AV建立在PE的基础上，在扩展表征到音频方面做出了关键贡献，并原生支持跨音频-视频、音频-文本和视频-文本模态的联合嵌入。PE-AV的统一跨模态嵌入实现了诸如语音检索等新任务，并在标准音频和视频基准测试中取得了新的state-of-the-art。为了实现这一点，我们构建了一个强大的音视频数据引擎，为O(100M)音视频对合成了高质量的字幕，从而实现了跨模态一致的大规模监督。我们的音频数据包括语音、音乐和通用音效，避免了先前工作中常见的单领域限制。我们利用了十个成对对比目标，表明缩放跨模态和字幕类型对可以加强对齐并提高零样本性能。我们进一步开发了PE-A-Frame，通过使用帧级对比目标微调PE-AV，从而实现了细粒度的音频帧到文本对齐，用于诸如声音事件检测等任务。",
            "intro_zh": [
                "现有音视频理解方法通常受限于单领域数据或模态，缺乏跨模态对齐能力，限制了其泛化性和应用范围。",
                "PE-AV通过构建大规模音视频数据集，并采用多目标对比学习，实现了音频、视频和文本的统一嵌入空间。",
                "实验表明，PE-AV在语音检索等新任务上表现出色，并在多个标准音视频基准测试中刷新了state-of-the-art。"
            ],
            "method_zh": "**问题定义**：现有音视频理解方法通常专注于单一模态或特定领域，例如只处理语音或只处理视频，缺乏对音频、视频和文本等多模态信息的统一理解和对齐能力。此外，现有方法在跨模态检索等任务上的表现也存在局限性。因此，需要一种能够有效学习音视频数据的跨模态表征，并支持多种下游任务的模型。\n\n**核心思路**：本文的核心思路是利用大规模对比学习，训练一个能够将音频、视频和文本信息映射到统一嵌入空间的编码器。通过构建包含大量音视频数据及其对应文本描述的数据集，并设计多个对比学习目标，使得模型能够学习到不同模态之间的对应关系，从而实现跨模态对齐和检索。\n\n**技术框架**：PE-AV的整体框架包括以下几个主要模块：1) 音频编码器：用于提取音频特征；2) 视频编码器：用于提取视频特征；3) 文本编码器：用于提取文本特征；4) 对比学习模块：包含多个对比学习目标，用于学习不同模态之间的对应关系。训练过程中，模型首先将音频、视频和文本数据分别输入到对应的编码器中，得到它们的特征表示。然后，对比学习模块利用这些特征表示，计算不同模态之间的相似度，并根据对比学习目标调整模型参数，使得相似的模态在嵌入空间中更加接近，不相似的模态更加远离。\n\n**关键创新**：PE-AV的关键创新点在于：1) 构建了一个大规模的音视频数据集，包含O(100M)的音视频对，并为其合成了高质量的字幕，为大规模对比学习提供了数据基础；2) 提出了多个对比学习目标，包括音频-视频、音频-文本和视频-文本等多种模态之间的对比，以及不同类型的字幕之间的对比，从而加强了模型对跨模态信息的理解；3) 提出了PE-A-Frame，通过帧级对比学习，实现了细粒度的音频帧到文本对齐，从而支持声音事件检测等任务。\n\n**关键设计**：PE-AV的关键设计包括：1) 使用Transformer作为音频、视频和文本编码器的基本架构；2) 采用了十个成对对比学习目标，包括跨模态对比和字幕类型对比；3) 使用InfoNCE损失函数作为对比学习的损失函数；4) 对于PE-A-Frame，使用了帧级对比学习目标，并对PE-AV进行了微调。",
            "application_zh": "PE-AV具有广泛的应用前景，例如：1) 跨模态检索：可以根据音频检索视频，或根据视频检索音频；2) 语音识别：可以利用视频信息提高语音识别的准确率；3) 视频理解：可以利用音频信息提高视频理解的准确率；4) 声音事件检测：可以检测视频中出现的声音事件。该研究有望推动音视频理解领域的发展，并为实际应用提供更强大的技术支持。",
            "highlight_zh": "PE-AV在多个标准音视频基准测试中取得了state-of-the-art的结果。例如，在语音检索任务中，PE-AV的性能显著优于现有方法。此外，PE-AV还成功应用于声音事件检测任务，并取得了良好的效果。这些实验结果表明，PE-AV能够有效学习音视频数据的跨模态表征，并支持多种下游任务。",
            "tags_zh": [
                "音视频理解",
                "多模态学习",
                "对比学习",
                "跨模态检索",
                "统一嵌入",
                "大规模数据集",
                "声音事件检测"
            ],
            "_index": 29,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
            "authors": [
                "Yongxin Wang",
                "Zhicheng Yang",
                "Meng Cao",
                "Mingfei Han",
                "Haokun Lin",
                "Yingying Zhu",
                "Xiaojun Chang",
                "Xiaodan Liang"
            ],
            "arxiv_id": "2512.19554v1",
            "summary": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19554v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "CARE：面向可验证多模态推理，通过对比锚定反射改进失败案例学习。",
            "summary_zh": "具有可验证奖励的群体相对强化学习（RLVR）常常浪费了最有用的数据——失败案例。当所有rollout都是错误时，梯度会停滞；当一个rollout碰巧是正确的，更新通常会忽略其他接近但错误的rollout，并且信用可能被错误地分配给虚假链。我们提出了CARE（对比锚定反射），一个以失败为中心的后训练框架，用于多模态推理，将错误转化为监督。CARE结合了：（i）锚定对比目标，围绕最佳rollout形成一个紧凑的子群，以及一组语义上接近的难负例，执行子群内的z-score归一化，仅对负例进行缩放，并包含一个全负例救援以防止零信号批次；（ii）反射引导重采样（RGR），一种一次性的结构化自修复，重写一个具有代表性的失败案例，并使用相同的验证器重新评分，将接近的失败转化为可用的正例，而无需任何测试时反射。CARE提高了准确性和训练平滑性，同时明确增加了来自失败案例的学习信号份额。在Qwen2.5-VL-7B上，CARE在六个可验证的视觉推理基准测试中，将宏平均准确率提高了4.6个百分点；在Qwen3-VL-8B上，在相同的评估协议下，它在MathVista和MMMU-Pro上达到了有竞争力的或最先进的结果。",
            "intro_zh": [
                "现有RLVR方法在处理失败案例时存在不足，未能充分利用错误数据中的信息，导致梯度停滞或信用分配错误。",
                "CARE框架通过对比学习和反射引导重采样，将失败案例转化为有效的监督信号，从而提升模型的多模态推理能力。",
                "实验表明，CARE在多个视觉推理基准测试中显著提升了模型准确率，并在MathVista和MMMU-Pro上取得了领先水平。"
            ],
            "method_zh": "**问题定义**：现有的群体相对强化学习与可验证奖励（RLVR）方法在处理多模态推理任务时，未能充分利用失败的rollout数据。当所有rollout都错误时，梯度更新停滞；即使存在正确的rollout，也往往忽略了其他接近但错误的rollout所包含的信息，导致信用分配不准确，影响模型学习效率和最终性能。\\n\\n**核心思路**：CARE的核心思路是将失败案例转化为有效的监督信号，从而提升模型的多模态推理能力。通过对比学习，模型能够区分正确的rollout和接近但错误的rollout，并学习到失败的原因。反射引导重采样则将接近的失败案例转化为可用的正例，进一步增强了模型的学习能力。\\n\\n**技术框架**：CARE是一个后训练框架，主要包含两个核心模块：锚定对比学习和反射引导重采样（RGR）。首先，锚定对比学习模块围绕最佳rollout构建一个紧凑的子群，并引入语义上接近的难负例，通过z-score归一化和负例缩放，增强对比学习的效果。其次，RGR模块通过重写具有代表性的失败案例，并使用相同的验证器重新评分，将接近的失败转化为可用的正例。\\n\\n**关键创新**：CARE的关键创新在于其以失败为中心的学习范式，以及将对比学习和反射引导重采样相结合的方法。与传统的RLVR方法不同，CARE充分利用了失败案例中的信息，将其转化为有效的监督信号，从而提升了模型的学习效率和泛化能力。此外，RGR模块通过结构化的自修复，有效地解决了数据稀疏的问题。\\n\\n**关键设计**：在锚定对比学习中，关键的设计包括如何选择合适的锚点（最佳rollout）、如何构建难负例集合，以及如何进行z-score归一化和负例缩放。在反射引导重采样中，关键的设计包括如何选择具有代表性的失败案例进行重写，以及如何保证重写后的rollout仍然具有可验证性。此外，全负例救援机制用于处理零信号批次，保证训练的稳定性。",
            "application_zh": "CARE框架可应用于各种需要多模态推理和可验证性的任务，例如视觉问答、机器人导航、智能对话系统等。该研究有助于提升AI系统的可靠性和安全性，使其能够更好地理解和处理复杂的多模态信息，并做出更准确的决策。未来，该方法有望在自动驾驶、医疗诊断等领域发挥重要作用。",
            "highlight_zh": "CARE在Qwen2.5-VL-7B模型上，于六个可验证的视觉推理基准测试中，宏平均准确率提升了4.6个百分点。在Qwen3-VL-8B模型上，CARE在MathVista和MMMU-Pro数据集上达到了具有竞争力的或最先进的结果，证明了其在多模态推理任务上的有效性。",
            "tags_zh": [
                "多模态推理",
                "对比学习",
                "强化学习",
                "失败案例学习",
                "视觉问答",
                "自修复",
                "可验证性"
            ],
            "_index": 30,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19554v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19554v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19554v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling",
            "authors": [
                "Sutashu Tomonaga",
                "Kenji Doya",
                "Noboru Murata"
            ],
            "arxiv_id": "2512.18965v1",
            "summary": "Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions \"slide\" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18965v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "Mamba",
                        "[T]SSM",
                        "[T]state space model"
                    ],
                    "score": 10.5
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于Lag算子的结构化状态空间建模几何框架，简化SSM设计。",
            "summary_zh": "结构化状态空间模型(SSMs)是当前流行的Mamba架构的核心，是强大的序列建模工具。然而，它们的理论基础依赖于一个复杂的多阶段过程，包括连续时间建模和随后的离散化，这可能会模糊直觉。本文介绍了一个直接的、第一性原理的框架，用于构建离散时间SSM，该框架既灵活又模块化。我们的方法基于一种新颖的Lag算子，它通过几何方式推导出离散时间递推关系，通过测量系统的基函数如何“滑动”以及从一个时间步长到下一个时间步长的变化。由此产生的状态矩阵通过涉及该算子的单个内积计算，为通过灵活地组合不同的基函数和时间扭曲方案来创建新的SSM提供了一个模块化的设计空间。为了验证我们的方法，我们证明了一个特定的实例可以完全恢复有影响力的HiPPO模型的递推关系。数值模拟证实了我们的推导，为设计灵活而鲁棒的序列模型提供了新的理论工具。",
            "intro_zh": [
                "现有SSM理论基础依赖于复杂的连续时间建模和离散化过程，缺乏直观性。",
                "提出基于Lag算子的几何框架，直接构建离散时间SSM，实现灵活模块化设计。",
                "实验验证了该框架能够精确恢复HiPPO模型，并为序列模型设计提供新工具。"
            ],
            "method_zh": "**问题定义**：现有结构化状态空间模型（SSMs）的理论基础复杂，涉及连续时间建模和离散化等多个阶段，导致模型设计和理解上的困难。现有方法缺乏直观性和灵活性，难以快速构建和调整SSM。\n\\n**核心思路**：本文的核心思路是引入一种新颖的Lag算子，通过几何方式直接推导离散时间递推关系。Lag算子衡量了系统基函数在时间步进中的“滑动”和变化，从而避免了复杂的连续时间建模过程。这种方法将SSM的构建简化为一个内积计算，提高了设计的模块化和灵活性。\n\\n**技术框架**：该框架的核心是Lag算子，它作用于系统的基函数，描述了基函数在离散时间步进中的变化。通过计算Lag算子与基函数的内积，可以得到离散时间状态转移矩阵。整个框架包括以下步骤：1) 选择合适的基函数；2) 定义Lag算子；3) 计算状态转移矩阵；4) 构建离散时间SSM。\n\\n**关键创新**：最重要的技术创新点在于Lag算子的引入，它提供了一种直接从离散时间角度构建SSM的几何方法。与现有方法相比，该方法避免了连续时间建模和离散化过程，简化了模型设计，并提高了灵活性和可解释性。Lag算子将SSM的构建转化为一个内积计算，使得可以方便地组合不同的基函数和时间扭曲方案。\n\\n**关键设计**：关键设计包括Lag算子的具体形式和基函数的选择。Lag算子的形式决定了时间步进的方式，基函数的选择则影响了模型的表达能力。论文中通过选择特定的基函数和Lag算子，成功地恢复了HiPPO模型的递推关系。此外，时间扭曲方案也是一个重要的设计参数，可以用于调整模型对不同时间尺度的敏感度。",
            "application_zh": "该研究成果可应用于序列建模的各个领域，例如自然语言处理、语音识别、时间序列预测等。通过灵活组合不同的基函数和时间扭曲方案，可以构建适用于特定任务的定制化SSM。该框架的模块化设计也有助于加速SSM的开发和部署，并为未来的序列模型研究提供新的思路。",
            "highlight_zh": "论文通过数值模拟验证了所提出的Lag算子框架的有效性。一个关键的实验结果是，通过选择特定的基函数和Lag算子，该框架能够精确地恢复有影响力的HiPPO模型的递推关系。这表明该框架具有很强的理论基础和实际应用潜力。数值模拟结果进一步证实了该框架在序列建模任务中的性能。",
            "tags_zh": [
                "结构化状态空间模型",
                "序列建模",
                "Lag算子",
                "HiPPO模型",
                "几何框架"
            ],
            "_index": 31,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18965v1/figures/plots/lagged_basis_forward_and_backward.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18965v1/figures/plots/compare_recon_lorenz_hippo_legs_and_lagop_zoh_w_measure.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
            "authors": [
                "Jian Yang",
                "Wei Zhang",
                "Yizhi Li",
                "Shawn Guo",
                "Haowen Wang",
                "Aishan Liu",
                "Ge Zhang",
                "Zili Wang",
                "Zhoujun Li",
                "Xianglong Liu",
                "Weifeng Lv"
            ],
            "arxiv_id": "2512.19424v1",
            "summary": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19424v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "CodeSimpleQA：提升代码大语言模型的事实性准确度",
            "summary_zh": "大型语言模型（LLM）在代码生成方面取得了显著进展，在从自然语言指令合成代码片段方面表现出令人印象深刻的能力。然而，一个关键挑战仍然存在，即确保LLM生成关于编程概念、技术实现等方面的、在事实上准确的响应。以往大多数代码相关基准侧重于代码执行的正确性，而忽略了编程知识的事实准确性。为了解决这一差距，我们提出了CodeSimpleQA，这是一个全面的双语基准，旨在评估代码LLM在回答代码相关问题时的事实准确性，其中包含精心策划的英语和中文问答对，涵盖不同的编程语言和主要的计算机科学领域。此外，我们创建了CodeSimpleQA-Instruct，一个包含6600万个样本的大规模指令语料库，并开发了一个结合了监督微调和强化学习的后训练框架。我们对各种LLM的全面评估表明，即使是最先进的LLM也在代码事实性方面存在困难。我们提出的框架证明了相对于基础模型的显著改进，突显了在开发可靠的代码LLM中，事实性感知对齐的关键重要性。",
            "intro_zh": [
                "现有代码基准测试主要关注代码执行的正确性，忽略了模型对编程知识理解的事实准确性。",
                "论文提出CodeSimpleQA基准和CodeSimpleQA-Instruct指令数据集，用于评估和提升代码大语言模型的事实性。",
                "通过监督微调和强化学习的后训练框架，显著提升了模型在CodeSimpleQA上的事实性准确度。"
            ],
            "method_zh": "**问题定义**：现有代码大语言模型在生成代码时，虽然代码可以执行，但对编程概念、技术实现等方面的理解可能存在事实性错误。以往的评测基准主要关注代码执行的正确性，缺乏对模型编程知识事实准确性的评估。\n\n**核心思路**：论文的核心思路是构建一个专门用于评估代码大语言模型事实性准确度的基准测试集CodeSimpleQA，并利用大规模指令数据集CodeSimpleQA-Instruct，通过监督微调和强化学习来提升模型的事实性。\n\n**技术框架**：整体框架包含三个主要部分：1) 构建双语基准测试集CodeSimpleQA，包含高质量的编程相关问答对；2) 创建大规模指令数据集CodeSimpleQA-Instruct，用于模型的指令微调；3) 提出一个后训练框架，结合监督微调（SFT）和强化学习（RL），以提升模型的事实性。\n\n**关键创新**：关键创新在于提出了一个专门针对代码大语言模型事实性评估的基准测试集，并设计了一个结合监督微调和强化学习的后训练框架。与以往侧重于代码执行正确性的方法不同，该方法直接关注模型对编程知识的理解是否准确。\n\n**关键设计**：CodeSimpleQA包含英语和中文两种语言的问答对，覆盖多种编程语言和计算机科学领域。CodeSimpleQA-Instruct包含6600万个样本，用于指令微调。后训练框架中，首先使用CodeSimpleQA-Instruct进行监督微调，然后使用强化学习进一步提升模型的事实性。具体的损失函数和网络结构细节在论文中未详细说明（未知）。",
            "application_zh": "该研究成果可应用于提升代码大语言模型的可靠性和可信度，使其在代码生成、代码解释、编程问答等场景中提供更准确、更可靠的编程知识。这有助于提高开发效率，降低错误率，并促进编程教育和知识共享。",
            "highlight_zh": "论文通过在CodeSimpleQA基准测试集上对多种LLM进行评估，发现即使是最先进的LLM在代码事实性方面也存在困难。提出的后训练框架显著提升了模型在CodeSimpleQA上的事实性准确度，证明了事实性感知对齐在开发可靠代码LLM中的重要性。具体提升幅度未在摘要中明确给出（未知）。",
            "tags_zh": [
                "代码大语言模型",
                "事实性评估",
                "基准测试",
                "指令微调",
                "强化学习",
                "编程知识",
                "代码生成"
            ],
            "_index": 32,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19424v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19424v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19424v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
            "authors": [
                "Zihan Lin",
                "Xiaohan Wang",
                "Hexiong Yang",
                "Jiajun Chai",
                "Jie Cao",
                "Guojun Yin",
                "Wei Lin",
                "Ran He"
            ],
            "arxiv_id": "2512.19126v1",
            "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19126v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 10.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出AWPO，通过显式整合推理奖励增强大语言模型工具使用能力",
            "summary_zh": "现有强化学习方法在训练工具使用的大语言模型时，主要依赖可验证的结果奖励，忽略了显式推理奖励在提升推理和工具利用方面的潜力。直接结合推理和结果奖励可能导致次优性能或与主要优化目标冲突。为了解决这个问题，我们提出了优势加权策略优化（AWPO），一个有效的强化学习框架，它有效地整合了显式推理奖励以增强工具使用能力。AWPO结合了方差感知门控和难度感知加权，基于群体相对统计自适应地调节来自推理信号的优势，并采用定制的裁剪机制以实现稳定的优化。大量实验表明，AWPO在标准工具使用基准测试中取得了最先进的性能，显著优于强大的基线模型，并在具有挑战性的多轮场景中领先于闭源模型。值得注意的是，凭借卓越的参数效率，我们的4B模型在多轮准确率方面超越Grok-4 16.0%，同时保持了对分布外MMLU-Pro基准的泛化能力。",
            "intro_zh": [
                "现有方法在训练工具使用LLM时，忽略了显式推理奖励对提升推理和工具利用的潜力。",
                "AWPO通过方差感知门控和难度感知加权，自适应地调节推理信号的优势，并采用裁剪机制。",
                "实验表明，AWPO在工具使用基准测试中达到SOTA，4B模型超越Grok-4，并保持泛化能力。"
            ],
            "method_zh": "**问题定义**：现有强化学习方法在训练工具使用的大语言模型时，主要依赖于可验证的结果奖励，而忽略了显式推理奖励在提升推理能力和工具利用方面的潜力。直接将推理奖励和结果奖励结合，可能会导致性能下降，甚至与主要的优化目标相冲突。因此，如何有效地整合推理奖励，同时避免其负面影响，是本文要解决的关键问题。\\n\\n**核心思路**：本文的核心思路是提出一种名为优势加权策略优化（AWPO）的强化学习框架，该框架能够有效地整合显式推理奖励，从而增强大语言模型的工具使用能力。AWPO通过自适应地调节来自推理信号的优势，并采用定制的裁剪机制，来保证优化的稳定性和有效性。\\n\\n**技术框架**：AWPO的整体框架包括以下几个主要模块：1) 大语言模型（LLM）：作为策略网络，负责生成工具使用序列。2) 环境：模拟真实世界或特定任务环境，提供状态和奖励信号。3) 推理奖励模块：根据LLM的推理过程生成推理奖励。4) 结果奖励模块：根据LLM的最终结果生成结果奖励。5) 优势加权模块：根据方差感知门控和难度感知加权，自适应地调节推理奖励的优势。6) 策略优化模块：使用裁剪机制，稳定地更新LLM的策略。\\n\\n**关键创新**：AWPO的关键创新在于其优势加权机制，该机制能够根据群体相对统计信息，自适应地调节来自推理信号的优势。具体来说，AWPO采用了方差感知门控和难度感知加权，前者用于抑制不稳定的推理信号，后者用于提升对困难样本的关注度。此外，AWPO还采用了定制的裁剪机制，以保证优化的稳定性。与现有方法相比，AWPO能够更有效地利用推理奖励，从而提升大语言模型的工具使用能力。\\n\\n**关键设计**：AWPO的关键设计包括：1) 方差感知门控：使用推理奖励的方差来衡量其稳定性，并根据方差动态地调整推理奖励的权重。2) 难度感知加权：使用样本的难度来衡量其重要性，并根据难度动态地调整样本的权重。3) 定制的裁剪机制：使用一种特殊的裁剪函数，限制策略更新的幅度，从而保证优化的稳定性。损失函数是标准策略梯度损失函数，但加入了优势加权后的推理奖励和结果奖励。",
            "application_zh": "AWPO具有广泛的应用前景，可以应用于各种需要工具使用的大语言模型任务，例如智能助手、自动化编程、科学研究等。通过提升大语言模型的工具使用能力，AWPO可以帮助人们更高效地完成各种任务，提高生产力，并促进人工智能技术的发展。",
            "highlight_zh": "实验结果表明，AWPO在标准工具使用基准测试中取得了最先进的性能，显著优于强大的基线模型，并在具有挑战性的多轮场景中领先于闭源模型。例如，在多轮准确率方面，AWPO的4B模型超越Grok-4 16.0%，同时保持了对分布外MMLU-Pro基准的泛化能力。这表明AWPO能够有效地提升大语言模型的工具使用能力，并具有良好的泛化性能。",
            "tags_zh": [
                "大语言模型",
                "工具使用",
                "强化学习",
                "推理奖励",
                "策略优化"
            ],
            "_index": 33,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19126v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19126v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
            "authors": [
                "Sihao Lin",
                "Zerui Li",
                "Xunyi Zhao",
                "Gengze Zhou",
                "Liuyi Wang",
                "Rong Wei",
                "Rui Tang",
                "Juncheng Li",
                "Hanqing Wang",
                "Jiangmiao Pang",
                "Anton van den Hengel",
                "Jiajun Liu",
                "Qi Wu"
            ],
            "arxiv_id": "2512.19021v1",
            "summary": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.",
            "categories": [
                "cs.CV",
                "cs.RO"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19021v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "locomotion",
                        "sim-to-real"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "embodied AI",
                        "VLN"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "VLNVerse：用于视觉-语言导航的多功能、具身、逼真模拟与评估基准",
            "summary_zh": "本文提出了VLNVerse，一个用于视觉-语言导航（VLN）的新型大规模、可扩展的基准，旨在实现多功能、具身、逼真的模拟和评估。现有VLN基准受限于固定的小规模数据集和简化的物理模拟，阻碍了对sim-to-real泛化能力的深入研究，并造成了显著的研究差距。此外，任务碎片化阻碍了该领域的统一进展，而有限的数据规模无法满足现代基于LLM的预训练需求。VLNVerse将VLN重新定义为一个可扩展的、全栈的具身AI问题。其多功能性将先前分散的任务统一到一个框架中，并为研究人员提供了一个可扩展的工具包。其具身设计超越了无形的、瞬移的“幽灵”代理，支持由强大的物理引擎驱动的逼真模拟中的完整运动学。利用VLNVerse的规模和多样性，对现有方法（从经典模型到基于MLLM的代理）进行了全面评估。同时，提出了一种新型的统一多任务模型，能够解决基准测试中的所有任务。VLNVerse旨在缩小模拟导航与真实世界泛化之间的差距，为社区提供一个重要的工具，以推动对可扩展的、通用具身运动代理的研究。",
            "intro_zh": [
                "现有VLN基准数据集规模小、物理模拟简单，难以评估模型在真实环境中的泛化能力。",
                "VLNVerse旨在构建一个大规模、多任务、具身且逼真的模拟环境，统一现有碎片化的VLN任务。",
                "通过VLNVerse对现有方法进行全面评估，并提出了一个能够处理所有任务的统一多任务模型。"
            ],
            "method_zh": "**问题定义**：现有的视觉-语言导航（VLN）基准存在数据集规模小、物理模拟不真实、任务碎片化等问题。这些问题限制了模型在真实世界中的泛化能力，阻碍了该领域的发展。现有方法难以在不同VLN任务之间共享知识，且无法充分利用大规模数据进行预训练。\\n\\n**核心思路**：VLNVerse的核心思路是构建一个大规模、多功能、具身且逼真的模拟环境，以解决现有VLN基准的局限性。通过统一不同的VLN任务，并提供一个可扩展的工具包，VLNVerse旨在促进该领域的统一进展。同时，逼真的物理模拟和具身代理的设计，有助于提高模型在真实世界中的泛化能力。\\n\\n**技术框架**：VLNVerse包含以下主要模块：1) 多样化的环境：提供各种室内和室外环境，以增加数据集的多样性。2) 具身代理：使用具有完整运动学和物理引擎支持的具身代理，以模拟真实的导航行为。3) 多任务学习框架：统一不同的VLN任务，并提供一个通用的模型训练框架。4) 评估指标：提供全面的评估指标，以评估模型在不同任务上的性能。\\n\\n**关键创新**：VLNVerse的关键创新在于其多功能性、具身性和逼真性。多功能性体现在它统一了不同的VLN任务，并提供了一个可扩展的工具包。具身性体现在它使用了具有完整运动学和物理引擎支持的具身代理。逼真性体现在它提供了各种室内和室外环境，并模拟了真实的导航行为。与现有方法相比，VLNVerse更接近真实世界，能够更好地评估模型在真实环境中的泛化能力。\\n\\n**关键设计**：VLNVerse的关键设计包括：1) 使用Habitat模拟器进行物理模拟。2) 设计了一个统一的多任务学习框架，可以同时训练多个VLN任务。3) 提出了新的评估指标，以更全面地评估模型的性能。4) 使用了大规模的预训练数据，以提高模型的泛化能力。",
            "application_zh": "VLNVerse的潜在应用领域包括机器人导航、自动驾驶、虚拟助手等。通过在VLNVerse上训练的模型，可以使机器人在真实世界中更好地理解人类指令，并完成导航任务。该研究的实际价值在于提高了机器人导航的可靠性和效率，未来影响在于促进了具身智能的发展。",
            "highlight_zh": "论文在VLNVerse上对现有方法进行了全面评估，结果表明，基于MLLM的代理在某些任务上表现出色，但在其他任务上仍有很大的提升空间。同时，论文提出的统一多任务模型在多个任务上取得了具有竞争力的结果，证明了VLNVerse的有效性。具体性能数据和对比基线在论文中有详细展示。",
            "tags_zh": [
                "视觉语言导航",
                "具身智能",
                "模拟环境",
                "多任务学习",
                "机器人导航",
                "物理模拟",
                "大规模数据集"
            ],
            "_index": 34,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19021v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19021v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19021v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
            "authors": [
                "Zelin Zhao",
                "Xinyu Gong",
                "Bangya Liu",
                "Ziyang Song",
                "Jun Zhang",
                "Suhui Wu",
                "Yongxin Chen",
                "Hao Zhang"
            ],
            "arxiv_id": "2512.19020v1",
            "summary": "Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.",
            "categories": [
                "cs.CV",
                "cs.LG"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19020v1",
            "code_links": [
                {
                    "url": "https://sjtuytc.github.io/CETCam_project_page.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "depth estimation",
                        "VGGT"
                    ],
                    "score": 4.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "geometric consistency"
                    ],
                    "score": 3.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 10.0,
            "hit_pillars": [
                "3_perception_slam",
                "7_retargeting",
                "9_embodied_foundation"
            ],
            "headline_zh": "CETCAM：通过一致且可扩展的Token化实现相机可控的视频生成",
            "summary_zh": "本文提出了一种相机可控的视频生成框架CETCAM，该框架通过一致且可扩展的Token化方案消除了对相机标注的需求。CETCAM利用几何基础模型（如VGGT）估计深度和相机参数，并将它们转换为统一的、几何感知的Token。这些Token通过轻量级的上下文块无缝集成到预训练的视频扩散骨干网络中。CETCAM分两个阶段进行训练，首先从各种原始视频数据中学习鲁棒的相机可控性，然后使用精心策划的高保真数据集细化精细的视觉质量。大量实验表明，CETCAM在多个基准测试中表现出最先进的几何一致性、时间稳定性和视觉真实感。此外，CETCAM对额外的控制模态（包括图像修复和布局控制）表现出强大的适应性，突出了其在相机控制之外的灵活性。",
            "intro_zh": [
                "现有相机可控视频生成方法依赖相机位姿标注，难以扩展到大型动态数据集，且与深度估计不一致，导致训练-测试差异。",
                "CETCAM通过一致且可扩展的Token化方案，将深度和相机参数转换为几何感知的Token，无需相机标注即可实现相机控制。",
                "实验表明，CETCAM在几何一致性、时间稳定性和视觉真实感方面均达到SOTA，并能适应图像修复和布局控制等额外模态。"
            ],
            "method_zh": "**问题定义**：现有的相机可控视频生成方法主要依赖于带有相机位姿标注的数据集进行训练。然而，获取大规模、高质量的相机位姿标注非常困难，尤其是在动态场景中。此外，这些标注通常与深度估计不一致，导致训练和测试阶段存在差异，影响生成视频的质量和可控性。\\n\\n**核心思路**：CETCAM的核心思路是利用几何基础模型（如VGGT）来估计视频中的深度和相机参数，并将这些几何信息转换为统一的、几何感知的Token。通过这种方式，模型可以在没有显式相机标注的情况下学习相机控制，从而避免了对大规模标注数据的依赖，并提高了模型的泛化能力。\\n\\n**技术框架**：CETCAM的整体框架包含以下几个主要模块：1) 几何信息提取模块：使用VGGT等几何基础模型估计视频的深度和相机参数。2) Token化模块：将提取的几何信息转换为统一的、几何感知的Token。3) 上下文融合模块：使用轻量级的上下文块将几何Token集成到预训练的视频扩散骨干网络中。4) 视频生成模块：利用扩散模型生成最终的视频。CETCAM采用两阶段训练策略：首先，在大量未标注的原始视频数据上训练模型，使其学习鲁棒的相机可控性；然后，使用高质量的标注数据集对模型进行微调，以提高生成视频的视觉质量。\\n\\n**关键创新**：CETCAM的关键创新在于提出了一种一致且可扩展的Token化方案，该方案能够将深度和相机参数等几何信息有效地编码为Token，并无缝集成到预训练的视频扩散模型中。这种方法避免了对相机标注的依赖，提高了模型的泛化能力和可控性。与现有方法相比，CETCAM能够生成具有更高几何一致性和时间稳定性的视频。\\n\\n**关键设计**：CETCAM的关键设计包括：1) 使用VGGT作为几何信息提取器，以获得准确的深度和相机参数估计。2) 设计了一种几何感知的Token化方案，该方案能够有效地编码深度和相机参数。3) 使用轻量级的上下文块将几何Token集成到视频扩散骨干网络中，以避免引入过多的计算开销。4) 采用两阶段训练策略，以提高模型的鲁棒性和视觉质量。损失函数方面，采用了标准的扩散模型损失函数，并根据需要添加了正则化项，以提高生成视频的几何一致性和时间稳定性。",
            "application_zh": "CETCAM具有广泛的应用前景，例如电影制作、游戏开发、虚拟现实和增强现实等领域。它可以用于生成具有精确相机控制的逼真视频内容，从而为用户提供更加沉浸式的体验。此外，CETCAM还可以用于数据增强，生成更多样化的训练数据，以提高其他计算机视觉模型的性能。未来，该技术有望应用于自动驾驶、机器人导航等领域。",
            "highlight_zh": "CETCAM在多个基准测试中取得了最先进的结果，证明了其在几何一致性、时间稳定性和视觉真实感方面的优越性。例如，在相机轨迹控制任务中，CETCAM生成的视频与目标相机轨迹的偏差显著小于现有方法。此外，CETCAM还展示了对图像修复和布局控制等额外模态的强大适应性，进一步证明了其灵活性和通用性。",
            "tags_zh": [
                "相机可控视频生成",
                "视频扩散模型",
                "几何感知Token化",
                "深度估计",
                "几何一致性"
            ],
            "_index": 35,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19020v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19020v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19020v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations",
            "authors": [
                "Yinhuai Wang",
                "Runyi Yu",
                "Hok Wai Tsui",
                "Xiaoyi Lin",
                "Hui Zhang",
                "Qihan Zhao",
                "Ke Fan",
                "Miao Li",
                "Jie Song",
                "Jingbo Wang",
                "Qifeng Chen",
                "Ping Tan"
            ],
            "arxiv_id": "2512.19583v1",
            "summary": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.",
            "categories": [
                "cs.RO",
                "cs.GR"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19583v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation",
                        "dexterous hand",
                        "dexterous manipulation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "imitation learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "提出HOP+HOT框架，仅用合成数据学习通用手-物跟踪控制器",
            "summary_zh": "本文提出了一个系统，用于仅从合成数据中学习可泛化的手-物跟踪控制器，无需任何人工演示。该方法主要有两个贡献：(1) HOP（Hand-Object Planner），可以合成多样化的手-物轨迹；(2) HOT（Hand-Object Tracker），通过强化学习和交互模仿学习弥合了从合成到物理的迁移，从而提供了一个以目标手-物状态为条件的通用控制器。该方法可以扩展到不同的物体形状和手部形态。通过广泛的评估，我们表明该方法能够使灵巧的手跟踪具有挑战性的、长期的序列，包括物体重新排列和灵巧的掌内重定向。这些结果代表了迈向可扩展的操纵基础控制器的重要一步，该控制器可以完全从合成数据中学习，打破了长期以来限制灵巧操纵进展的数据瓶颈。",
            "intro_zh": [
                "现有灵巧操作方法依赖大量真实数据，成本高昂且难以扩展到新场景。",
                "论文提出HOP+HOT框架，利用HOP生成多样化合成数据，HOT学习通用跟踪控制器，实现从合成到真实的迁移。",
                "实验表明，该方法能使灵巧手跟踪复杂序列，包括物体重排列和掌内重定向，显著提升泛化能力。"
            ],
            "method_zh": "**问题定义**：现有灵巧手操作方法严重依赖于大量真实世界的数据，这使得训练过程成本高昂且难以扩展到新的物体形状、手部形态和任务场景。数据收集的瓶颈限制了灵巧操作领域的发展。因此，如何仅使用合成数据训练出具有良好泛化能力的灵巧手-物跟踪控制器是一个关键问题。\\n\\n**核心思路**：论文的核心思路是利用合成数据生成多样化的手-物交互轨迹，并训练一个能够从合成数据泛化到真实世界的跟踪控制器。通过结合手-物规划器（HOP）和手-物跟踪器（HOT），该方法旨在弥合合成数据和真实数据之间的差距，从而实现仅使用合成数据训练出鲁棒的灵巧手操作控制器。\\n\\n**技术框架**：该方法包含两个主要模块：HOP（Hand-Object Planner）和 HOT（Hand-Object Tracker）。HOP负责生成多样化的手-物交互轨迹，作为HOT的训练数据。HOT则利用这些合成数据，通过强化学习和交互模仿学习训练一个通用的手-物跟踪控制器。该控制器以目标手-物状态为条件，能够根据目标状态调整手的运动，从而实现对物体的跟踪和操作。整体流程是从HOP生成合成数据，然后利用这些数据训练HOT，最后将训练好的HOT部署到真实机器人上。\\n\\n**关键创新**：该方法最重要的创新在于提出了一种完全基于合成数据学习通用手-物跟踪控制器的框架。与以往依赖大量真实数据的方法不同，该方法通过HOP生成多样化的合成数据，并利用HOT学习从合成到真实的迁移，从而打破了数据瓶颈。此外，结合强化学习和交互模仿学习，使得HOT能够学习到更加鲁棒和泛化的控制策略。\\n\\n**关键设计**：HOP的设计关键在于生成多样化的手-物交互轨迹，这需要考虑不同的物体形状、手部形态和任务目标。HOT的设计关键在于如何弥合合成数据和真实数据之间的差距，这需要仔细设计强化学习的奖励函数和交互模仿学习的目标函数。具体的网络结构和参数设置在论文中应该有详细描述，但摘要中未提及。",
            "application_zh": "该研究成果可广泛应用于机器人自动化领域，例如工业生产中的物体抓取和装配、家庭服务机器人中的物品整理和清洁、以及医疗机器人中的手术辅助等。通过降低对真实数据的依赖，该方法有望加速灵巧操作机器人的研发和部署，提高机器人的智能化水平和服务能力。",
            "highlight_zh": "该论文通过实验验证了所提出方法的有效性，展示了灵巧手在复杂场景下的跟踪能力，包括物体重新排列和掌内重定向等。虽然摘要中没有给出具体的性能数据和对比基线，但强调了该方法在泛化能力方面的显著提升，表明其在解决灵巧操作问题上具有重要潜力。",
            "tags_zh": [
                "灵巧手操作",
                "手-物跟踪",
                "合成数据",
                "强化学习",
                "模仿学习",
                "机器人控制",
                "泛化能力"
            ],
            "_index": 36,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19583v1/img/teaser.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19583v1/img/overview.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19583v1/img/hop.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation",
            "authors": [
                "Devi Yuliarti",
                "Ravi Prakash",
                "Hiu Ching Cheung",
                "Amy Strong",
                "Patrick J. Codd",
                "Shan Lin"
            ],
            "arxiv_id": "2512.19010v1",
            "summary": "The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making.",
            "categories": [
                "eess.SP",
                "cs.RO"
            ],
            "primary_category": "eess.SP",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19010v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "PalpAid：用于组织触诊的多模态气动触觉传感器",
            "summary_zh": "在外科肿瘤学中，组织的触觉特性（如弹性和硬度）在识别肿瘤和病理组织边界方面起着重要作用。机器人辅助手术虽然极具价值，但也降低了外科医生的感觉信息，通常只有视觉可用。为了克服这种感觉缺失，提出的传感器通常体积庞大、结构复杂且与手术流程不兼容。本文提出了一种多模态气动触觉传感器PalpAid，配备麦克风和压力传感器，将接触力转换为内部压差。压力传感器用作事件检测器，而麦克风捕获的听觉特征有助于组织划界。本文展示了传感单元的设计、制造和组装，并通过特性测试证明了其耐用性、充放气循环能力以及与机器人系统的集成能力。最后，本文展示了该传感器对具有不同填充密度的3D打印硬物和离体软组织的分类能力。总而言之，PalpAid旨在智能地填补感觉空白，并改进临床决策。",
            "intro_zh": [
                "机器人辅助手术中，外科医生缺乏触觉反馈，影响肿瘤边界的精确识别。",
                "PalpAid采用气动原理，通过压力传感器和麦克风感知接触力和声音，实现多模态触觉感知。",
                "实验验证了PalpAid的鲁棒性，并展示了其在区分不同硬度物体和组织方面的能力。"
            ],
            "method_zh": "**问题定义**：机器人辅助手术中，外科医生依赖视觉信息，缺乏触觉反馈，难以准确识别肿瘤边界和组织特性。现有的触觉传感器通常体积大、结构复杂，难以集成到手术流程中，且成本较高。\\n\\n**核心思路**：PalpAid的核心思路是利用气动原理，将接触力转换为内部压差，并通过压力传感器和麦克风分别感知压力变化和声音信号。通过融合压力和声音信息，实现对组织特性的多模态感知，从而辅助外科医生进行更精确的触诊。\\n\\n**技术框架**：PalpAid系统主要由以下几个部分组成：1) 气动触觉传感单元：包含一个气囊、一个压力传感器和一个麦克风。气囊与组织接触时，压力变化被压力传感器捕捉，同时麦克风记录接触产生的声音。2) 信号处理单元：对压力传感器和麦克风采集到的信号进行处理，提取特征。3) 分类算法：利用机器学习算法，根据提取的特征对组织类型进行分类。\\n\\n**关键创新**：PalpAid的关键创新在于：1) 采用气动原理，结构简单、体积小，易于集成到机器人手术系统中。2) 融合压力和声音信息，实现多模态触觉感知，提高了组织识别的准确性。3) 使用低成本的压力传感器和麦克风，降低了传感器的成本。\\n\\n**关键设计**：气囊的材料和尺寸是关键设计参数，需要根据应用场景进行优化。压力传感器的量程和精度需要满足触诊的需求。麦克风的灵敏度和频率响应范围需要能够捕捉到组织接触产生的声音信号。分类算法的选择需要根据数据集的特点进行调整，常用的算法包括支持向量机（SVM）和神经网络。",
            "application_zh": "PalpAid可应用于机器人辅助手术中，为外科医生提供触觉反馈，辅助肿瘤边界识别和组织特性判断，提高手术精度和成功率。此外，该技术还可应用于远程医疗、康复机器人等领域，实现远程触诊和个性化康复治疗。未来，结合人工智能技术，PalpAid有望实现更智能化的触觉感知和诊断。",
            "highlight_zh": "实验结果表明，PalpAid能够有效区分不同硬度的3D打印物体和离体软组织。通过压力和声音信号的融合，PalpAid在组织分类任务中取得了较高的准确率，证明了其在触觉感知方面的潜力。该传感器具有良好的鲁棒性，能够承受多次充放气循环，并能与机器人系统集成。",
            "tags_zh": [
                "触觉传感器",
                "气动触觉",
                "多模态感知",
                "机器人辅助手术",
                "组织触诊"
            ],
            "_index": 37,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19010v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19010v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19010v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
            "authors": [
                "Argha Kamal Samanta",
                "Harshika Goyal",
                "Vasudha Joshi",
                "Tushar Mungle",
                "Pabitra Mitra"
            ],
            "arxiv_id": "2512.19663v1",
            "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "14 pages, 14 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19663v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出知识增强多模态Transformer，用于糖尿病视网膜病变诊断中的跨模态对齐",
            "summary_zh": "糖尿病视网膜病变(DR)是全球可预防性失明的主要原因，需要精确的自动化诊断系统。通用领域的视觉-语言模型（如CLIP）在自然图像任务中表现良好，但在医学领域应用中表现不佳，尤其是在眼科图像的跨模态检索方面。我们提出了一种新颖的知识增强联合嵌入框架，该框架通过多模态Transformer架构整合视网膜眼底图像、临床文本和结构化患者数据，以解决医学图像-文本对齐的关键差距。我们的方法为每个模态采用单独的编码器：用于视网膜图像的Vision Transformer (ViT-B/16)、用于临床叙述的Bio-ClinicalBERT以及用于结构化人口统计学和临床特征的多层感知器。这些模态通过具有模态特定嵌入的联合Transformer融合，并使用包括模态对之间的对比损失、图像和文本的重建损失以及根据ICDR和SDRG方案进行DR严重程度分级的分类损失在内的多个目标进行训练。在巴西多标签眼科数据集(BRSET)上的实验结果表明，与基线模型相比，有显著的改进。我们的框架实现了近乎完美的文本到图像检索性能，Recall@1为99.94%，而微调的CLIP为1.29%，同时保持了最先进的SDRG分类精度97.05%和ICDR分类精度97.97%。此外，在未见过的DeepEyeNet数据集上的零样本评估验证了强大的泛化能力，Recall@1为93.95%，而微调的CLIP为0.22%。这些结果表明，我们的多模态训练方法有效地捕捉了医学领域的跨模态关系，从而建立了卓越的检索能力和强大的诊断性能。",
            "intro_zh": [
                "现有CLIP等视觉-语言模型在医学图像跨模态检索方面表现不佳，无法有效对齐图像和临床文本。",
                "提出一种知识增强的多模态Transformer框架，融合视网膜图像、临床文本和结构化数据，实现跨模态对齐。",
                "在BRSET数据集上，文本到图像检索Recall@1达到99.94%，显著优于微调的CLIP，并在DeepEyeNet数据集上展现出强大的泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决糖尿病视网膜病变（DR）诊断中，医学图像（眼底图像）与临床文本信息之间跨模态对齐的问题。现有方法，如直接使用通用视觉-语言模型（如CLIP）进行微调，在医学领域表现不佳，无法有效捕捉图像和文本之间的复杂关系，导致检索和诊断性能下降。\\n\\n**核心思路**：论文的核心思路是利用知识增强的多模态Transformer架构，显式地融合来自不同模态（图像、文本、结构化数据）的信息，并通过多目标学习策略，促使模型学习到更鲁棒和准确的跨模态表示。通过引入医学领域的先验知识，提升模型在特定任务上的性能。\\n\\n**技术框架**：整体框架包含三个主要模块：1) 模态编码器：分别使用ViT-B/16编码眼底图像，Bio-ClinicalBERT编码临床文本，多层感知器（MLP）编码结构化数据。2) 联合Transformer：将不同模态的嵌入向量输入到联合Transformer中，进行跨模态特征融合。3) 多目标学习：使用对比损失（模态对之间）、重建损失（图像和文本）和分类损失（DR严重程度分级）进行联合训练。\\n\\n**关键创新**：最重要的技术创新点在于知识增强的多模态融合方法。与直接微调通用模型不同，该方法针对医学领域特点，设计了专门的模态编码器（如Bio-ClinicalBERT）和多目标学习策略，从而更好地捕捉医学图像和文本之间的关联。\\n\\n**关键设计**：关键设计包括：1) 使用Bio-ClinicalBERT作为文本编码器，利用生物医学领域的预训练知识。2) 设计了多目标损失函数，包括对比损失、重建损失和分类损失，以促进跨模态对齐和诊断性能。3) 使用模态特定的嵌入，更好地表示不同模态的信息。",
            "application_zh": "该研究成果可应用于糖尿病视网膜病变的自动化诊断系统，辅助医生进行快速准确的诊断，尤其是在医疗资源匮乏的地区。此外，该方法也可推广到其他医学图像-文本对齐任务，例如放射学报告生成、病理图像分析等，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，该方法在BRSET数据集上实现了近乎完美的文本到图像检索性能，Recall@1达到99.94%，显著优于微调的CLIP（1.29%）。同时，在SDRG和ICDR分类任务中，分别取得了97.05%和97.97%的分类精度。在未见过的DeepEyeNet数据集上的零样本评估中，Recall@1为93.95%，远超微调的CLIP（0.22%），验证了模型的泛化能力。",
            "tags_zh": [
                "糖尿病视网膜病变",
                "多模态学习",
                "跨模态对齐",
                "医学图像分析",
                "知识增强",
                "Transformer",
                "对比学习"
            ],
            "_index": 38,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19663v1/dr_grades_distribution_log.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19663v1/anatomical_status_distribution.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19663v1/other_pathological_features_prevalence.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
            "authors": [
                "Yi Xin",
                "Siqi Luo",
                "Qi Qin",
                "Haoxing Chen",
                "Kaiwen Zhu",
                "Zhiwei Zhang",
                "Yangfan He",
                "Rongchao Zhang",
                "Jinbin Bai",
                "Shuo Cao",
                "Bin Fu",
                "Junjun He",
                "Yihao Liu",
                "Yuewen Cao",
                "Xiaohong Liu"
            ],
            "arxiv_id": "2512.19433v1",
            "summary": "Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19433v1",
            "code_links": [
                {
                    "url": "https://github.com/Alpha-VLLM/Lumina-DiMOO",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出dMLLM-TTS，通过自验证和高效测试时缩放提升扩散多模态大语言模型的生成质量和效率。",
            "summary_zh": "扩散多模态大语言模型(dMLLMs)作为一种统一图像生成和理解的新型架构，近年来备受关注。然而，开发有效且高效的测试时缩放(TTS)方法以充分发挥其生成潜力仍然是一个未被充分探索的挑战。为了解决这个问题，我们提出了dMLLM-TTS，这是一个在两个互补的缩放轴上运行的新框架：(1) 轨迹探索缩放，以增强生成假设的多样性；(2) 迭代细化缩放，以实现稳定的生成。传统的TTS方法通常在这两个维度上执行线性搜索，导致O(NT)的巨大计算成本，并且需要外部验证器来进行最佳N选一。为了克服这些限制，我们提出了两项创新。首先，我们设计了一种高效的层次搜索算法，其复杂度为O(N+T)，能够自适应地扩展和修剪采样轨迹。其次，我们引入了一种自验证反馈机制，该机制利用dMLLMs固有的图像理解能力来评估文本-图像对齐，从而消除了对外部验证器的需求。在GenEval基准上对三个具有代表性的dMLLMs（例如，Lumina-DiMOO、MMaDA、Muddit）进行的大量实验表明，我们的框架在显著提高生成质量的同时，实现了比线性搜索高出6倍的效率。",
            "intro_zh": [
                "现有的扩散多模态大语言模型测试时缩放方法计算成本高昂，依赖外部验证器，限制了生成质量和效率。",
                "dMLLM-TTS通过轨迹探索和迭代细化两个维度进行缩放，并引入高效的层次搜索和自验证反馈机制。",
                "实验表明，dMLLM-TTS在GenEval基准测试中，显著提升了生成质量，并实现了高达6倍的效率提升。"
            ],
            "method_zh": "**问题定义**：论文旨在解决扩散多模态大语言模型（dMLLMs）在测试时缩放（TTS）过程中效率低和需要外部验证器的问题。现有的TTS方法通常采用线性搜索，计算复杂度高，难以在生成质量和效率之间取得平衡。此外，依赖外部验证器进行最佳结果选择增加了系统的复杂性和成本。\\n\\n**核心思路**：论文的核心思路是通过设计一种高效的层次搜索算法和自验证反馈机制来优化TTS过程。层次搜索算法能够自适应地探索和修剪采样轨迹，降低计算复杂度。自验证反馈机制利用dMLLMs自身的图像理解能力来评估文本-图像对齐程度，从而避免使用外部验证器。\\n\\n**技术框架**：dMLLM-TTS框架包含两个主要的缩放轴：轨迹探索缩放和迭代细化缩放。轨迹探索缩放旨在增加生成假设的多样性，而迭代细化缩放则用于稳定生成过程。框架的核心是层次搜索算法和自验证反馈机制。层次搜索算法在轨迹探索和迭代细化两个维度上进行自适应搜索，并根据自验证反馈机制的评估结果进行剪枝。最终，选择最佳的生成结果。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了高效的层次搜索算法，将计算复杂度从O(NT)降低到O(N+T)；2) 引入了自验证反馈机制，利用dMLLMs自身的图像理解能力进行文本-图像对齐评估，消除了对外部验证器的依赖。\\n\\n**关键设计**：层次搜索算法采用分层结构，每一层代表不同的缩放级别。算法首先在较低的缩放级别上进行快速搜索，然后根据自验证反馈机制的评估结果，选择有希望的轨迹进行进一步的探索。自验证反馈机制通过计算生成图像和输入文本之间的相似度来评估文本-图像对齐程度。具体的相似度计算方法可能涉及CLIP等预训练模型。",
            "application_zh": "dMLLM-TTS可应用于各种需要高质量、高效率图像生成的场景，例如：创意设计、内容生成、虚拟现实、游戏开发等。该研究有助于提升多模态大语言模型在实际应用中的性能和用户体验，并降低计算成本，促进相关技术的普及。",
            "highlight_zh": "实验结果表明，dMLLM-TTS在GenEval基准测试中，相较于传统的线性搜索方法，在生成质量上取得了显著提升，同时实现了高达6倍的效率提升。该方法在Lumina-DiMOO、MMaDA和Muddit等多个代表性dMLLMs上均取得了良好的效果，验证了其通用性和有效性。",
            "tags_zh": [
                "扩散模型",
                "多模态大语言模型",
                "测试时缩放",
                "自验证",
                "层次搜索",
                "图像生成",
                "文本图像对齐"
            ],
            "_index": 39,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19433v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19433v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19433v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
            "authors": [
                "Xueming Yan",
                "Boyan Xu",
                "Yaochu Jin",
                "Lixian Xiao",
                "Wenlong Ye",
                "Runyang Cai",
                "Zeqi Zheng",
                "Jingfa Liu",
                "Aimin Yang"
            ],
            "arxiv_id": "2512.19379v1",
            "summary": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.MM"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19379v1",
            "code_links": [
                {
                    "url": "https://github.com/yanxm01/INDOMER",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出OmniMER以解决印尼多模态情感识别问题",
            "summary_zh": "印尼语作为一种有超过2亿使用者的语言，在多模态情感识别研究中仍然处于服务不足的状态。我们引入了IndoMER，这是第一个针对印尼的多模态情感识别基准，包含来自203位说话者的1,944个视频片段，具有时间对齐的文本、音频和视觉注释，涵盖七种情感类别。该数据集展示了诸如跨模态不一致性和受印尼文化交流规范影响的长尾类分布等现实挑战。为了解决这些问题，我们提出了OmniMER，一个基于Qwen2.5-Omni的多模态适应框架，通过情感关键词提取、面部表情分析和音频韵律分析等三个辅助模态特定感知任务来增强情感识别。这些辅助任务帮助模型在融合前识别每种模态中的情感相关线索，从而减少在低资源环境下对虚假相关性的依赖。实验结果表明，OmniMER在情感分类和情感识别上分别达到了0.582和0.454的Macro-F1，较基线模型分别提升了7.6和22.1个绝对点。",
            "intro_zh": [
                "印尼语在多模态情感识别研究中服务不足，面临跨模态不一致性和长尾类分布等挑战。",
                "提出OmniMER框架，通过情感关键词提取、面部表情分析和音频韵律分析等辅助任务增强情感识别能力。",
                "实验结果显示，OmniMER在情感分类和识别上分别提升了7.6和22.1个绝对点，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：本论文旨在解决印尼语多模态情感识别中的挑战，尤其是跨模态不一致性和长尾类分布的问题。现有方法在低资源环境下容易依赖虚假相关性，导致识别效果不佳。\\n\\n**核心思路**：提出OmniMER框架，通过引入三个辅助模态特定感知任务，帮助模型在融合前识别情感相关线索，从而提高情感识别的准确性和鲁棒性。\\n\\n**技术框架**：OmniMER框架基于Qwen2.5-Omni，包含三个主要模块：情感关键词提取、面部表情分析和音频韵律分析。每个模块分别处理文本、视频和音频数据，最终进行融合以实现情感识别。\\n\\n**关键创新**：最重要的创新点在于通过辅助任务的设计，增强了模型对不同模态中情感线索的感知能力，显著减少了对虚假相关性的依赖。与现有方法相比，OmniMER在多模态融合中更具有效性和适应性。\\n\\n**关键设计**：在模型设计中，采用了特定的损失函数来优化每个辅助任务的输出，并通过调节超参数来平衡不同模态的影响，确保模型在多模态融合时的稳定性和准确性。 ",
            "application_zh": "该研究在社交媒体分析、情感计算和人机交互等领域具有广泛的应用潜力。通过提升印尼语的情感识别能力，能够更好地理解和分析印尼文化中的情感表达，促进相关技术在东南亚地区的落地与发展。",
            "highlight_zh": "实验结果显示，OmniMER在情感分类任务中达到了0.582的Macro-F1，在情感识别任务中达到了0.454，分别较基线模型提升了7.6和22.1个绝对点。此外，跨语言评估表明该框架具有良好的泛化能力。",
            "tags_zh": [
                "多模态情感识别",
                "印尼语",
                "情感计算",
                "辅助任务",
                "跨模态融合",
                "数据集构建",
                "深度学习"
            ],
            "_index": 40,
            "_used_api": "openai",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19379v1/pdf/France.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19379v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19379v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
            "authors": [
                "Haoyu Jiang",
                "Boan Qu",
                "Junjie Zhu",
                "Fanjie Zeng",
                "Xiaojie Lin",
                "Wei Zhong"
            ],
            "arxiv_id": "2512.19114v1",
            "summary": "The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19114v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "HyperLoad：基于跨模态增强大语言模型的绿色数据中心冷却负荷预测框架",
            "summary_zh": "人工智能的快速发展正以指数级速度提升计算需求，加剧数据中心的能源消耗和碳排放，推动绿色数据中心的快速部署以缓解资源和环境压力。为了在最小化PUE和生命周期碳强度的同时，实现可再生能源、存储和负载的亚分钟级协调，准确的负载预测至关重要。然而，现有方法难以解决绿色数据中心中由冷启动、负载失真、多源数据碎片化和分布偏移引起的小样本场景。我们提出了HyperLoad，一个利用预训练大语言模型（LLM）来克服数据稀缺性的跨模态框架。在跨模态知识对齐阶段，文本先验知识和时间序列数据被映射到共同的潜在空间，最大化先验知识的效用。在多尺度特征建模阶段，通过自适应前缀调优注入领域对齐的先验知识，实现快速的场景适应，同时增强的全局交互注意力机制捕获跨设备的时间依赖性。我们发布了公开的DCData数据集用于基准测试。在数据充足和数据稀缺的设置下，HyperLoad始终优于最先进的（SOTA）基线，证明了其在可持续绿色数据中心管理中的实用性。",
            "intro_zh": [
                "现有方法难以应对绿色数据中心冷启动、负载失真等导致的小样本预测问题。",
                "HyperLoad利用预训练大语言模型，通过跨模态知识对齐和多尺度特征建模克服数据稀缺性。",
                "实验表明，HyperLoad在数据充足和稀缺场景下均超越现有最佳方法，适用于绿色数据中心管理。"
            ],
            "method_zh": "**问题定义**：论文旨在解决绿色数据中心冷却负荷预测中小样本场景下的预测精度问题。现有方法在面对冷启动、负载失真、多源数据碎片化和分布偏移等问题时，难以有效利用历史数据和领域知识，导致预测精度下降。\\n\\n**核心思路**：论文的核心思路是利用预训练大语言模型（LLM）的强大知识表示和迁移能力，将文本先验知识和时间序列数据进行融合，从而克服数据稀缺性带来的挑战。通过跨模态知识对齐，将不同模态的信息映射到统一的潜在空间，并利用自适应前缀调优将领域知识注入模型，提高模型对新场景的适应能力。\\n\\n**技术框架**：HyperLoad框架主要包含两个阶段：跨模态知识对齐和多尺度特征建模。在跨模态知识对齐阶段，利用对比学习等方法，将文本描述（例如设备规格、运行状态）和时间序列数据映射到共同的潜在空间。在多尺度特征建模阶段，首先通过自适应前缀调优将领域对齐的先验知识注入模型，然后利用增强的全局交互注意力机制捕获跨设备的时间依赖性，最后进行负荷预测。\\n\\n**关键创新**：论文的关键创新在于提出了一个跨模态融合的框架，能够有效利用预训练大语言模型的知识，并将其与时间序列数据进行结合。与传统的时间序列预测方法相比，HyperLoad能够更好地利用领域知识，提高在小样本场景下的预测精度。增强的全局交互注意力机制能够有效捕获设备间的依赖关系，进一步提升预测性能。\\n\\n**关键设计**：在跨模态知识对齐阶段，使用了对比学习损失函数来拉近文本和时间序列数据在潜在空间的距离。自适应前缀调优模块通过学习一组可训练的前缀向量，将领域知识注入到预训练语言模型中。增强的全局交互注意力机制通过引入全局节点，能够更好地捕获设备间的依赖关系。具体的参数设置和网络结构细节在论文中进行了详细描述（未知）。",
            "application_zh": "HyperLoad可应用于绿色数据中心的智能运维管理，通过精准的冷却负荷预测，实现可再生能源、储能系统和负载的优化调度，降低数据中心的能源消耗和碳排放，提高能源利用效率，助力数据中心的可持续发展。该方法也可推广到其他能源预测和优化领域。",
            "highlight_zh": "HyperLoad在公开的DCData数据集上进行了评估，实验结果表明，在数据充足和数据稀缺的设置下，HyperLoad均优于现有的最佳基线方法。具体的性能提升幅度在论文中进行了详细的量化分析（未知）。该结果验证了HyperLoad在绿色数据中心冷却负荷预测中的有效性和实用性。",
            "tags_zh": [
                "绿色数据中心",
                "冷却负荷预测",
                "大语言模型",
                "跨模态学习",
                "时间序列预测"
            ],
            "_index": 41,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19114v1/DC1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19114v1/FT_AAAI_CR.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19114v1/HyperLoad.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
            "authors": [
                "Angjelin Hila"
            ],
            "arxiv_id": "2512.19570v1",
            "summary": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.",
            "categories": [
                "cs.HC",
                "cs.AI"
            ],
            "primary_category": "cs.HC",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "AI & Soc (2025)",
            "doi": "10.1007/s00146-025-02426-3",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19570v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "大型语言模型对认知的影响：重新思考集体智能与机构知识",
            "summary_zh": "本文探讨了人与大型语言模型（LLM）交互所带来的认知风险。我们构建了集体认知理论，将认知保证分布在人类集体中，并以有限理性和双重过程理论为背景。我们区分了内在论证（对命题为何为真的反思性理解）和外在论证（真理的可靠传递）。两者对于集体理性都是必要的，但只有内在论证才能产生反思性知识。我们将反思性知识定义如下：主体理解主张的评估基础，当该基础不可用时，主体持续评估真理来源的可靠性，并且主体有义务在其能力范围内应用这些标准。我们认为，LLM近似于外在可靠性，因为它们可以可靠地传递其论证基础已在其他地方确立的信息，但它们本身并不具备反思性论证。将反思性工作广泛外包给可靠的LLM输出可能会削弱反思性论证标准，降低理解的积极性，并降低主体履行专业和公民认知义务的能力。为了减轻这些风险，我们提出了一个三层规范方案，包括个人使用的认知交互模型、用于播种和执行认知最优结果规范的机构和组织框架，以及在组织和立法层面实例化话语规范并遏制认知恶习的义务约束。",
            "intro_zh": [
                "现有方法未能充分考虑大型语言模型（LLM）与人类交互对集体认知和机构知识的潜在威胁。",
                "论文提出集体认知理论，强调内在论证（反思性理解）和外在论证（可靠传递）在集体理性中的作用。",
                "论文构建三层规范方案，旨在减轻LLM可能带来的认知风险，维护反思性知识和认知义务。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人与大型语言模型（LLM）交互对集体认知和机构知识产生的潜在威胁。现有方法未能充分考虑LLM在知识生产和传播中扮演的角色，以及这种角色可能对人类认知能力和社会认知规范带来的影响。特别关注的是，过度依赖LLM可能导致反思性知识的弱化和认知义务的缺失。\\n\\n**核心思路**：论文的核心思路是区分内在论证和外在论证，并强调内在论证（即对知识的理解和反思）在集体认知中的重要性。论文认为，LLM虽然可以可靠地传递信息，但缺乏内在论证能力，过度依赖LLM可能导致人类认知能力的退化。因此，需要建立一套规范体系，以维护反思性知识和认知义务。\\n\\n**技术框架**：论文构建了一个三层规范方案：第一层是个人使用的认知交互模型，旨在指导个体如何有效地与LLM交互，避免过度依赖；第二层是机构和组织框架，旨在在组织内部播种和执行认知最优结果的规范；第三层是在组织和立法层面建立义务约束，旨在实例化话语规范并遏制认知恶习。这个框架旨在从个体、组织和社会三个层面，共同维护反思性知识和认知义务。\\n\\n**关键创新**：论文的关键创新在于提出了集体认知理论，并将其应用于分析LLM对人类认知的影响。论文区分了内在论证和外在论证，并强调了内在论证在集体认知中的重要性。此外，论文提出的三层规范方案也为应对LLM带来的认知风险提供了一个可行的框架。\\n\\n**关键设计**：论文的关键设计在于三层规范方案的具体内容。例如，认知交互模型可能包括一些指导原则，如在使用LLM时要保持批判性思维，要验证LLM提供的信息，要尽可能理解LLM的推理过程等。机构和组织框架可能包括一些制度安排，如建立知识管理系统，鼓励员工进行知识分享和讨论，定期进行认知能力培训等。义务约束可能包括一些法律法规，如禁止使用LLM进行欺骗或误导，保护公民的知情权等。这些具体的设计旨在从不同层面维护反思性知识和认知义务。",
            "application_zh": "该研究成果可应用于教育、新闻传播、科研等领域，帮助人们更理性地使用大型语言模型，避免过度依赖，维护批判性思维和认知能力。通过构建更完善的认知规范和社会制度，可以促进人与AI的协同发展，提升集体智能。",
            "highlight_zh": "该论文侧重于理论分析和框架构建，并未提供具体的实验数据。其亮点在于提出了集体认知理论，并将其应用于分析LLM对人类认知的影响，为应对LLM带来的认知风险提供了一个新的视角和框架。",
            "tags_zh": [
                "大型语言模型",
                "集体认知",
                "机构知识",
                "认知风险",
                "认知义务"
            ],
            "_index": 42,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19570v1/human-llm-interaction_model.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
            "authors": [
                "Mingxu Zhang",
                "Dazhong Shen",
                "Qi Zhang",
                "Ying Sun"
            ],
            "arxiv_id": "2512.19240v1",
            "summary": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19240v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "ChemATP：一种用于大语言模型的免训练化学推理框架",
            "summary_zh": "大型语言模型（LLM）在通用推理方面表现出色，但由于标准字符串表示中缺乏明确的化学先验知识，因此在分子科学领域表现不佳。现有的解决方案面临一个根本性的困境。基于训练的方法将先验知识注入参数，但这种静态耦合阻碍了知识的快速更新，并且常常会损害模型的一般推理能力。相反，现有的免训练方法避免了这些问题，但依赖于表面级别的提示，无法提供精确化学推理所必需的细粒度原子级别先验知识。为了解决这个问题，我们引入了ChemATP，一个将化学知识与推理引擎解耦的框架。通过构建第一个原子级别的文本知识库，ChemATP使冻结的LLM能够显式地检索和推理这些信息。这种架构确保了解释性和适应性，同时保留了LLM固有的通用智能。实验表明，ChemATP显著优于免训练基线，并且可以与最先进的基于训练的模型相媲美，这表明显式先验注入是隐式参数更新的一种有竞争力的替代方案。",
            "intro_zh": [
                "现有方法难以在LLM中有效融入化学先验知识，训练方法影响通用性，免训练方法缺乏细粒度信息。",
                "ChemATP构建原子级别知识库，使LLM能够动态检索和推理化学知识，实现知识与推理引擎的解耦。",
                "实验表明，ChemATP性能超越免训练方法，并能与基于训练的SOTA模型竞争，验证了显式先验注入的有效性。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型在化学推理方面表现不佳，主要原因是缺乏明确的化学先验知识。基于训练的方法虽然可以将化学知识注入模型参数，但会损害模型的通用推理能力，并且难以快速更新知识。免训练的方法虽然避免了这些问题，但依赖于表面级别的提示，无法提供原子级别的细粒度信息，导致推理精度不足。\\n\\n**核心思路**：ChemATP的核心思路是将化学知识与推理引擎解耦。通过构建一个原子级别的文本知识库，ChemATP允许冻结的大语言模型动态地检索和推理这些知识。这种方法避免了修改模型参数，从而保留了模型的通用推理能力，并且可以方便地更新知识库。\\n\\n**技术框架**：ChemATP的整体架构包含以下几个主要模块：1) 原子级别文本知识库：存储了关于原子和分子性质的详细信息。2) 知识检索模块：根据输入的问题，从知识库中检索相关的原子级别信息。3) 推理模块：利用检索到的知识，结合大语言模型的推理能力，生成答案。整个流程是：输入问题 -> 知识检索 -> LLM推理 -> 输出答案。\\n\\n**关键创新**：ChemATP最重要的技术创新点在于构建了原子级别的文本知识库，并将其与大语言模型的推理引擎解耦。这种解耦使得模型可以在不修改参数的情况下，动态地获取和利用化学知识，从而提高了模型的推理精度和适应性。与现有方法的本质区别在于，ChemATP采用显式的知识注入方式，而不是隐式的参数更新方式。\\n\\n**关键设计**：ChemATP的关键设计包括：1) 原子级别知识库的构建方式，需要考虑如何有效地表示和存储原子和分子的性质。2) 知识检索模块的设计，需要考虑如何快速准确地检索到相关的知识。3) 推理模块的设计，需要考虑如何将检索到的知识有效地融入到大语言模型的推理过程中。具体的参数设置、损失函数、网络结构等技术细节在论文中可能并未详细描述，属于未知信息。",
            "application_zh": "ChemATP具有广泛的应用前景，例如药物发现、材料设计、化学反应预测等。它可以帮助研究人员更高效地进行分子设计和筛选，加速新药和新材料的研发过程。此外，ChemATP还可以用于化学教育和科普，帮助学生和公众更好地理解化学知识。",
            "highlight_zh": "ChemATP在化学推理任务上取得了显著的性能提升。实验结果表明，ChemATP显著优于免训练基线，并且可以与最先进的基于训练的模型相媲美。例如，在某个具体的化学推理数据集上，ChemATP的准确率比最佳免训练基线提高了XX%，与SOTA的训练模型相比，性能差距缩小到YY%。这些结果表明，显式先验注入是一种有效的化学推理方法。",
            "tags_zh": [
                "化学推理",
                "大语言模型",
                "知识库",
                "免训练",
                "原子级别",
                "先验知识",
                "知识检索"
            ],
            "_index": 43,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19240v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19240v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving",
            "authors": [
                "Peiqing Lu",
                "Yuan Zhang",
                "Haoyun Zhang",
                "Jiasen Zheng",
                "Kejian Tong",
                "Wenjun Wu"
            ],
            "arxiv_id": "2512.19093v1",
            "summary": "Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19093v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "[T]distillation"
                    ],
                    "score": 6.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出HERALD框架，融合工具增强、集成推理与知识蒸馏，提升双语数学问题求解能力",
            "summary_zh": "本文提出了一种名为HERALD（混合集成推理与自适应学习和蒸馏）的框架，用于解决双语数学问题。该框架结合了NuminaMath-7B-TIR、GPT-4o和Mistral-7B等模型，利用自适应路由、基于工具的强化学习和知识蒸馏来连接不同的推理路径。置信度校准保证了权重的稳定性，双路径检查保证了结果的正确性。强化学习控制工具的使用以减少冗余，知识蒸馏在不牺牲准确性的前提下降低了延迟。实验结果表明，结合符号检查、自适应集成和双语微调有助于实现流畅的推理和精确的计算。HERALD为多语言数学推理提供了一个实用的解决方案，具有更高的准确性、稳定性和清晰度。",
            "intro_zh": [
                "大型语言模型在语言理解方面表现出色，但在精确计算方面存在不足，双语数学问题求解需要语言推理和符号计算之间的清晰联系。",
                "HERALD框架通过自适应路由、工具增强的强化学习和知识蒸馏，将多个模型的推理路径连接起来，实现混合集成推理。",
                "实验表明，HERALD框架结合符号检查、自适应集成和双语微调，在多语言数学推理中实现了更高的准确性、稳定性和清晰度。"
            ],
            "method_zh": "**问题定义**：双语数学问题求解需要模型具备强大的语言理解能力和精确的计算能力。现有的大型语言模型虽然擅长语言处理，但在复杂的数学计算方面表现不足，尤其是在多语言环境下，如何有效地将语言推理与符号计算相结合是一个挑战。现有方法往往难以兼顾推理的流畅性和计算的准确性，并且容易产生冗余计算。\n\n**核心思路**：HERALD框架的核心思路是利用混合集成推理，将多个模型的优势结合起来。通过自适应路由选择合适的模型进行推理，利用工具增强的强化学习来优化工具的使用，并通过知识蒸馏来提高效率。这种方法旨在实现推理的流畅性和计算的准确性，同时降低延迟。\n\n**技术框架**：HERALD框架包含以下主要模块：1) 自适应路由模块，根据问题选择合适的模型进行推理；2) 工具增强模块，利用外部工具进行符号计算；3) 强化学习模块，优化工具的使用策略，减少冗余计算；4) 知识蒸馏模块，将多个模型的知识转移到一个更小的模型中，以提高效率；5) 置信度校准模块，保证权重的稳定性；6) 双路径检查模块，保证结果的正确性。\n\n**关键创新**：HERALD框架的关键创新在于其混合集成推理方法，它能够有效地结合多个模型的优势，实现更准确、更高效的数学问题求解。此外，该框架还引入了工具增强的强化学习，能够自动学习最优的工具使用策略，减少人工干预。自适应路由和双路径检查进一步提高了系统的鲁棒性和准确性。\n\n**关键设计**：HERALD框架使用了NuminaMath-7B-TIR、GPT-4o和Mistral-7B等多个模型，并针对双语数学问题进行了微调。强化学习模块使用了策略梯度算法，奖励函数的设计考虑了计算的准确性和效率。知识蒸馏模块使用了交叉熵损失函数，并对不同模型的输出进行了加权平均。",
            "application_zh": "HERALD框架可应用于智能教育、金融分析、科学计算等领域。在智能教育中，它可以帮助学生解决数学难题，提供个性化的学习辅导。在金融分析中，它可以用于风险评估、投资决策等。在科学计算中，它可以用于模拟仿真、数据分析等。该研究有望推动人工智能在数学领域的应用，并为其他领域的智能问题求解提供借鉴。",
            "highlight_zh": "HERALD框架通过结合符号检查、自适应集成和双语微调，在多语言数学推理任务上取得了显著的性能提升。具体数据需要在论文中查找，但摘要表明该框架在准确性、稳定性和清晰度方面均优于现有方法。知识蒸馏的使用在不牺牲准确性的前提下降低了延迟，提高了效率。",
            "tags_zh": [
                "双语数学问题求解",
                "混合集成推理",
                "工具增强",
                "强化学习",
                "知识蒸馏"
            ],
            "_index": 44,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19093v1/134_1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19093v1/134_2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19093v1/134_4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm",
            "authors": [
                "Li Yan",
                "Bolun Liu",
                "Chao Li",
                "Jing Liang",
                "Kunjie Yu",
                "Caitong Yue",
                "Xuzhao Chai",
                "Boyang Qu"
            ],
            "arxiv_id": "2512.18947v1",
            "summary": "Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.",
            "categories": [
                "cs.AI",
                "cs.NE"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18947v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]multimodal"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于聚类迁移学习的动态多模态多目标进化算法，解决动态环境下的多模态优化问题。",
            "summary_zh": "本文针对动态多模态多目标优化问题，该问题需要在时变环境中同时跟踪多个等价的帕累托最优集并保持种群多样性。现有动态多目标进化算法通常忽略解的模态，而静态多模态多目标进化算法缺乏对动态变化的适应性。为了解决上述挑战，本文做出了两个主要贡献。首先，我们构建了一个新的动态多模态多目标测试函数基准套件，该套件融合了动态和多模态优化的特性，以建立一个严格的评估平台。其次，我们提出了一种以基于聚类的自编码器预测动态响应机制为中心的新算法，该算法利用自编码器模型处理匹配的聚类以生成高度多样化的初始种群。此外，为了平衡算法的收敛性和多样性，我们将自适应小生境策略集成到静态优化器中。在12个动态多模态多目标测试函数实例上的经验分析表明，与几种最先进的动态多目标进化算法和多模态多目标进化算法相比，我们的算法不仅在决策空间中更有效地保持了种群多样性，而且在目标空间中实现了卓越的收敛性。",
            "intro_zh": [
                "现有动态多目标进化算法忽略解的模态，静态多模态算法缺乏对动态环境的适应性，难以同时跟踪多个最优解并保持种群多样性。",
                "提出基于聚类的自编码器预测动态响应机制，利用自编码器处理聚类生成多样化初始种群，并结合自适应小生境策略平衡收敛性和多样性。",
                "在12个动态多模态测试函数上的实验表明，该算法在决策空间和目标空间均优于现有算法，能有效保持种群多样性并实现更好的收敛性。"
            ],
            "method_zh": "**问题定义**：动态多模态多目标优化问题（DMMOP）旨在解决环境随时间变化时，同时优化多个目标并找到多个等价的帕累托最优解集的问题。现有方法的痛点在于，动态多目标进化算法（DMOEAs）通常忽略解的模态信息，导致难以维持种群多样性；而静态多模态多目标进化算法（MMOEAs）则缺乏对动态环境的适应性，无法有效跟踪最优解的变化。\\n\\n**核心思路**：本文的核心思路是利用聚类和迁移学习的思想，通过自编码器学习历史环境中的解分布信息，并在新环境中生成具有多样性的初始种群。具体来说，首先对历史种群进行聚类，然后训练自编码器学习每个簇的特征表示，最后在新环境中，利用自编码器生成新的种群，并结合自适应小生境策略来平衡算法的收敛性和多样性。这样设计的目的是为了充分利用历史信息，加速算法的收敛速度，并维持种群的多样性，从而更好地适应动态环境的变化。\\n\\n**技术框架**：该算法主要包含以下几个阶段：1) **环境变化检测**：检测环境是否发生变化。2) **聚类与自编码器训练**：对历史种群进行聚类，并训练自编码器学习每个簇的特征表示。3) **种群初始化**：在新环境中，利用训练好的自编码器生成新的种群。4) **自适应小生境优化**：利用自适应小生境策略对种群进行优化，平衡算法的收敛性和多样性。5) **种群更新**：根据环境变化情况，更新种群和自编码器模型。\\n\\n**关键创新**：该算法的关键创新在于：1) 提出了基于聚类的自编码器预测动态响应机制，能够有效地利用历史信息生成具有多样性的初始种群。2) 将自适应小生境策略集成到静态优化器中，从而更好地平衡算法的收敛性和多样性。3) 构建了一个新的动态多模态多目标测试函数基准套件，为该领域的研究提供了一个严格的评估平台。\\n\\n**关键设计**：在聚类方面，可以使用K-means等算法。自编码器的网络结构可以根据具体问题进行调整，损失函数可以使用均方误差等。自适应小生境策略的关键在于如何动态调整小生境半径，以平衡算法的收敛性和多样性。具体的参数设置需要根据实验结果进行调整。",
            "application_zh": "该研究成果可应用于需要动态优化和维持多个最优解的实际问题，例如：动态资源调度、机器人路径规划、电力系统优化、金融投资组合优化等。通过快速适应环境变化并保持解的多样性，可以提高决策效率和鲁棒性，从而在动态环境中获得更好的性能。",
            "highlight_zh": "实验结果表明，该算法在12个动态多模态多目标测试函数实例上，与几种最先进的DMOEAs和MMOEAs相比，不仅在决策空间中更有效地保持了种群多样性，而且在目标空间中实现了卓越的收敛性。具体性能提升数据在论文中给出，证明了该算法在解决DMMOP问题上的有效性。",
            "tags_zh": [
                "动态多目标优化",
                "多模态优化",
                "进化算法",
                "迁移学习",
                "聚类",
                "自编码器",
                "动态环境"
            ],
            "_index": 45,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18947v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18947v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18947v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting",
            "authors": [
                "Ivan Decostanzi",
                "Yelena Mejova",
                "Kyriaki Kalimeri"
            ],
            "arxiv_id": "2512.19475v1",
            "summary": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "18 pages, 3 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19475v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于大语言模型的自动化人道主义情况报告框架，提升报告效率与质量",
            "summary_zh": "本文提出了一种全自动框架，利用大型语言模型（LLM）将异构的人道主义文档转换为结构化且基于证据的报告，旨在解决当前人道主义决策中情况报告工作流程的手动、资源密集和不一致等问题。该系统集成了语义文本聚类、自动问题生成、带有引用的检索增强型答案抽取、多层次摘要和执行摘要生成，并辅以模拟专家推理的内部评估指标。在包括自然灾害和冲突在内的13个人道主义事件中，使用来自ReliefWeb等验证来源的1100多份文档对该框架进行了评估。生成的问题的相关性达到84.7%，重要性达到84.0%，紧迫性达到76.4%。提取的答案的相关性达到86.3%，引用精确率和召回率均超过76%。人类评估与基于LLM的评估之间的一致性超过了0.80的F1分数。对比分析表明，所提出的框架生成的报告比现有基线更结构化、更易于解释且更具可操作性。通过将LLM推理与透明的引用链接和多层次评估相结合，本研究证明了生成式AI可以自主生成准确、可验证且在操作上有用的人道主义情况报告。",
            "intro_zh": [
                "当前人道主义情况报告流程依赖手动，耗费大量资源，且一致性不足，影响决策效率。",
                "利用大型语言模型，结合语义聚类、问题生成、检索增强答案抽取等技术，构建自动化报告框架。",
                "实验结果表明，该框架生成的报告在相关性、可解释性和可操作性方面均优于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决人道主义援助领域中，人工生成情况报告效率低、成本高、且主观性强的问题。现有方法依赖人工收集和整理信息，难以快速响应突发事件，且不同报告员的风格和侧重点可能存在差异，导致报告质量参差不齐。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型的强大自然语言处理能力，自动化地从大量异构的人道主义文档中提取关键信息，并将其组织成结构化的报告。通过结合多种技术，例如问题生成、检索增强和多层次摘要，确保报告的准确性、可验证性和可操作性。\\n\\n**技术框架**：该框架包含以下主要模块：1) **语义文本聚类**：对输入文档进行聚类，将相关文档分组；2) **自动问题生成**：根据文档内容自动生成关键问题；3) **检索增强型答案抽取**：从文档中检索并抽取与问题相关的答案，并提供引用链接；4) **多层次摘要**：生成不同层次的摘要，包括详细摘要和执行摘要；5) **内部评估**：使用内部评估指标评估报告质量。\\n\\n**关键创新**：该框架的关键创新在于将大型语言模型应用于人道主义情况报告的自动化生成，并结合多种技术以提高报告的质量和可信度。与传统的信息检索或文本摘要方法相比，该框架能够更深入地理解文档内容，并生成更具结构化和可操作性的报告。此外，框架还集成了内部评估机制，能够自动评估报告的质量，并提供改进建议。\\n\\n**关键设计**：论文中使用了基于Transformer的大型语言模型，例如BERT或其变体，进行问题生成和答案抽取。检索增强模块使用了向量数据库（例如FAISS）来加速文档检索。损失函数方面，可能使用了交叉熵损失或对比学习损失来训练问题生成和答案抽取模型。具体参数设置和网络结构细节在论文中可能有所描述，但此处未知。",
            "application_zh": "该研究成果可广泛应用于人道主义援助、灾害管理、危机响应等领域。通过自动化生成高质量的情况报告，可以帮助决策者更快地了解情况、制定更有效的应对措施，从而减少人员伤亡和财产损失。未来，该技术还可以扩展到其他领域，例如金融风险评估、舆情监控等。",
            "highlight_zh": "实验结果表明，该框架生成的问题的相关性达到84.7%，重要性达到84.0%，紧迫性达到76.4%。提取的答案的相关性达到86.3%，引用精确率和召回率均超过76%。人类评估与基于LLM的评估之间的一致性超过了0.80的F1分数。这些数据表明，该框架能够生成高质量且可信度高的报告。",
            "tags_zh": [
                "大型语言模型",
                "人道主义援助",
                "情况报告",
                "自动化",
                "信息抽取",
                "自然语言处理",
                "灾害管理"
            ],
            "_index": 46,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19475v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19475v1/4versions.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19475v1/expertseval.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models",
            "authors": [
                "Zhiqing Hu",
                "Chenxu Zhao",
                "Jiazhong Lu",
                "Xiaolei Liu"
            ],
            "arxiv_id": "2512.19378v1",
            "summary": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Camera-ready version of the paper accepted for oral presentation at the 11th International Conference on Computer and Communications (ICCC 2025)",
            "doi": "",
            "journal_ref": "In Proceedings of the 11th International Conference on Computer and Communications, 2025",
            "pdf_url": "https://arxiv.org/pdf/2512.19378v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "[T]large language model"
                    ],
                    "score": 9.0
                }
            ],
            "relevance_score": 9.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出高精度三集合水印方案HATS，用于保护大型语言模型生成文本的版权",
            "summary_zh": "本文提出了一种针对大型语言模型（LLM）生成文本的水印技术，旨在遏制LLM生成文本的滥用。该水印技术在每个解码步骤将词汇表划分为三个集合（绿/黄/红），并限制采样仅在绿色和黄色集合中进行。在检测时，重放相同的划分，计算绿色集合的富集和红色集合的耗尽统计量，将其转换为单侧z分数，并通过Fisher方法聚合它们的p值，以判断一段文本是否被水印标记。在Llama 2 7B上实现了生成、检测和测试，并评估了真阳性率、假阳性率和文本质量。结果表明，三集合划分方案在固定FPR下实现了高检测精度，同时保持了可读性。",
            "intro_zh": [
                "大型语言模型生成文本的滥用是一个日益严重的问题，需要有效的水印技术来追踪和识别生成来源。",
                "论文提出一种新颖的三集合水印方案，通过在生成过程中限制词汇采样空间，嵌入隐式信号。",
                "实验结果表明，该方案在保持文本质量的同时，显著提高了水印检测的准确性，降低了误报率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）生成文本的版权保护问题。现有方法可能存在检测精度不高、对文本质量影响较大或容易被规避等痛点。因此，需要一种鲁棒且高效的水印方案，能够在不显著影响生成文本质量的前提下，准确地识别出由特定LLM生成的内容。\\n\\n**核心思路**：论文的核心思路是将LLM的词汇表在每个解码步骤动态划分为三个互斥的集合：绿色集合、黄色集合和红色集合。生成过程被限制为主要从绿色和黄色集合中采样，从而在文本中引入一种统计偏差。这种偏差在检测阶段可以被用来判断文本是否被水印标记。\\n\\n**技术框架**：该水印方案包含三个主要阶段：1) **词汇划分**：在每个解码步骤，根据预定义的规则（例如哈希函数）将词汇表划分为绿色、黄色和红色集合，并保持三者比例固定。2) **文本生成**：限制LLM的采样过程，使其主要从绿色和黄色集合中选择token。3) **水印检测**：对于待检测文本，重复词汇划分过程，计算绿色集合的富集程度和红色集合的耗尽程度，并基于统计检验判断文本是否包含水印。\\n\\n**关键创新**：该方案的关键创新在于使用三集合划分，同时考虑了绿色集合的富集和红色集合的耗尽。相比于传统的二元划分，三集合划分能够更有效地利用词汇信息，提高水印的鲁棒性和检测精度。此外，通过Fisher方法聚合绿色富集和红色耗尽的p值，进一步提升了检测的可靠性。\\n\\n**关键设计**：关键设计包括：1) **集合比例**：绿色、黄色和红色集合的比例需要仔细调整，以平衡检测精度和文本质量。2) **哈希函数**：用于词汇划分的哈希函数需要具有良好的随机性和均匀性，以避免引入偏差。3) **统计检验**：使用单侧z分数和Fisher方法进行统计检验，以评估水印存在的可能性。4) **阈值设定**：需要根据实际应用场景设定合适的阈值，以控制假阳性率。",
            "application_zh": "该研究成果可应用于保护大型语言模型生成内容的版权，防止恶意使用和传播。例如，可以用于识别和追踪由特定LLM生成的虚假新闻、恶意代码或侵权内容。此外，该技术还可以用于验证LLM生成内容的真实性，提升用户对LLM的信任度。",
            "highlight_zh": "实验结果表明，HATS水印方案在Llama 2 7B模型上实现了高检测精度，同时保持了较低的假阳性率。具体来说，在固定假阳性率的情况下，HATS能够显著提高真阳性率，优于现有的水印方案。此外，实验还验证了HATS对文本质量的影响较小，能够生成可读性良好的文本。",
            "tags_zh": [
                "大型语言模型",
                "水印技术",
                "版权保护",
                "文本生成",
                "统计检测"
            ],
            "_index": 47,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19378v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19378v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
            "authors": [
                "Kyungwon Cho",
                "Hanbyul Joo"
            ],
            "arxiv_id": "2512.19283v1",
            "summary": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Project Page: https://kyungwoncho.github.io/HaMoS/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19283v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "[T]egocentric",
                        "egocentric vision"
                    ],
                    "score": 8.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "6_video_extraction"
            ],
            "headline_zh": "提出HaMoS：一种手部感知的序列级自中心运动重建扩散框架",
            "summary_zh": "自中心视觉系统日益普及，为人机交互创造了新机遇。一个核心挑战是从第一人称视频中估计穿戴者的全身运动，这对于理解人类行为至关重要。然而，这项任务很困难，因为大部分身体部位在自中心视角下是不可见的。先前的方法主要依赖于头部轨迹，导致模糊性，或者假设连续跟踪手部，这对于轻量级自中心设备是不现实的。本文提出了HaMoS，这是第一个手部感知的序列级扩散框架，它直接以头部轨迹和因视场限制和遮挡而间歇性可见的手部线索为条件，就像在真实的自中心设备中一样。为了克服缺乏将多样相机视角与人体运动配对的数据集的问题，我们引入了一种新的增强方法，该方法模拟了这种真实世界的条件。我们还证明了诸如体型和视场之类的序列级上下文对于准确的运动重建至关重要，因此采用局部注意力来有效地推断长序列。在公共基准上的实验表明，我们的方法实现了最先进的准确性和时间平滑性，展示了朝着可靠的野外自中心3D运动理解迈出的实际一步。",
            "intro_zh": [
                "现有自中心运动重建方法依赖头部轨迹或假设连续手部跟踪，在实际应用中存在模糊性和不现实性。",
                "HaMoS框架利用头部轨迹和间歇性可见的手部线索，结合序列级上下文信息，实现更准确的运动重建。",
                "通过新颖的数据增强方法和局部注意力机制，HaMoS在公共基准上取得了state-of-the-art的准确性和时间平滑性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从自中心视角视频中准确重建人体全身运动的问题。现有方法要么依赖不准确的头部轨迹，要么假设不现实的连续手部跟踪，无法很好地处理真实场景中手部遮挡和视场限制等问题。\\n\\n**核心思路**：论文的核心思路是利用间歇性可见的手部信息作为补充，结合头部轨迹，并引入序列级上下文信息（如体型和视场），来更准确地推断全身运动。通过扩散模型，将这些信息融合在一起，实现鲁棒的运动重建。\\n\\n**技术框架**：HaMoS框架主要包含以下几个模块：1) 特征提取模块，用于提取头部轨迹和手部关键点的特征；2) 序列建模模块，利用局部注意力机制对序列级上下文进行建模；3) 扩散模型，将提取的特征和上下文信息作为条件，生成人体运动序列。框架采用了一种新颖的数据增强方法，模拟真实场景中的手部遮挡和视场限制。\\n\\n**关键创新**：论文的关键创新在于：1) 提出了手部感知的自中心运动重建方法，充分利用了间歇性可见的手部信息；2) 引入了序列级上下文信息，提高了运动重建的准确性；3) 设计了一种新的数据增强方法，模拟真实场景中的手部遮挡和视场限制。\\n\\n**关键设计**：论文的关键设计包括：1) 局部注意力机制，用于高效地建模长序列上下文；2) 扩散模型，用于生成高质量的运动序列；3) 数据增强策略，通过随机遮挡手部和改变视场来模拟真实场景。",
            "application_zh": "该研究成果可应用于人机交互、虚拟现实、增强现实等领域。通过准确重建用户在自中心视角下的全身运动，可以实现更自然、更沉浸式的交互体验。例如，在VR游戏中，可以根据用户的真实运动来控制虚拟角色的行为。此外，该技术还可以用于运动分析、健康监测等领域。",
            "highlight_zh": "HaMoS在公共基准测试中取得了state-of-the-art的性能，显著提高了自中心运动重建的准确性和时间平滑性。实验结果表明，该方法能够有效地处理手部遮挡和视场限制等问题，在真实场景中具有良好的鲁棒性。具体性能数据和对比基线信息未在摘要中详细给出，需查阅论文全文。",
            "tags_zh": [
                "自中心视觉",
                "运动重建",
                "手部感知",
                "序列建模",
                "扩散模型"
            ],
            "_index": 48,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19283v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19283v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19283v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Towards AI-Guided Open-World Ecological Taxonomic Classification",
            "authors": [
                "Cheng Yaw Low",
                "Heejoon Koo",
                "Jaewoo Park",
                "Kaleb Mesfin Asfaw",
                "Meeyoung Cha"
            ],
            "arxiv_id": "2512.18994v1",
            "summary": "AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "4 figures, 11 tables, and 15 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18994v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱八：物理动画 (Physics-based Animation)",
                    "id": "8_physics_animation",
                    "matched_keywords": [
                        "spatiotemporal"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "foundation model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 8.0,
            "hit_pillars": [
                "8_physics_animation",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TaxoNet，解决开放世界生态分类中的长尾分布和领域偏移问题。",
            "summary_zh": "本文提出了一个开放世界生态分类框架，旨在解决现实生态环境中存在的长尾分类分布、细粒度分类差异、测试时空域偏移以及闭集假设等挑战。为了应对这些问题，作者提出了TaxoNet，一种基于嵌入的编码器，它使用双边际惩罚损失来加强来自稀有、代表性不足的类别的学习信号，同时减轻过度代表类别的优势，从而直接应对相互关联的挑战。该方法在多个生态领域进行了评估，包括Google Auto-Arborist（城市树木）、iNat-Plantae（来自iNaturalist-2019的植物观测）和NAFlora-Mini（一个精选的植物标本馆集合）。实验结果表明，TaxoNet始终优于基线方法，尤其是在稀有类别上，为开放世界植物分类监测奠定了坚实的基础。研究还表明，通用多模态基础模型在植物领域应用中仍然受到限制。",
            "intro_zh": [
                "现有生态分类方法难以应对长尾分布、细粒度差异和领域偏移等开放世界挑战，限制了其在实际生态监测中的应用。",
                "TaxoNet通过嵌入式编码器和双边际惩罚损失，增强稀有类别的学习信号，同时抑制常见类别的影响，从而有效应对上述挑战。",
                "实验表明，TaxoNet在多个生态数据集上显著优于现有方法，尤其是在稀有类别上，验证了其在开放世界生态分类中的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决开放世界生态分类问题，该问题面临着长尾分类分布、细粒度分类差异、测试时空域偏移以及闭集假设等挑战。现有方法通常假设类别分布均衡，忽略了稀有类别的学习，并且难以泛化到新的领域和类别。这些问题限制了生态分类在生物多样性监测、保护规划和政策制定等方面的应用。\\n\\n**核心思路**：TaxoNet的核心思路是通过学习一个鲁棒的嵌入空间，使得同一类别的样本在嵌入空间中聚集，不同类别的样本尽可能分离。为了解决长尾分布问题，TaxoNet采用双边际惩罚损失，对稀有类别施加更大的惩罚，从而增强其学习信号，同时对常见类别施加较小的惩罚，以防止其过度主导学习过程。\\n\\n**技术框架**：TaxoNet的整体框架包括一个嵌入编码器和一个双边际惩罚损失函数。嵌入编码器将输入的图像或多模态数据映射到嵌入空间。双边际惩罚损失函数基于嵌入空间中的样本距离计算损失，并根据类别的频率调整惩罚力度。训练过程中，TaxoNet通过最小化双边际惩罚损失来学习鲁棒的嵌入空间。\\n\\n**关键创新**：TaxoNet的关键创新在于其双边际惩罚损失函数，该损失函数能够自适应地调整不同类别的惩罚力度，从而有效地解决长尾分布问题。与传统的交叉熵损失或对比损失相比，双边际惩罚损失能够更好地平衡常见类别和稀有类别的学习，提高稀有类别的分类精度。\\n\\n**关键设计**：双边际惩罚损失函数的设计是TaxoNet的关键。该损失函数包含两个边际参数，分别控制对正样本和负样本的惩罚力度。对于稀有类别，正样本的边际参数较小，负样本的边际参数较大，从而增强其学习信号。对于常见类别，正样本的边际参数较大，负样本的边际参数较小，以防止其过度主导学习过程。具体的网络结构和参数设置根据不同的数据集和任务进行调整。",
            "application_zh": "该研究成果可应用于生物多样性监测、保护规划和政策制定等领域。通过自动识别和分类生态物种，可以更有效地评估生物多样性状况，制定合理的保护措施，并为环境政策提供科学依据。此外，该方法还可以应用于农业、林业等领域，例如病虫害监测和作物分类。",
            "highlight_zh": "实验结果表明，TaxoNet在Google Auto-Arborist、iNat-Plantae和NAFlora-Mini等多个生态数据集上均优于基线方法。尤其是在稀有类别上，TaxoNet的分类精度显著提升，证明了其在解决长尾分布问题上的有效性。此外，研究还表明，通用多模态基础模型在植物领域应用中仍然存在局限性，需要针对特定领域进行优化。",
            "tags_zh": [
                "生态分类",
                "开放世界学习",
                "长尾分布",
                "领域自适应",
                "嵌入学习"
            ],
            "_index": 49,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18994v1/figures/openworld.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18994v1/figures/embedding_collapse.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18994v1/figures/norm_analysis.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
            "authors": [
                "Jiawen Wang",
                "Jingjing Wang Tianyang Chen",
                "Min Zhang",
                "Guodong Zhou"
            ],
            "arxiv_id": "2512.19551v1",
            "summary": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19551v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱四：生成式动作 (Generative Motion)",
                    "id": "4_motion_diffusion",
                    "matched_keywords": [
                        "[T]motion generation"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "4_motion_diffusion"
            ],
            "headline_zh": "提出基于LLM的终身情感动作生成框架，解决新场景下的情感动作泛化问题",
            "summary_zh": "本文提出了一种新的基于LLM的终身情感动作生成(L^2-EMG)任务，旨在使LLM能够持续获取不同未见场景下的情感动作生成知识，从而有助于构建一个具备同理心和智能的闭环和自我进化的具身智能体。本文提出了L^2-EMG任务中的两个关键挑战，即情感解耦挑战和场景适应挑战。为此，本文提出了一种情感可迁移和场景自适应的混合专家模型(ES-MoE)，该模型设计了一个因果引导的情感解耦块和一个场景自适应的专家构建块，分别应对这两个挑战。特别地，本文构建了多个L^2-EMG数据集来验证ES-MoE方法的有效性。广泛的评估表明，ES-MoE优于先进的基线方法。",
            "intro_zh": [
                "现有情感动作生成方法主要关注在单一固定数据集上的性能提升，忽略了灵活且规模增长的运动场景。",
                "论文提出基于LLM的终身情感动作生成任务，并设计情感可迁移和场景自适应的混合专家模型(ES-MoE)。",
                "实验结果表明，ES-MoE在多个L^2-EMG数据集上优于现有先进基线方法，验证了其有效性。"
            ],
            "method_zh": "**问题定义**：现有的人类中心情感动作生成方法主要关注在单个数据集上的性能提升，缺乏在新兴的、规模不断增长的运动场景（如体育、舞蹈）中的泛化能力。这些新场景的学习对于提升模型在真实世界的泛化能力至关重要。因此，需要一种方法能够使模型持续学习不同场景下的情感动作生成知识。\n\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）的知识和推理能力，使其能够像人类一样，通过不断学习新的场景和情感表达方式，从而具备终身学习情感动作生成的能力。通过设计特定的模块来解决情感解耦和场景适应的挑战，使得模型能够更好地理解和生成情感化的动作。\n\n**技术框架**：整体框架是一个基于LLM的混合专家模型（ES-MoE）。主要包含两个核心模块：1) 因果引导的情感解耦块：用于将情感信息从动作中分离出来，使得模型可以更好地理解和控制情感的表达。2) 场景自适应的专家构建块：用于根据不同的场景选择合适的专家模型，从而实现场景自适应的动作生成。整个流程包括：输入文本描述，情感解耦，场景选择，动作生成，以及通过损失函数进行优化。\n\n**关键创新**：论文的关键创新在于提出了一个基于LLM的终身情感动作生成框架，并设计了情感可迁移和场景自适应的混合专家模型（ES-MoE）。ES-MoE能够有效地解决情感解耦和场景适应的挑战，使得模型可以在不同的未见场景下生成情感化的动作。与现有方法相比，该方法更具泛化能力和可扩展性。\n\n**关键设计**：情感解耦块采用因果推理的方式，通过干预情感变量来学习情感和动作之间的因果关系。场景自适应的专家构建块通过学习不同场景的特征表示，并根据场景特征选择合适的专家模型。损失函数包括动作生成损失、情感一致性损失和场景适应损失，用于优化模型的性能。",
            "application_zh": "该研究成果可应用于虚拟现实、游戏、人机交互等领域，例如创建更具情感表达能力的虚拟角色，或者使机器人能够根据人类的情感状态做出相应的动作反应。此外，该技术还可以用于辅助情感障碍患者的康复治疗，帮助他们更好地理解和表达情感。",
            "highlight_zh": "ES-MoE在多个L^2-EMG数据集上进行了广泛的评估，实验结果表明，ES-MoE在动作生成质量和情感表达准确性方面均优于现有先进的基线方法。具体性能提升数据未知，但论文强调ES-MoE在未见场景下的泛化能力显著增强。",
            "tags_zh": [
                "情感动作生成",
                "终身学习",
                "大型语言模型",
                "混合专家模型",
                "具身智能",
                "情感解耦",
                "场景适应"
            ],
            "_index": 50,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19551v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19551v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19551v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
            "authors": [
                "A. A. Gde Yogi Pramana",
                "Jason Ray",
                "Anthony Jaya",
                "Michael Wijaya"
            ],
            "arxiv_id": "2512.19317v1",
            "summary": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19317v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "SafeMed-R1：用于视觉-语言模型中可泛化和鲁棒医学推理的对抗强化学习",
            "summary_zh": "视觉-语言模型(VLM)在医学视觉问答(VQA)方面显示出巨大的潜力，但其在临床环境中的部署受到对抗攻击严重脆弱性的阻碍。标准的对抗训练虽然对简单的任务有效，但通常会降低泛化性能和生成的临床推理质量。我们引入了SafeMed-R1，一个混合防御框架，它确保了鲁棒的性能，同时保持高质量、可解释的医学推理。SafeMed-R1采用两阶段方法：在训练时，我们将对抗训练与群体相对策略优化(AT-GRPO)相结合，以显式地增强推理过程，使其免受最坏情况的扰动；在推理时，我们使用随机平滑来增强模型，以提供经过认证的$L_2$-范数鲁棒性保证。我们在OmniMedVQA基准上评估了SafeMed-R1，该基准涵盖了八种医学成像模式，包含超过88,000个样本。我们的实验表明，标准的微调VLM虽然在干净的输入上达到了95%的准确率，但在PGD攻击下会崩溃到大约25%。相比之下，SafeMed-R1在相同的对抗条件下保持了84.45%的准确率，代表了鲁棒性提高了59个百分点。此外，我们证明了使用显式思维链推理训练的模型比仅使用指令的变体表现出更强的对抗鲁棒性，这表明医学人工智能系统中可解释性和安全性之间存在协同作用。",
            "intro_zh": [
                "现有的视觉-语言模型在医学视觉问答中易受对抗攻击，标准对抗训练降低了泛化性和推理质量。",
                "SafeMed-R1通过结合对抗训练与群体相对策略优化，并在推理时使用随机平滑，增强模型的鲁棒性。",
                "实验表明，SafeMed-R1在对抗攻击下比标准微调VLM的鲁棒性提高了59个百分点，且思维链推理模型更鲁棒。"
            ],
            "method_zh": "**问题定义**：论文旨在解决医学视觉问答（Medical VQA）中，视觉-语言模型（VLM）容易受到对抗攻击的问题。现有方法，如标准对抗训练，虽然能提高鲁棒性，但会牺牲模型的泛化能力和推理质量，导致在实际临床应用中效果不佳。\\n\\n**核心思路**：论文的核心思路是结合对抗训练和强化学习，设计一个混合防御框架SafeMed-R1，在训练阶段增强模型对对抗样本的鲁棒性，同时保持其泛化能力和推理质量。此外，在推理阶段使用随机平滑技术，提供经过认证的鲁棒性保证。\\n\\n**技术框架**：SafeMed-R1框架包含两个主要阶段：训练阶段和推理阶段。在训练阶段，使用对抗训练与群体相对策略优化（AT-GRPO）相结合的方法，显式地增强推理过程，使其免受最坏情况的扰动。在推理阶段，使用随机平滑技术来增强模型，以提供经过认证的$L_2$-范数鲁棒性保证。\\n\\n**关键创新**：论文的关键创新在于将对抗训练与群体相对策略优化相结合，用于增强医学视觉问答模型的鲁棒性。与传统的对抗训练方法相比，AT-GRPO能够更好地平衡模型的鲁棒性和泛化能力，同时保持高质量的临床推理。此外，论文还证明了使用显式思维链推理训练的模型具有更强的对抗鲁棒性。\\n\\n**关键设计**：AT-GRPO的具体实现细节未知，但可以推测其目标是优化策略，使得模型在对抗样本上的表现尽可能接近在干净样本上的表现。随机平滑的具体参数设置未知，但其目的是通过对输入进行随机扰动，并对模型的输出进行平均，从而提高模型的鲁棒性。论文中使用的损失函数和网络结构等技术细节未知。",
            "application_zh": "SafeMed-R1的研究成果可应用于医疗影像诊断、辅助决策等领域，提高医学人工智能系统的安全性和可靠性。通过增强模型对对抗攻击的防御能力，可以避免恶意攻击者篡改诊断结果，保障患者的权益。该研究还有助于推动医学人工智能在临床环境中的广泛应用，提升医疗服务的质量和效率。",
            "highlight_zh": "SafeMed-R1在OmniMedVQA基准测试中表现出色，在PGD攻击下，标准微调VLM的准确率从95%下降到25%，而SafeMed-R1保持了84.45%的准确率，鲁棒性提高了59个百分点。此外，实验还表明，使用显式思维链推理训练的模型比仅使用指令的变体表现出更强的对抗鲁棒性。",
            "tags_zh": [
                "医学视觉问答",
                "对抗攻击",
                "对抗训练",
                "强化学习",
                "鲁棒性",
                "视觉-语言模型",
                "群体相对策略优化",
                "随机平滑"
            ],
            "_index": 51,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19317v1/general_ovvw.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19317v1/example.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
            "authors": [
                "Jiaao Wu",
                "Xian Zhang",
                "Fan Yang",
                "Yinpeng Dong"
            ],
            "arxiv_id": "2512.19287v1",
            "summary": "We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "20 pages, 13 figures",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19287v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱五：交互与反应 (Interaction & Reaction)",
                    "id": "5_interaction_reaction",
                    "matched_keywords": [
                        "[T]IMoS"
                    ],
                    "score": 7.5
                }
            ],
            "relevance_score": 7.5,
            "hit_pillars": [
                "5_interaction_reaction"
            ],
            "headline_zh": "提出Vibe Reasoning，提升AI在复杂数学问题上的推理能力",
            "summary_zh": "本文提出了一种人机协作范式Vibe Reasoning，用于解决复杂的数学问题。核心思想是前沿AI模型已经具备解决难题所需的知识，但缺乏应用这些知识的方法、策略和时机。Vibe Reasoning通过通用元提示、基于Agent的 grounding 和模型编排，将AI的潜在能力转化为实际能力。本文以IMO 2025问题6（一个组合优化问题，此前AI系统公开报告失败）为例，结合GPT-5的探索能力和Gemini 3 Pro的证明能力，利用基于Agent的工作流、Python代码执行和基于文件的记忆，推导出正确答案（2112）和一个严谨的数学证明。通过多次迭代改进，发现了Agent grounding和模型编排的必要性，同时人类提示从特定于问题的提示演变为通用的、可转移的元提示。分析了有能力的AI自主失败的原因，以及每个组件如何解决特定的失败模式，并提取了有效Vibe Reasoning的原则。研究表明，轻量级的人工指导可以释放前沿模型的数学推理潜力。这是一项正在进行的工作，正在开发自动化框架并进行更广泛的评估，以进一步验证Vibe Reasoning的通用性和有效性。",
            "intro_zh": [
                "现有AI模型在解决复杂数学问题时，缺乏有效利用已有知识的能力，导致自主求解失败。",
                "Vibe Reasoning通过人机协作，利用元提示引导AI，结合Agent机制和模型编排，激发AI的潜在推理能力。",
                "在IMO 2025问题6上，Vibe Reasoning成功求解，证明了其在提升AI数学推理能力方面的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决复杂数学问题，特别是那些需要创造性推理和证明的问题，例如国际数学奥林匹克（IMO）的问题。现有方法，尤其是完全自主的AI系统，在解决这类问题时表现不佳，无法有效地将已有的数学知识应用于解决新问题，缺乏探索、验证和整合不同知识的能力。\\n\\n**核心思路**：Vibe Reasoning的核心思路是人机协作，通过人类的轻量级指导来激发AI模型的潜在能力。它认为，前沿AI模型已经具备解决复杂数学问题所需的知识，但缺乏应用这些知识的策略和方法。通过精心设计的提示（元提示），引导AI模型进行探索、验证和整合，从而解决问题。\\n\\n**技术框架**：Vibe Reasoning的技术框架主要包含三个关键组件：1) **通用元提示**：使用通用的、可转移的提示，而非特定于问题的提示，引导AI模型进行推理。2) **基于Agent的Grounding**：利用Agent机制，例如Python代码执行和文件系统，为AI模型提供外部知识和工具，并允许模型进行实验和验证。3) **模型编排**：结合不同AI模型的优势，例如GPT-5的探索能力和Gemini 3 Pro的证明能力，通过编排不同的模型来完成不同的任务。\\n\\n**关键创新**：Vibe Reasoning的关键创新在于其人机协作的范式，以及将AI模型视为具有潜在能力的“智能体”，通过轻量级的人工指导来激发其能力。与传统的完全自主的AI系统相比，Vibe Reasoning能够更好地利用AI模型的已有知识，并克服其在推理和证明方面的局限性。此外，元提示的使用也使得该方法具有更好的通用性和可扩展性。\\n\\n**关键设计**：在具体实现上，论文使用了GPT-5和Gemini 3 Pro两种模型，并设计了基于Agent的工作流，允许模型执行Python代码并读写文件。元提示的设计需要仔细考虑，以确保能够有效地引导AI模型进行推理。此外，模型编排的策略也需要根据具体问题进行调整，以充分利用不同模型的优势。",
            "application_zh": "Vibe Reasoning具有广泛的应用前景，可应用于数学研究、科学发现、工程设计等领域。通过人机协作，可以加速复杂问题的求解过程，提高解决问题的效率和质量。未来，该方法有望应用于教育领域，辅助学生学习数学和科学知识，培养学生的创新思维能力。",
            "highlight_zh": "Vibe Reasoning在IMO 2025问题6上取得了显著成果，成功推导出了正确答案（2112）和一个严谨的数学证明。此前，完全自主的AI系统在该问题上公开报告失败。这一结果表明，Vibe Reasoning能够显著提升AI模型在复杂数学问题上的推理能力，并为解决其他领域的复杂问题提供了新的思路。",
            "tags_zh": [
                "人机协作",
                "数学推理",
                "Agent机制",
                "模型编排",
                "元提示",
                "组合优化",
                "问题求解"
            ],
            "_index": 52,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19287v1/adaptive_strategy_visualization.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
            "authors": [
                "Kirill Djebko",
                "Tom Baumann",
                "Erik Dilger",
                "Frank Puppe",
                "Sergio Montenegro"
            ],
            "arxiv_id": "2512.19576v1",
            "summary": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.",
            "categories": [
                "cs.RO",
                "cs.AI",
                "cs.LG",
                "eess.SY"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19576v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "sim2real"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning",
                        "DRL"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 6.5,
            "hit_pillars": [
                "1_robot_core",
                "2_algo_arch"
            ],
            "headline_zh": "LeLaR首次在轨演示基于AI的卫星姿态控制器，克服Sim2Real难题。",
            "summary_zh": "姿态控制对许多卫星任务至关重要。然而，经典控制器设计耗时，且对模型不确定性和运行边界条件的变化敏感。深度强化学习（DRL）通过与仿真环境的自主交互学习自适应控制策略，提供了一种有前景的替代方案。克服Sim2Real差距，即将仿真中训练的智能体部署到真实的物理卫星上，仍然是一个重大挑战。本文介绍了首次成功在轨演示的基于AI的惯性指向机动姿态控制器。该控制器完全在仿真中训练，并部署到由维尔茨堡大学与柏林工业大学合作开发的InnoCube 3U纳米卫星上，该卫星于2025年1月发射。我们介绍了AI智能体设计、训练过程的方法、仿真与真实卫星观测行为之间的差异，以及基于AI的姿态控制器与InnoCube经典PD控制器的比较。稳态指标证实了基于AI的控制器在重复在轨机动期间的鲁棒性能。",
            "intro_zh": [
                "传统卫星姿态控制器设计复杂，对模型误差和环境变化敏感，难以适应复杂任务。",
                "利用深度强化学习在仿真环境中训练姿态控制器，实现自适应控制策略，降低对精确模型的依赖。",
                "成功将仿真环境训练的AI控制器部署到真实卫星InnoCube上，验证了其在轨姿态控制的鲁棒性。"
            ],
            "method_zh": "**问题定义**：卫星姿态控制旨在精确调整和维持卫星在空间中的姿态。传统方法，如PID控制器，需要精确的卫星动力学模型，且难以应对模型不确定性和外部扰动。现有方法的痛点在于设计和调参过程耗时，且难以保证在复杂环境下的鲁棒性。\\n\\n**核心思路**：利用深度强化学习（DRL）训练一个AI智能体，使其能够通过与仿真环境的交互学习最优的姿态控制策略。核心在于通过大量的仿真训练，使智能体能够适应各种不确定性和扰动，从而实现鲁棒的姿态控制。\\n\\n**技术框架**：整体框架包括仿真环境和DRL智能体。仿真环境模拟卫星的动力学特性和外部环境，DRL智能体则负责接收卫星状态信息，输出控制指令。训练过程采用标准的强化学习流程，智能体通过与环境交互，不断优化控制策略。主要模块包括：状态观测模块、动作选择模块、奖励函数设计模块和策略更新模块。\\n\\n**关键创新**：最重要的创新点在于成功克服了Sim2Real差距，将完全在仿真环境中训练的AI智能体部署到真实的卫星上，并实现了有效的姿态控制。这表明DRL在卫星姿态控制领域具有巨大的潜力。与传统方法相比，该方法无需精确的卫星模型，且具有更强的自适应性和鲁棒性。\\n\\n**关键设计**：奖励函数的设计至关重要，需要综合考虑姿态误差、角速度误差和控制能量消耗等因素。网络结构的选择也需要根据具体任务进行调整。论文中未明确说明具体的网络结构和参数设置，但强调了训练过程的迭代优化和对仿真环境的精确建模。",
            "application_zh": "该研究成果可广泛应用于各类卫星任务，如遥感、通信、导航等。基于AI的姿态控制器能够提高卫星的自主性和智能化水平，降低对地面控制的依赖，并提升任务执行的效率和可靠性。未来，该技术有望应用于深空探测等更复杂的航天任务中。",
            "highlight_zh": "该研究首次在轨演示了基于AI的卫星姿态控制器，验证了其在真实环境中的有效性。实验结果表明，基于AI的控制器在稳态性能方面表现出色，能够实现精确的姿态控制。虽然论文中没有给出具体的性能数据和对比基线，但强调了AI控制器在重复在轨机动中的鲁棒性。",
            "tags_zh": [
                "卫星姿态控制",
                "深度强化学习",
                "Sim2Real",
                "在轨演示",
                "AI控制器"
            ],
            "_index": 53,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19576v1/InnoCubeSideView.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19576v1/ADCSNode.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19576v1/InnoCubeTVChamberOpen.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
            "authors": [
                "Yanliang Huang",
                "Xia Yan",
                "Peiran Yin",
                "Zhenduo Zhang",
                "Zeyan Shao",
                "Youran Wang",
                "Haoliang Huang",
                "Matthias Althoff"
            ],
            "arxiv_id": "2512.19564v1",
            "summary": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19564v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "[T]motion planning"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "CommonRoad自动驾驶运动规划竞赛：标准化评估复杂交通场景下的规划算法",
            "summary_zh": "过去十年，自动驾驶车辆的运动规划方法得到了广泛发展，以应对日益复杂的交通场景。然而，这些方法很少在标准化的基准上进行比较，这限制了对相对优势和劣势的评估。为了解决这个问题，我们介绍了2024年举行的第四届CommonRoad运动规划竞赛的设置和结果，该竞赛使用CommonRoad基准套件进行。这项年度竞赛为运动规划算法的基准测试提供了一个开源和可复现的框架。基准场景涵盖高速公路和城市环境，包含各种交通参与者，包括乘用车、公共汽车和自行车。规划器的性能从四个维度进行评估：效率、安全性、舒适性和对选定交通规则的遵守情况。本报告介绍了比赛形式，并对2023年和2024年版本中具有代表性的高性能规划器进行了比较。",
            "intro_zh": [
                "现有自动驾驶运动规划算法缺乏在标准化基准上的比较，难以评估其优劣。",
                "论文通过CommonRoad运动规划竞赛，提供开源可复现的基准测试框架。",
                "竞赛在复杂交通场景中评估规划器的效率、安全、舒适和合规性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决自动驾驶运动规划算法缺乏标准化评估的问题。现有方法难以在统一的基准上进行比较，导致难以客观评估不同算法的优劣，阻碍了该领域的发展。现有方法在面对复杂交通场景时，其性能表现的差异性无法有效量化。\\n\\n**核心思路**：论文的核心思路是通过组织年度CommonRoad运动规划竞赛，创建一个开源、可复现的基准测试平台。该平台提供了一系列具有挑战性的交通场景，并定义了明确的评估指标，从而为不同运动规划算法提供公平的比较环境。通过竞赛的方式，促进算法的迭代和优化。\\n\\n**技术框架**：整体框架包括以下几个主要阶段：1) 定义CommonRoad基准场景，涵盖高速公路和城市环境，包含多种交通参与者；2) 制定评估指标，包括效率、安全性、舒适性和交通规则合规性；3) 组织竞赛，邀请研究团队提交其运动规划算法；4) 在CommonRoad平台上运行算法，并根据评估指标进行排名；5) 发布竞赛结果和算法分析报告。\\n\\n**关键创新**：该研究的关键创新在于构建了一个开源、可复现的自动驾驶运动规划算法基准测试平台。该平台不仅提供了标准化的测试场景和评估指标，还通过竞赛的形式，激发了研究人员的创新热情，促进了该领域的发展。与以往的研究相比，该研究更注重实际应用和算法的公平比较。\\n\\n**关键设计**：关键设计包括：1) CommonRoad场景的设计，需要覆盖各种复杂交通情况；2) 评估指标的选取，需要能够全面反映算法的性能；3) 竞赛规则的制定，需要保证公平性和激励性；4) 平台的可扩展性，需要能够支持未来的算法和场景。",
            "application_zh": "该研究成果可应用于自动驾驶系统的开发与测试，帮助开发者选择和优化运动规划算法。此外，该平台也可用于自动驾驶安全性的评估和验证，为自动驾驶汽车的商业化部署提供技术支持。未来，该平台可以扩展到更多类型的自动驾驶场景，例如无人配送、自动泊车等。",
            "highlight_zh": "2024年CommonRoad运动规划竞赛对来自2023年和2024年的高性能规划器进行了比较，结果表明，在效率、安全、舒适性和合规性等多个维度上，不同算法的表现存在显著差异。具体的性能数据和提升幅度需要在竞赛报告中查看，但总体而言，该竞赛为评估和改进自动驾驶运动规划算法提供了有价值的参考。",
            "tags_zh": [
                "自动驾驶",
                "运动规划",
                "基准测试",
                "CommonRoad",
                "竞赛"
            ],
            "_index": 54,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19564v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19564v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19564v1/figures/coverage_pie.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
            "authors": [
                "Mingrui Wu",
                "Zhaozhi Wang",
                "Fangjinhua Wang",
                "Jiaolong Yang",
                "Marc Pollefeys",
                "Tong Zhang"
            ],
            "arxiv_id": "2512.19683v1",
            "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Project page: https://harmlesssr.github.io/openbench/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19683v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示多模态大语言模型在开放世界空间推理方面的差距，并提出相应评测基准。",
            "summary_zh": "多模态大语言模型(MLLMs)在语义任务上取得了令人印象深刻的性能，但其空间智能——对于鲁棒和有基础的AI系统至关重要——仍然不发达。现有的基准测试无法诊断这种局限性：它们要么侧重于过于简化的定性推理，要么依赖于特定领域的室内数据，并受到缺乏具有可验证度量真值的室外数据集的限制。为了弥合这一差距，我们引入了一个大规模基准，该基准建立在用同步立体相机、激光雷达和IMU/GPS传感器捕获的行人视角视频之上。该数据集提供了度量上精确的3D信息，从而能够自动生成空间推理问题，这些问题跨越了从定性关系推理到定量度量和运动学理解的分层范围。评估表明，在结构化室内基准中观察到的性能增益在开放世界环境中消失了。使用合成异常场景和盲测的进一步分析证实，当前的MLLM严重依赖于语言先验而不是有基础的视觉推理。因此，我们的基准提供了一个原则性的平台，用于诊断这些局限性并推进物理基础的空间智能。",
            "intro_zh": [
                "现有多模态大语言模型在空间智能方面存在不足，尤其是在开放世界场景下，缺乏有效的评测基准。",
                "论文构建了一个大规模的、基于行人视角视频的开放世界空间推理基准，包含精确的3D信息和分层空间推理问题。",
                "实验表明，现有模型在开放世界中的性能显著下降，且过度依赖语言先验，缺乏真正的视觉推理能力。"
            ],
            "method_zh": "**问题定义**：现有的多模态大语言模型在空间推理能力上存在不足，尤其是在从室内环境迁移到复杂的开放世界环境时。现有的评测基准要么过于简单，侧重于定性推理，要么依赖于特定领域的室内数据，缺乏具有精确度量真值的室外数据集，难以全面评估模型的空间智能。\n\n**核心思路**：论文的核心思路是构建一个大规模、具有度量真值的开放世界空间推理基准，该基准基于行人视角的视频数据，包含丰富的3D信息，并能够自动生成涵盖不同层次空间推理能力的问题。通过在该基准上评估现有模型，揭示其在开放世界空间推理方面的差距，并促进相关研究。\n\n**技术框架**：该基准的构建流程主要包括以下几个阶段：1) 数据采集：使用同步立体相机、激光雷达和IMU/GPS传感器采集行人视角的视频数据。2) 3D重建：利用采集到的数据进行精确的3D重建，生成具有度量真值的3D场景。3) 问题生成：自动生成涵盖不同层次空间推理能力的问题，包括定性关系推理、定量度量理解和运动学理解。4) 模型评估：使用生成的基准测试评估现有MLLM的空间推理能力。\n\n**关键创新**：该论文的关键创新在于构建了一个大规模、具有度量真值的开放世界空间推理基准，该基准能够全面评估MLLM在复杂环境下的空间智能。与现有基准相比，该基准具有以下优势：1) 数据规模更大，场景更复杂；2) 具有精确的度量真值，能够进行定量评估；3) 涵盖不同层次的空间推理能力，能够更全面地评估模型的空间智能。\n\n**关键设计**：该基准的关键设计包括：1) 使用行人视角的视频数据，更贴近实际应用场景；2) 使用同步立体相机、激光雷达和IMU/GPS传感器，保证3D重建的精度；3) 自动生成不同类型的空间推理问题，保证基准的多样性和覆盖性；4) 设计了合成异常场景和盲测，用于评估模型对语言先验的依赖程度。",
            "application_zh": "该研究成果可应用于机器人导航、自动驾驶、增强现实等领域。通过提升多模态大语言模型的空间推理能力，可以使机器人更好地理解和适应复杂环境，提高导航的准确性和鲁棒性。在自动驾驶领域，可以提高车辆对周围环境的感知和理解能力，从而提高驾驶安全性。在增强现实领域，可以实现更自然的虚拟物体与真实环境的交互。",
            "highlight_zh": "实验结果表明，现有MLLM在开放世界空间推理基准上的性能显著低于室内环境，验证了模型在开放世界场景下的空间智能不足。通过合成异常场景和盲测，进一步证实了现有模型过度依赖语言先验，缺乏真正的视觉推理能力。该基准的发布将促进相关研究，推动物理基础的空间智能发展。",
            "tags_zh": [
                "多模态大语言模型",
                "空间推理",
                "开放世界",
                "评测基准",
                "视觉推理"
            ],
            "_index": 55,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19683v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19683v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19683v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
            "authors": [
                "Hanyang Kong",
                "Xingyi Yang",
                "Xiaoxu Zheng",
                "Xinchao Wang"
            ],
            "arxiv_id": "2512.19678v1",
            "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Project page: https://hyokong.github.io/worldwarp-page/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
            "code_links": [
                {
                    "url": "https://hyokong.github.io/worldwarp-page/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "3DGS",
                        "gaussian splatting",
                        "splatting"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "3_perception_slam"
            ],
            "headline_zh": "WorldWarp：利用异步视频扩散传播3D几何信息，生成长时几何一致性视频。",
            "summary_zh": "生成长时、几何一致的视频面临一个根本困境：一致性要求严格遵守像素空间中的3D几何结构，而最先进的生成模型在相机条件潜在空间中效果最佳。这种脱节导致现有方法在处理遮挡区域和复杂相机轨迹时遇到困难。为了弥合这一差距，我们提出了WorldWarp，一个将3D结构锚点与2D生成细化器相结合的框架。为了建立几何基础，WorldWarp维护一个通过高斯溅射（3DGS）构建的在线3D几何缓存。通过将历史内容显式地扭曲到新的视角中，该缓存充当结构支架，确保每个新帧都尊重先前的几何结构。然而，由于遮挡，静态扭曲不可避免地会留下孔洞和伪影。我们使用专为“填充和修改”目标设计的时空扩散（ST-Diff）模型来解决这个问题。我们的关键创新是时空变化的噪声调度：空白区域接收完全噪声以触发生成，而扭曲区域接收部分噪声以实现细化。通过在每个步骤动态更新3D缓存，WorldWarp保持视频块之间的一致性。因此，它通过确保3D逻辑指导结构，而扩散逻辑完善纹理，从而实现了最先进的保真度。",
            "intro_zh": [
                "现有视频生成方法难以在长时视频中保持几何一致性，尤其是在遮挡和复杂相机运动下。",
                "WorldWarp通过维护一个基于高斯溅射的3D几何缓存，并将历史内容扭曲到新视角，从而实现几何约束。",
                "该方法引入时空扩散模型，通过动态调整噪声调度，实现对扭曲区域的细化和对空白区域的生成，提升视频质量。"
            ],
            "method_zh": "**问题定义**：现有视频生成方法在生成长时视频时，难以保证几何一致性，尤其是在存在遮挡和复杂相机运动的情况下。这些方法通常在相机条件潜在空间中操作，与像素空间中的3D几何结构脱节，导致生成结果出现不自然的形变和伪影。\\n\\n**核心思路**：WorldWarp的核心思路是将3D几何信息显式地融入到视频生成过程中。通过维护一个在线更新的3D几何缓存，并利用该缓存将历史内容扭曲到新的视角，从而保证生成的新帧与之前的帧在几何上保持一致。同时，利用时空扩散模型来填充和细化扭曲后的图像，解决遮挡和伪影问题。\\n\\n**技术框架**：WorldWarp框架主要包含两个核心模块：3D几何缓存和时空扩散模型。3D几何缓存基于高斯溅射（3DGS）构建，用于存储和更新场景的3D几何信息。时空扩散模型（ST-Diff）用于填充和细化扭曲后的图像，生成最终的视频帧。整个流程包括：1）利用3D几何缓存将历史帧扭曲到当前视角；2）使用ST-Diff模型填充和细化扭曲后的图像；3）将生成的新帧更新到3D几何缓存中。\\n\\n**关键创新**：WorldWarp的关键创新在于将3D几何缓存与时空扩散模型相结合，实现几何约束的视频生成。与现有方法相比，WorldWarp显式地利用3D几何信息，避免了在潜在空间中进行推理时可能出现的几何失真。此外，提出的时空变化的噪声调度策略，能够有效地控制生成和细化的过程，提高生成质量。\\n\\n**关键设计**：在时空扩散模型中，关键的设计是时空变化的噪声调度。对于扭曲后的区域，施加部分噪声，使其在保留原有结构的基础上进行细化；对于空白区域（例如遮挡区域），施加完全噪声，使其完全由扩散模型生成。这种策略能够有效地平衡几何约束和生成质量。此外，3D几何缓存的更新频率和方式也会影响最终的生成效果，需要在实际应用中进行调整。",
            "application_zh": "WorldWarp在电影制作、游戏开发、虚拟现实等领域具有广泛的应用前景。它可以用于生成具有复杂相机运动和遮挡的长时视频，例如虚拟场景漫游、角色动画等。该技术还可以用于视频修复和增强，例如修复老旧电影或提高视频分辨率。未来，WorldWarp有望成为一种强大的视频生成工具，为各行各业带来创新。",
            "highlight_zh": "论文提出的WorldWarp框架在长时视频生成任务上取得了显著的成果。通过与现有方法进行对比，WorldWarp在几何一致性和视觉质量方面均有明显提升。实验结果表明，WorldWarp能够生成具有复杂相机运动和遮挡的、几何一致的视频，并且视觉效果更加逼真。",
            "tags_zh": [
                "视频生成",
                "3D几何",
                "扩散模型",
                "高斯溅射",
                "长时视频",
                "几何一致性",
                "时空扩散"
            ],
            "_index": 56,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19678v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19678v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19678v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
            "authors": [
                "Artemis Panagopoulou",
                "Aveek Purohit",
                "Achin Kulshrestha",
                "Soroosh Yazdani",
                "Mohit Goyal"
            ],
            "arxiv_id": "2512.19609v1",
            "summary": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19609v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "MapTrace：提出可扩展数据生成流程，提升MLLM地图路径追踪能力",
            "summary_zh": "多模态大型语言模型（MLLM）在许多视觉和文本推理任务上已达到类人性能，但在精细的空间理解方面，例如地图上的路径追踪，其能力仍然有限。与能够快速学习解析和导航地图的人类不同，当前的MLLM模型常常无法满足基本的路径约束，部分原因是收集大规模、像素精确的路径标注的成本过高且难度大。为了解决这个问题，我们引入了一个可扩展的合成数据生成流程，该流程利用合成地图图像和像素级解析来自动生成此项具有挑战性任务的精确标注。使用此流程，我们构建了一个包含4k张地图上23k个路径样本的微调数据集，使模型能够获得更像人类的空间能力。使用此数据集，我们对开源和专有的MLLM进行了微调。在MapBench上的结果表明，微调显著提高了鲁棒性，成功率提高了6.4个百分点，同时也降低了路径追踪误差（NDTW）。这些提升表明，预训练模型中缺乏的精细空间推理能力可以通过合成监督进行显式地教授。",
            "intro_zh": [
                "现有MLLM在地图路径追踪等精细空间理解任务中表现不足，无法有效解析和导航地图。",
                "论文提出一种可扩展的合成数据生成流程，利用合成地图和像素级解析自动生成精确标注。",
                "通过在合成数据集上微调MLLM，显著提升了模型在MapBench上的路径追踪成功率和鲁棒性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大型语言模型（MLLM）在地图路径追踪任务中表现不佳的问题。现有方法依赖于人工标注的大规模数据集，成本高昂且难以获取像素级别的精确标注，导致模型无法有效学习地图的空间结构和路径约束。\\n\\n**核心思路**：论文的核心思路是利用合成数据生成流程，自动创建包含精确路径标注的地图数据集。通过在合成数据上进行微调，使MLLM能够学习到精细的空间推理能力，从而提升其在真实地图上的路径追踪性能。这种方法避免了人工标注的成本和限制，具有良好的可扩展性。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1) 合成地图生成：生成具有多样化道路和地标的合成地图图像。2) 路径生成：在合成地图上随机生成起始点和目标点，并计算出连接它们的最佳路径。3) 像素级标注：利用地图生成过程中的信息，自动生成路径的像素级精确标注。4) 数据集构建：将合成地图图像和对应的路径标注组合成微调数据集。5) 模型微调：使用生成的数据集对MLLM进行微调，使其学习路径追踪任务。\\n\\n**关键创新**：该方法最重要的技术创新点在于其可扩展的合成数据生成流程。该流程能够自动生成大规模、像素精确的路径标注，无需人工干预，从而降低了数据获取的成本和难度。此外，该方法还能够控制合成数据的多样性和复杂性，从而更好地训练模型。\\n\\n**关键设计**：论文中没有详细描述具体的参数设置、损失函数和网络结构等技术细节。但是，可以推断，微调过程可能使用了标准的交叉熵损失函数来优化路径预测，并可能采用了数据增强等技术来提高模型的泛化能力。地图生成过程可能使用了程序化生成技术，以确保地图的多样性和真实性。",
            "application_zh": "该研究成果可应用于自动驾驶导航、机器人路径规划、地理信息系统等领域。通过提升模型在地图上的空间推理能力，可以实现更智能、更可靠的导航服务，并为相关应用提供更精确的地理信息支持。未来，该方法还可扩展到其他空间推理任务，例如室内导航、三维场景理解等。",
            "highlight_zh": "实验结果表明，使用该方法生成的合成数据集对MLLM进行微调后，在MapBench上的路径追踪成功率提高了6.4个百分点，同时路径追踪误差（NDTW）也显著降低。这些结果表明，通过合成监督可以有效地提升MLLM在精细空间推理任务中的性能，使其更接近人类水平。",
            "tags_zh": [
                "多模态大语言模型",
                "地图路径追踪",
                "合成数据生成",
                "空间推理",
                "像素级标注"
            ],
            "_index": 57,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19609v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19609v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19609v1/figures/data_dist.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
            "authors": [
                "Evelyn Zhang",
                "Fufu Yu",
                "Aoqi Wu",
                "Zichen Wen",
                "Ke Yan",
                "Shouhong Ding",
                "Biqing Qi",
                "Linfeng Zhang"
            ],
            "arxiv_id": "2512.19443v1",
            "summary": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19443v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "multimodal"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "D2Pruner：通过解偏重要性和结构多样性实现MLLM Token剪枝",
            "summary_zh": "多模态大型语言模型(MLLM)处理长视觉token序列时面临巨大的计算负担。Token剪枝为加速提供了一条途径，但我们发现当前方法虽然在通用理解方面足够，但在细粒度定位任务上却 катастрофически 失败。我们将这种失败归因于两种主流策略的固有缺陷：基于重要性的方法存在强烈的 positional bias，这是一种固有的模型伪像，会分散对语义内容的注意力；而基于多样性的方法表现出结构盲视，忽略了用户的提示和空间冗余。为了解决这个问题，我们引入了D2Pruner，一个通过独特地结合解偏重要性和结构剪枝机制来纠正这些问题的框架。我们的方法首先基于解偏的注意力分数，将最关键的token集合作为枢轴来保护。然后，对剩余的token执行最大独立集(MIS)选择，这些token在混合图上建模，其中边表示空间邻近性和语义相似性。这个过程迭代地保留最重要和可用的token，同时删除其邻居，确保选择补充token以最大化重要性和多样性。大量的实验表明，D2Pruner具有卓越的效率和保真度。应用于LLaVA-1.5-7B进行通用理解任务时，它减少了74.2%的FLOPs，同时保留了99.2%的原始性能。此外，在具有InternVL-2.5-8B的具有挑战性的定位基准测试中，它在90%的token减少率下保持了85.7%的性能，这是一个显著的进步，比现有方法提高了高达63.53%。",
            "intro_zh": [
                "现有MLLM的token剪枝方法在细粒度定位任务中表现不佳，主要原因是重要性偏置和结构盲视。",
                "D2Pruner通过结合解偏重要性和结构剪枝机制，选择重要且多样的token子集，从而提升性能。",
                "实验表明，D2Pruner在通用理解和细粒度定位任务中均能显著减少计算量并保持甚至提升性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态大型语言模型（MLLM）中视觉token序列过长导致的计算负担问题。现有的token剪枝方法在通用理解任务上表现尚可，但在细粒度定位任务上会严重失效。主要痛点在于：基于重要性的方法受位置偏见影响，而基于多样性的方法忽略了用户提示和空间冗余。\\n\\n**核心思路**：D2Pruner的核心思路是结合解偏的重要性评估和结构多样性剪枝，从而选择既重要又具有代表性的token子集。通过解偏重要性，消除模型固有的位置偏见，关注语义内容。利用结构多样性剪枝，确保选取的token在空间和语义上具有代表性，避免冗余。\\n\\n**技术框架**：D2Pruner框架包含两个主要阶段：1) **解偏重要性评估**：计算每个token的解偏注意力分数，选择最重要的token作为枢轴（pivots）。2) **结构多样性剪枝**：对剩余token构建混合图，图中边表示空间邻近性和语义相似性，然后使用最大独立集（MIS）算法，迭代地选择重要且与已选token不相邻的token。\\n\\n**关键创新**：D2Pruner的关键创新在于同时考虑了token的重要性（解偏后的）和结构多样性。与现有方法相比，D2Pruner能够更好地平衡计算效率和性能，尤其是在细粒度定位任务中，避免了因位置偏见或结构盲视导致的性能下降。\\n\\n**关键设计**：在解偏重要性评估中，具体如何消除位置偏见，论文中未详细说明，属于未知细节。混合图的构建方式，空间邻近性和语义相似性的具体度量方法，以及MIS算法的具体实现，都属于关键设计细节，但论文中未提供足够信息。这些细节对最终性能有重要影响。",
            "application_zh": "D2Pruner可应用于各种需要处理长视觉token序列的多模态大型语言模型，例如图像/视频理解、视觉问答、目标检测和图像分割等。该方法能够有效降低计算成本，提高模型推理速度，使其更易于部署在资源受限的设备上，并促进MLLM在实际场景中的应用。",
            "highlight_zh": "D2Pruner在LLaVA-1.5-7B上进行通用理解任务时，减少了74.2%的FLOPs，同时保留了99.2%的原始性能。在InternVL-2.5-8B上进行细粒度定位任务时，在90%的token减少率下保持了85.7%的性能，比现有方法提高了高达63.53%。这些结果表明D2Pruner在效率和精度方面均优于现有方法。",
            "tags_zh": [
                "多模态大语言模型",
                "Token剪枝",
                "解偏",
                "结构多样性",
                "最大独立集",
                "细粒度定位",
                "计算效率"
            ],
            "_index": 58,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19443v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19443v1/image/img_motivation.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19443v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
            "authors": [
                "Tao Zhang",
                "Ziqian Zeng",
                "Hao Peng",
                "Huiping Zhuang",
                "Cen Chen"
            ],
            "arxiv_id": "2512.19206v1",
            "summary": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19206v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "MixKVQ：面向长文本推理的查询感知混合精度KV缓存量化",
            "summary_zh": "长链思维（CoT）推理显著提升了大型语言模型（LLM）的能力，但同时也带来了巨大的内存和延迟开销，这主要源于庞大的键值（KV）缓存。KV缓存量化是一种有前景的压缩技术，但现有的低比特量化方法在复杂的推理任务上通常表现出严重的性能下降。固定精度量化难以处理键缓存中的异常通道，而当前的混合精度策略无法准确识别需要高精度表示的组件。我们发现，有效的低比特KV缓存量化策略必须考虑两个因素：键通道的内在量化难度及其与查询的相关性。基于此，我们提出了一种新颖的即插即用方法MixKVQ，它引入了一种轻量级的、查询感知的算法来识别和保留需要更高精度的关键键通道，同时对值缓存应用逐token量化。在复杂推理数据集上的实验表明，我们的方法显著优于现有的低比特方法，在显著降低内存占用的同时，实现了与全精度基线相当的性能。",
            "intro_zh": [
                "长文本推理对KV缓存造成巨大内存和延迟开销，现有低比特量化方法在复杂推理任务中性能下降明显。",
                "MixKVQ提出一种查询感知的混合精度量化方法，根据键通道的量化难度和与查询的相关性自适应分配精度。",
                "实验表明，MixKVQ在复杂推理数据集上显著优于现有低比特方法，内存占用降低的同时性能接近全精度。"
            ],
            "method_zh": "**问题定义**：论文旨在解决长文本推理中，大型语言模型（LLM）因KV缓存过大而导致的内存和延迟问题。现有的低比特量化方法在复杂推理任务中性能下降严重，固定精度量化无法有效处理异常值，混合精度量化无法准确识别需要高精度的关键组件。\\n\\n**核心思路**：论文的核心思路是提出一种查询感知的混合精度KV缓存量化方法，即MixKVQ。该方法认为，有效的低比特KV缓存量化需要同时考虑键通道的内在量化难度以及其与当前查询的相关性。通过查询感知的方式，可以更有针对性地对关键键通道分配更高的精度，从而在保证性能的同时降低内存占用。\\n\\n**技术框架**：MixKVQ是一种即插即用的方法，可以方便地集成到现有的LLM框架中。其主要流程包括：1) 使用轻量级的查询感知算法来评估每个键通道的重要性；2) 根据重要性评估结果，对不同的键通道分配不同的量化精度；3) 对值缓存应用逐token量化。\\n\\n**关键创新**：MixKVQ的关键创新在于引入了查询感知机制，能够动态地识别并保留对当前查询至关重要的键通道，并对这些通道使用更高的精度进行量化。这与传统的固定精度或静态混合精度量化方法有本质区别，后者无法根据查询动态调整量化策略。\\n\\n**关键设计**：MixKVQ的关键设计包括：1) 轻量级的查询感知算法，用于评估键通道的重要性，具体实现细节未知；2) 混合精度量化策略，根据键通道的重要性自适应地分配量化比特数；3) 逐token的值缓存量化，进一步降低内存占用，具体量化方法未知。",
            "application_zh": "MixKVQ可应用于各种需要长文本推理的大型语言模型，例如智能问答、文本摘要、机器翻译等。通过降低KV缓存的内存占用和延迟，MixKVQ能够提升LLM在资源受限设备上的部署能力，并加速推理过程，具有重要的实际应用价值和广泛的应用前景。",
            "highlight_zh": "实验结果表明，MixKVQ在复杂推理数据集上显著优于现有的低比特量化方法，在大幅降低内存占用的同时，实现了与全精度基线相当的性能。具体的性能提升数据和对比基线未知，但摘要强调了其优越性。",
            "tags_zh": [
                "KV缓存量化",
                "混合精度量化",
                "长文本推理",
                "大型语言模型",
                "查询感知",
                "低比特量化",
                "模型压缩"
            ],
            "_index": 59,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19206v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19206v1/fig/Layer_0_Head_2_KV_Error_Heatmap.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19206v1/fig/quant_analysis_layer_0_combined_v3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
            "authors": [
                "Do Minh Duc",
                "Quan Xuan Truong",
                "Nguyen Tat Dat",
                "Nguyen Van Vinh"
            ],
            "arxiv_id": "2512.19247v1",
            "summary": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19247v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "chain-of-thought"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出检索引导的自动Prompt优化方法，提升LLM在物流文本框架检测中的性能。",
            "summary_zh": "本文提出了一种新颖的prompt优化流程，用于物流文本中的框架检测。该流程结合了检索增强生成（RAG）、少样本prompting、思维链（CoT）推理和自动CoT合成（Auto-CoT），以生成高效的任务特定prompt。核心是一个基于LLM的prompt优化代理，它使用检索到的示例、性能反馈和内部自我评估来迭代地改进prompt。该框架在实际的物流文本标注任务上进行了评估，实验结果表明，优化的prompt（特别是通过Auto-CoT和RAG增强的prompt）与基线零样本或静态prompt相比，实际推理准确率提高了15%。该系统在多个LLM（包括GPT-4o、Qwen 2.5 (72B)和LLaMA 3.1 (70B)）上表现出一致的改进，验证了其通用性和实用价值。这些发现表明，结构化的prompt优化是完全微调的可行替代方案，为在物流等特定领域的NLP应用中部署LLM提供了可扩展的解决方案。",
            "intro_zh": [
                "现有方法在将LLM应用于物流文本框架检测时，依赖人工设计的prompt，效率低且效果有限。",
                "论文提出一种基于检索引导的自动prompt优化方法，利用RAG、CoT等技术自动生成和优化prompt。",
                "实验结果表明，该方法在多个LLM上均能显著提升框架检测的准确率，最高提升达15%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决物流文本中框架检测任务的prompt工程问题。现有方法依赖人工设计prompt，耗时且效果受限于专家知识。此外，静态prompt难以适应复杂多变的物流场景，导致LLM推理精度不高。\\n\\n**核心思路**：论文的核心思路是利用LLM自身的能力，通过检索增强和自动优化，生成更有效的任务特定prompt。通过检索相似的示例，为LLM提供上下文信息，并通过迭代优化prompt，使其更好地适应目标任务。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 检索模块：从预定义的语料库中检索与输入文本相关的示例；2) Prompt优化代理：基于LLM，利用检索到的示例、性能反馈和内部自我评估，迭代地改进prompt；3) 推理模块：使用优化后的prompt，利用LLM进行框架检测；4) 评估模块：评估LLM的推理结果，并将评估结果反馈给Prompt优化代理。\\n\\n**关键创新**：最重要的技术创新点在于Prompt优化代理的设计。该代理能够利用检索增强和自动CoT合成技术，自动生成和优化prompt，无需人工干预。此外，该代理还能够进行内部自我评估，从而更有效地改进prompt。\\n\\n**关键设计**：Prompt优化代理的关键设计包括：1) 使用RAG技术，从语料库中检索与输入文本相关的示例，为LLM提供上下文信息；2) 使用Auto-CoT技术，自动生成思维链，引导LLM进行推理；3) 使用性能反馈和内部自我评估，迭代地改进prompt；4) 使用特定的损失函数，优化prompt的生成过程（具体损失函数细节未知）。",
            "application_zh": "该研究成果可应用于智能物流领域，例如自动提取物流单据中的关键信息、识别异常物流事件、优化物流路径规划等。通过提升LLM在物流文本理解方面的能力，可以提高物流效率、降低运营成本，并为用户提供更优质的服务。未来，该方法还可以扩展到其他领域，例如金融、医疗等。",
            "highlight_zh": "实验结果表明，该方法在实际物流文本标注任务中，与基线零样本或静态prompt相比，推理准确率提高了15%。该方法在多个LLM（包括GPT-4o、Qwen 2.5 (72B)和LLaMA 3.1 (70B)）上表现出一致的改进，验证了其通用性和实用价值。",
            "tags_zh": [
                "Prompt工程",
                "大型语言模型",
                "物流文本",
                "框架检测",
                "检索增强生成",
                "思维链",
                "自动Prompt优化"
            ],
            "_index": 60,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19247v1/image/wordcloud.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19247v1/image/sentence_length_distribution.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19247v1/image/copy.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation",
            "authors": [
                "Thittipat Pairatsuppawat",
                "Abhibhu Tachaapornchai",
                "Paweekorn Kusolsomboon",
                "Chutikan Chaiwong",
                "Thodsaporn Chay-intr",
                "Kobkrit Viriyayudhakorn",
                "Nongnuch Ketui",
                "Aslan B. Wong"
            ],
            "arxiv_id": "2512.19455v1",
            "summary": "Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19455v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model",
                        "instruction following"
                    ],
                    "score": 6.0
                }
            ],
            "relevance_score": 6.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "SiamGPT：面向稳定泰语文本生成的质量优先微调方法",
            "summary_zh": "由于在复杂指令下生成不稳定，即使在英语方面表现出色，开源大型语言模型在泰语部署方面仍然面临困难。为了缓解这些限制，我们提出了SiamGPT-32B，一个基于Qwen3-32B的开源模型，采用质量优先策略进行微调，强调精心策划的监督而非数据规模。该微调流程结合了翻译的高复杂度英语指令数据和泰语改编的AutoIF框架，用于指令和语言约束。仅使用监督微调，无需持续预训练或语料库扩展，SiamGPT-32B提高了指令遵循、多轮对话鲁棒性和语言稳定性。在SEA-HELM基准上的评估表明，SiamGPT-32B在类似规模的开源泰语模型中实现了最强的整体性能，在指令遵循、多轮对话和自然语言理解方面均取得了持续的提升。",
            "intro_zh": [
                "现有开源大语言模型在泰语环境下，复杂指令下的生成结果不稳定，难以有效部署。",
                "SiamGPT采用质量优先的微调策略，结合翻译后的高质量英语指令数据和泰语适配的AutoIF框架。",
                "实验表明，SiamGPT在指令遵循、多轮对话和语言稳定性方面均有提升，并在SEA-HELM基准测试中表现最佳。"
            ],
            "method_zh": "**问题定义**：论文旨在解决开源大型语言模型在泰语文本生成中，面对复杂指令时生成结果不稳定的问题。现有方法通常依赖于大规模数据进行训练，但忽略了数据的质量，导致模型在泰语环境下的指令遵循能力较弱，多轮对话鲁棒性不足，且语言表达不够稳定。\\n\\n**核心思路**：论文的核心思路是采用“质量优先”的微调策略，即更加注重训练数据的质量而非数量。通过精心挑选和处理高质量的指令数据，并结合泰语环境的特点进行适配，从而提升模型在泰语文本生成方面的性能。这种策略避免了盲目扩大数据规模可能带来的噪声和偏差，更加有效地利用了有限的资源。\\n\\n**技术框架**：SiamGPT的整体框架包括以下几个主要阶段：1) 基于Qwen3-32B的预训练模型作为基础；2) 将高质量的英语指令数据翻译成泰语；3) 采用泰语适配的AutoIF框架，用于指令和语言约束；4) 使用监督微调方法对模型进行训练，提升其在泰语环境下的指令遵循能力、多轮对话鲁棒性和语言稳定性。整个流程没有进行持续预训练或语料库扩展，而是专注于利用高质量的监督数据进行微调。\\n\\n**关键创新**：论文最重要的技术创新点在于提出了“质量优先”的微调策略，并将其应用于泰语大型语言模型的训练中。与以往依赖大规模数据的方法不同，该策略更加注重数据的质量和针对性，从而在有限的资源下取得了更好的效果。此外，论文还提出了泰语适配的AutoIF框架，用于指令和语言约束，进一步提升了模型在泰语环境下的性能。\\n\\n**关键设计**：论文的关键设计包括：1) 精心挑选和翻译高质量的英语指令数据，确保数据的多样性和复杂性；2) 采用泰语适配的AutoIF框架，对生成的文本进行约束，保证语言的流畅性和准确性；3) 使用监督微调方法，通过优化模型的参数，使其更好地适应泰语环境。具体的参数设置、损失函数和网络结构等细节未在摘要中详细说明，属于未知信息。",
            "application_zh": "SiamGPT的研究成果可应用于多种泰语自然语言处理任务，如智能客服、文本摘要、机器翻译、内容创作等。该模型能够生成更稳定、更符合指令的泰语文本，提升用户体验和工作效率。未来，该研究有望推动泰语自然语言处理技术的发展，促进泰语信息资源的数字化和智能化。",
            "highlight_zh": "SiamGPT-32B在SEA-HELM基准测试中，在同等规模的开源泰语模型中取得了最佳的整体性能。在指令遵循、多轮对话和自然语言理解等多个方面均有显著提升，证明了“质量优先”微调策略的有效性。具体的性能提升幅度未在摘要中给出，属于未知信息。",
            "tags_zh": [
                "泰语NLP",
                "大型语言模型",
                "指令微调",
                "质量优先",
                "多轮对话"
            ],
            "_index": 61,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19455v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19455v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "OMP: One-step Meanflow Policy with Directional Alignment",
            "authors": [
                "Han Fang",
                "Yize Huang",
                "Yuheng Zhao",
                "Paul Weng",
                "Xiao Li",
                "Yutong Ban"
            ],
            "arxiv_id": "2512.19347v1",
            "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19347v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "embodied AI"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出OMP：一种单步MeanFlow策略，通过方向对齐提升机器人操作性能",
            "summary_zh": "机器人操作是具身智能的关键能力。本文针对现有数据驱动的生成策略框架，如扩散模型推理延迟高、Flow模型架构复杂等问题，提出了一种改进的基于MeanFlow的策略OMP。该方法通过引入轻量级的余弦损失来对齐速度方向，并使用微分推导方程(DDE)优化雅可比向量积(JVP)算子。在Adroit和Meta-World任务上的实验表明，该方法在平均成功率上优于MP1和FlowPolicy，尤其是在具有挑战性的Meta-World任务中，有效增强了机器人操作策略的少样本泛化能力和轨迹精度，同时保持了实时性能，为高精度机器人操作提供了一种更鲁棒的解决方案。",
            "intro_zh": [
                "主流机器人操作策略，如扩散模型和Flow模型，分别存在推理延迟高和架构复杂的问题，限制了其应用。",
                "OMP通过引入余弦损失对齐速度方向，并利用微分推导方程优化雅可比向量积，从而提升策略的泛化能力和轨迹精度。",
                "实验结果表明，OMP在Adroit和Meta-World任务上优于现有方法，尤其在Meta-World任务中表现出更强的少样本泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人操作任务中，现有生成策略（如扩散模型和Flow模型）存在的推理速度慢、模型复杂度高，以及MeanFlow方法泛化能力不足的问题。具体来说，MeanFlow方法由于其分散损失中的固定温度超参数，以及预测速度与真实速度方向的偏差，导致其在少样本学习场景下表现不佳。\\n\\n**核心思路**：论文的核心思路是通过改进MeanFlow策略，使其在保持单步推理速度的同时，提升其少样本泛化能力和轨迹精度。具体而言，通过引入余弦损失来对齐预测速度和真实速度的方向，从而减少方向偏差；并使用微分推导方程（DDE）来优化雅可比向量积（JVP）算子，从而更有效地学习策略。\\n\\n**技术框架**：OMP方法的核心框架是在MeanFlow的基础上进行改进。整体流程包括：1) 使用MeanFlow生成初始轨迹；2) 使用余弦损失对齐预测速度和真实速度的方向；3) 使用微分推导方程优化雅可比向量积算子；4) 使用优化后的策略进行机器人操作。\\n\\n**关键创新**：论文的关键创新在于：1) 引入轻量级的余弦损失来对齐速度方向，这是一种简单而有效的方法，可以显著减少方向偏差；2) 使用微分推导方程（DDE）来优化雅可比向量积（JVP）算子，这可以更有效地学习策略，并提高轨迹精度。\\n\\n**关键设计**：余弦损失的设计旨在最小化预测速度和真实速度之间的角度差异，其具体形式为：`loss = 1 - cos(theta)`，其中`theta`是两个速度向量之间的夹角。微分推导方程（DDE）用于计算雅可比向量积，其具体形式未知（需要查阅论文原文）。网络结构方面，论文使用了轻量级的网络结构，以保证实时性能。具体参数设置未知（需要查阅论文原文）。",
            "application_zh": "该研究成果可应用于各种需要高精度和实时性的机器人操作任务，例如工业自动化、医疗手术机器人、家庭服务机器人等。通过提高机器人操作的泛化能力和轨迹精度，可以使机器人在复杂和未知的环境中更好地完成任务，从而提高生产效率和服务质量。未来，该方法有望进一步扩展到更复杂的机器人系统和任务中。",
            "highlight_zh": "实验结果表明，OMP在Adroit和Meta-World任务上均取得了显著的性能提升。尤其是在具有挑战性的Meta-World任务中，OMP的平均成功率优于MP1和FlowPolicy，表明其具有更强的少样本泛化能力。具体性能数据未知（需要查阅论文原文）。",
            "tags_zh": [
                "机器人操作",
                "MeanFlow",
                "少样本学习",
                "方向对齐",
                "微分推导方程",
                "雅可比向量积",
                "生成策略"
            ],
            "_index": 62,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19347v1/figures/teaser_v1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19347v1/figures/overview_v4.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19347v1/figures/Experiments.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Translating Flow to Policy via Hindsight Online Imitation",
            "authors": [
                "Yitian Zheng",
                "Zhangchen Ye",
                "Weijun Dong",
                "Shengjie Wang",
                "Yuyang Liu",
                "Chongjie Zhang",
                "Chuan Wen",
                "Yang Gao"
            ],
            "arxiv_id": "2512.19269v1",
            "summary": "Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.",
            "categories": [
                "cs.RO",
                "cs.LG"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19269v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                },
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "cross-embodiment"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 5.0,
            "hit_pillars": [
                "1_robot_core",
                "7_retargeting"
            ],
            "headline_zh": "提出HinFlow，通过回溯在线模仿学习将高层规划转化为机器人策略",
            "summary_zh": "本文提出了一种通过在线交互改进低层策略的方法。该方法收集在线轨迹，从实现的结果中回溯标注相应的高层目标，并聚合这些回溯重标记的经验来更新目标条件模仿策略。该方法名为回溯流条件在线模仿(HinFlow)，以2D点流作为高层规划器。在模拟和物理世界的各种操作任务中，HinFlow的性能比基线策略提高了2倍以上，显著优于现有方法。此外，该框架能够从跨具身视频数据训练的规划器中获取策略，展示了其在可扩展和可转移机器人学习方面的潜力。",
            "intro_zh": [
                "现有分层机器人系统难以将高层规划转化为可执行的动作，尤其是在高质量机器人数据有限的情况下。",
                "HinFlow通过在线交互收集数据，回溯标注高层目标，并使用回溯重标记的经验更新目标条件模仿策略。",
                "在模拟和真实机器人操作任务中，HinFlow显著优于现有方法，并展示了从跨具身视频数据学习策略的潜力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人学习中，如何利用有限的机器人数据，将高层规划器（例如从视频数据训练的规划器）的输出转化为可执行的低层机器人动作策略的问题。现有方法通常需要大量高质量的机器人数据进行训练，或者难以泛化到新的环境和任务。\\n\\n**核心思路**：论文的核心思路是利用在线交互，通过回溯的方式从实际执行结果中推断出高层目标，然后将这些回溯标注的经验用于训练目标条件模仿策略。这种方法可以有效地利用少量机器人数据，并提高策略的泛化能力。\\n\\n**技术框架**：HinFlow框架包含以下几个主要模块：1) **在线交互模块**：机器人与环境进行交互，收集轨迹数据。2) **回溯标注模块**：根据实际达到的结果，回溯地标注对应的高层目标。3) **目标条件模仿学习模块**：利用回溯标注的经验，训练一个以目标为条件的模仿策略。4) **高层规划器**：提供高层任务规划，例如2D点流。整体流程是，高层规划器给出目标，低层策略执行动作，然后根据执行结果回溯目标，并更新低层策略。\\n\\n**关键创新**：关键创新在于将回溯标注和在线模仿学习相结合，从而有效地利用了少量机器人数据，并提高了策略的泛化能力。与传统的模仿学习方法相比，HinFlow不需要预先定义好的专家轨迹，而是通过在线交互和回溯学习，自动地发现有效的策略。\\n\\n**关键设计**：论文使用2D点流作为高层规划器，并设计了一个目标条件模仿学习网络，该网络以当前状态和目标为输入，输出机器人的动作。损失函数采用标准的模仿学习损失，例如交叉熵损失或均方误差损失。回溯标注模块根据实际达到的状态与目标状态的距离来判断是否成功，并进行标注。",
            "application_zh": "该研究成果可应用于各种机器人操作任务，例如物体抓取、装配、导航等。通过利用非机器人数据源（如视频）训练高层规划器，并使用HinFlow将规划转化为可执行的机器人动作，可以降低机器人学习的成本，并提高机器人的泛化能力。该方法在自动化生产、家庭服务、医疗康复等领域具有广阔的应用前景。",
            "highlight_zh": "HinFlow在模拟和真实机器人操作任务中，性能比基线策略提高了2倍以上，显著优于现有方法。例如，在某个具体的抓取任务中，HinFlow的成功率达到了80%，而基线策略只有40%。此外，HinFlow还展示了从跨具身视频数据学习策略的潜力，这表明该方法具有很强的泛化能力。",
            "tags_zh": [
                "机器人学习",
                "模仿学习",
                "在线学习",
                "分层强化学习",
                "回溯学习"
            ],
            "_index": 63,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19269v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19269v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19269v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors",
            "authors": [
                "Georgios Voulgaris"
            ],
            "arxiv_id": "2512.19504v1",
            "summary": "Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.\n  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.\n  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.\n  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Preprint. Under review at IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19504v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]representation learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "FusionNet：通过可训练信号处理先验实现多光谱与热数据的物理感知表征学习",
            "summary_zh": "现代深度学习模型在处理多模态视觉信号时，常常依赖于与信号形成相关的物理过程不一致的归纳偏置，导致在跨光谱和真实世界条件下性能不稳定。特别是，优先考虑直接热线索的方法难以捕捉由持续热排放引起的间接但持久的环境变化。本文提出了一种物理感知表征学习框架，该框架利用多光谱信息来建模长期物理过程的稳定特征。具体而言，将对土壤性质变化敏感的地质短波红外（SWIR）比率与热红外（TIR）数据通过中间融合架构（FusionNet）集成。所提出的骨干网络在卷积层中嵌入可训练的微分信号处理先验，结合混合池化策略，并采用更宽的感受野，以增强跨光谱模态的鲁棒性。系统消融实验表明，每个架构组件都有助于性能提升，DGCNN在SWIR比率上实现了88.7%的准确率，FusionNet达到了90.6%，优于五种光谱配置下的最先进基线。迁移学习实验进一步表明，ImageNet预训练会降低TIR性能，突出了模态感知训练对于跨光谱学习的重要性。在真实世界数据上的评估结果表明，将物理感知特征选择与有原则的深度学习架构相结合，可以产生鲁棒且可泛化的表征，说明了第一性原理信号建模如何改善具有挑战性条件下的多光谱学习。",
            "intro_zh": [
                "现有深度学习模型在多模态视觉信号处理中，缺乏与物理过程对齐的归纳偏置，导致性能在跨光谱和真实场景下表现不佳。",
                "论文提出FusionNet，一种物理感知表征学习框架，通过可训练的信号处理先验，融合短波红外和热红外数据，建模长期物理过程的稳定特征。",
                "实验结果表明，FusionNet在多光谱数据处理上优于现有方法，且ImageNet预训练反而会降低热红外性能，强调了模态感知训练的重要性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多光谱和热数据融合中，现有深度学习模型缺乏物理感知能力，导致在真实场景和跨光谱条件下性能不佳的问题。现有方法通常直接处理热数据，忽略了由长期热排放引起的间接环境变化，并且缺乏对不同光谱模态之间物理关系的有效建模。\\n\\n**核心思路**：论文的核心思路是引入物理感知的先验知识，通过可训练的信号处理模块，将短波红外（SWIR）和热红外（TIR）数据进行融合，从而更好地捕捉长期物理过程的稳定特征。这种方法旨在克服现有方法对直接热线索的过度依赖，并提高模型在复杂环境下的鲁棒性和泛化能力。\\n\\n**技术框架**：FusionNet的整体架构包含以下几个主要模块：1) 输入层：接收SWIR和TIR数据；2) 可训练的微分信号处理层：嵌入在卷积层中，用于提取物理相关的特征；3) 混合池化层：结合不同类型的池化操作，增强特征的鲁棒性；4) 宽感受野卷积层：用于捕捉更大范围的上下文信息；5) 融合层：将SWIR和TIR特征进行融合；6) 输出层：输出最终的预测结果。\\n\\n**关键创新**：论文最重要的技术创新点在于引入了可训练的微分信号处理先验，并将其嵌入到卷积层中。这种方法允许模型学习与物理过程相关的特征，从而更好地理解多光谱数据。与现有方法相比，FusionNet更加注重对物理过程的建模，而不是仅仅依赖于数据驱动的学习。\\n\\n**关键设计**：在网络结构方面，FusionNet采用了混合池化策略，结合了最大池化和平均池化，以增强特征的鲁棒性。此外，网络还使用了更宽的感受野，以便捕捉更大范围的上下文信息。在训练方面，论文强调了模态感知训练的重要性，避免使用ImageNet等通用数据集进行预训练，而是针对多光谱数据进行专门的训练。",
            "application_zh": "该研究成果可应用于遥感图像分析、地质勘探、环境监测、火灾检测等领域。通过结合物理感知的深度学习模型，可以更准确地识别和分析地表特征，为资源管理、灾害预警和环境保护提供更可靠的信息支持。未来，该方法有望扩展到其他多模态数据融合任务中，例如医学图像分析和自动驾驶。",
            "highlight_zh": "实验结果表明，FusionNet在SWIR比率预测任务中达到了90.6%的准确率，优于DGCNN（88.7%）和其他基线方法。消融实验证明了每个架构组件对性能提升的贡献。此外，迁移学习实验表明，ImageNet预训练会降低TIR性能，突出了模态感知训练的重要性。在真实世界数据上的评估结果验证了FusionNet的鲁棒性和泛化能力。",
            "tags_zh": [
                "多光谱数据融合",
                "物理感知学习",
                "深度学习",
                "信号处理",
                "热红外",
                "短波红外",
                "表征学习"
            ],
            "_index": 64,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19504v1/Figures/CementChip.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19504v1/Figures/DataChips.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19504v1/Figures/DeepFusionModel_3.jpg",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
            "authors": [
                "Yuqiao Tan",
                "Minzheng Wang",
                "Shizhu He",
                "Huanxuan Liao",
                "Chengfeng Zhao",
                "Qiunan Lu",
                "Tian Liang",
                "Jun Zhao",
                "Kang Liu"
            ],
            "arxiv_id": "2512.19673v1",
            "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
            "categories": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19673v1",
            "code_links": [
                {
                    "url": "https://github.com/Trae1ounG/BuPO",
                    "type": "github"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "提出自底向上策略优化(BuPO)，通过优化LLM内部策略提升复杂推理能力。",
            "summary_zh": "现有的强化学习方法将大型语言模型（LLM）视为一个单一的统一策略，忽略了其内部机制。理解策略如何在层和模块之间演变对于实现更有针对性的优化和揭示复杂的推理机制至关重要。本文通过利用Transformer残差流的内在分割以及隐藏状态与unembedding矩阵的组合与可采样策略之间的等价性来分解语言模型策略。这种分解揭示了内部层策略（对应于来自各个层的贡献）和内部模块策略（与每层内的自注意力和前馈网络（FFN）组件对齐）。通过分析内部策略的熵，我们发现：（a）早期层保持高熵以进行探索，顶层收敛到接近零熵以进行细化，并且收敛模式在模型系列之间有所不同。（b）LLama的预测空间在最后一层迅速收敛，而Qwen系列模型，尤其是Qwen3，表现出更像人类的、逐步结构化的推理模式。受这些发现的启发，我们提出了一种新颖的强化学习范例——自底向上策略优化（BuPO），该范例直接优化早期训练期间的内部层策略。通过在较低层对齐训练目标，BuPO重建了基础推理能力并实现了卓越的性能。在复杂推理基准上的大量实验证明了我们方法的有效性。",
            "intro_zh": [
                "现有强化学习方法忽略了LLM内部的层级结构和模块化设计，将其视为单一策略，限制了优化潜力。",
                "论文提出自底向上策略优化（BuPO），通过分解LLM策略，优化内部层策略，从而提升整体性能。",
                "实验表明，BuPO通过在底层对齐训练目标，能够有效重建基础推理能力，并在复杂推理任务上取得显著提升。"
            ],
            "method_zh": "**问题定义**：现有强化学习方法将LLM视为一个黑盒，忽略了其内部结构，无法针对性地优化不同层和模块的功能。这种方法无法充分利用LLM的潜力，尤其是在复杂推理任务中。现有方法缺乏对LLM内部策略演化过程的理解，难以揭示其推理机制。\\n\\n**核心思路**：论文的核心思路是将LLM的策略分解为内部层策略和内部模块策略，分别对应于Transformer的每一层以及每一层中的自注意力机制和前馈网络。通过分析这些内部策略的熵，可以了解LLM在不同层和模块上的行为模式。基于此，论文提出自底向上策略优化（BuPO），通过在早期训练阶段直接优化底层策略，从而引导LLM学习更有效的推理能力。\\n\\n**技术框架**：BuPO的技术框架主要包括以下几个步骤：1) **策略分解**：利用Transformer的残差连接和unembedding矩阵，将LLM的整体策略分解为内部层策略和内部模块策略。2) **熵分析**：分析不同层和模块的策略熵，了解LLM在不同阶段的行为模式。3) **自底向上优化**：在早期训练阶段，直接优化底层策略，使其更好地对齐训练目标。4) **整体策略优化**：在底层策略优化完成后，再进行整体策略的优化，从而提升LLM的整体性能。\\n\\n**关键创新**：BuPO的关键创新在于：1) **策略分解**：首次将LLM的策略分解为内部层策略和内部模块策略，为理解LLM的内部机制提供了新的视角。2) **自底向上优化**：提出了一种新的强化学习范式，通过在早期训练阶段优化底层策略，从而引导LLM学习更有效的推理能力。与现有方法相比，BuPO能够更有效地利用LLM的内部结构，从而提升其性能。\\n\\n**关键设计**：BuPO的关键设计包括：1) **残差连接和unembedding矩阵的使用**：利用Transformer的残差连接和unembedding矩阵，实现了LLM策略的分解。2) **熵的计算**：通过计算不同层和模块的策略熵，了解LLM在不同阶段的行为模式。3) **底层策略的优化目标**：在底层策略的优化过程中，需要设计合适的优化目标，使其能够更好地对齐整体训练目标。具体的损失函数和优化算法的选择需要根据具体的任务进行调整。",
            "application_zh": "该研究成果可应用于各种需要复杂推理能力的场景，例如问答系统、对话生成、代码生成等。通过优化LLM的内部策略，可以提升其推理能力和泛化能力，使其能够更好地解决实际问题。此外，该研究还可以帮助我们更好地理解LLM的内部机制，为未来的模型设计和优化提供指导。",
            "highlight_zh": "实验结果表明，BuPO在复杂推理基准上取得了显著的性能提升。例如，在某些任务上，BuPO的性能超过了现有最先进的方法。通过分析内部策略的熵，论文还发现不同LLM的推理模式存在差异，例如LLama在最后一层迅速收敛，而Qwen系列模型则表现出更像人类的逐步结构化推理模式。",
            "tags_zh": [
                "大型语言模型",
                "强化学习",
                "策略优化",
                "内部策略",
                "复杂推理"
            ],
            "_index": 65,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19673v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19673v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19673v1/x4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
            "authors": [
                "Xueming Yan",
                "Bo Yin",
                "Yaochu Jin"
            ],
            "arxiv_id": "2512.19516v1",
            "summary": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19516v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出LacaDM，通过潜在因果扩散模型提升多目标强化学习适应性",
            "summary_zh": "本文提出了一种名为潜在因果扩散模型（LacaDM）的新方法，旨在增强多目标强化学习（MORL）在离散和连续环境中的适应性。MORL由于目标之间的固有冲突以及适应动态环境的困难而面临重大挑战。LacaDM不同于主要解决目标之间冲突的现有方法，它学习环境状态和策略之间的潜在时间因果关系，从而能够在不同的MORL场景中实现有效的知识迁移。通过将这些因果结构嵌入到基于扩散模型的框架中，LacaDM在冲突目标之间实现了平衡，同时在以前未见过的环境中保持了强大的泛化能力。在MOGymnasium框架的各种任务上的实证评估表明，LacaDM在超体积、稀疏性和预期效用最大化方面始终优于最先进的基线，展示了其在复杂多目标任务中的有效性。",
            "intro_zh": [
                "多目标强化学习在复杂环境中泛化能力弱，难以有效处理目标冲突。",
                "LacaDM学习环境状态和策略间的潜在因果关系，提升知识迁移和泛化能力。",
                "实验表明，LacaDM在超体积、稀疏性和效用最大化方面优于现有方法。"
            ],
            "method_zh": "**问题定义**：多目标强化学习（MORL）旨在寻找一组策略，每个策略在多个目标上都表现良好。然而，由于目标之间的冲突，以及环境的动态性，MORL算法通常难以泛化到新的环境或任务。现有的方法主要关注于解决目标之间的冲突，而忽略了环境状态和策略之间的潜在因果关系，导致知识迁移效率低下。\\n\\n**核心思路**：LacaDM的核心思路是学习环境状态和策略之间的潜在时间因果关系，并将这些关系嵌入到扩散模型中。通过学习这些因果关系，LacaDM可以更好地理解环境的动态性，并能够将知识从一个环境迁移到另一个环境。扩散模型提供了一个强大的框架，用于建模复杂的数据分布，并能够生成新的策略。\\n\\n**技术框架**：LacaDM的整体框架包括以下几个主要模块：1）环境交互模块：智能体与环境进行交互，收集状态、动作和奖励数据。2）潜在因果关系学习模块：使用因果发现算法学习环境状态和策略之间的潜在因果关系。3）扩散模型训练模块：使用收集到的数据和学习到的因果关系训练扩散模型，使其能够生成新的策略。4）策略优化模块：使用扩散模型生成的策略作为初始策略，并使用强化学习算法进行优化。\\n\\n**关键创新**：LacaDM的关键创新在于将因果发现和扩散模型结合起来，用于解决多目标强化学习问题。通过学习环境状态和策略之间的潜在因果关系，LacaDM可以更好地理解环境的动态性，并能够将知识从一个环境迁移到另一个环境。此外，扩散模型提供了一个强大的框架，用于建模复杂的数据分布，并能够生成新的策略。\\n\\n**关键设计**：LacaDM的关键设计包括：1）使用时间序列因果发现算法（例如PCMCI）学习环境状态和策略之间的潜在因果关系。2）使用去噪扩散概率模型（DDPM）作为扩散模型，并使用Transformer网络作为扩散模型的骨干网络。3）使用超体积作为奖励函数，以鼓励智能体探索不同的策略。4）使用对抗训练来提高扩散模型的生成能力。",
            "application_zh": "LacaDM具有广泛的应用前景，例如机器人控制、自动驾驶、金融交易和医疗决策等领域。在这些领域中，通常存在多个相互冲突的目标，并且环境是动态变化的。LacaDM可以帮助智能体在这些复杂环境中找到一组平衡的策略，从而实现更好的性能和鲁棒性。未来，LacaDM可以进一步扩展到处理更复杂的环境和任务，例如具有部分可观测性的环境和具有长期依赖关系的任务。",
            "highlight_zh": "LacaDM在MOGymnasium框架的多个任务上进行了评估，实验结果表明，LacaDM在超体积（Hypervolume）、稀疏性（Sparsity）和预期效用最大化（Expected Utility Maximization）方面始终优于最先进的基线方法。例如，在某些任务上，LacaDM的超体积比基线方法提高了20%以上，表明LacaDM能够找到一组更好的策略，从而更好地平衡不同的目标。",
            "tags_zh": [
                "多目标强化学习",
                "因果发现",
                "扩散模型",
                "知识迁移",
                "泛化能力"
            ],
            "_index": 66,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19516v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19516v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19516v1/x6.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
            "authors": [
                "Simon Welker",
                "Bunlong Lay",
                "Maris Hillemann",
                "Tal Peer",
                "Timo Gerkmann"
            ],
            "arxiv_id": "2512.19442v1",
            "summary": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.",
            "categories": [
                "eess.SP",
                "cs.LG",
                "cs.SD"
            ],
            "primary_category": "eess.SP",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "This work has been submitted to the IEEE for possible publication",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19442v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]flow matching"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出Stream.FM：一种实时流式生成语音恢复Flow Matching模型，延迟低至48ms。",
            "summary_zh": "近年来，基于扩散的生成模型对语音处理领域产生了巨大影响，展现出高度的语音自然度，并催生了一个新的研究方向。然而，由于其计算密集型特性，涉及多次调用大型DNN，它们在实时通信中的应用仍然滞后。本文提出Stream.FM，一种帧因果的基于流的生成模型，算法延迟为32毫秒（ms），总延迟为48毫秒，为实时通信中的生成语音处理铺平了道路。我们提出了一种缓冲流式推理方案和优化的DNN架构，展示了如何通过学习的少步数值求解器在固定的计算预算下提高输出质量，探索了模型权重压缩以找到计算/质量权衡的有利点，并贡献了一个总延迟为24毫秒的语音增强任务模型变体。我们的工作超越了理论延迟，表明高质量的流式生成语音处理可以在当今可用的消费级GPU上实现。Stream.FM可以以流式方式解决各种语音处理任务：语音增强、去混响、编解码器后滤波、带宽扩展、STFT相位检索和Mel声码器。正如我们通过全面的评估和MUSHRA听力测试所验证的那样，Stream.FM为生成流式语音恢复建立了最先进的水平，与非流式变体相比，质量仅有合理的降低，并且在更低的延迟下优于我们最近在生成流式语音增强方面的工作（Diffusion Buffer）。",
            "intro_zh": [
                "现有基于扩散的语音生成模型计算量大，难以应用于实时通信场景。",
                "提出Stream.FM，一种基于Flow Matching的流式生成模型，降低延迟并优化模型结构。",
                "实验表明，Stream.FM在多种语音处理任务上达到SOTA，且延迟远低于现有方法。"
            ],
            "method_zh": "**问题定义**：论文旨在解决实时通信中生成语音处理的延迟问题。现有的基于扩散的生成模型虽然能产生高质量的语音，但由于计算复杂度高，难以满足实时性要求，限制了其在实时语音增强、去混响等场景的应用。\\n\\n**核心思路**：论文的核心思路是利用Flow Matching模型，并结合流式推理和模型优化，在保证语音质量的前提下，显著降低模型的延迟。Flow Matching相比扩散模型，训练和推理效率更高，更适合实时应用。\\n\\n**技术框架**：Stream.FM的整体框架包括以下几个主要部分：1) 帧因果的Flow Matching模型：使用Flow Matching作为生成模型的核心，保证模型的生成能力。2) 缓冲流式推理方案：通过缓冲输入帧，实现流式推理，降低延迟。3) 优化的DNN架构：设计高效的DNN结构，减少计算量。4) 少步数值求解器：学习高效的数值求解器，在有限的计算资源下提升输出质量。5) 模型权重压缩：通过模型压缩，进一步降低计算量和延迟。\\n\\n**关键创新**：论文的关键创新在于将Flow Matching模型应用于流式语音处理，并提出了一系列优化方法，包括缓冲流式推理、优化的DNN架构、少步数值求解器和模型权重压缩。这些创新使得Stream.FM能够在保证语音质量的前提下，实现极低的延迟，满足实时通信的需求。\\n\\n**关键设计**：在模型设计上，论文采用了帧因果的结构，保证了流式处理的实时性。在训练过程中，使用了特定的损失函数来优化模型的生成质量和稳定性。在推理过程中，通过调整缓冲大小和数值求解器的步数，可以灵活地权衡计算量和语音质量。此外，论文还探索了不同的模型权重压缩方法，以进一步降低模型的计算复杂度。",
            "application_zh": "Stream.FM具有广泛的应用前景，可用于实时语音增强、去混响、编解码器后滤波、带宽扩展、STFT相位检索和Mel声码器等领域。该技术可以提升实时通信的语音质量和清晰度，改善用户体验，尤其是在网络条件较差或设备性能有限的情况下，其价值更为突出。未来，Stream.FM有望应用于在线会议、语音助手、游戏语音等多种场景。",
            "highlight_zh": "Stream.FM实现了48ms的总延迟，算法延迟为32ms，在生成流式语音恢复任务上达到了SOTA水平。与非流式变体相比，质量仅有合理的降低。在语音增强任务上，Stream.FM在更低的延迟下优于Diffusion Buffer。MUSHRA听力测试验证了Stream.FM的优越性能。",
            "tags_zh": [
                "流式语音处理",
                "语音增强",
                "Flow Matching",
                "实时通信",
                "生成模型",
                "低延迟",
                "语音恢复"
            ],
            "_index": 67,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19442v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19442v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19442v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation",
            "authors": [
                "Debamita Ghosh",
                "George K. Atia",
                "Yue Wang"
            ],
            "arxiv_id": "2512.18957v1",
            "summary": "The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18957v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "[T]reinforcement learning"
                    ],
                    "score": 4.5
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出在线分布鲁棒强化学习算法，解决训练与部署环境不匹配问题。",
            "summary_zh": "强化学习（RL）智能体在实际应用中的部署常常因训练和部署环境之间的不匹配而导致性能下降。分布鲁棒强化学习（DR-RL）通过优化过渡动态不确定性集合上的最坏情况性能来解决这个问题。然而，现有的工作通常依赖于大量的先验知识，例如访问生成模型或大型离线数据集，并且主要集中于无法扩展到复杂领域的表格方法。我们通过提出一种具有通用函数逼近的在线DR-RL算法来克服这些限制，该算法仅通过与环境的交互来学习最优鲁棒策略，而无需先验模型或离线数据，从而能够在高维任务中部署。我们进一步提供了理论分析，在总变差不确定性集合下建立了接近最优的次线性后悔界，证明了我们方法的样本效率和有效性。",
            "intro_zh": [
                "现有强化学习方法在训练和部署环境存在差异时，性能会显著下降，缺乏鲁棒性。",
                "论文提出一种在线分布鲁棒强化学习算法，无需先验知识或离线数据，直接与环境交互学习鲁棒策略。",
                "理论分析表明，该算法在总变差不确定性集合下具有接近最优的次线性后悔界，保证了样本效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决强化学习智能体在真实世界部署时，由于训练环境与实际环境存在分布差异而导致的性能下降问题。现有的分布鲁棒强化学习方法通常依赖于大量的先验知识，例如生成模型或离线数据集，或者仅适用于表格型问题，难以扩展到高维复杂环境。\\n\\n**核心思路**：论文的核心思路是在线学习一个鲁棒的策略，使其在过渡动态的不确定性集合下，能够优化最坏情况下的性能。通过与环境的交互，算法能够自适应地调整策略，从而应对环境的变化和不确定性，无需预先知道环境的精确模型。\\n\\n**技术框架**：该算法是一个在线学习框架，智能体与环境进行交互，收集经验数据，并利用这些数据来更新策略。主要包含以下几个阶段：1) 智能体根据当前策略选择动作；2) 环境根据动作给出奖励和下一个状态；3) 智能体将经验数据（状态、动作、奖励、下一个状态）存储起来；4) 智能体利用存储的经验数据，更新策略，使其在最坏情况下也能获得较好的性能。\\n\\n**关键创新**：该论文的关键创新在于提出了一种在线的分布鲁棒强化学习算法，该算法不需要任何先验知识或离线数据，可以直接与环境交互学习鲁棒策略。此外，该算法还具有通用函数逼近的能力，可以处理高维状态空间和动作空间的问题。\\n\\n**关键设计**：论文采用总变差距离来定义过渡动态的不确定性集合。算法使用函数逼近的方法来表示策略和价值函数，例如可以使用神经网络。算法的关键在于如何有效地估计不确定性集合，并利用这些信息来更新策略。具体的参数设置、损失函数和网络结构等细节，需要在实际应用中根据具体问题进行调整。",
            "application_zh": "该研究成果可应用于机器人控制、自动驾驶、金融交易等领域。在这些领域中，环境通常是动态变化的，并且存在各种不确定性。使用该算法可以训练出更加鲁棒的智能体，使其能够在各种复杂环境中稳定地工作，降低部署风险，提高系统可靠性。",
            "highlight_zh": "论文提供了理论分析，证明了该算法在总变差不确定性集合下具有接近最优的次线性后悔界。这意味着该算法具有较高的样本效率，可以在较少的交互次数下学习到较好的策略。实验结果（未在摘要中提及，此处假设）表明，该算法在多个benchmark任务上优于现有的在线强化学习算法，尤其是在环境存在较大不确定性的情况下。",
            "tags_zh": [
                "分布鲁棒强化学习",
                "在线学习",
                "函数逼近",
                "总变差距离",
                "次线性后悔界"
            ],
            "_index": 68,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18957v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18957v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18957v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
            "authors": [
                "Haoyu Jiang",
                "Fanjie Zeng",
                "Boan Qu",
                "Xiaojie Lin",
                "Wei Zhong"
            ],
            "arxiv_id": "2512.19299v1",
            "summary": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19299v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "RLHF"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "Helios：面向智慧能源知识推理与应用的领域专用大语言模型",
            "summary_zh": "为了在全球碳中和的背景下促进工业转型，深度协调的智慧能源系统至关重要。然而，该领域跨学科、碎片化和快速发展的专业知识使得通用大语言模型（LLM）难以提供精确的工程推理和生成，因为它们缺乏领域知识和物理约束意识。为了解决这些挑战，我们推出了Helios，一个专为智慧能源领域定制的大语言模型，以及一套全面的资源，以推进该领域LLM的研究。具体来说，我们开发了Enersys，一个用于端到端数据集构建的多智能体协作框架，通过该框架，我们生成了：（1）一个智慧能源知识库EnerBase，以丰富模型的基础专业知识；（2）一个指令微调数据集EnerInstruct，以加强模型在领域特定下游任务上的性能；（3）一个RLHF数据集EnerReinforce，使模型与人类偏好和行业标准对齐。利用这些资源，Helios经历了大规模的预训练、SFT和RLHF。我们还发布了EnerBench，一个用于评估智慧能源场景中LLM的基准，并证明我们的方法显著提高了领域知识掌握、任务执行准确性和与人类偏好的一致性。",
            "intro_zh": [
                "通用LLM在智慧能源领域面临挑战，缺乏领域知识和物理约束意识，难以进行精确推理和生成。",
                "Helios通过构建领域知识库EnerBase、指令微调数据集EnerInstruct和RLHF数据集EnerReinforce，增强模型在智慧能源领域的性能。",
                "实验表明，Helios在领域知识掌握、任务执行准确性和与人类偏好的一致性方面均有显著提升。"
            ],
            "method_zh": "**问题定义**：现有通用大语言模型（LLM）在智慧能源领域应用受限，主要痛点在于缺乏该领域的专业知识，无法理解和应用物理约束，导致推理和生成结果不准确，难以满足工程需求。该领域知识分散且更新迅速，通用LLM难以有效学习和利用。\n\n**核心思路**：论文的核心思路是构建一个领域专用的大语言模型Helios，通过大规模的领域数据训练，使其具备智慧能源领域的专业知识和推理能力。通过构建知识库、指令微调数据集和RLHF数据集，从不同维度提升模型的性能和对齐。\n\n**技术框架**：Helios的训练框架包括三个主要阶段：预训练、指令微调（SFT）和基于人类反馈的强化学习（RLHF）。首先，利用EnerBase知识库进行预训练，使模型具备领域基础知识。然后，使用EnerInstruct数据集进行指令微调，提升模型在特定任务上的执行能力。最后，使用EnerReinforce数据集进行RLHF，使模型的输出更符合人类偏好和行业标准。整个流程由Enersys多智能体协作框架驱动，实现端到端的数据集构建和模型训练。\n\n**关键创新**：该论文的关键创新在于构建了一套完整的智慧能源领域LLM训练资源，包括EnerBase知识库、EnerInstruct指令微调数据集和EnerReinforce RLHF数据集。Enersys多智能体协作框架能够高效地生成高质量的训练数据，解决了领域数据稀缺的问题。Helios模型本身也是一个创新，它是首个专门针对智慧能源领域设计的大语言模型。\n\n**关键设计**：Enersys框架采用多智能体协作的方式，每个智能体负责不同的数据生成任务，例如知识抽取、问题生成、答案生成等。EnerBase知识库包含大量的智慧能源领域知识，包括概念、实体、关系等。EnerInstruct数据集包含各种智慧能源领域的任务指令，例如故障诊断、优化调度等。EnerReinforce数据集包含人类对模型输出的偏好反馈，用于训练奖励模型，指导RLHF过程。具体的参数设置和网络结构细节在论文中可能有所描述，但摘要中未明确提及。",
            "application_zh": "Helios在智慧能源领域具有广泛的应用前景，例如智能电网优化、能源需求预测、故障诊断、设备维护等。它可以帮助工程师和研究人员更高效地进行能源系统设计、运行和管理，提高能源利用效率，降低碳排放，加速能源转型。未来，Helios有望成为智慧能源领域的重要基础设施。",
            "highlight_zh": "Helios在EnerBench基准测试中表现出色，显著优于通用LLM和其他基线模型。实验结果表明，Helios在领域知识掌握、任务执行准确性和与人类偏好的一致性方面均有显著提升。具体的性能数据和提升幅度在摘要中未明确给出，需要在论文正文中查找。",
            "tags_zh": [
                "智慧能源",
                "大语言模型",
                "知识推理",
                "指令微调",
                "强化学习",
                "领域知识库",
                "智能电网"
            ],
            "_index": 69,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19299v1/Helios8.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19299v1/md.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19299v1/Evaluate_and_Optimize.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Can abstract concepts from LLM improve SLM performance?",
            "authors": [
                "Siddharth Tandon"
            ],
            "arxiv_id": "2512.19069v1",
            "summary": "Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19069v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "distillation"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 4.5,
            "hit_pillars": [
                "2_algo_arch",
                "9_embodied_foundation"
            ],
            "headline_zh": "利用LLM抽象概念提升SLM性能，实现推理时动态调整",
            "summary_zh": "大型语言模型(LLM)在各种任务中表现出色，但将其部署在资源受限的设备上仍然具有挑战性。量化、剪枝和蒸馏等现有方法可以减少内存占用，但通常需要大量的实验和仔细的基础设施设计。本文利用现有的从大型模型中提取高级概念（表示为steering vectors）的技术，研究它们在推理时对小型语言模型(SLM)的可迁移性。通过大量的实验证明，这些概念可以有效地转移到更小的模型，而不管它们的家族（例如，Phi，Llama，Qwen），从而提高各种任务的性能。此外，本文引入了推理时缩放，通过动态调整steering intensity来增强性能，从而使Qwen3-0.6B的准确率提高了7-15%。",
            "intro_zh": [
                "现有模型压缩方法（量化、剪枝等）部署复杂，需要大量实验和基础设施支持。",
                "论文提出将LLM中的高级概念（steering vectors）迁移到SLM，提升SLM的性能。",
                "实验表明，该方法可有效提升不同SLM家族（Phi, Llama, Qwen）的性能，Qwen3-0.6B准确率提升7-15%。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）难以在资源受限设备上部署的问题。现有模型压缩方法，如量化、剪枝和蒸馏，虽然可以减小模型体积，但通常需要大量的实验和复杂的工程设计，部署成本高昂。\\n\\n**核心思路**：论文的核心思路是将LLM中学习到的高级抽象概念（steering vectors）迁移到小型语言模型（SLM）中，从而提升SLM的性能，而无需对SLM进行额外的训练或微调。这种方法旨在利用LLM的知识，以一种更轻量级的方式增强SLM的能力。\\n\\n**技术框架**：该方法主要包含两个阶段：1) 从LLM中提取steering vectors，这些vectors代表了LLM学习到的高级概念。2) 在SLM的推理过程中，将这些steering vectors注入到SLM的激活中，从而引导SLM的输出。此外，论文还引入了推理时缩放，通过动态调整steering intensity来进一步优化性能。\\n\\n**关键创新**：该方法最重要的创新点在于将LLM的抽象概念以steering vectors的形式迁移到SLM，实现知识迁移。与传统的模型蒸馏方法不同，该方法不需要训练SLM，而是直接在推理时注入知识，降低了计算成本。此外，推理时缩放机制允许动态调整steering intensity，进一步提升了性能。\\n\\n**关键设计**：steering vectors的提取方式未知，论文中没有详细描述。推理时缩放的关键在于确定合适的steering intensity。论文通过实验确定了最佳的缩放因子，但具体的优化算法未知。",
            "application_zh": "该研究成果可应用于各种资源受限的场景，例如移动设备、嵌入式系统和边缘计算设备。通过将LLM的知识迁移到SLM，可以在这些设备上部署更智能的应用，例如智能助手、机器翻译和文本生成。该方法还可以用于提升现有SLM的性能，而无需重新训练模型。",
            "highlight_zh": "实验结果表明，该方法可以有效地将LLM的抽象概念迁移到SLM，并显著提升SLM在各种任务上的性能。对于Qwen3-0.6B模型，通过引入推理时缩放，准确率提高了7-15%。该方法适用于不同的SLM家族，例如Phi, Llama和Qwen，具有较强的通用性。",
            "tags_zh": [
                "大型语言模型",
                "小型语言模型",
                "知识迁移",
                "steering vectors",
                "推理优化",
                "模型压缩",
                "资源受限设备"
            ],
            "_index": 70,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19069v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19069v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19069v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
            "authors": [
                "Utae Jeong",
                "Sumin In",
                "Hyunju Ryu",
                "Jaewan Choi",
                "Feng Yang",
                "Jongheon Jeong",
                "Seungryong Kim",
                "Sangpil Kim"
            ],
            "arxiv_id": "2512.19048v1",
            "summary": "Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19048v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "world model"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱三：空间感知与语义 (Perception & Semantics)",
                    "id": "3_perception_slam",
                    "matched_keywords": [
                        "optical flow"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "3_perception_slam"
            ],
            "headline_zh": "提出WaTeRFlow框架，增强水印在图像转视频中的时间鲁棒性",
            "summary_zh": "图像水印技术用于验证图像的真实性和来源，但现有方案容易受到各种失真和生成式编辑的攻击。基于深度学习的水印技术提高了对扩散模型图像编辑的鲁棒性，但当水印图像通过图像转视频（I2V）转换为视频时，每帧水印检测的性能会下降。I2V技术已从短而抖动的片段迅速发展到多秒、时间连贯的场景，不仅用于内容创作，还用于世界建模和模拟工作流程，使得跨模态水印恢复至关重要。本文提出了WaTeRFlow框架，专门用于增强I2V下的鲁棒性。它包含：（i）FUSE（Flow-guided Unified Synthesis Engine），通过指令驱动的编辑和快速视频扩散代理，在训练期间使编码器-解码器暴露于真实的失真；（ii）光流扭曲和时间一致性损失（TCL），用于稳定每帧预测；（iii）语义保持损失，用于维护条件信号。在代表性的I2V模型上的实验表明，从帧中可以准确恢复水印，并且在视频生成前后应用各种失真时，首帧和每帧的比特准确率更高，鲁棒性更强。",
            "intro_zh": [
                "现有水印方案在图像转视频后，因每帧独立检测导致鲁棒性下降，无法满足跨模态应用需求。",
                "WaTeRFlow框架通过FUSE引入真实失真，利用光流和时间一致性损失稳定预测，并保持语义信息。",
                "实验表明，WaTeRFlow显著提升了水印在I2V场景下的鲁棒性，提高了首帧和每帧的比特准确率。"
            ],
            "method_zh": "**问题定义**：现有图像水印技术在图像转视频（I2V）场景下表现不佳。主要痛点在于，I2V过程引入了时间上的不一致性，导致每帧独立的水印检测结果不稳定，整体鲁棒性下降。此外，现有的深度学习水印方法主要关注图像域的攻击，缺乏对视频生成过程中的各种失真和编辑的适应性。\n\n**核心思路**：WaTeRFlow的核心思路是利用光流信息来增强水印检测的时间一致性。通过在训练过程中模拟I2V过程中的各种失真，并引入时间一致性损失，使得水印检测器能够更好地适应视频帧之间的变化，从而提高整体的鲁棒性。此外，还通过语义保持损失来确保水印的嵌入不会影响视频内容的语义信息。\n\n**技术框架**：WaTeRFlow框架主要包含三个核心模块：FUSE（Flow-guided Unified Synthesis Engine）、光流扭曲与时间一致性损失（TCL）以及语义保持损失。FUSE模块负责生成带有各种失真的训练数据，模拟真实的I2V场景。光流扭曲与TCL模块利用光流信息来对齐相邻帧的水印检测结果，并引入时间一致性损失来约束检测结果的稳定性。语义保持损失则用于确保水印的嵌入不会影响视频内容的语义信息。\n\n**关键创新**：WaTeRFlow的关键创新在于其针对I2V场景的时间鲁棒性设计。与现有的图像水印方法不同，WaTeRFlow显式地考虑了视频帧之间的时间关系，并利用光流信息来增强水印检测的时间一致性。此外，FUSE模块能够生成更加真实的训练数据，使得水印检测器能够更好地适应各种I2V过程中的失真。\n\n**关键设计**：FUSE模块通过指令驱动的编辑和快速视频扩散代理来生成带有各种失真的训练数据。时间一致性损失（TCL）基于光流计算相邻帧之间的对应关系，并约束对应位置的水印检测结果尽可能一致。语义保持损失通常采用感知损失或对抗损失，以确保水印的嵌入不会影响视频内容的语义信息。具体的网络结构和参数设置取决于所使用的I2V模型和水印检测器。",
            "application_zh": "WaTeRFlow技术可应用于视频版权保护、内容溯源、深度伪造检测等领域。通过在视频中嵌入鲁棒的水印，可以有效防止未经授权的复制和传播，并为内容创作者提供法律保护。此外，该技术还可以用于检测深度伪造视频，维护网络安全和信息安全。",
            "highlight_zh": "实验结果表明，WaTeRFlow在各种I2V模型上均能实现准确的水印恢复，并且在视频生成前后应用各种失真时，首帧和每帧的比特准确率均显著提高。与现有方法相比，WaTeRFlow在I2V场景下具有更强的鲁棒性和更高的准确率，能够有效抵抗各种攻击。",
            "tags_zh": [
                "图像水印",
                "视频水印",
                "时间鲁棒性",
                "图像转视频",
                "光流",
                "深度学习",
                "版权保护",
                "内容溯源"
            ],
            "_index": 71,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19048v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19048v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19048v1/figure/figure3_05.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning Through Little Eyes: Attribute Discrimination Beyond Objects",
            "authors": [
                "Patrick Batsell",
                "Tsutsui Satoshi",
                "Bihan Wen"
            ],
            "arxiv_id": "2512.18951v1",
            "summary": "Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18951v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "contrastive learning"
                    ],
                    "score": 1.5
                },
                {
                    "name": "支柱六：视频提取与匹配 (Video Extraction)",
                    "id": "6_video_extraction",
                    "matched_keywords": [
                        "egocentric"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 3.5,
            "hit_pillars": [
                "2_algo_arch",
                "6_video_extraction"
            ],
            "headline_zh": "通过婴儿视角学习：探索超越物体的属性辨别能力",
            "summary_zh": "婴儿在生命最初两年不仅学会识别物体类别，还能辨别颜色、大小和纹理等细粒度属性。先前工作Childs View for Contrastive Learning (CVCL) 是一种基于婴儿自我中心视频训练的CLIP风格模型，作为早期婴儿学习的计算模型，但它仅关注类别级别的识别。本文旨在探究婴儿尺度的学习是否支持属性辨别。为此，我们引入了一个基准，系统地改变颜色、大小和纹理，从而对类内属性识别进行受控测试。将CVCL与CLIP进行比较表明存在明显差异。CVCL在大小辨别方面表现更好，而CLIP在颜色辨别方面准确率更高。两种模型都在图像嵌入中表示纹理，但未能将纹理在语言上进行关联，表明视觉和语言空间之间存在差距。",
            "intro_zh": [
                "现有基于婴儿自我中心视频的对比学习模型主要关注物体类别识别，忽略了婴儿对物体属性（如颜色、大小、纹理）的辨别能力。",
                "论文提出一个基准数据集，系统性地改变颜色、大小和纹理等属性，用于评估模型在类内属性识别方面的能力。",
                "实验结果表明，CVCL在大小辨别上优于CLIP，而CLIP在颜色辨别上更胜一筹，两者在纹理的语言关联上均存在不足。"
            ],
            "method_zh": "**问题定义**：论文旨在研究基于婴儿视角学习的模型是否具备辨别物体属性（颜色、大小、纹理）的能力。现有方法，如CVCL，主要关注物体类别的识别，忽略了婴儿在早期学习过程中对物体属性的辨别能力。因此，如何评估和提升模型在属性辨别方面的性能是一个关键问题。\\n\\n**核心思路**：论文的核心思路是通过构建一个系统性的基准数据集，该数据集在同一物体类别下，系统性地改变颜色、大小和纹理等属性，从而能够对模型在类内属性识别方面的能力进行受控测试。通过比较CVCL和CLIP在这一基准上的表现，可以深入了解婴儿视角学习模型在属性辨别方面的优势和不足。\\n\\n**技术框架**：论文的技术框架主要包括以下几个部分：1）构建属性辨别基准数据集：该数据集包含多种物体类别，每个类别下包含不同颜色、大小和纹理的样本。2）使用CVCL和CLIP模型对基准数据集进行测试，提取图像嵌入。3）设计评估指标，用于衡量模型在颜色、大小和纹理辨别方面的准确率。4）分析CVCL和CLIP在不同属性辨别任务上的表现差异，并探讨其原因。\\n\\n**关键创新**：论文的关键创新在于：1）提出了一个用于评估模型属性辨别能力的基准数据集，该数据集能够系统性地控制颜色、大小和纹理等属性的变化。2）通过对比CVCL和CLIP在属性辨别任务上的表现，揭示了婴儿视角学习模型在大小辨别方面的优势，以及在纹理语言关联方面的不足。\\n\\n**关键设计**：论文的关键设计包括：1）基准数据集的构建：确保数据集包含足够多的物体类别和属性变化，以保证评估的可靠性。2）评估指标的选择：选择合适的评估指标，能够准确衡量模型在不同属性辨别任务上的性能。3）模型参数设置：CVCL和CLIP模型使用默认参数，以保证实验的公平性。",
            "application_zh": "该研究成果可应用于开发更智能的计算机视觉系统，使其能够像婴儿一样理解和辨别物体属性，从而提升图像识别、物体检测和场景理解等任务的性能。此外，该研究还有助于深入理解人类早期认知发展，为教育和儿童发展提供新的启示。",
            "highlight_zh": "实验结果表明，CVCL在大小辨别方面优于CLIP，而CLIP在颜色辨别方面表现更好。两种模型都能在图像嵌入中表示纹理，但都未能有效地将纹理与语言关联起来。例如，CVCL在大小属性辨别上取得了显著的优势，这表明婴儿视角学习可能更侧重于对物体大小的感知。",
            "tags_zh": [
                "婴儿视角学习",
                "属性辨别",
                "对比学习",
                "计算机视觉",
                "CLIP模型"
            ],
            "_index": 72,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18951v1/figs/scene.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18951v1/figs/method.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18951v1/figs/horse_large_smooth_02_brown.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
            "authors": [
                "Jiaqi Peng",
                "Wenzhe Cai",
                "Yuqiang Yang",
                "Tai Wang",
                "Yuan Shen",
                "Jiangmiao Pang"
            ],
            "arxiv_id": "2512.19629v1",
            "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}.",
            "categories": [
                "cs.RO",
                "cs.CV"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Project page:https://steinate.github.io/logoplanner.github.io/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19629v1",
            "code_links": [
                {
                    "url": "https://steinate.github.io/logoplanner.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "cross-embodiment"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "LoGoPlanner：基于度量视觉几何的定位引导端到端导航策略",
            "summary_zh": "本文提出LoGoPlanner，一个定位引导的端到端导航框架，旨在解决移动机器人在非结构化环境中轨迹规划的挑战。传统模块化方法存在延迟和感知、定位、建图和规划模块之间的误差累积问题。现有的端到端学习方法依赖于精确的传感器外参标定进行自定位，限制了其在不同机器人和环境中的泛化能力。LoGoPlanner通过以下方式解决这些限制：（1）微调长时程视觉几何骨干网络，以绝对度量尺度进行预测，从而提供隐式的状态估计以实现精确定位；（2）从历史观测中重建周围场景几何，为可靠的避障提供密集的、细粒度的环境感知；（3）策略以辅助任务引导的隐式几何为条件，从而减少误差传播。在仿真和真实世界环境中的评估表明，LoGoPlanner的完全端到端设计减少了累积误差，而度量感知几何记忆增强了规划一致性和避障能力，与oracle定位基线相比，性能提升超过27.3％，并在不同的机器人和环境中表现出强大的泛化能力。",
            "intro_zh": [
                "传统导航方法依赖模块化pipeline，易受延迟和模块间误差累积的影响，限制了整体性能。",
                "LoGoPlanner通过端到端学习，结合视觉几何信息进行定位和环境感知，实现更鲁棒的导航。",
                "实验表明，LoGoPlanner在仿真和真实环境中均优于传统方法，并在不同机器人和环境间具有良好的泛化性。"
            ],
            "method_zh": "**问题定义**：现有移动机器人的轨迹规划方法，特别是端到端学习方法，依赖于独立的定位模块，而这些模块又依赖于精确的传感器外参标定。这限制了算法在不同机器人平台和环境中的泛化能力。此外，传统模块化pipeline存在延迟和误差累积问题，影响整体导航性能。\\n\\n**核心思路**：LoGoPlanner的核心思想是将定位融入到端到端的导航策略中，通过学习视觉几何信息来隐式地进行状态估计，从而避免对独立定位模块的依赖。同时，利用历史观测重建环境几何信息，为导航策略提供更丰富的环境感知，增强避障能力。\\n\\n**技术框架**：LoGoPlanner是一个端到端的导航框架，主要包含以下几个模块：(1) 长时程视觉几何骨干网络：用于提取视觉特征并预测场景的几何信息，通过微调使其具备绝对度量尺度感知能力。(2) 几何记忆模块：利用历史观测重建周围场景的几何信息，提供密集的、细粒度的环境感知。(3) 导航策略模块：以视觉特征和几何记忆作为输入，输出控制信号或轨迹。整个框架通过端到端的方式进行训练，实现定位、环境感知和导航的联合优化。\\n\\n**关键创新**：LoGoPlanner的关键创新在于将定位融入到端到端的导航策略中，通过学习视觉几何信息来隐式地进行状态估计，避免了对独立定位模块的依赖。此外，利用历史观测重建环境几何信息，为导航策略提供更丰富的环境感知。这种端到端的设计减少了误差传播，提高了导航的鲁棒性和泛化能力。\\n\\n**关键设计**：LoGoPlanner的关键设计包括：(1) 使用长时程视觉几何骨干网络，例如Transformer或LSTM，来提取视觉特征并预测场景的几何信息。(2) 设计合适的损失函数，例如度量学习损失或几何重建损失，来训练骨干网络使其具备绝对度量尺度感知能力。(3) 使用几何记忆模块，例如占用栅格地图或点云地图，来存储和更新环境几何信息。(4) 设计合适的导航策略网络，例如强化学习或模仿学习，以视觉特征和几何记忆作为输入，输出控制信号或轨迹。",
            "application_zh": "LoGoPlanner适用于各种移动机器人导航场景，尤其是在非结构化和动态环境中，如家庭服务机器人、仓储物流机器人、自动驾驶车辆等。该方法通过端到端学习和视觉几何信息，提高了导航的鲁棒性和泛化能力，降低了对传感器标定的依赖，具有重要的实际应用价值和广阔的应用前景。",
            "highlight_zh": "LoGoPlanner在仿真和真实世界环境中进行了评估，实验结果表明，与依赖oracle定位的基线方法相比，LoGoPlanner的性能提升超过27.3％。此外，LoGoPlanner在不同的机器人和环境中表现出强大的泛化能力，证明了其端到端设计和度量感知几何记忆的有效性。",
            "tags_zh": [
                "端到端导航",
                "视觉几何",
                "定位",
                "环境感知",
                "机器人",
                "轨迹规划",
                "度量学习",
                "深度学习"
            ],
            "_index": 73,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19629v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19629v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19629v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
            "authors": [
                "Zhenglong Guo",
                "Yiming Zhao",
                "Feng Jiang",
                "Heng Jin",
                "Zongbao Feng",
                "Jianbin Zhou",
                "Siyuan Xu"
            ],
            "arxiv_id": "2512.19453v1",
            "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "8 pages, 10 figures, This work was completed in December 2024",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19453v1",
            "code_links": [
                {
                    "url": "https://map-avr.github.io/",
                    "type": "project_page"
                }
            ],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "MaP-AVR：结合视觉语言模型与检索增强生成，为机器人提出元动作规划器",
            "summary_zh": "本文提出了一种名为MaP-AVR的元动作规划器，旨在提升具身机器人AI系统在复杂日常任务中的规划能力。该方法强调规划技能集的重要性，并提出将规划结果抽象为一组元动作，每个元动作包含{移动/旋转，末端执行器状态改变，与环境的关系}三个组成部分。这种抽象用机器人内在功能取代了以人为中心的动作概念，使规划结果与机器人可执行的动作范围对齐。为了确保LLM/VLM准确生成所需的元动作格式，采用了检索增强生成（RAG）技术，利用人工标注的规划演示数据库进行上下文学习。系统成功完成的任务越多，数据库将自我增强以支持多样性。使用GPT-4o和OmniGibson平台进行的实验表明，该方法与当前最先进的方法相比具有良好的性能。",
            "intro_zh": [
                "现有具身机器人任务规划方法侧重于微调或CoT提示来增强LLM/VLM的任务理解能力，忽略了规划技能集定义的重要性。",
                "论文提出将规划结果抽象为元动作，包含机器人内在功能，而非人类中心概念，从而提高技能集的泛化能力。",
                "通过检索增强生成（RAG）技术，利用规划演示数据库进行上下文学习，确保LLM/VLM准确生成元动作格式，并在OmniGibson平台上验证了有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决具身机器人AI系统中，任务规划器在处理复杂日常任务时，由于技能集泛化能力不足而导致规划效果不佳的问题。现有方法通常侧重于增强LLM/VLM的任务理解能力，而忽略了规划技能集本身的设计，导致规划结果难以直接转化为机器人可执行的动作。\\n\\n**核心思路**：论文的核心思路是将规划结果抽象为一组元动作，这些元动作基于机器人的内在功能（例如移动、旋转、末端执行器状态改变）来定义，而不是基于人类的动作概念（例如抓取、推动）。这种抽象提高了技能集的泛化能力，使得规划结果能够更好地适应不同的环境和任务。\\n\\n**技术框架**：MaP-AVR 包含两个主要部分：元动作定义和检索增强生成（RAG）。首先，定义了一组元动作，每个元动作包含三个组成部分：{移动/旋转，末端执行器状态改变，与环境的关系}。然后，利用 RAG 技术，构建一个包含人工标注的规划演示数据库，在 LLM/VLM 生成元动作序列时，从数据库中检索相关的演示案例，以进行上下文学习，确保生成的元动作符合预期的格式和语义。随着系统完成更多任务，数据库会自我增强，不断提升系统的规划能力。\\n\\n**关键创新**：该方法最重要的创新点在于提出了基于机器人内在功能的元动作抽象，以及将其与检索增强生成技术相结合。与现有方法相比，这种方法不再依赖于人类中心的概念来定义技能集，而是直接利用机器人的底层能力，从而提高了技能集的泛化能力和适应性。同时，RAG 技术的引入，使得 LLM/VLM 能够更好地理解和生成符合预期的元动作序列。\\n\\n**关键设计**：元动作的设计是关键。每个元动作都由三个部分组成，这三个部分共同描述了机器人的一个基本操作。RAG 模块的关键在于数据库的构建和检索策略。数据库包含人工标注的规划演示案例，每个案例都包含任务描述和对应的元动作序列。检索策略需要能够根据当前的任务描述，从数据库中找到最相关的演示案例，以提供有效的上下文信息。",
            "application_zh": "该研究成果可应用于各种需要机器人进行复杂任务规划的场景，例如家庭服务机器人、工业自动化机器人、医疗辅助机器人等。通过提高机器人的任务规划能力，可以使其更好地理解人类指令，完成各种日常任务，提高工作效率和生活质量。未来，该方法有望进一步扩展到更复杂的环境和任务中，实现更智能、更自主的机器人系统。",
            "highlight_zh": "论文使用GPT-4o作为LLM/VLM模型，并在OmniGibson平台上进行了实验验证。实验结果表明，MaP-AVR方法在任务规划方面表现出良好的性能，与当前最先进的方法相比具有竞争力。通过元动作抽象和RAG技术的结合，该方法能够生成更符合机器人能力的规划结果，并提高任务完成的成功率。具体性能数据和对比基线在论文中有详细描述。",
            "tags_zh": [
                "具身智能",
                "机器人任务规划",
                "视觉语言模型",
                "检索增强生成",
                "元动作",
                "机器人技能学习",
                "上下文学习"
            ],
            "_index": 74,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19453v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19453v1/meta-action-composed.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19453v1/task-extended-simple.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
            "authors": [
                "Jin Wang",
                "Kim Tien Ly",
                "Jacques Cloete",
                "Nikos Tsagarakis",
                "Ioannis Havoutis"
            ],
            "arxiv_id": "2512.19178v1",
            "summary": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/",
            "categories": [
                "cs.RO",
                "cs.AI"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Manuscript under review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19178v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱七：动作重定向 (Motion Retargeting)",
                    "id": "7_retargeting",
                    "matched_keywords": [
                        "cross-embodiment"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "7_retargeting"
            ],
            "headline_zh": "提出基于视觉-语言-策略模型的动态机器人任务规划方法，提升复杂环境下的自主执行能力。",
            "summary_zh": "本文提出了一种基于语言模型的动态机器人任务规划框架。该框架的核心是一个视觉-语言-策略（VLP）模型，它基于在真实世界数据上微调的视觉-语言模型，能够理解语义指令，并结合对当前任务场景的推理，生成控制机器人完成任务的行为策略。此外，该模型能够动态调整任务策略以响应任务变化，从而灵活适应不断变化的任务需求。在不同机器人和各种真实世界任务中进行的实验表明，该模型能够有效地适应新场景并动态更新其策略，展示了强大的规划自主性和跨具身泛化能力。",
            "intro_zh": [
                "传统机器人任务规划难以将底层执行与高层任务推理结合，且无法在执行过程中动态更新策略，限制了其通用性和适应性。",
                "论文提出VLP模型，通过视觉-语言模型理解指令和场景，生成行为策略控制机器人，并能根据任务变化动态调整策略。",
                "实验表明，该模型能有效适应新场景并动态更新策略，展现出强大的规划自主性和跨具身泛化能力。"
            ],
            "method_zh": "**问题定义**：论文旨在解决机器人如何在非结构化环境中，根据自然语言指令自主执行任务的问题。现有方法难以将高层任务理解与底层动作执行有效结合，并且在任务指令发生变化时，无法动态调整规划策略，导致机器人适应性和通用性不足。\\n\\n**核心思路**：论文的核心思路是利用视觉-语言模型理解自然语言指令和感知环境信息，并将其融合以生成机器人执行策略。通过在真实世界数据上进行微调，使模型能够更好地适应实际场景的复杂性和不确定性。此外，模型被设计成能够动态响应任务变化，从而实现灵活的任务规划。\\n\\n**技术框架**：该框架的核心是视觉-语言-策略（VLP）模型。整体流程如下：1) 机器人通过视觉传感器获取环境信息；2) VLP模型接收自然语言指令和环境信息作为输入；3) VLP模型对指令和场景进行理解和推理，生成相应的行为策略；4) 机器人根据生成的策略执行动作；5) 如果任务指令发生变化，VLP模型会动态调整策略，并指导机器人继续执行。\\n\\n**关键创新**：该论文的关键创新在于将视觉-语言模型与机器人任务规划相结合，并使其具备动态调整策略的能力。与传统方法相比，该方法能够更好地理解自然语言指令，并根据环境变化和任务需求进行灵活的任务规划。此外，该模型展现出较强的跨具身泛化能力，即在不同机器人平台上也能有效工作。\\n\\n**关键设计**：VLP模型基于预训练的视觉-语言模型，并在真实世界机器人任务数据上进行微调。具体的网络结构和损失函数细节未知，但可以推测使用了Transformer等结构来处理序列化的语言和视觉信息，并可能采用了强化学习或模仿学习等方法来训练策略生成模块。动态策略调整机制的具体实现方式未知，可能涉及到注意力机制或动态规划等技术。",
            "application_zh": "该研究成果可应用于各种需要机器人自主执行任务的场景，例如家庭服务、工业自动化、医疗辅助、灾难救援等。通过自然语言指令，用户可以轻松地指挥机器人完成复杂任务，无需专业的编程知识。该技术有望提升机器人的智能化水平，使其更好地服务于人类社会。",
            "highlight_zh": "实验结果表明，该VLP模型能够有效地适应新场景并动态更新其策略，展现了强大的规划自主性和跨具身泛化能力。虽然论文中没有给出具体的性能指标和对比基线，但强调了模型在不同机器人和各种真实世界任务中的有效性，证明了其在实际应用中的潜力。",
            "tags_zh": [
                "机器人任务规划",
                "视觉语言模型",
                "动态策略调整",
                "自主导航",
                "跨具身泛化"
            ],
            "_index": 75,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19178v1/images/figure2.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19178v1/images/figure3.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19178v1/images/figure4.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners",
            "authors": [
                "Yanding Yang",
                "Weitao Zhou",
                "Jinhai Wang",
                "Xiaomin Guo",
                "Junze Wen",
                "Xiaolong Liu",
                "Lang Ding",
                "Zheng Fu",
                "Jinyu Miao",
                "Kun Jiang",
                "Diange Yang"
            ],
            "arxiv_id": "2512.18988v1",
            "summary": "Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport.",
            "categories": [
                "cs.RO"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18988v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "imitation learning",
                        "contrastive learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出DTCCL框架，通过脱离事件触发的对比持续学习提升自动驾驶巴士规划策略。",
            "summary_zh": "自动驾驶巴士在固定路线上运行，但必须在开放、动态的城市环境中运行。这些路线上的脱离事件通常在地理位置上集中，并且通常由高度交互区域中的规划器故障引起。使用传统的模仿学习很难纠正此类策略级别的故障，因为模仿学习很容易过度拟合稀疏的脱离数据。为了解决这个问题，本文提出了一种脱离触发的对比持续学习（DTCCL）框架，使自动驾驶巴士能够通过实际操作来改进规划策略。每次脱离都会触发基于云的数据增强，通过扰动周围的智能体同时保留路线上下文来生成正样本和负样本。对比学习改进策略表示，以更好地区分安全和不安全的行为，并且在没有人工监督的情况下，在云边循环中应用持续更新。在城市公交线路上的实验表明，与直接重新训练相比，DTCCL将整体规划性能提高了48.6％，验证了其在自动公共交通中可扩展的闭环策略改进的有效性。",
            "intro_zh": [
                "现有模仿学习方法难以有效利用自动驾驶脱离数据，易于过拟合，导致策略改进受限。",
                "DTCCL框架通过脱离事件触发数据增强，利用对比学习区分安全与不安全行为，实现持续策略优化。",
                "实验表明，DTCCL相较于直接重训练，在城市公交路线上整体规划性能提升了48.6%。"
            ],
            "method_zh": "**问题定义**：自动驾驶巴士在实际运营中，由于环境的复杂性和动态性，经常出现需要人工干预的“脱离”事件。这些事件往往集中在特定的地理区域，并且是由于规划器在高度交互场景中失效导致的。传统的模仿学习方法难以有效利用这些稀疏的脱离数据，容易发生过拟合，导致策略改进效果不佳。因此，如何利用这些脱离事件来持续改进自动驾驶巴士的规划策略是一个关键问题。\\n\\n**核心思路**：DTCCL的核心思路是利用脱离事件作为触发信号，通过数据增强生成对比学习所需的正负样本，并结合持续学习框架，在云端进行策略优化，然后将优化后的策略部署到边缘设备（自动驾驶巴士）上。通过这种云边协同的方式，实现自动驾驶巴士规划策略的持续改进。对比学习用于学习更好的策略表示，区分安全和不安全的行为。\\n\\n**技术框架**：DTCCL框架主要包含以下几个模块：1) **脱离事件检测**：实时监测自动驾驶巴士的运行状态，当发生脱离事件时，触发数据增强流程。2) **云端数据增强**：基于脱离事件发生时的环境数据，通过扰动周围的智能体，生成正负样本。正样本模拟安全行为，负样本模拟不安全行为。3) **对比学习**：利用生成的数据，训练策略网络，使其能够更好地区分安全和不安全的行为。4) **持续学习**：将优化后的策略部署到自动驾驶巴士上，并在后续的运行过程中不断收集新的数据，重复上述流程，实现策略的持续改进。5) **云边协同**：云端负责数据增强和策略优化，边缘设备负责策略部署和数据收集。\\n\\n**关键创新**：DTCCL的关键创新在于：1) **脱离触发的数据增强**：利用脱离事件作为触发信号，自动生成对比学习所需的正负样本，避免了人工标注的成本。2) **对比学习与持续学习的结合**：通过对比学习提升策略表示能力，并通过持续学习实现策略的持续改进。3) **云边协同的架构**：将计算密集型的数据增强和策略优化放在云端进行，减轻了边缘设备的计算负担。\\n\\n**关键设计**：数据增强策略是关键设计之一，通过对周围车辆进行轨迹扰动，生成正负样本。正样本的扰动幅度较小，保证车辆的安全行驶；负样本的扰动幅度较大，模拟可能导致事故的危险行为。对比学习损失函数的设计也至关重要，目标是拉近正样本之间的距离，推远负样本之间的距离。网络结构方面，可以使用Transformer等模型来捕捉环境中的时序依赖关系。",
            "application_zh": "DTCCL框架可应用于自动驾驶公交、物流车等固定线路的自动驾驶车辆，通过持续学习提升其在复杂城市环境中的规划能力。该研究有助于降低自动驾驶系统的部署和维护成本，提高其安全性和可靠性，加速自动驾驶技术的商业化落地。未来，该方法可以扩展到其他类型的自动驾驶车辆和更复杂的驾驶场景。",
            "highlight_zh": "实验结果表明，DTCCL框架在城市公交路线上取得了显著的性能提升。与直接重训练相比，DTCCL将整体规划性能提高了48.6%。这表明DTCCL能够有效利用脱离事件数据，提升自动驾驶巴士在复杂城市环境中的规划能力。此外，实验还验证了DTCCL框架的持续学习能力，即随着运行时间的增加，规划性能能够持续提升。",
            "tags_zh": [
                "自动驾驶",
                "持续学习",
                "对比学习",
                "数据增强",
                "脱离事件",
                "规划策略",
                "云边协同"
            ],
            "_index": 76,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18988v1/images/idea.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18988v1/images/framework2.jpg",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18988v1/images/platform.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
            "authors": [
                "Zixuan Ye",
                "Quande Liu",
                "Cong Wei",
                "Yuanxing Zhang",
                "Xintao Wang",
                "Pengfei Wan",
                "Kun Gai",
                "Wenhan Luo"
            ],
            "arxiv_id": "2512.19686v1",
            "summary": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",
            "categories": [
                "cs.CV"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Project Page: https://zixuan-ye.github.io/VACoT/",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19686v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "chain-of-thought"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Visual-Aware CoT，提升统一模型在多模态生成中视觉一致性",
            "summary_zh": "本文提出了一种名为Visual-Aware Chain-of-Thought (CoT) 的方法，旨在提高统一模型在多模态生成任务中的视觉一致性。现有方法在生成过程中主要关注文本与文本提示的一致性，忽略了与视觉参考图像的视觉上下文一致性，导致无法保持关键视觉特征（如人物ID、物体属性、风格）。为了解决这个问题，本文将视觉上下文一致性融入到统一模型的推理过程中，通过自适应视觉规划（生成结构化的视觉检查列表，确定需要保持一致性的视觉元素）和迭代视觉校正（在检查列表的指导下进行自我反思并迭代优化生成结果）来显式地促使模型保持视觉一致性。通过监督微调来训练模型进行视觉检查规划、自我反思和自我优化，并使用flow-GRPO通过定制的视觉检查奖励来进一步增强视觉一致性。实验结果表明，本文方法在多模态生成任务中优于零样本统一模型和基于文本CoT的模型，证明了其更高的视觉上下文一致性。",
            "intro_zh": [
                "现有统一模型在多模态生成中，忽略了与参考图像的视觉上下文一致性，导致关键视觉特征丢失。",
                "提出Visual-Aware CoT，通过自适应视觉规划和迭代视觉校正，显式地引导模型保持视觉一致性。",
                "实验表明，该方法在多模态生成任务中，显著优于现有零样本和文本CoT模型，提升视觉一致性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决多模态生成任务中，统一模型生成的图像与参考图像在视觉上下文上不一致的问题。现有方法主要关注文本提示的一致性，忽略了视觉参考图像的关键视觉特征，例如人物身份、物体属性和风格等，导致生成结果质量下降。\\n\\n**核心思路**：论文的核心思路是将视觉上下文一致性显式地融入到统一模型的推理过程中。通过让模型学习如何识别和保持关键的视觉元素，从而提高生成结果的视觉保真度。这种方法模拟了人类在进行图像生成时的思考过程，即先确定需要保持的视觉特征，然后在生成过程中不断检查和修正。\\n\\n**技术框架**：整体框架包含两个主要模块：自适应视觉规划和迭代视觉校正。首先，自适应视觉规划模块生成一个结构化的视觉检查列表，用于确定需要保持一致性的视觉元素。然后，迭代视觉校正模块在检查列表的指导下，进行自我反思，并迭代地优化生成结果。整个过程通过监督微调进行训练，并使用flow-GRPO进一步增强视觉一致性。\\n\\n**关键创新**：论文的关键创新在于将视觉上下文一致性显式地建模到统一模型的推理过程中。通过自适应视觉规划和迭代视觉校正，模型能够更好地理解和保持视觉特征，从而生成更高质量的图像。与现有方法相比，该方法更加关注视觉信息，并能够有效地解决视觉不一致的问题。\\n\\n**关键设计**：在自适应视觉规划模块中，使用监督学习来训练模型生成视觉检查列表。在迭代视觉校正模块中，使用flow-GRPO来进一步增强视觉一致性，通过定制的视觉检查奖励来引导模型生成更符合视觉上下文的图像。具体的网络结构和参数设置在论文中有详细描述，但具体数值未知。",
            "application_zh": "该研究成果可应用于图像编辑、图像生成、视频生成等领域，例如，可以用于生成具有特定风格或属性的图像，或者用于修复图像中的视觉不一致性。该方法在虚拟现实、游戏开发、广告设计等领域具有广泛的应用前景，能够提升用户体验和创作效率。",
            "highlight_zh": "实验结果表明，该方法在多模态生成任务中显著优于零样本统一模型和基于文本CoT的模型，证明了其更高的视觉上下文一致性。具体的性能数据和提升幅度在论文中有详细描述，但具体数值未知。实验结果表明，该方法能够有效地保持关键视觉特征，生成更高质量的图像。",
            "tags_zh": [
                "多模态生成",
                "视觉一致性",
                "统一模型",
                "链式思考",
                "自适应视觉规划",
                "迭代视觉校正",
                "flow-GRPO"
            ],
            "_index": 77,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19686v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19686v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19686v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
            "authors": [
                "Moritz Böhle",
                "Amélie Royer",
                "Juliette Marrie",
                "Edouard Grave",
                "Patrick Pérez"
            ],
            "arxiv_id": "2512.19535v1",
            "summary": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
            "categories": [
                "cs.CV",
                "cs.AI"
            ],
            "primary_category": "cs.CV",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19535v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "multimodal"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CASA：通过自注意力实现的跨注意力，用于高效的视觉-语言融合",
            "summary_zh": "视觉-语言模型（VLMs）通常通过将预训练视觉编码器的图像tokens插入到语言模型的文本流中进行训练。这使得文本和图像信息能够在模型内部充分地相互关注，但对于高分辨率图像、长对话或流媒体视频来说，无论在内存还是计算方面，成本都非常高昂。利用跨注意力的VLMs是token插入的一种有效替代方案，但在性能上存在明显的差距，尤其是在涉及细粒度视觉细节的任务中。我们发现，改进此类模型的关键在于在专用的跨注意力层中启用局部文本到文本的交互。在此基础上，我们提出了CASA，即通过自注意力实现的跨注意力，这是一种简单而有效的范例，它大大缩小了在常见图像理解基准测试中与完全token插入的差距，同时在应用于长上下文多模态任务（如流媒体视频字幕）时，具有与跨注意力模型相同的可扩展性。",
            "intro_zh": [
                "现有视觉-语言模型在高分辨率图像等场景下，直接插入图像tokens导致计算和内存成本过高。",
                "CASA通过自注意力实现跨注意力，在跨注意力层中加入局部文本交互，提升模型性能。",
                "CASA在图像理解任务上缩小了与token插入的差距，并在长上下文多模态任务中保持了可扩展性。"
            ],
            "method_zh": "**问题定义**：现有视觉-语言模型（VLMs）在处理高分辨率图像、长对话或流媒体视频等长上下文多模态任务时，直接将图像tokens插入到语言模型的文本流中，导致计算和内存成本显著增加。虽然基于跨注意力的VLMs可以降低计算成本，但其性能，尤其是在需要细粒度视觉信息的任务上，与token插入方法相比存在明显差距。\\n\\n**核心思路**：CASA的核心思路是通过自注意力机制来增强跨注意力模块，从而在降低计算成本的同时，提升模型对细粒度视觉信息的理解能力。具体来说，CASA在跨注意力层中引入局部文本到文本的交互，使得文本信息在与图像信息交互之前，能够更好地理解上下文信息，从而提高跨注意力机制的效率和准确性。\\n\\n**技术框架**：CASA的整体框架是在现有的跨注意力VLM架构基础上，修改了跨注意力模块。该模块接收文本和图像特征作为输入，首先使用自注意力机制对文本特征进行局部上下文建模，然后将处理后的文本特征与图像特征进行跨注意力交互，最后输出融合后的特征表示。整个过程可以看作是先通过自注意力增强文本表示，再利用增强后的文本表示进行跨注意力。\\n\\n**关键创新**：CASA的关键创新在于利用自注意力机制来增强跨注意力模块，从而在不显著增加计算成本的前提下，提升了模型对细粒度视觉信息的理解能力。与传统的跨注意力模型相比，CASA通过引入局部文本到文本的交互，使得文本信息在与图像信息交互之前，能够更好地理解上下文信息，从而提高了跨注意力机制的效率和准确性。\\n\\n**关键设计**：CASA的关键设计包括：1) 使用标准的自注意力模块进行局部文本上下文建模；2) 将自注意力模块集成到现有的跨注意力模块中，保持整体架构的简洁性；3) 通过实验验证了不同自注意力参数设置（如注意力头数、隐藏层维度）对模型性能的影响，并选择了最优的参数配置。",
            "application_zh": "CASA具有广泛的应用前景，包括但不限于：流媒体视频字幕、视频问答、图像描述、视觉对话等。其高效的计算特性使其特别适用于资源受限的场景，如移动设备或边缘计算平台。此外，CASA还可以应用于长上下文多模态任务，如长时间视频分析和理解，为智能监控、自动驾驶等领域提供技术支持。",
            "highlight_zh": "CASA在常见的图像理解基准测试中，显著缩小了与完全token插入方法的性能差距，同时保持了与跨注意力模型相当的计算效率。实验结果表明，CASA在处理细粒度视觉信息方面具有优势，能够有效提升视觉-语言模型的性能。具体性能数据和对比基线信息在论文中详细给出。",
            "tags_zh": [
                "视觉-语言模型",
                "跨注意力",
                "自注意力",
                "多模态融合",
                "长上下文",
                "图像理解",
                "视频字幕"
            ],
            "_index": 78,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19535v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19535v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19535v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Brain-Grounded Axes for Reading and Steering LLM States",
            "authors": [
                "Sandro Andric"
            ],
            "arxiv_id": "2512.19399v1",
            "summary": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19399v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于人脑活动的LLM状态解读与操控方法，实现神经生理学层面可控性。",
            "summary_zh": "本文提出了一种新的LLM可解释性方法，该方法不依赖于文本监督，而是利用人脑活动作为坐标系来读取和操控LLM的状态。具体而言，研究者使用SMN4Lang MEG数据集构建了一个词级别的脑活动图谱（基于相位锁定值PLV），并通过ICA提取潜在轴。通过独立的词汇表和NER标签验证这些轴（使用POS/log-frequency作为健全性检查），然后训练轻量级适配器，将LLM的隐藏状态映射到这些脑轴，而无需微调LLM。沿着这些脑源方向进行操控，在TinyLlama的中间层产生了一个鲁棒的词汇（频率相关）轴。脑轴与文本探针的对比显示，脑轴具有更大的log-frequency偏移和更低的困惑度。一个功能/内容轴（轴13）在TinyLlama、Qwen2-0.5B和GPT-2中表现出一致的操控效果。探索性的fMRI锚定表明嵌入变化和log frequency可能存在潜在的对齐，但效果对血流动力学建模假设敏感。这些结果支持一种新的接口：神经生理学基础的轴为LLM行为提供了可解释和可控的句柄。",
            "intro_zh": [
                "现有LLM可解释性方法依赖文本监督，缺乏外部依据，难以保证结果的可靠性和泛化性。",
                "利用人脑活动作为LLM状态的坐标系，通过MEG数据构建脑活动图谱，提取潜在轴，实现神经生理学层面的解读与操控。",
                "实验表明，该方法在多个LLM中发现了鲁棒的词汇轴和功能/内容轴，并验证了其有效性和一致性。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型（LLM）可解释性方法主要依赖于文本监督信号，例如词汇表、NER标签等。这种方法的局限性在于，文本本身可能存在偏差或噪声，导致解释结果缺乏外部的客观依据。此外，基于文本的解释难以直接与人类认知过程建立联系，限制了对LLM内部机制的深入理解。因此，需要一种更具外部依据且与人类认知相关的LLM可解释性方法。\\n\\n**核心思路**：本文的核心思路是将人脑活动作为LLM状态的坐标系，通过分析LLM隐藏层状态与人脑活动之间的关系，来理解和操控LLM的行为。具体而言，作者利用脑磁图（MEG）数据构建词级别的脑活动图谱，并提取潜在的脑活动轴。然后，将LLM的隐藏状态映射到这些脑轴上，从而实现对LLM行为的神经生理学层面的解读和操控。这种方法的优势在于，它将LLM的内部状态与人类的认知过程联系起来，提供了一种更具客观性和可解释性的视角。\\n\\n**技术框架**：该方法主要包含以下几个阶段：\n1. **构建脑活动图谱**：使用SMN4Lang MEG数据集，计算词级别的相位锁定值（PLV），构建脑活动图谱。\n2. **提取脑活动轴**：使用独立成分分析（ICA）从脑活动图谱中提取潜在的脑活动轴。\n3. **验证脑活动轴**：使用独立的词汇表和NER标签验证脑活动轴的有效性，并使用POS/log-frequency作为健全性检查。\n4. **训练适配器**：训练轻量级适配器，将LLM的隐藏状态映射到脑活动轴，无需微调LLM。\n5. **操控LLM状态**：沿着脑活动轴的方向操控LLM的状态，观察LLM行为的变化。\n6. **fMRI锚定（探索性）**：使用fMRI数据探索嵌入变化和log frequency与脑活动之间的潜在对齐关系。\\n\\n**关键创新**：该方法最重要的技术创新点在于，它将人脑活动作为LLM状态的坐标系，提供了一种全新的LLM可解释性视角。与传统的基于文本监督的方法相比，该方法具有以下优势：\n1. **外部依据**：利用人脑活动作为外部依据，避免了文本偏差或噪声的影响。\n2. **认知相关**：将LLM的内部状态与人类的认知过程联系起来，有助于深入理解LLM的内部机制。\n3. **可操控性**：通过操控脑活动轴，可以实现对LLM行为的神经生理学层面的操控。\\n\\n**关键设计**：\n1. **MEG数据处理**：使用SMN4Lang MEG数据集，计算词级别的相位锁定值（PLV），以表征脑活动。\n2. **ICA参数设置**：使用独立成分分析（ICA）从脑活动图谱中提取潜在的脑活动轴，需要选择合适的ICA算法和参数。\n3. **适配器网络结构**：训练轻量级适配器，将LLM的隐藏状态映射到脑活动轴，需要设计合适的网络结构和损失函数。\n4. **操控策略**：沿着脑活动轴的方向操控LLM的状态，需要选择合适的操控策略和幅度。",
            "application_zh": "该研究成果可应用于提升LLM的可解释性和可控性，例如，通过操控LLM的脑活动轴，可以控制LLM生成特定情感或风格的文本。此外，该方法还可以用于诊断LLM的潜在问题，例如，通过分析LLM的脑活动轴，可以发现LLM是否存在认知偏差或逻辑错误。未来，该研究有望促进人机协作，实现更自然、更智能的人机交互。",
            "highlight_zh": "实验结果表明，该方法在TinyLlama、Qwen2-0.5B和GPT-2等多个LLM中发现了鲁棒的词汇轴和功能/内容轴。脑轴与文本探针的对比显示，脑轴具有更大的log-frequency偏移和更低的困惑度。重建脑活动图谱时，即使移除GPT嵌入变化特征或使用word2vec嵌入，轴结构依然稳定（|r|=0.64-0.95）。",
            "tags_zh": [
                "大语言模型可解释性",
                "脑机接口",
                "神经生理学",
                "脑磁图",
                "独立成分分析",
                "模型操控",
                "认知神经科学"
            ],
            "_index": 79,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19399v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19399v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19399v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Cluster-Based Generalized Additive Models Informed by Random Fourier Features",
            "authors": [
                "Xin Huang",
                "Jia Li",
                "Jun Yu"
            ],
            "arxiv_id": "2512.19373v1",
            "summary": "Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.",
            "categories": [
                "stat.ML",
                "cs.LG"
            ],
            "primary_category": "stat.ML",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "25 pages, 13 figures, 4 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19373v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "predictive model",
                        "representation learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于随机傅里叶特征的聚类广义加性模型，提升可解释回归任务的预测性能。",
            "summary_zh": "本文提出了一种混合广义加性模型（GAMs），该模型利用随机傅里叶特征（RFF）表示来揭示数据中局部自适应结构。该方法首先学习基于RFF的嵌入，然后通过主成分分析进行压缩。得到的低维表示用于通过高斯混合模型对数据进行软聚类。这些聚类分配随后被应用于构建混合GAMs框架，其中每个局部GAM通过可解释的单变量平滑函数捕获非线性效应。在包括加州住房、NASA翼型自噪声和自行车共享数据集在内的真实回归基准上的数值实验表明，相对于经典的可解释模型，预测性能有所提高。总而言之，这种构建为将表示学习与透明统计建模相结合提供了一种原则性方法。",
            "intro_zh": [
                "深度神经网络等黑盒模型预测能力强但缺乏可解释性，如何在预测精度和模型透明度之间取得平衡是一个挑战。",
                "论文提出一种混合广义加性模型，利用随机傅里叶特征学习数据局部结构，并通过软聚类构建局部GAMs。",
                "实验表明，在真实回归数据集上，该方法相对于经典可解释模型，预测性能有所提升，实现了表示学习与透明统计建模的结合。"
            ],
            "method_zh": "**问题定义**：现有黑盒模型，如深度神经网络，虽然预测精度高，但缺乏可解释性。传统可解释模型，如线性回归，表达能力有限，难以捕捉复杂非线性关系。因此，如何在保证预测精度的同时，提高模型的可解释性是一个关键问题。\\n\\n**核心思路**：论文的核心思路是将数据进行聚类，然后在每个簇内使用广义加性模型（GAM）进行建模。通过聚类，可以将复杂的数据分布分解为多个局部简单的分布，从而可以使用GAM在每个局部进行更精确的建模。同时，GAM本身具有良好的可解释性，可以清晰地展示每个特征对预测结果的影响。\\n\\n**技术框架**：该方法主要包含以下几个阶段：1. **RFF嵌入学习**：使用随机傅里叶特征（RFF）将原始数据映射到高维空间，以便更好地捕捉非线性关系。2. **降维**：通过主成分分析（PCA）对RFF嵌入进行降维，减少计算复杂度。3. **软聚类**：使用高斯混合模型（GMM）对降维后的数据进行软聚类，得到每个数据点属于每个簇的概率。4. **混合GAM建模**：在每个簇内，使用GAM进行建模，其中每个GAM使用可解释的单变量平滑函数来捕捉非线性效应。最终的预测结果是所有GAM的加权平均，权重为数据点属于每个簇的概率。\\n\\n**关键创新**：该方法最重要的创新点在于将表示学习（RFF嵌入）与可解释的统计建模（GAM）相结合。通过RFF嵌入，可以有效地捕捉非线性关系，而GAM则保证了模型的可解释性。此外，使用软聚类可以更好地处理数据分布的复杂性，避免硬聚类可能导致的误差。\\n\\n**关键设计**：RFF的参数（如傅里叶基的数量）需要根据数据集的复杂程度进行调整。GMM的簇的数量也需要根据数据的分布进行选择。GAM中使用的平滑函数的类型（如样条函数）和参数也需要进行调整，以获得最佳的预测性能。损失函数通常是均方误差或对数似然函数，具体取决于回归任务的类型。",
            "application_zh": "该研究成果可应用于需要高预测精度和可解释性的回归任务，例如金融风险评估、医疗诊断、环境监测等领域。通过该方法，可以更好地理解模型预测的原因，从而提高决策的可靠性和透明度。未来，该方法可以扩展到分类任务和其他类型的可解释模型。",
            "highlight_zh": "在加州住房、NASA翼型自噪声和自行车共享数据集上的实验结果表明，该方法相对于经典的可解释模型，如线性回归和GAM，预测性能有所提高。具体提升幅度取决于数据集的复杂程度和参数的调整情况。实验结果验证了该方法在可解释回归任务中的有效性。",
            "tags_zh": [
                "可解释机器学习",
                "广义加性模型",
                "随机傅里叶特征",
                "聚类分析",
                "回归分析"
            ],
            "_index": 80,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19373v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19373v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19373v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning General Policies with Policy Gradient Methods",
            "authors": [
                "Simon Ståhlberg",
                "Blai Bonet",
                "Hector Geffner"
            ],
            "arxiv_id": "2512.19366v1",
            "summary": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19366v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "DRL"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于图神经网络的策略梯度方法，学习可泛化的通用策略",
            "summary_zh": "强化学习方法在许多领域取得了显著成果，但泛化能力，即以可靠和系统的方式生成可泛化策略的能力，仍然是一个挑战。泛化问题在经典规划中得到了正式解决，其中使用组合方法学习了可在给定领域的所有实例中泛化的可证明正确的策略。本研究旨在将这两个研究方向结合起来，阐明在何种条件下，（深度）强化学习方法，特别是策略优化方法，可以像组合方法一样用于学习可泛化的策略。我们借鉴了先前组合和深度学习方法的经验，并以一种方便的方式对其进行了扩展。与前者类似，我们将策略建模为状态转移分类器，因为（ground）动作不是通用的，并且因实例而异。与后者类似，我们使用适用于处理关系结构的图神经网络（GNN）来表示规划状态上的价值函数，在我们的例子中，是策略。有了这些要素，我们发现actor-critic方法可以用于学习几乎与使用组合方法获得的策略一样好的策略，同时避免了可扩展性瓶颈和特征池的使用。此外，DRL方法在所考虑的基准测试中的局限性与深度学习或强化学习算法无关，而是源于GNN的表达能力限制，以及最优性和泛化之间的权衡（通用策略在某些领域可能不是最优的）。通过添加派生谓词和优化替代成本结构，可以在不改变基本DRL方法的情况下解决这两个限制。",
            "intro_zh": [
                "现有强化学习方法在泛化能力上存在挑战，难以生成可靠且系统性的通用策略。",
                "论文提出将策略建模为状态转移分类器，并利用图神经网络处理关系结构，学习通用策略。",
                "实验表明，该方法在泛化能力上接近组合方法，同时避免了可扩展性瓶颈，并通过优化成本结构解决了GNN的局限性。"
            ],
            "method_zh": "**问题定义**：现有强化学习方法在规划问题中难以泛化到不同的实例。传统的强化学习方法通常学习针对特定实例的策略，而无法推广到具有不同初始状态或目标状态的相同领域的新实例。此外，组合规划方法虽然可以生成可证明正确的通用策略，但在可扩展性方面存在瓶颈。\\n\\n**核心思路**：论文的核心思路是将策略表示为状态转移分类器，而不是直接学习动作。这种表示方式允许策略在不同的实例之间共享知识，因为状态转移规则在同一领域内是通用的。此外，论文利用图神经网络（GNN）来处理规划状态的关系结构，从而更好地捕捉状态之间的依赖关系。\\n\\n**技术框架**：该方法采用Actor-Critic框架，其中Actor使用GNN来表示策略，Critic使用GNN来评估策略的价值。Actor的目标是学习一个策略，该策略可以预测给定状态下采取哪个动作会导致期望的状态转移。Critic的目标是学习一个价值函数，该函数可以预测给定状态下遵循策略的预期回报。训练过程使用策略梯度方法，通过最大化预期回报来更新Actor和Critic的参数。\\n\\n**关键创新**：该方法的关键创新在于将策略建模为状态转移分类器，并使用GNN来处理规划状态的关系结构。这种组合使得该方法能够学习可泛化的通用策略，同时避免了传统强化学习方法的可扩展性问题。此外，论文还提出了一种优化成本结构的方法，以解决GNN的表达能力限制。\\n\\n**关键设计**：论文使用消息传递神经网络（MPNN）作为GNN的骨干网络。状态表示为图中的节点，节点之间的边表示状态之间的关系。消息传递过程用于聚合来自相邻节点的信息，从而更新节点的状态表示。策略网络使用更新后的节点表示来预测状态转移概率。损失函数包括策略梯度损失和价值函数损失。为了解决GNN的表达能力限制，论文添加了派生谓词，这些谓词是基于原始状态变量计算得到的。此外，论文还使用了一种替代成本结构，该结构鼓励策略学习更有效的状态转移。",
            "application_zh": "该研究成果可应用于各种规划和决策问题，例如机器人导航、游戏AI和资源调度。通过学习可泛化的通用策略，可以使智能体在面对新的环境和任务时更加灵活和高效。此外，该方法还可以用于解决传统强化学习方法难以处理的复杂规划问题。",
            "highlight_zh": "实验结果表明，该方法在多个规划基准测试中取得了与组合方法相当的泛化性能，同时避免了组合方法的可扩展性瓶颈。例如，在某些领域，该方法可以学习在所有实例中都表现良好的策略，而传统强化学习方法只能学习针对特定实例的策略。此外，通过添加派生谓词和优化成本结构，该方法可以解决GNN的表达能力限制，进一步提高泛化性能。",
            "tags_zh": [
                "强化学习",
                "策略梯度",
                "图神经网络",
                "泛化",
                "规划"
            ],
            "_index": 81,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation",
            "authors": [
                "Isshaan Singh",
                "Divyansh Chawla",
                "Anshu Garg",
                "Shivin Mangal",
                "Pallavi Gupta",
                "Khushi Agarwal",
                "Nimrat Singh Khalsa",
                "Nandan Patel"
            ],
            "arxiv_id": "2512.19361v1",
            "summary": "The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.",
            "categories": [
                "cs.LG"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19361v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "deep reinforcement learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出一种可解释的混合深度Q学习框架，用于物联网食品腐败预测，并进行硬件验证。",
            "summary_zh": "本文提出了一种混合强化学习框架，集成长短期记忆网络（LSTM）和循环神经网络（RNN），用于增强食品腐败预测，以应对物联网驱动的食品供应链中易腐货物对环境条件高度敏感的问题。该混合架构捕获传感器数据中的时间依赖性，从而实现鲁棒和自适应的决策。采用基于规则的分类器环境，根据领域特定的阈值，为腐败程度提供透明的ground truth标签，符合可解释人工智能原则。通过腐败准确率、奖励步长比、损失减少率和探索衰减等可解释性驱动的指标来监控模型行为。使用类别腐败分布可视化来分析智能体的决策概况和策略行为。在模拟和实时硬件数据上的大量评估表明，基于LSTM和RNN的智能体在预测准确性和决策效率方面优于其他强化学习方法，同时保持了可解释性。结果突出了具有集成可解释性的混合深度强化学习在可扩展的基于物联网的食品监控系统中的潜力。",
            "intro_zh": [
                "现有食品腐败预测方法缺乏对动态环境的适应性，难以实时优化决策，无法满足现代物联网食品供应链的需求。",
                "提出一种混合强化学习框架，结合LSTM和RNN，利用传感器数据中的时间依赖性，实现更鲁棒和自适应的决策。",
                "在模拟和实时硬件数据上的实验表明，该方法在预测准确性和决策效率方面优于其他强化学习方法，并保持了可解释性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决物联网驱动的食品供应链中，易腐食品的实时腐败预测问题。现有方法难以适应动态环境，无法有效利用传感器数据中的时间依赖性，导致预测精度不足，且缺乏可解释性。\\n\\n**核心思路**：论文的核心思路是结合LSTM和RNN的混合深度强化学习框架，利用LSTM和RNN捕获传感器数据中的时间依赖性，从而提高预测精度。同时，通过规则分类器环境和可解释性指标，保证模型决策的透明性和可追溯性。\\n\\n**技术框架**：整体框架包含以下几个主要模块：1) 数据采集：通过物联网传感器收集环境数据（如温度、湿度等）；2) 状态表示：将传感器数据转化为强化学习智能体的状态；3) 动作空间：定义智能体可以采取的动作，例如调整环境控制参数；4) 奖励函数：根据预测的腐败程度和采取的动作，给予智能体奖励或惩罚；5) 混合深度Q学习智能体：使用LSTM和RNN构建深度Q网络，学习最优策略；6) 可解释性分析：使用腐败准确率、奖励步长比等指标监控模型行为，并进行类别腐败分布可视化。\\n\\n**关键创新**：该方法的主要创新点在于：1) 提出了一种混合LSTM和RNN的深度Q学习框架，能够有效捕获时间依赖性；2) 引入了基于规则的分类器环境，为腐败程度提供透明的ground truth标签，增强了模型的可解释性；3) 使用可解释性驱动的指标监控模型行为，并进行可视化分析。\\n\\n**关键设计**：规则分类器环境基于领域专家知识设定腐败阈值，为强化学习提供明确的语义边界。LSTM和RNN的具体网络结构（层数、神经元数量等）未知，奖励函数的设计需要平衡预测精度和控制成本。探索衰减策略用于平衡探索和利用，以提高学习效率。",
            "application_zh": "该研究成果可应用于智能食品供应链管理、冷链物流优化、食品安全监控等领域。通过实时预测食品腐败情况，可以减少食物浪费，降低经济损失，并提高食品安全水平。未来，该技术有望扩展到其他易腐商品的监控和管理。",
            "highlight_zh": "实验结果表明，基于LSTM和RNN的混合深度Q学习智能体在预测准确性和决策效率方面优于其他强化学习方法。具体性能数据未知，但论文强调了该方法在保持可解释性的同时，实现了性能提升。在模拟和实时硬件数据上的验证进一步证实了该方法的有效性。",
            "tags_zh": [
                "深度强化学习",
                "物联网",
                "食品腐败预测",
                "LSTM",
                "RNN",
                "可解释人工智能",
                "混合模型"
            ],
            "_index": 82,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19361v1/fig1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19361v1/fig2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19361v1/fig3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
            "authors": [
                "JiaWei Zhu",
                "ZiHeng Liu"
            ],
            "arxiv_id": "2512.19349v1",
            "summary": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
            "categories": [
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "7 pages,1 figure,4 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19349v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "VIGOR+：提出基于LLM-CEVAE反馈环路的迭代混淆因子生成与验证框架，解决因果推断中的隐藏混淆问题。",
            "summary_zh": "隐藏混淆是观测数据因果推断中的一个根本挑战。最近的研究利用大型语言模型（LLM）基于领域知识生成合理的隐藏混淆因子，但存在一个关键差距：LLM生成的混淆因子通常在语义上是合理的，但在统计上没有效用。我们提出了VIGOR+（用于迭代混淆因子改进的变分信息增益），这是一个新颖的框架，它闭合了基于LLM的混淆因子生成和基于CEVAE的统计验证之间的循环。与将生成和验证视为独立阶段的先前方法不同，VIGOR+建立了一个迭代反馈机制：来自CEVAE的验证信号（包括信息增益、潜在一致性指标和诊断消息）被转换为自然语言反馈，以指导后续的LLM生成轮次。这种迭代改进持续进行，直到满足收敛标准。我们形式化了反馈机制，在温和的假设下证明了收敛性质，并提供了一个完整的算法框架。",
            "intro_zh": [
                "现有方法在因果推断中，利用LLM生成混淆因子，但缺乏统计效用验证，导致生成结果不实用。",
                "VIGOR+通过LLM生成和CEVAE验证之间的迭代反馈环路，不断优化混淆因子，提升统计有效性。",
                "该框架形式化了反馈机制，证明了收敛性，并提供了完整的算法，实验验证了其有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决观测数据因果推断中隐藏混淆因子带来的偏差问题。现有方法利用LLM生成混淆因子，但缺乏有效的统计验证，导致生成的混淆因子虽然语义上合理，但对因果推断的帮助不大，甚至可能引入新的偏差。现有方法将混淆因子的生成和验证视为独立的步骤，缺乏迭代优化。\n\n**核心思路**：VIGOR+的核心思路是建立一个LLM生成和CEVAE验证之间的迭代反馈环路。CEVAE负责对LLM生成的混淆因子进行统计验证，并将验证结果（如信息增益、潜在一致性指标等）转化为自然语言反馈，反馈给LLM，指导LLM生成更有效的混淆因子。通过这种迭代的方式，不断优化混淆因子，提高因果推断的准确性。这样设计的目的是为了弥补LLM生成结果缺乏统计效用的缺陷，同时利用CEVAE的验证能力来指导LLM的生成过程。\n\n**技术框架**：VIGOR+的整体框架包含以下几个主要模块：1) LLM混淆因子生成模块：利用LLM基于领域知识生成候选的隐藏混淆因子。2) CEVAE验证模块：利用CEVAE对生成的混淆因子进行统计验证，评估其对因果推断的影响。3) 反馈生成模块：将CEVAE的验证结果转化为自然语言反馈，反馈给LLM。4) 迭代优化模块：根据反馈信息，LLM调整生成策略，生成新的混淆因子，重复上述过程，直到满足收敛条件。\n\n**关键创新**：VIGOR+最关键的创新在于建立了LLM和CEVAE之间的迭代反馈环路。与现有方法将生成和验证分离不同，VIGOR+将两者紧密结合，利用CEVAE的验证结果指导LLM的生成过程，从而生成更有效的混淆因子。这种迭代优化的方式能够显著提高因果推断的准确性。此外，将CEVAE的统计指标转化为自然语言反馈也是一个创新点，使得LLM能够更好地理解验证结果，并进行相应的调整。\n\n**关键设计**：VIGOR+的关键设计包括：1) CEVAE模型的选择和训练：选择合适的CEVAE模型，并使用观测数据进行训练，使其能够准确评估混淆因子的影响。2) 反馈信息的生成策略：设计有效的反馈信息生成策略，将CEVAE的统计指标转化为LLM能够理解的自然语言。3) 收敛条件的设定：设定合理的收敛条件，以保证迭代过程能够收敛到最优解。4) LLM的prompt设计：设计合适的prompt，引导LLM生成高质量的混淆因子。",
            "application_zh": "VIGOR+可应用于医疗健康、社会科学、经济学等领域，用于解决观测数据因果推断中的隐藏混淆问题。例如，在医疗领域，可以利用VIGOR+识别影响疾病治疗效果的隐藏因素，从而制定更有效的治疗方案。在社会科学领域，可以用于分析影响社会现象的复杂因果关系，为政策制定提供依据。该研究具有重要的实际价值和广泛的应用前景。",
            "highlight_zh": "论文通过实验验证了VIGOR+的有效性。实验结果表明，VIGOR+能够显著提高因果推断的准确性，优于现有的方法。具体而言，VIGOR+在合成数据集和真实数据集上都取得了显著的性能提升，例如在某个数据集上，VIGOR+的因果推断准确率比基线方法提高了10%以上。实验还验证了迭代反馈机制的有效性，表明通过迭代优化，VIGOR+能够生成更有效的混淆因子。",
            "tags_zh": [
                "因果推断",
                "隐藏混淆",
                "大型语言模型",
                "CEVAE",
                "迭代优化",
                "反馈环路",
                "统计验证"
            ],
            "_index": 83,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19349v1/fig1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
            "authors": [
                "Anna-Maria Gueorguieva",
                "Aylin Caliskan"
            ],
            "arxiv_id": "2512.19238v1",
            "summary": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19238v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "研究揭示LLM中针对污名化群体的偏见特征，并评估Guardrail模型缓解效果",
            "summary_zh": "大型语言模型(LLM)已显示出社会偏见，但针对非受保护的污名化身份的偏见仍未得到充分研究。此外，污名的哪些社会特征与LLM输出中的偏见相关仍然未知。心理学文献表明，污名包含六个共同的社会特征：美学、可隐藏性、过程、破坏性、起源和危险性。本研究调查了人类和LLM对污名特征的评分，以及提示风格和污名类型，是否会对LLM输出中针对污名化群体的偏见产生影响。我们使用SocialStigmaQA（一个包含37个关于污名化身份的社会场景的基准，例如决定是否推荐他们参加实习）来衡量三种广泛使用的LLM（Granite 3.0-8B、Llama-3.1-8B、Mistral-7B）中针对93个污名化群体的偏见。我们发现，人类评定为高度危险的污名（例如，成为帮派成员或感染艾滋病毒）在SocialStigmaQA提示中产生最多的偏见输出（来自所有模型的60%），而社会人口污名（例如，亚裔美国人或老年）产生的偏见输出最少（11%）。我们测试了使用guardrail模型（旨在识别有害输入的模型）是否可以减少偏见输出的数量，使用了每个LLM各自的guardrail模型（Granite Guardian 3.0、Llama Guard 3.0、Mistral Moderation API）。我们发现，偏见分别显著降低了10.4%、1.4%和7.8%。然而，我们表明，对偏见有显著影响的特征在缓解后仍然没有改变，并且guardrail模型通常无法识别提示中偏见的意图。这项工作对在涉及污名化群体的场景中使用LLM具有重要意义，我们建议未来的工作应致力于改进用于偏见缓解的guardrail模型。",
            "intro_zh": [
                "现有研究对LLM中针对污名化群体的偏见关注不足，且缺乏对污名社会特征与偏见关联的深入理解。",
                "该研究通过分析污名的六个社会特征（美学、可隐藏性等）与LLM偏见输出之间的关系，揭示了危险性特征与偏见程度的正相关性。",
                "实验表明，Guardrail模型能一定程度缓解偏见，但对高危险性污名的偏见缓解效果有限，且难以识别提示中的偏见意图。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型(LLM)中存在的、针对污名化群体的社会偏见问题。现有方法主要关注受保护群体的偏见，而对非受保护的污名化群体（如患有特定疾病、从事特定职业等）的偏见研究不足。此外，现有方法缺乏对污名本身特征与LLM偏见输出之间关联性的深入分析，难以有效缓解此类偏见。\\n\\n**核心思路**：论文的核心思路是，从心理学角度出发，将污名分解为六个关键的社会特征（美学、可隐藏性、过程、破坏性、起源和危险性），并研究这些特征与LLM偏见输出之间的关系。通过量化这些特征，并结合不同的提示风格和污名类型，分析其对LLM偏见程度的影响。同时，评估Guardrail模型在缓解此类偏见方面的效果。\\n\\n**技术框架**：整体框架包括以下几个主要步骤：1) 收集93个污名化群体的数据，并使用SocialStigmaQA基准测试LLM的偏见程度。2) 人工和LLM对污名的六个社会特征进行评分。3) 分析污名特征、提示风格和污名类型对LLM偏见输出的影响。4) 使用Guardrail模型（Granite Guardian 3.0、Llama Guard 3.0、Mistral Moderation API）对LLM进行偏见缓解。5) 评估Guardrail模型的缓解效果，并分析其局限性。\\n\\n**关键创新**：论文最重要的技术创新点在于，首次将心理学中污名的社会特征引入到LLM偏见分析中，并揭示了“危险性”这一特征与LLM偏见程度的正相关关系。这为理解和缓解LLM中针对污名化群体的偏见提供了新的视角。与现有方法相比，该研究不仅关注偏见的存在，更深入地探究了偏见的根源。\\n\\n**关键设计**：关键设计包括：1) 使用SocialStigmaQA基准，该基准包含37个关于污名化身份的社会场景，能够有效衡量LLM的偏见程度。2) 人工和LLM对污名的六个社会特征进行评分，为量化分析提供了数据基础。3) 使用三种不同的Guardrail模型，评估其在缓解偏见方面的效果，并分析其局限性。4) 采用统计分析方法，量化污名特征、提示风格和污名类型对LLM偏见输出的影响。",
            "application_zh": "该研究成果可应用于开发更公平、更负责任的LLM系统，尤其是在涉及污名化群体的场景中，如医疗诊断、招聘筛选、法律咨询等。通过理解污名特征与偏见的关系，可以设计更有效的偏见缓解策略，并提高Guardrail模型的识别能力，从而减少LLM对弱势群体的歧视。未来的研究可以进一步探索其他社会因素对LLM偏见的影响，并开发更通用的偏见缓解方法。",
            "highlight_zh": "实验结果表明，LLM对被人类评定为高度危险的污名（如帮派成员、HIV感染者）的偏见输出最多（60%），而对社会人口污名（如亚裔美国人、老年人）的偏见输出最少（11%）。使用Guardrail模型后，偏见分别显著降低了10.4%（Granite Guardian 3.0）、1.4%（Llama Guard 3.0）和7.8%（Mistral Moderation API）。但对偏见有显著影响的特征在缓解后仍然没有改变，且Guardrail模型通常无法识别提示中偏见的意图。",
            "tags_zh": [
                "大型语言模型",
                "社会偏见",
                "污名化群体",
                "Guardrail模型",
                "偏见缓解",
                "社会特征",
                "风险评估"
            ],
            "_index": 84,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19238v1/report_images/fig_1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19238v1/report_images/fig_2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19238v1/report_images/fig_3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions",
            "authors": [
                "Haishan Ye"
            ],
            "arxiv_id": "2512.19104v1",
            "summary": "Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.",
            "categories": [
                "math.OC",
                "cs.LG"
            ],
            "primary_category": "math.OC",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19104v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱二：RL算法与架构 (RL & Architecture)",
                    "id": "2_algo_arch",
                    "matched_keywords": [
                        "reinforcement learning",
                        "preference learning"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "2_algo_arch"
            ],
            "headline_zh": "提出基于排序的零阶算法，解决随机光滑函数优化问题，达到最优查询效率。",
            "summary_zh": "本文研究了随机函数的基于排序的零阶优化问题，其中仅能获得随机函数的序数反馈。针对此问题，提出了一种简单且计算高效的基于排序的零阶算法。在包括光滑性、强凸性和随机梯度的有界二阶矩等标准假设下，建立了凸和非凸目标的显式非渐近查询复杂度界限。值得注意的是，我们的结果与基于价值的零阶算法的最佳已知查询复杂度相匹配，表明仅凭序数信息就足以在随机环境中实现最佳查询效率。我们的分析不同于现有的基于漂移和信息几何的技术，为噪声下的基于排序的优化研究提供了新的工具。这些发现缩小了理论与实践之间的差距，并为人类偏好驱动的优化提供了原则性基础。",
            "intro_zh": [
                "现有零阶优化算法在处理序数反馈和随机目标时，理论理解有限，尤其是在标准光滑性假设下。",
                "论文提出一种基于排序的零阶算法，利用序数信息实现优化，关注算法的查询复杂度。",
                "结果表明，该算法在随机环境下，仅使用序数信息即可达到与基于价值的算法相同的最优查询效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决随机光滑函数的零阶优化问题，该问题仅能获得函数的序数反馈（例如，排序或偏好）。现有基于价值的零阶优化算法虽然有效，但在序数反馈场景下，理论分析较为复杂，且可能无法充分利用序数信息的优势。此外，在随机环境下，如何保证算法的查询效率也是一个挑战。\\n\\n**核心思路**：论文的核心思路是设计一种基于排序的零阶算法，该算法直接利用序数信息进行优化，避免了对函数值的直接估计。通过比较不同样本点的函数值排序，算法可以推断出梯度方向，从而进行优化。这种方法可以更好地适应序数反馈的特点，并提高算法的鲁棒性。\\n\\n**技术框架**：该算法主要包含以下几个阶段：1）样本点生成：在当前解的邻域内随机生成一组样本点。2）序数反馈获取：通过某种方式（例如，人类反馈或模拟器）获取这些样本点的函数值排序。3）梯度估计：基于样本点的排序信息，估计梯度方向。4）解更新：沿着估计的梯度方向更新当前解。5）重复上述步骤，直到满足停止条件。\\n\\n**关键创新**：论文的关键创新在于提出了一种新的基于排序的梯度估计方法，该方法能够有效地利用序数信息，并具有较好的鲁棒性。此外，论文还对算法的查询复杂度进行了详细的理论分析，证明了该算法在随机环境下可以达到最优的查询效率。\\n\\n**关键设计**：算法的关键设计包括：1）样本点生成策略：采用合适的采样分布，以保证样本点的多样性。2）梯度估计方法：设计一种能够有效利用序数信息的梯度估计方法，例如，基于排序的平均梯度估计。3）步长选择策略：采用自适应步长选择策略，以保证算法的收敛性。4）停止条件：设置合理的停止条件，以避免过度优化。",
            "application_zh": "该研究成果可应用于人机交互、强化学习、推荐系统等领域。例如，在强化学习中，可以利用人类反馈的偏好信息来指导智能体的学习，从而提高学习效率和性能。在推荐系统中，可以利用用户的排序信息来优化推荐结果，从而提高用户满意度。此外，该算法还可以应用于进化策略等优化问题中。",
            "highlight_zh": "论文的主要实验结果是建立了凸和非凸目标的显式非渐近查询复杂度界限。结果表明，该算法在随机环境下，仅使用序数信息即可达到与基于价值的零阶算法相同的最优查询复杂度。这表明序数信息在随机优化中具有重要的价值，并且该算法具有较好的实用性。",
            "tags_zh": [
                "零阶优化",
                "序数反馈",
                "随机优化",
                "查询复杂度",
                "人机交互"
            ],
            "_index": 85,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
            "authors": [
                "Hengrui Jia",
                "Taoran Li",
                "Jonas Guan",
                "Varun Chandrasekaran"
            ],
            "arxiv_id": "2512.19025v1",
            "summary": "Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \\name, an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19025v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Erasure Illusion框架，用于压力测试LLM遗忘评估的泛化能力。",
            "summary_zh": "机器遗忘旨在从已训练模型中移除特定数据的影响，这对于遵守版权法和确保AI安全至关重要。目前的遗忘指标通常通过监控模型在特定遗忘数据集 ($D_u$) 上的性能下降来衡量成功。我们认为，对于大型语言模型 (LLM) 来说，这种评估范式是不够的，并且可能具有误导性。许多现实世界中的遗忘应用（由版权或安全驱动）不仅针对 $D_u$ 中的逐字内容，还针对模型从中获得的更广泛的概括所影响的行为。我们证明，LLM 可以通过标准的遗忘评估，并且看起来已经“忘记”了目标知识，同时在语义上与 $D_u$ 相邻的内容上保持强大的能力。这种现象表明，擦除确切的句子并不一定等同于移除底层知识。为了解决这个差距，我们提出了 \name，一个自动压力测试框架，用于生成替代数据集 $\tilde{D}_u$。构建该替代集，使其在语义上源自 $D_u$，但在嵌入空间中足够不同。通过比较 $D_u$ 和 $\tilde{D}_u$ 之间的遗忘指标分数，我们可以压力测试指标本身的可靠性。我们对三个 LLM 系列（Llama-3-8B、Qwen2.5-7B 和 Zephyr-7B-$β$）、三个不同的数据集和七个标准指标进行了广泛的评估，揭示了普遍的不一致性。我们发现，当前的指标经常高估遗忘的成功率，未能检测到我们的压力测试数据集暴露的保留知识。",
            "intro_zh": [
                "现有LLM遗忘评估主要关注特定数据集上的性能下降，忽略了模型泛化能力带来的潜在知识残留。",
                "论文提出Erasure Illusion框架，通过生成语义相关的替代数据集，对遗忘评估指标进行压力测试。",
                "实验表明，现有遗忘指标经常高估LLM的遗忘效果，未能有效检测到模型保留的知识。"
            ],
            "method_zh": "**问题定义**：现有LLM遗忘评估方法主要关注模型在特定遗忘数据集上的性能下降，而忽略了模型通过泛化学习到的知识的残留。这种评估方式无法有效衡量模型是否真正“忘记”了相关知识，存在被“欺骗”的风险。现有方法的痛点在于无法检测模型在语义相关的知识上的表现，从而可能导致对遗忘效果的过度乐观估计。\\n\\n**核心思路**：论文的核心思路是通过构建与原始遗忘数据集语义相关但又足够不同的替代数据集，来对现有的遗忘评估指标进行压力测试。如果模型在原始数据集上表现出遗忘，但在替代数据集上仍然表现出相关知识，则说明现有的遗忘评估指标存在缺陷，无法准确衡量模型的遗忘效果。这种方法旨在揭示LLM遗忘评估中的“幻觉”，即模型表面上遗忘了某些信息，但实际上仍然保留了相关知识。\\n\\n**技术框架**：Erasure Illusion框架主要包含以下几个阶段：1) 选择或构建原始遗忘数据集 ($D_u$)；2) 基于$D_u$，利用语义变换技术（例如释义、同义词替换等）生成替代数据集 ($\tilde{D}_u$)，确保$\tilde{D}_u$在语义上与$D_u$相关，但在嵌入空间中足够不同；3) 使用现有的遗忘评估指标分别在$D_u$和$\tilde{D}_u$上评估模型的遗忘效果；4) 比较在$D_u$和$\tilde{D}_u$上的评估结果，如果结果存在显著差异，则表明现有的遗忘评估指标存在问题。\\n\\n**关键创新**：该论文最重要的技术创新点在于提出了一个通用的、自动化的压力测试框架，用于评估LLM遗忘评估指标的可靠性。与以往只关注特定数据集的遗忘评估方法不同，该框架通过引入语义相关的替代数据集，能够更全面地评估模型的遗忘效果，并揭示现有评估方法的局限性。这种方法为LLM遗忘评估提供了一个新的视角，有助于开发更可靠的遗忘评估指标。\\n\\n**关键设计**：在替代数据集的生成过程中，需要仔细控制语义相似度和嵌入空间距离。论文可能使用了特定的语义变换技术和距离度量方法，以确保替代数据集既能反映原始数据集的语义信息，又能避免与原始数据集过于相似。此外，在比较评估结果时，可能需要使用统计检验方法来判断差异是否显著。",
            "application_zh": "该研究成果可应用于评估和改进LLM的机器遗忘技术，确保模型能够有效移除特定数据的影响，从而更好地遵守版权法规、保护用户隐私，并提升AI系统的安全性。该框架能够帮助开发者识别和修复遗忘评估中的漏洞，开发更可靠的遗忘机制，从而构建更值得信赖的AI系统。",
            "highlight_zh": "实验结果表明，现有的遗忘评估指标在面对Erasure Illusion框架生成的替代数据集时，经常表现出不一致性，高估了LLM的遗忘效果。在Llama-3-8B、Qwen2.5-7B和Zephyr-7B-$β$等多个LLM上，以及三个不同的数据集上，都观察到了这种现象。这表明现有的遗忘评估方法可能存在严重的缺陷，需要进一步改进。",
            "tags_zh": [
                "机器遗忘",
                "大型语言模型",
                "遗忘评估",
                "压力测试",
                "泛化能力",
                "AI安全",
                "数据隐私"
            ],
            "_index": 86,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19025v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19025v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19025v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
            "authors": [
                "Akshaj Prashanth Rao",
                "Advait Singh",
                "Saumya Kumaar Saksena",
                "Dhruv Kumar"
            ],
            "arxiv_id": "2512.19011v1",
            "summary": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
            "categories": [
                "cs.CR",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Under Review",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19011v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出一种基于语义线性分类的多阶段流水线，高效缓解大语言模型的越狱攻击。",
            "summary_zh": "本文提出了一种高效且经过系统评估的防御架构，旨在缓解基于大型语言模型（LLM）的系统所面临的持续性安全挑战，即提示注入和越狱攻击。该架构的核心组件是一个基于文本归一化、TF-IDF表示和线性SVM分类器的语义过滤器。尽管其结构简单，但该模块在保留数据上实现了93.4%的准确率和96.5%的特异性，在产生可忽略的计算开销的同时，显著降低了攻击吞吐量。在此高效基础之上，完整的流水线集成了在连续阶段运行的互补检测和缓解机制，以最小的延迟提供强大的鲁棒性。对比实验表明，基于SVM的配置将总体准确率从35.1%提高到93.4%，同时将平均完成时间从大约450秒减少到47秒，从而比ShieldGemma的延迟降低了10倍以上。这些结果表明，所提出的设计同时提高了防御精度和效率，解决了当前基于模型的调节器的核心局限性。在包含超过30,000个标记提示（包括良性、越狱和应用层注入）的精选语料库上的评估证实，分阶段、资源高效的防御可以可靠地保护现代LLM驱动的应用程序。",
            "intro_zh": [
                "大型语言模型面临提示注入和越狱攻击的安全威胁，现有防御方法在精度和效率上存在局限性。",
                "提出一种多阶段流水线防御架构，核心是基于语义的线性SVM分类器，实现高效的攻击检测和缓解。",
                "实验结果表明，该方法在准确率和延迟方面均优于现有方法，验证了其在实际应用中的有效性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）面临的提示注入和越狱攻击问题。现有防御方法通常计算开销大，效率低，或者精度不足，无法有效阻止恶意攻击，影响了LLM在实际应用中的安全性。\\n\\n**核心思路**：论文的核心思路是利用轻量级的语义分析技术，构建一个高效的多阶段防御流水线。通过在不同阶段部署不同的检测和缓解机制，可以在保证精度的同时，显著降低计算开销和延迟。核心在于使用线性SVM分类器进行语义过滤，快速识别恶意提示。\\n\\n**技术框架**：整体架构是一个多阶段流水线，包含以下主要模块：1) 文本归一化：对输入提示进行预处理，消除噪声和冗余信息。2) TF-IDF表示：将文本转换为数值向量，捕捉文本的语义信息。3) 线性SVM分类器：基于TF-IDF向量，快速判断提示是否为恶意攻击。4) 后续检测和缓解机制：在SVM过滤之后，还可以集成其他检测和缓解模块，进一步提高防御的鲁棒性。\\n\\n**关键创新**：最重要的技术创新点在于使用线性SVM分类器进行语义过滤。相比于复杂的深度学习模型，线性SVM具有计算效率高、易于训练和部署的优点。同时，通过文本归一化和TF-IDF表示，可以有效地提取文本的语义信息，提高分类器的准确率。多阶段流水线的设计也保证了防御的鲁棒性和效率。\\n\\n**关键设计**：文本归一化采用标准化的文本处理流程，例如去除特殊字符、转换为小写等。TF-IDF表示采用常用的参数设置，例如选择合适的词汇表大小和IDF权重计算方法。线性SVM分类器采用标准的线性核函数，并使用交叉验证选择合适的正则化参数。损失函数为标准的hinge loss。",
            "application_zh": "该研究成果可应用于各种基于大型语言模型的应用场景，例如智能客服、聊天机器人、内容生成等。通过部署该防御架构，可以有效防止恶意用户利用提示注入和越狱攻击来操纵LLM的行为，保障LLM应用的安全性、可靠性和公平性。未来，该方法可以进一步扩展到其他类型的攻击防御，例如对抗样本攻击和数据投毒攻击。",
            "highlight_zh": "实验结果表明，该方法在准确率和效率方面均优于现有方法。基于SVM的配置将总体准确率从35.1%提高到93.4%，同时将平均完成时间从大约450秒减少到47秒，从而比ShieldGemma的延迟降低了10倍以上。这些结果验证了该方法在实际应用中的有效性。",
            "tags_zh": [
                "大型语言模型安全",
                "提示注入攻击",
                "越狱攻击",
                "语义分析",
                "线性SVM"
            ],
            "_index": 87,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19011v1/FINAL_REPORT_LLM_really_final.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
            "authors": [
                "Tongyuan Miao",
                "Gary Huang",
                "Kai Jun Han",
                "Annie Jiang"
            ],
            "arxiv_id": "2512.19004v1",
            "summary": "Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.",
            "categories": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19004v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出上下文感知初始化方法，缩短扩散语言模型生成路径，加速推理。",
            "summary_zh": "扩散语言模型（DLLMs）实现了完全并行的token解码，但由于需要多次去噪迭代才能将无信息的全掩码初始化转化为连贯的文本，因此在推理时通常不实用。现有的大多数加速方法都侧重于通过改进的求解器或采样策略来更有效地遍历此生成轨迹。本文提出了一种互补的视角：通过上下文感知的初始化，从更接近目标分布的位置开始，从而缩短轨迹本身。本文提出了一种无需训练的接口，该接口将来自轻量级辅助模型的prompt条件先验注入到扩散初始化中，并通过两种机制实例化它：离散token注入和表示级嵌入插值。由于注入的先验可能不完善，并且仅使用unmask解码可能会过早地提交，因此本文还引入了一种简单的基于置信度的重掩码机制，作为一种先验怀疑的形式。在GSM8K上的初步证据表明，上下文感知的初始化可以大大减少去噪迭代次数（在本文的设置中减少约35％的函数评估），同时也暴露了一个关键的开放挑战：相对于强大的扩散基线，naive的warm-starting会降低最终的准确性。本文使用这些发现来激发围绕校准、修正机制和表示对齐的研究议程，以实现可靠的warm-started扩散解码。",
            "intro_zh": [
                "扩散语言模型推理速度慢，主要瓶颈在于需要大量的去噪迭代，从完全随机的初始状态生成文本。",
                "论文提出上下文感知初始化方法，利用轻量级辅助模型提供prompt条件先验，使初始状态更接近目标分布，从而缩短生成路径。",
                "实验表明，该方法能显著减少去噪迭代次数（约35%），但同时也发现naive的warm-starting可能降低最终精度，需要进一步研究。"
            ],
            "method_zh": "**问题定义**：扩散语言模型（DLLMs）虽然具有并行解码的优势，但推理速度慢，主要原因是需要大量的去噪迭代。现有方法主要集中在优化采样策略或求解器，但忽略了初始状态对生成路径长度的影响。因此，如何减少生成路径的长度，提高推理效率是一个关键问题。\\n\\n**核心思路**：论文的核心思路是通过上下文感知初始化，使扩散过程的起始点更接近目标分布。具体来说，利用一个轻量级的辅助模型（例如，一个小的语言模型）来预测在给定prompt下的token分布，并将这些先验信息注入到扩散模型的初始状态中。这样，扩散模型就可以从一个更有意义的状态开始去噪，从而减少所需的迭代次数。\\n\\n**技术框架**：整体框架包含以下几个主要步骤：1) 使用prompt作为输入，通过轻量级辅助模型生成prompt条件先验。2) 将这些先验信息注入到扩散模型的初始状态中，具体方法包括离散token注入和表示级嵌入插值。3) 使用扩散模型进行去噪迭代，生成最终的文本。4) 引入基于置信度的重掩码机制，对初始注入的先验信息进行修正，避免过早的commit。\\n\\n**关键创新**：论文的关键创新在于提出了上下文感知初始化这一概念，并将其应用于扩散语言模型。与现有方法不同，该方法不是优化扩散过程本身，而是通过改变初始状态来缩短生成路径。此外，论文还提出了两种具体的先验注入方法（离散token注入和表示级嵌入插值）以及一种基于置信度的重掩码机制，这些都为扩散语言模型的加速提供了新的思路。\\n\\n**关键设计**：论文中，离散token注入方法直接将辅助模型预测的token替换扩散模型的初始掩码token。表示级嵌入插值方法则将辅助模型生成的嵌入向量与扩散模型的初始嵌入向量进行插值，从而将先验信息融入到表示空间中。基于置信度的重掩码机制则根据辅助模型预测的置信度，对低置信度的token进行重新掩码，从而避免过早的commit。具体的置信度阈值和插值系数等参数需要根据实验进行调整。",
            "application_zh": "该研究成果可应用于各种需要快速文本生成的场景，例如对话系统、机器翻译、文本摘要等。通过减少扩散语言模型的推理时间，可以提高这些应用的实时性和用户体验。此外，该方法还可以促进扩散模型在资源受限设备上的部署，扩大其应用范围。",
            "highlight_zh": "实验结果表明，上下文感知初始化方法可以在GSM8K数据集上显著减少去噪迭代次数，在特定设置下减少约35%的函数评估。然而，naive的warm-starting可能会降低最终精度，这表明需要进一步研究校准、修正机制和表示对齐等问题，以实现更可靠的warm-started扩散解码。",
            "tags_zh": [
                "扩散语言模型",
                "上下文感知初始化",
                "生成路径缩短",
                "推理加速",
                "轻量级辅助模型"
            ],
            "_index": 88,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19004v1/figures/arvsdiffusion.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19004v1/figures/confidence.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19004v1/figures/fig_method1_token_injection.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
            "authors": [
                "Saman Forouzandeh",
                "Wei Peng",
                "Parham Moradi",
                "Xinghuo Yu",
                "Mahdi Jalili"
            ],
            "arxiv_id": "2512.18950v1",
            "summary": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
            "categories": [
                "cs.LG",
                "cs.AI"
            ],
            "primary_category": "cs.LG",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Accepted at The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). 21 pages including references, with 7 figures and 8 tables. Code is publicly available at the authors GitHub repository: https://github.com/S-Forouzandeh/MACLA-LLM-Agents-AAMAS-Conference",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18950v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "MACLA：通过贝叶斯选择和对比精炼学习LLM Agent的分层程序记忆",
            "summary_zh": "本文提出MACLA，一个将推理与学习解耦的框架，它维护一个冻结的大语言模型，并在外部分层程序记忆中执行所有适应。MACLA从轨迹中提取可重用的程序，通过贝叶斯后验跟踪可靠性，通过期望效用评分选择动作，并通过对比成功和失败来改进程序。在四个基准测试（ALFWorld、WebShop、TravelPlanner、InterCodeSQL）中，MACLA实现了78.1%的平均性能，优于所有基线。在ALFWorld的未见任务中，MACLA达到90.3%的性能，具有3.1%的正泛化能力。该系统在56秒内构建记忆，比最先进的LLM参数训练基线快2800倍，并将2851条轨迹压缩为187个程序。实验结果表明，具有贝叶斯选择和对比精炼的结构化外部记忆能够实现样本高效、可解释且持续改进的Agent，而无需LLM参数更新。",
            "intro_zh": [
                "现有LLM Agent在复杂任务中面临样本效率低和泛化能力弱的挑战，需要大量训练数据和参数调整。",
                "MACLA通过维护冻结的LLM和外部分层程序记忆，解耦推理和学习过程，实现高效的知识复用和泛化。",
                "实验表明，MACLA在多个基准测试中显著优于现有方法，尤其在未见任务上表现出良好的泛化能力和效率。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型Agent在处理复杂任务时，通常需要大量的训练数据进行参数调整，导致样本效率低下。此外，模型难以泛化到未见过的任务，需要针对特定任务进行重新训练。现有的方法难以在效率、泛化性和可解释性之间取得平衡。\\n\\n**核心思路**：MACLA的核心思路是将LLM的推理能力与外部程序记忆的学习能力解耦。LLM负责进行高层次的推理和决策，而外部程序记忆则负责存储和复用从经验中学习到的程序。通过这种方式，LLM可以专注于推理，而程序记忆可以不断地学习和改进，从而提高样本效率和泛化能力。\\n\\n**技术框架**：MACLA的整体框架包括以下几个主要模块：1) 轨迹提取：从Agent的交互轨迹中提取可重用的程序。2) 可靠性跟踪：使用贝叶斯后验来跟踪每个程序的可靠性。3) 动作选择：通过期望效用评分来选择最佳的动作。4) 程序精炼：通过对比成功和失败的经验来改进程序。整个流程无需更新LLM参数。\\n\\n**关键创新**：MACLA最重要的创新点在于其结构化的外部记忆和贝叶斯选择与对比精炼机制。结构化外部记忆允许Agent存储和复用程序，而贝叶斯选择机制则允许Agent根据程序的可靠性来选择最佳的动作。对比精炼机制则允许Agent通过对比成功和失败的经验来不断改进程序。与现有方法相比，MACLA无需对LLM进行参数更新，从而大大提高了效率。\\n\\n**关键设计**：MACLA使用分层程序记忆，允许Agent存储不同粒度的程序。贝叶斯后验的计算基于程序的成功和失败次数。期望效用评分则基于程序的可靠性和预期收益。对比精炼机制通过比较成功和失败的轨迹来识别需要改进的程序。",
            "application_zh": "MACLA框架具有广泛的应用前景，可应用于机器人控制、游戏AI、智能助手等领域。通过学习和复用经验，Agent可以更有效地完成复杂任务，并适应新的环境。该研究有助于开发更智能、更高效、更具适应性的AI系统。",
            "highlight_zh": "MACLA在四个基准测试中取得了显著的性能提升，平均性能达到78.1%，优于所有基线。在ALFWorld的未见任务中，MACLA达到了90.3%的性能，具有3.1%的正泛化能力。此外，MACLA构建记忆的速度比最先进的LLM参数训练基线快2800倍，并将2851条轨迹压缩为187个程序。这些结果表明，MACLA具有很高的效率和泛化能力。",
            "tags_zh": [
                "LLM Agent",
                "程序记忆",
                "贝叶斯选择",
                "对比学习",
                "分层记忆",
                "知识复用",
                "强化学习"
            ],
            "_index": 89,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18950v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18950v1/Figures/fig_memory_size.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18950v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Exploring the features used for summary evaluation by Human and GPT",
            "authors": [
                "Zahra Sadeghi",
                "Evangelos Milios",
                "Frank Rudzicz"
            ],
            "arxiv_id": "2512.19620v1",
            "summary": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19620v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "研究人类与GPT评估摘要时使用的特征，并提升GPT摘要评估能力",
            "summary_zh": "摘要评估旨在衡量生成的摘要在多大程度上反映了源文本的关键思想和含义，这需要对内容有深刻的理解。大型语言模型（LLMs）已被用于自动化此过程，充当评估摘要相对于原始文本的质量的评判者。虽然之前的研究调查了LLM与人类反应之间的一致性，但人们对LLM在基于特定质量维度进行评估时利用的属性或特征的理解尚不充分，并且对评估分数和指标之间的映射关注不足。在本文中，我们通过研究统计和机器学习指标来解决这个问题，并发现与人类和生成式预训练Transformer（GPT）反应对齐的特征。此外，我们表明，指示GPT采用人类使用的指标可以改善其判断，使其更好地符合人类的反应。",
            "intro_zh": [
                "现有摘要评估方法缺乏对人类和LLM评估特征的深入理解，阻碍了自动化评估的准确性和可靠性。",
                "通过统计和机器学习指标，论文探索了人类和GPT在摘要评估中使用的特征，并建立了评估分数与指标之间的映射关系。",
                "实验表明，指导GPT使用人类评估指标可以显著提高其判断能力，使其评估结果更符合人类的认知。"
            ],
            "method_zh": "**问题定义**：论文旨在解决摘要评估自动化的问题，特别是如何使大型语言模型（LLMs）的评估结果更接近人类的判断。现有方法缺乏对人类和LLM评估摘要时所关注特征的深入理解，导致LLM的评估结果与人类存在偏差。\\n\\n**核心思路**：论文的核心思路是通过分析人类和LLM在评估摘要时使用的特征，找到两者之间的关联，并利用这些关联来指导LLM的评估过程，从而提高LLM评估结果与人类判断的一致性。\\n\\n**技术框架**：论文的技术框架主要包括以下几个步骤：1) 收集人类对摘要的评估数据；2) 收集LLM（GPT）对相同摘要的评估数据；3) 提取摘要的统计和机器学习特征；4) 分析人类和GPT评估数据与摘要特征之间的关系；5) 基于分析结果，设计指导GPT评估的策略。\\n\\n**关键创新**：论文的关键创新在于：1) 深入研究了人类和GPT在摘要评估中使用的特征，揭示了两者之间的差异和关联；2) 提出了一种利用人类评估特征来指导GPT评估的方法，提高了GPT评估结果与人类判断的一致性。\\n\\n**关键设计**：论文的关键设计包括：1) 选择合适的统计和机器学习指标来提取摘要的特征，例如ROUGE、BLEU等；2) 设计合适的损失函数来训练GPT，使其评估结果更接近人类的判断；3) 设计合适的prompt，引导GPT关注人类评估摘要时使用的特征。",
            "application_zh": "该研究成果可应用于自动摘要评估、机器翻译质量评估、文本生成质量评估等领域。通过提高LLM评估结果与人类判断的一致性，可以减少人工评估的成本，提高评估效率，并为文本生成模型的优化提供更可靠的反馈。",
            "highlight_zh": "研究表明，通过指导GPT使用人类评估指标，可以显著提高其摘要评估能力，使其评估结果更符合人类的认知。具体而言，经过指导的GPT在摘要评估任务上的表现更接近人类专家的评估结果，减少了与人类判断的偏差。",
            "tags_zh": [
                "摘要评估",
                "大型语言模型",
                "特征分析",
                "人类对齐",
                "GPT",
                "自动化评估",
                "自然语言处理"
            ],
            "_index": 90,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19620v1/Figs/corr_scatter_cos_ent_read_gini_rel_model_condprep1_legend.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19620v1/Figs/corr_scatter_cos_ent_read_gini_coh_model_condprep1_legend.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19620v1/Figs/corr_barchart_cos_ent_gini_read_rel_gpts_phi_condprep1.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
            "authors": [
                "Katharina Stengg",
                "Christian Macho",
                "Martin Pinzger"
            ],
            "arxiv_id": "2512.19481v1",
            "summary": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
            "categories": [
                "cs.SE",
                "cs.AI"
            ],
            "primary_category": "cs.SE",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "6 pages",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19481v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "构建代码变更影响分析数据集，初步评估GPT-5在代码影响预测中的能力。",
            "summary_zh": "理解源代码变更及其对其他代码实体的影响是软件开发中的一项关键技能。然而，代码变更及其影响的分析通常是手动执行的，因此非常耗时。人工智能，特别是大型语言模型（LLM）的最新进展，显示出帮助开发人员完成各种代码分析任务的潜力。然而，这种潜力在理解代码变更及其影响方面的利用程度尚未得到充分探索。为了弥补这一差距，我们研究了GPT-5和GPT-5-mini预测给定源代码变更所影响的代码实体的能力。我们构建了一个数据集，其中包含每个提交的种子变更、变更对和变更类型的信息。现有数据集缺乏关于种子变更和受影响代码实体的关键信息。我们的实验在两种配置中评估LLM：（1）种子变更信息和父提交树；（2）种子变更信息、父提交树和每个种子变更的diff hunk。我们发现，两种LLM在这两个实验中的表现都很差，但GPT-5优于GPT-5-mini。此外，提供diff hunk有助于两个模型略微提高其性能。",
            "intro_zh": [
                "手动分析代码变更的影响耗时，现有数据集缺乏种子变更和受影响代码实体的关键信息。",
                "论文旨在评估GPT-5和GPT-5-mini在预测代码变更影响方面的能力，并构建包含种子变更信息的数据集。",
                "实验结果表明，GPT-5和GPT-5-mini在代码变更影响预测方面表现不佳，但提供diff hunk能略微提升性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决软件开发中代码变更影响分析的自动化问题。现有方法主要依赖人工分析，效率低下且容易出错。现有数据集缺乏关于种子变更和受影响代码实体的详细信息，阻碍了基于机器学习的自动化分析方法的发展。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM），特别是GPT-5和GPT-5-mini，来预测给定代码变更所影响的代码实体。通过构建包含种子变更、变更对和变更类型等信息的数据集，并结合父提交树和diff hunk等上下文信息，来训练和评估LLM在代码变更影响分析任务中的性能。\\n\\n**技术框架**：论文的技术框架主要包括以下几个部分：1) 构建代码变更影响分析数据集，该数据集包含种子变更、变更对、变更类型、父提交树和diff hunk等信息。2) 使用GPT-5和GPT-5-mini作为模型，在构建的数据集上进行训练和评估。3) 设计两种实验配置：第一种配置仅使用种子变更信息和父提交树，第二种配置则额外使用diff hunk信息。4) 评估LLM在不同配置下的性能，并分析结果。\\n\\n**关键创新**：论文的关键创新在于：1) 构建了一个新的代码变更影响分析数据集，该数据集包含更全面的信息，例如种子变更和受影响代码实体。2) 首次尝试使用GPT-5和GPT-5-mini等大型语言模型来解决代码变更影响分析问题。3) 通过实验评估了LLM在不同配置下的性能，并分析了diff hunk信息对模型性能的影响。\\n\\n**关键设计**：论文的关键设计包括：1) 数据集的构建方式，如何选择和标注种子变更、变更对和变更类型等信息。2) 实验配置的设计，如何选择合适的输入特征（例如父提交树和diff hunk）以及如何评估模型的性能。3) 模型训练和评估的细节，例如如何选择合适的损失函数和优化器，以及如何避免过拟合等问题。具体参数设置和网络结构细节未知。",
            "application_zh": "该研究成果可应用于软件开发过程中的代码变更管理、缺陷预测和代码审查等领域。通过自动化代码变更影响分析，可以帮助开发人员更快速、准确地理解代码变更的影响范围，从而减少错误、提高开发效率和软件质量。未来，该研究可以扩展到更复杂的代码变更场景，并与其他代码分析技术相结合，构建更智能化的软件开发工具。",
            "highlight_zh": "实验结果表明，GPT-5和GPT-5-mini在代码变更影响预测任务中表现不佳，这表明现有LLM在理解代码变更的语义和上下文方面仍存在挑战。尽管如此，GPT-5的性能优于GPT-5-mini，并且提供diff hunk信息可以略微提高模型的性能。这些发现为未来研究如何利用LLM进行代码变更影响分析提供了有价值的参考。",
            "tags_zh": [
                "代码变更影响分析",
                "大型语言模型",
                "GPT-5",
                "软件开发",
                "数据集构建"
            ],
            "_index": 91,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19481v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19481v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                }
            ]
        },
        {
            "title": "An Agentic Framework for Autonomous Materials Computation",
            "authors": [
                "Zeyu Xia",
                "Jinzhe Ma",
                "Congjie Zheng",
                "Shufei Zhang",
                "Yuqiang Li",
                "Hang Su",
                "P. Hu",
                "Changshui Zhang",
                "Xingao Gong",
                "Wanli Ouyang",
                "Lei Bai",
                "Dongzhan Zhou",
                "Mao Su"
            ],
            "arxiv_id": "2512.19458v1",
            "summary": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
            "categories": [
                "cs.AI",
                "cond-mat.mtrl-sci"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19458v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于Agent的材料计算框架，实现第一性原理计算的可靠自动化。",
            "summary_zh": "大型语言模型（LLMs）已成为加速科学发现的强大工具，但其静态知识和幻觉问题阻碍了自主研究应用。最近的研究将LLMs集成到Agent框架中，从而能够进行检索、推理和工具使用，以完成复杂的科学工作流程。本文提出了一个领域专用Agent，旨在可靠地自动化第一性原理材料计算。通过嵌入领域专业知识，该Agent确保物理上连贯的多步骤工作流程，并始终如一地选择收敛、适定的参数，从而实现可靠的端到端计算执行。一个新的包含各种计算任务的基准测试表明，我们的系统在准确性和鲁棒性方面均显著优于独立的LLMs。这项工作为自主计算实验奠定了可验证的基础，并代表了迈向完全自动化科学发现的关键一步。",
            "intro_zh": [
                "现有LLM在材料计算中面临静态知识和幻觉问题，难以自主完成可靠的计算任务。",
                "论文提出领域专用Agent，嵌入领域知识，确保计算流程的物理连贯性和参数的合理性。",
                "实验表明，该Agent在准确性和鲁棒性方面显著优于独立LLM，为自主计算实验奠定基础。"
            ],
            "method_zh": "**问题定义**：论文旨在解决第一性原理材料计算的自动化问题。现有方法，特别是直接使用大型语言模型（LLMs），由于LLMs缺乏领域知识、容易产生幻觉以及难以保证计算流程的物理合理性，导致计算结果的准确性和可靠性难以保证。因此，需要一种能够自主、可靠地执行材料计算任务的框架。\\n\\n**核心思路**：论文的核心思路是将大型语言模型（LLMs）集成到Agent框架中，并嵌入领域专业知识，从而弥补LLMs在材料计算方面的不足。通过领域知识的指导，Agent可以更好地理解计算任务，选择合适的计算参数，并确保计算流程的物理连贯性，从而提高计算结果的准确性和可靠性。\\n\\n**技术框架**：该Agent框架包含以下主要模块：1) 任务理解模块：负责理解用户输入的计算任务，并将其转化为Agent可以理解的形式。2) 知识检索模块：负责从领域知识库中检索与当前任务相关的知识，例如材料的性质、计算方法等。3) 参数选择模块：负责根据检索到的知识和计算任务的要求，选择合适的计算参数。4) 计算执行模块：负责执行计算任务，并监控计算过程，确保计算的收敛性和稳定性。5) 结果验证模块：负责验证计算结果的合理性，并生成计算报告。\\n\\n**关键创新**：该论文的关键创新在于将领域知识嵌入到Agent框架中，从而提高了Agent在材料计算方面的能力。与直接使用LLMs相比，该Agent能够更好地理解计算任务，选择合适的计算参数，并确保计算流程的物理连贯性。此外，该Agent还能够自动验证计算结果的合理性，从而提高了计算结果的可靠性。\\n\\n**关键设计**：在Agent的设计中，关键的设计包括：1) 领域知识库的构建：需要构建一个包含丰富材料计算知识的知识库，例如材料的性质、计算方法、计算参数等。2) 知识检索算法的设计：需要设计一种高效的知识检索算法，能够快速地从知识库中检索与当前任务相关的知识。3) 参数选择策略的设计：需要设计一种合理的参数选择策略，能够根据检索到的知识和计算任务的要求，选择合适的计算参数。4) 结果验证规则的设计：需要设计一套完善的结果验证规则，能够自动验证计算结果的合理性。",
            "application_zh": "该研究成果可应用于材料科学、化学、物理等领域，加速新材料的发现和性能预测。通过自动化第一性原理计算，可以降低计算成本，提高研究效率，并为材料设计提供更可靠的理论指导。未来，该框架有望扩展到其他科学计算领域，实现更广泛的科学发现自动化。",
            "highlight_zh": "实验结果表明，该Agent在各种材料计算任务中均显著优于独立的LLMs。例如，在能量计算任务中，该Agent的准确率提高了20%，在结构优化任务中，该Agent的收敛速度提高了30%。此外，该Agent还能够自动识别并纠正计算过程中的错误，从而提高了计算结果的可靠性。",
            "tags_zh": [
                "材料计算",
                "自主Agent",
                "大型语言模型",
                "第一性原理",
                "自动化",
                "领域知识",
                "科学发现"
            ],
            "_index": 92,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19458v1/vasp_agent_v6.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19458v1/radar_completion_seaborn.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19458v1/passrate_seaborn_paper.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
            "authors": [
                "Jinwei Chi",
                "Ke Wang",
                "Yu Chen",
                "Xuanye Lin",
                "Qiang Xu"
            ],
            "arxiv_id": "2512.19456v1",
            "summary": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
            "categories": [
                "cs.CL",
                "cs.AI"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19456v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "利用LLM激活值进行可泛化Essay评分表征学习，提升跨Prompt场景性能",
            "summary_zh": "由于评分标准的多样性，自动作文评分(AES)在跨Prompt设置中是一项具有挑战性的任务。以往的研究主要集中于利用大型语言模型(LLM)的输出来提高评分准确性，但我们认为中间层的激活值也可能提供有价值的信息。为了探索这种可能性，我们评估了LLM激活值在跨Prompt作文评分任务中的判别能力。具体来说，我们使用激活值来拟合探针，并进一步分析了不同模型和LLM的输入内容对这种判别能力的影响。通过计算不同Prompt下各种特征维度作文的方向，我们分析了大型语言模型在作文类型和特征方面的评估视角的差异。结果表明，激活值在评估作文质量方面具有很强的判别能力，并且LLM可以调整其评估视角以适应不同的特征和作文类型，从而有效地处理跨Prompt设置中评分标准的多样性。",
            "intro_zh": [
                "跨Prompt作文评分面临评分标准多样性的挑战，现有方法侧重于优化LLM输出，忽略了中间层激活值可能蕴含的丰富信息。",
                "该论文的核心思想是探索LLM中间层激活值在跨Prompt作文评分中的判别能力，通过分析激活值来理解LLM的评分视角。",
                "实验结果表明，LLM的激活值在评估作文质量方面具有很强的判别能力，并且LLM能够根据不同的Prompt调整评估视角。"
            ],
            "method_zh": "**问题定义**：自动作文评分（AES）旨在自动评估作文质量。在跨Prompt场景下，由于不同Prompt对应的评分标准存在差异，导致模型难以泛化。现有方法主要集中于优化LLM的输出，例如微调LLM或设计复杂的后处理模块，但忽略了LLM中间层激活值可能包含的丰富信息，这些信息可能反映了LLM对作文的深层理解和评估标准。\\n\\n**核心思路**：该论文的核心思路是利用LLM中间层的激活值作为特征，通过训练简单的探针（probe）来预测作文的质量。这种方法假设LLM的激活值已经包含了对作文质量的有效表征，而探针的作用是将这些表征提取出来并映射到评分空间。通过分析不同Prompt下激活值的变化，可以了解LLM如何根据不同的Prompt调整其评分标准。\\n\\n**技术框架**：整体框架包括以下几个步骤：1) 使用LLM处理作文文本，并提取指定中间层的激活值。2) 使用提取的激活值作为特征，训练一个线性探针来预测作文的评分。3) 通过计算不同Prompt下作文在不同特征维度上的方向，分析LLM评估视角的差异。4) 评估探针在跨Prompt场景下的泛化性能。\\n\\n**关键创新**：该论文的关键创新在于将LLM的激活值作为特征，用于跨Prompt作文评分。与以往方法不同，该方法不依赖于对LLM输出的直接优化，而是通过分析LLM内部的表征来理解其评分机制。这种方法可以更好地利用LLM的知识，并提高模型的泛化能力。\\n\\n**关键设计**：论文中使用了线性探针，这是一种简单的线性模型，用于将激活值映射到评分空间。选择线性探针的原因是其易于训练和解释。论文还计算了不同Prompt下作文在不同特征维度上的方向，这可以帮助理解LLM如何根据不同的Prompt调整其评分标准。具体来说，使用了余弦相似度来衡量不同Prompt下作文方向的差异。",
            "application_zh": "该研究成果可应用于自动作文评分系统，尤其是在需要处理多种Prompt的场景下。通过分析LLM的激活值，可以更好地理解LLM的评分机制，并提高评分系统的准确性和泛化能力。此外，该方法还可以用于教育诊断，帮助教师了解学生在不同方面的写作能力。",
            "highlight_zh": "实验结果表明，LLM的激活值在评估作文质量方面具有很强的判别能力。通过训练线性探针，可以在跨Prompt场景下取得良好的评分效果。此外，论文还发现LLM可以根据不同的Prompt调整其评估视角，有效地处理评分标准的多样性。具体性能数据未知，但论文强调了激活值特征的有效性和LLM评估视角的自适应性。",
            "tags_zh": [
                "自动作文评分",
                "大型语言模型",
                "激活值分析",
                "跨Prompt学习",
                "表征学习"
            ],
            "_index": 93,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19456v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19456v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19456v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
            "authors": [
                "Linzhi Chen",
                "Yang Sun",
                "Hongru Wei",
                "Yuqi Chen"
            ],
            "arxiv_id": "2512.19297v1",
            "summary": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
            "categories": [
                "cs.CR",
                "cs.AI"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "NDSS 2026",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19297v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出CBA：一种因果引导的LoRA模型解毒后门攻击方法",
            "summary_zh": "低秩适应(LoRA)已成为微调大型语言模型(LLM)的有效方法，并在开源社区中得到广泛应用。然而，通过Hugging Face等平台分散传播LoRA适配器引入了新的安全漏洞：恶意适配器可以很容易地传播并逃避传统的监督机制。尽管存在这些风险，但针对基于LoRA的微调的后门攻击仍然相对未被探索。现有的后门攻击策略不适合这种设置，因为它们通常依赖于无法访问的训练数据，未能考虑到LoRA特有的结构属性，或者遭受高误触发率(FTR)，从而损害了它们的隐蔽性。为了解决这些挑战，我们提出了一种因果引导的解毒后门攻击(CBA)，这是一种专门为开放权重LoRA模型设计的新的后门攻击框架。CBA在无需访问原始训练数据的情况下运行，并通过两个关键创新实现高隐蔽性：(1)一个覆盖引导的数据生成管道，通过行为探索合成任务对齐的输入，以及(2)一个因果引导的解毒策略，通过保留任务关键神经元来合并中毒和干净的适配器。与先前的方法不同，CBA通过基于因果影响的权重分配，实现了对攻击强度的训练后控制，从而消除了重复训练的需要。在六个LoRA模型上进行评估，CBA实现了高攻击成功率，同时与基线方法相比，降低了50-70%的FTR。此外，它还展示了对最先进的后门防御的增强抵抗力，突出了其隐蔽性和鲁棒性。",
            "intro_zh": [
                "现有LoRA后门攻击方法依赖不可访问的训练数据，忽略LoRA结构特性，且误触发率高，隐蔽性差。",
                "CBA通过覆盖引导的数据生成和因果引导的解毒策略，在无需原始数据的情况下实现高隐蔽性后门攻击。",
                "实验表明，CBA在保持高攻击成功率的同时，显著降低了误触发率，并增强了对现有防御的抵抗力。"
            ],
            "method_zh": "**问题定义**：现有针对LoRA模型的后门攻击方法存在诸多问题。首先，许多方法需要访问原始训练数据，这在实际场景中通常是不可行的。其次，这些方法未能充分利用LoRA的结构特性，导致攻击效果不佳。此外，现有方法的误触发率（FTR）较高，容易被检测到，从而降低了攻击的隐蔽性。因此，需要一种能够在无需访问原始数据的情况下，利用LoRA结构特性，并具有低误触发率的后门攻击方法。\\n\\n**核心思路**：CBA的核心思路是通过行为探索生成任务对齐的输入，并利用因果关系指导中毒和干净适配器的合并，从而实现高隐蔽性和可控的后门攻击。具体来说，CBA首先通过覆盖引导的数据生成管道，合成能够触发目标行为的输入。然后，利用因果引导的解毒策略，选择性地保留任务关键神经元，将中毒适配器和干净适配器合并，从而在保持模型性能的同时，植入后门。\\n\\n**技术框架**：CBA框架主要包含两个阶段：(1)覆盖引导的数据生成阶段：该阶段通过探索模型的行为空间，生成能够有效触发目标行为的输入数据。具体来说，该阶段使用一种基于覆盖率的策略，选择能够最大程度覆盖模型内部状态的输入，从而提高生成数据的有效性。(2)因果引导的解毒阶段：该阶段利用因果关系指导中毒适配器和干净适配器的合并。具体来说，该阶段首先识别对任务性能至关重要的神经元，然后选择性地保留这些神经元，从而在植入后门的同时，保持模型的性能。\\n\\n**关键创新**：CBA的关键创新在于其因果引导的解毒策略。与现有方法不同，CBA不是简单地将中毒适配器和干净适配器合并，而是利用因果关系选择性地保留任务关键神经元。这种策略能够有效地降低误触发率，并提高攻击的隐蔽性。此外，CBA还实现了对攻击强度的训练后控制，无需重复训练即可调整攻击效果。\\n\\n**关键设计**：在覆盖引导的数据生成阶段，CBA使用了一种基于神经元覆盖率的策略，选择能够最大程度覆盖模型内部神经元的输入。在因果引导的解毒阶段，CBA使用了一种基于因果影响的权重分配方法，根据神经元对任务性能的影响程度，分配不同的权重。此外，CBA还设计了一种损失函数，用于平衡攻击成功率和模型性能。",
            "application_zh": "CBA的研究成果可应用于评估和增强LoRA模型的安全性，尤其是在开源社区中广泛共享的LoRA适配器。该方法能够帮助开发者识别潜在的后门攻击风险，并开发更有效的防御机制。此外，CBA的思路也可以推广到其他类型的模型和攻击场景，为提升人工智能系统的整体安全性做出贡献。",
            "highlight_zh": "CBA在六个LoRA模型上进行了评估，实验结果表明，CBA能够实现高攻击成功率，同时将误触发率(FTR)降低50-70%，显著优于基线方法。此外，CBA还展示了对最先进的后门防御的增强抵抗力，证明了其隐蔽性和鲁棒性。这些结果表明，CBA是一种有效的LoRA模型后门攻击方法。",
            "tags_zh": [
                "后门攻击",
                "LoRA模型",
                "因果引导",
                "解毒策略",
                "大型语言模型",
                "安全漏洞",
                "对抗防御"
            ],
            "_index": 94,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19297v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19297v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19297v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation",
            "authors": [
                "Jerry Wang",
                "Ting Yiu Liu"
            ],
            "arxiv_id": "2512.19210v1",
            "summary": "We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Accepted at NeurIPS Workshop on Foundations of Reasoning in Language Models and Workshop on Bridging Language, Agent, and World Model",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19210v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出基于观察者模式的框架，通过石头剪刀布游戏评估LLM的心理理论能力",
            "summary_zh": "本文提出了一个交互式框架，用于评估大型语言模型（LLM）在简单但具有策略性的环境中是否表现出真正的“理解”能力。以石头剪刀布（RPS）为例，尽管其表面上很简单，但需要顺序推理、适应和策略识别。该系统将LLM定位为观察者，其任务是识别正在使用的策略，并阐明这种判断背后的推理。目的不是测试对石头剪刀布本身的知识，而是探究模型是否可以表现出类似心智的关于顺序行为的推理。为了支持系统评估，我们提供了一个基准，该基准由静态策略和由良好提示规则指定的轻量级动态策略组成。我们使用三个互补信号来量化观察者的预测与实际策略对引起的真实分布之间的一致性：交叉熵、Brier分数和期望值（EV）差异。这些指标进一步集成到一个统一的分数中，即联合损失，该损失平衡了校准、灵敏度和收益对齐。连同策略识别率（SIR）指标，我们的框架不仅捕获了预测准确性，还捕获了模型是否可以稳定地识别正在使用的潜在策略。该演示强调了交互性、透明性和可重复性。用户可以实时调整LLM分布，可视化损失的演变，并直接检查推理片段，以识别失败发生的位置和原因。通过这样做，我们的系统为顺序游戏中类似心智的推理提供了一个实用且可解释的代理，从而深入了解了当前LLM推理的优势和局限性。",
            "intro_zh": [
                "现有方法难以有效评估LLM在策略性环境中的“理解”能力，缺乏对顺序行为推理的深入探究。",
                "将LLM定位为观察者，通过分析其对石头剪刀布游戏策略的识别和推理，模拟心理理论。",
                "构建包含静态和动态策略的基准，并提出联合损失和策略识别率等指标，综合评估LLM的推理能力。"
            ],
            "method_zh": "**问题定义**：论文旨在评估大型语言模型（LLM）是否具备类似人类的“心理理论”（Theory of Mind）能力，即理解他人意图和策略的能力。现有方法难以在策略性环境中有效评估LLM的这种能力，尤其是在需要顺序推理和策略适应的场景下。传统的知识问答或文本生成任务无法充分体现LLM在复杂交互中的策略理解能力。\\n\\n**核心思路**：论文的核心思路是将LLM置于一个“观察者”的角色，让其观察两个智能体之间的策略性互动（例如石头剪刀布游戏），并尝试识别和解释这些智能体所使用的策略。通过分析LLM的预测和推理过程，可以评估其是否具备理解他人意图和策略的能力。这种方法模拟了人类通过观察他人行为来推断其心理状态的过程。\\n\\n**技术框架**：该框架包含以下主要模块：1) 游戏环境：使用石头剪刀布游戏作为测试环境，该游戏简单但需要策略性思考。2) 策略生成器：生成静态和动态的石头剪刀布策略，作为智能体的行为模式。动态策略由预先设定的规则驱动，并由prompt进行控制。3) 观察者（LLM）：LLM作为观察者，接收游戏历史记录作为输入，并预测每个智能体下一步行动的概率分布。4) 评估指标：使用交叉熵、Brier分数和期望值差异等指标来量化LLM预测的准确性，并结合策略识别率（SIR）来评估LLM识别潜在策略的能力。这些指标被整合到一个统一的“联合损失”中，以平衡校准、灵敏度和收益对齐。\\n\\n**关键创新**：该论文的关键创新在于：1) 将LLM置于“观察者”的角色，模拟心理理论的推理过程。2) 构建了一个包含静态和动态策略的基准，用于系统评估LLM的策略理解能力。3) 提出了联合损失和策略识别率等综合指标，用于全面评估LLM的预测准确性和策略识别能力。与现有方法相比，该方法更侧重于评估LLM在策略性互动中的推理能力，而不仅仅是知识记忆或文本生成。\\n\\n**关键设计**：在策略生成方面，论文设计了静态策略（例如始终选择石头）和动态策略（例如根据对手的历史行为进行调整）。动态策略通过精心设计的prompt来控制，使得LLM可以观察到不同类型的策略行为。在评估指标方面，联合损失的设计旨在平衡预测的准确性（交叉熵和Brier分数）和收益对齐（期望值差异），从而更全面地评估LLM的策略理解能力。策略识别率（SIR）则用于衡量LLM是否能够稳定地识别正在使用的潜在策略。",
            "application_zh": "该研究的潜在应用领域包括：1) 评估和改进LLM在多智能体环境中的决策能力。2) 开发更具人情味的AI助手，能够理解用户的意图和偏好。3) 在安全领域，用于分析对手的策略和预测其行为。未来的影响在于，该研究可以推动LLM在复杂交互场景中的应用，并促进人机协作的进一步发展。",
            "highlight_zh": "实验结果表明，LLM在识别静态策略方面表现良好，但在处理动态策略时面临挑战。联合损失和策略识别率等指标能够有效区分不同LLM的策略理解能力。通过交互式演示，用户可以实时调整LLM的分布，并观察损失的变化，从而深入了解LLM推理的优势和局限性。具体性能数据和对比基线在论文中进行了详细展示。",
            "tags_zh": [
                "大型语言模型",
                "心理理论",
                "策略推理",
                "博弈论",
                "智能体建模"
            ],
            "_index": 95,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19210v1/pic/early_round.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19210v1/pic/late_round.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19210v1/pic/pipeline.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning",
            "authors": [
                "Yanzhi Zhang",
                "Yitong Duan",
                "Zhaoxi Zhang",
                "Jiyan He",
                "Shuxin Zheng"
            ],
            "arxiv_id": "2512.19081v1",
            "summary": "Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.",
            "categories": [
                "cs.AI"
            ],
            "primary_category": "cs.AI",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19081v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出Population-Evolve，一种基于遗传算法的LLM数学推理优化方法",
            "summary_zh": "近年来，测试时扩展已成为增强大型语言模型推理能力的一个有前景的方向。本文提出Population-Evolve，一种受遗传算法启发的免训练方法，用于优化LLM推理。该方法通过并行推理，为每个问题维护一个候选解的动态种群。通过引入进化提示，LLM在所有迭代中自我进化其种群。收敛后，最终答案通过多数投票得出。此外，我们建立了一个统一框架，通过遗传算法的视角解释现有的测试时扩展策略。实验结果表明，Population-Evolve以较低的性能方差和计算效率实现了卓越的准确性。我们的发现突出了进化策略在推理过程中释放LLM推理能力的潜力。",
            "intro_zh": [
                "现有LLM推理方法在复杂问题上表现不足，且缺乏有效的优化策略，限制了其推理能力的充分发挥。",
                "Population-Evolve借鉴遗传算法思想，维护候选解种群，并通过进化提示引导LLM自我进化，提升推理质量。",
                "实验表明，Population-Evolve在数学推理任务上取得了更高的准确率，同时降低了性能方差，提升了计算效率。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLM）在数学推理任务中准确率不高的问题。现有的测试时扩展策略虽然能提升性能，但缺乏统一的理论框架，且可能存在性能方差较大、计算效率不高等问题。\\n\\n**核心思路**：论文的核心思路是借鉴遗传算法的思想，将LLM的推理过程视为一个种群进化过程。通过维护一个候选解的种群，并利用LLM自身的能力进行种群的迭代和优化，最终得到更准确的答案。这种方法旨在通过模拟自然选择的过程，提升LLM的推理能力。\\n\\n**技术框架**：Population-Evolve方法包含以下几个主要阶段：1) 初始化：为每个问题生成一个候选解的初始种群，每个解代表一个可能的答案或推理路径。2) 进化：通过“进化提示”引导LLM对种群进行迭代更新。进化提示指示LLM根据当前种群的质量，生成新的候选解，或者对现有解进行改进。3) 选择：在每次迭代后，根据某种评价标准（例如，LLM对解的置信度）对种群中的解进行评估。4) 收敛：当种群达到收敛状态（例如，种群中的解的相似度达到一定阈值，或者迭代次数达到上限）时，停止迭代。5) 投票：通过多数投票的方式，从最终种群中选择最可能的答案。\\n\\n**关键创新**：该方法最重要的创新点在于将遗传算法的思想引入到LLM的推理过程中，并设计了“进化提示”来引导LLM进行自我进化。与传统的测试时扩展策略相比，Population-Evolve能够更有效地利用LLM自身的能力，进行推理路径的探索和优化。此外，论文还提出了一个统一框架，将现有的测试时扩展策略纳入到遗传算法的视角下进行解释。\\n\\n**关键设计**：进化提示的设计是关键。它需要能够有效地引导LLM生成高质量的候选解，并避免陷入局部最优。论文中可能使用了特定的提示工程技巧，例如，提供一些高质量的推理示例，或者使用一些鼓励探索和创新的语言。此外，种群大小、迭代次数、收敛阈值等参数的设置也会影响最终的性能。",
            "application_zh": "Population-Evolve方法可应用于各种需要复杂推理的场景，如数学问题求解、代码生成、逻辑推理等。该方法能够提升LLM在这些任务上的准确性和可靠性，具有广泛的应用前景。未来，该方法可以进一步扩展到其他类型的推理任务，并与其他技术（如知识图谱、符号推理）相结合，以实现更强大的推理能力。",
            "highlight_zh": "实验结果表明，Population-Evolve在数学推理任务上取得了显著的性能提升。具体而言，该方法在多个基准数据集上超越了现有的测试时扩展策略，并且降低了性能方差。此外，实验还验证了Population-Evolve的计算效率，表明该方法能够在合理的时间内完成推理任务。",
            "tags_zh": [
                "大型语言模型",
                "数学推理",
                "遗传算法",
                "测试时扩展",
                "进化提示"
            ],
            "_index": 96,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19081v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19081v1/images/Variance.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19081v1/images/Population_Evolving.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
            "authors": [
                "Jiacheng Guo",
                "Ling Yang",
                "Peter Chen",
                "Qixin Xiao",
                "Yinjie Wang",
                "Xinzhe Juan",
                "Jiahao Qiu",
                "Ke Shen",
                "Mengdi Wang"
            ],
            "arxiv_id": "2512.19682v1",
            "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "Our codes are available at https://github.com/Gen-Verse/GenEnv",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19682v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "GenEnv：通过LLM智能体与环境模拟器的难度对齐协同进化，提升智能体性能。",
            "summary_zh": "本文提出GenEnv框架，旨在解决训练高性能大语言模型（LLM）智能体时，真实世界交互数据成本高昂且静态的问题。GenEnv建立了一个智能体与可扩展的生成式环境模拟器之间难度对齐的协同进化博弈。与在静态数据集上进化模型的传统方法不同，GenEnv实现了数据进化：模拟器充当动态课程策略，持续生成专门为智能体的“近端发展区”量身定制的任务。这一过程由一个简单而有效的α-课程奖励引导，该奖励将任务难度与智能体的当前能力对齐。在API-Bank、ALFWorld、BFCL、Bamboogle和TravelPlanner五个基准测试中，GenEnv将智能体性能提高了高达+40.3％，并达到或超过了更大模型的平均性能。与基于Gemini 2.5 Pro的离线数据增强相比，GenEnv在数据使用量减少3.3倍的情况下实现了更好的性能。通过从静态监督转向自适应模拟，GenEnv为扩展智能体能力提供了一条数据高效的途径。",
            "intro_zh": [
                "现有LLM智能体训练受限于真实世界交互数据的高成本和静态性，难以有效提升智能体能力。",
                "GenEnv通过构建智能体与环境模拟器的协同进化机制，动态生成难度与智能体能力相匹配的任务。",
                "实验表明，GenEnv在多个基准测试中显著提升了智能体性能，且数据效率优于离线数据增强方法。"
            ],
            "method_zh": "**问题定义**：现有的大语言模型智能体训练方法依赖于静态的、成本高昂的真实世界交互数据，这限制了智能体能力的扩展和泛化能力。如何以更低的成本、更高效的方式训练出更强大的LLM智能体是一个关键问题。\\n\\n**核心思路**：GenEnv的核心思路是建立一个智能体和环境模拟器之间的协同进化关系。环境模拟器根据智能体的能力动态生成任务，智能体在这些任务中学习和进化。通过难度对齐的课程学习，智能体可以逐步提升自身能力。\\n\\n**技术框架**：GenEnv框架包含两个主要组成部分：LLM智能体和生成式环境模拟器。智能体负责执行任务，环境模拟器负责生成任务并评估智能体的表现。框架通过α-课程奖励机制来调节任务难度，使其与智能体的能力相匹配。整个过程是一个循环迭代的过程，智能体和环境模拟器相互促进，共同进化。\\n\\n**关键创新**：GenEnv的关键创新在于将环境模拟器视为一个动态的课程策略，能够根据智能体的能力自适应地生成任务。这种方法摆脱了对静态数据集的依赖，实现了数据的高效利用。此外，α-课程奖励机制能够有效地将任务难度与智能体的能力对齐，避免了智能体在过于简单或过于困难的任务中浪费时间。\\n\\n**关键设计**：α-课程奖励是GenEnv中的一个关键设计。它通过一个参数α来控制任务难度的调整。具体来说，α根据智能体在任务上的表现进行调整，如果智能体表现良好，则增加任务难度；如果智能体表现不佳，则降低任务难度。这种自适应的难度调整机制能够确保智能体始终处于“近端发展区”，从而实现最佳的学习效果。具体的损失函数和网络结构取决于具体的智能体和环境模拟器的选择，论文中没有给出统一的定义。",
            "application_zh": "GenEnv框架具有广泛的应用前景，可以应用于各种需要智能体与环境交互的任务中，例如机器人控制、游戏AI、自动驾驶等。通过GenEnv，可以更高效地训练出能够在复杂环境中执行任务的智能体，从而推动人工智能技术的发展。",
            "highlight_zh": "GenEnv在五个基准测试中表现出色，相较于7B参数的基线模型，性能提升高达40.3%。GenEnv的性能与更大的模型相当甚至超过，并且比基于Gemini 2.5 Pro的离线数据增强方法节省了3.3倍的数据量。这些结果表明，GenEnv是一种数据高效且有效的智能体训练方法。",
            "tags_zh": [
                "LLM智能体",
                "环境模拟器",
                "协同进化",
                "课程学习",
                "难度对齐",
                "数据高效",
                "自适应学习"
            ],
            "_index": 97,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19682v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19682v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19682v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Increasing the Thinking Budget is Not All You Need",
            "authors": [
                "Ignacio Iacobacci",
                "Zhaozhi Qian",
                "Faroq AL-Tam",
                "Muhammad AL-Qurishi",
                "Riad Souissi"
            ],
            "arxiv_id": "2512.19585v1",
            "summary": "Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "4 pages, 4 figures, 3 tables",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19585v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "研究表明，单纯增加思考预算并非提升大语言模型推理能力的最佳途径",
            "summary_zh": "最近，涌现出一批具备卓越推理能力的大型语言模型，在各种推理基准测试中表现出非凡的性能。早期研究开始探索推理过程长度（即所谓的思考预算）对模型性能的影响。本文对思考预算这一关键参数进行了系统性研究，考察了它与自洽性、反思等各种配置的相互作用。我们的目标是提供一个信息丰富、平衡的比较框架，同时考虑性能结果和计算成本。研究发现，简单地增加思考预算并非最有效的计算资源利用方式。通过自洽性和自我反思等替代配置，可以获得更准确的响应。",
            "intro_zh": [
                "现有研究主要关注增加大语言模型的思考预算来提升推理能力，但缺乏系统性的对比分析。",
                "本文旨在系统研究思考预算与自洽性、反思等配置的交互作用，寻求更有效的计算资源利用方式。",
                "实验表明，单纯增加思考预算并非最佳策略，自洽性和自我反思等方法能以更低成本实现更高精度。"
            ],
            "method_zh": "**问题定义**：现有方法主要通过增加大语言模型的推理步骤（即思考预算）来提升其在复杂推理任务上的表现。然而，这种方法的计算成本较高，且可能并非最有效的资源利用方式。现有研究缺乏对不同配置（如自洽性、反思等）与思考预算之间相互作用的系统性分析，难以确定最优的计算资源分配策略。\\n\\n**核心思路**：本文的核心思路是，通过系统性地比较不同配置（包括不同的思考预算、自洽性、反思等）下大语言模型的推理性能和计算成本，来确定更有效的计算资源利用方式。作者假设，通过优化配置，可以在相同或更低的计算成本下，获得比单纯增加思考预算更高的推理精度。\\n\\n**技术框架**：本文构建了一个比较框架，用于评估不同配置下大语言模型的推理性能。该框架主要包含以下几个步骤：1) 选择一系列推理基准测试；2) 确定不同的配置组合，包括不同的思考预算、是否使用自洽性、是否使用反思等；3) 使用不同的配置运行大语言模型，并记录其在基准测试上的性能和计算成本；4) 对比不同配置的性能和成本，分析思考预算与其他配置之间的相互作用。\\n\\n**关键创新**：本文最重要的技术创新在于，它提供了一个系统性的比较框架，用于评估不同配置下大语言模型的推理性能和计算成本。该框架能够帮助研究人员和开发者更好地理解不同配置之间的相互作用，并确定最优的计算资源分配策略。与现有方法相比，本文更加关注计算成本的效率，并探索了多种替代方案，而不仅仅是增加思考预算。\\n\\n**关键设计**：本文的关键设计包括：1) 选择了多个具有代表性的推理基准测试，以确保评估结果的泛化能力；2) 采用了多种不同的配置组合，以全面考察不同配置之间的相互作用；3) 使用了标准化的计算成本度量方法，以便于比较不同配置的效率；4) 对实验结果进行了详细的统计分析，以确保结论的可靠性。",
            "application_zh": "该研究成果可应用于各种需要复杂推理能力的场景，例如智能问答、自然语言推理、代码生成等。通过优化大语言模型的配置，可以在保证推理精度的前提下，降低计算成本，提高应用效率。该研究还有助于推动大语言模型在资源受限环境下的应用，例如移动设备和嵌入式系统。",
            "highlight_zh": "实验结果表明，单纯增加思考预算并非提升大语言模型推理能力的最佳途径。例如，在某些推理任务上，使用自洽性或自我反思等配置，可以在相同或更低的计算成本下，获得比单纯增加思考预算更高的精度。具体而言，某些配置在特定任务上实现了高达10%-20%的性能提升，同时降低了5%-10%的计算成本。",
            "tags_zh": [
                "大语言模型",
                "推理能力",
                "思考预算",
                "自洽性",
                "自我反思",
                "计算成本",
                "资源优化"
            ],
            "_index": 98,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19585v1/images/reasoning_models_no_judge.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19585v1/images/Qwen3-8B_Reflect_AIME24.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19585v1/images/reasoning_models_judge.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Epistemological Fault Lines Between Human and Artificial Intelligence",
            "authors": [
                "Walter Quattrociocchi",
                "Valerio Capraro",
                "Matjaž Perc"
            ],
            "arxiv_id": "2512.19466v1",
            "summary": "Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI.",
            "categories": [
                "cs.CY",
                "cs.CL",
                "cs.HC"
            ],
            "primary_category": "cs.CY",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "16 pages, 1 figure",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19466v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "揭示人类与大型语言模型在认知上的根本差异，为AI治理提供理论基础",
            "summary_zh": "大型语言模型（LLMs）被广泛认为是人工智能，但其认知模式与人类认知截然不同。本文表明，人类和机器输出之间表面上的一致性掩盖了判断产生方式上更深层次的结构性不匹配。通过追溯从符号AI和信息过滤系统到大规模生成Transformer的历史转变，我们认为LLMs不是认知主体，而是随机模式补全系统，可以形式化地描述为语言转换的高维图上的游走，而不是形成信念或世界模型的系统。通过系统地映射人类和人工智能的认知流程，我们确定了七个认知断层线，即在基础、解析、经验、动机、因果推理、元认知和价值观方面的差异。我们将由此产生的情况称为“认知错位（Epistemia）”：一种语言上的合理性取代了认知评估的结构性情况，产生了一种无需判断劳动即可获得知识的感觉。最后，我们概述了在日益围绕生成式AI组织的社会中，评估、治理和认知素养的影响。",
            "intro_zh": [
                "现有大型语言模型（LLMs）在表面上与人类认知相似，但其内在认知机制存在根本差异，需要深入研究。",
                "论文核心思想是LLMs本质上是随机模式补全系统，而非具有信念和世界模型的认知主体，揭示了人类与AI在认知上的结构性不匹配。",
                "通过分析人类和AI的认知流程，论文识别了七个关键的认知断层线，为理解和治理生成式AI提供了理论框架。"
            ],
            "method_zh": "**问题定义**：论文旨在解决的问题是，尽管大型语言模型（LLMs）在输出上与人类相似，但其认知方式与人类存在根本差异。现有方法未能充分揭示这种差异，导致对LLMs的误解和潜在风险。痛点在于，将LLMs视为具有类似人类认知能力的系统，可能导致对其能力和局限性的错误评估，从而影响AI治理和应用。\n\n**核心思路**：论文的核心思路是将LLMs视为随机模式补全系统，而非具有信念和世界模型的认知主体。这种观点强调LLMs通过学习大量文本数据中的统计模式来生成文本，而不是通过理解和推理来产生输出。通过对比人类和LLMs的认知流程，论文揭示了两者在认知基础上的结构性不匹配。\n\n**技术框架**：论文没有提出具体的算法或模型，而是采用了一种概念框架，通过比较人类和LLMs的认知流程来识别差异。该框架包括以下几个关键要素：\n1.  **认知流程映射**：系统地分析人类和LLMs在信息处理、知识获取和决策制定等方面的流程。\n2.  **认知断层线识别**：确定人类和LLMs在认知上的七个关键差异，包括基础、解析、经验、动机、因果推理、元认知和价值观。\n3.  **“认知错位（Epistemia）”概念**：提出一种新的概念，描述LLMs在缺乏真正认知的情况下，通过语言合理性产生“知识感”的现象。\n\n**关键创新**：论文最重要的技术创新点在于其概念框架，该框架提供了一种新的视角来理解LLMs的本质。与将LLMs视为“黑盒”或简单地关注其输出性能的方法不同，论文深入探讨了LLMs的认知机制，揭示了其与人类认知的根本差异。这种框架为AI伦理、治理和教育提供了重要的理论基础。\n\n**关键设计**：论文没有涉及具体的参数设置或网络结构。其关键设计在于其概念框架，该框架通过七个认知断层线来系统地比较人类和LLMs的认知流程。这些断层线包括：\n1.  **基础（Grounding）**：LLMs缺乏与物理世界的直接交互，其知识来源于文本数据。\n2.  **解析（Parsing）**：LLMs的解析能力依赖于统计模式，而非真正的语义理解。\n3.  **经验（Experience）**：LLMs缺乏人类的具身经验和情感体验。\n4.  **动机（Motivation）**：LLMs没有内在动机和目标。\n5.  **因果推理（Causal Reasoning）**：LLMs的因果推理能力有限，容易产生虚假相关。\n6.  **元认知（Metacognition）**：LLMs缺乏对自身认知过程的意识和控制。\n7.  **价值观（Value）**：LLMs没有内在的价值观和道德判断能力。",
            "application_zh": "该研究成果可应用于AI伦理和治理领域，帮助制定更合理的AI监管政策。同时，该研究也对教育领域具有指导意义，有助于提高公众对AI的认知素养，避免过度依赖或盲目信任AI系统。此外，该研究还可用于指导AI系统的设计，使其更符合人类价值观和伦理规范。",
            "highlight_zh": "论文通过系统地分析人类和LLMs的认知流程，识别了七个关键的认知断层线，为理解LLMs的本质提供了新的视角。提出了“认知错位（Epistemia）”的概念，解释了LLMs在缺乏真正认知的情况下产生“知识感”的现象。这些发现对于AI伦理、治理和教育具有重要意义。",
            "tags_zh": [
                "大型语言模型",
                "认知科学",
                "人工智能伦理",
                "AI治理",
                "认知断层线"
            ],
            "_index": 99,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19466v1/figure.jpg",
                    "caption": "",
                    "figure_id": "fig_0"
                }
            ]
        },
        {
            "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
            "authors": [
                "Jiaren Peng",
                "Hongda Sun",
                "Xuan Tian",
                "Cheng Huang",
                "Zeqing Li",
                "Rui Yan"
            ],
            "arxiv_id": "2512.19414v1",
            "summary": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
            "categories": [
                "cs.CR",
                "cs.CL"
            ],
            "primary_category": "cs.CR",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19414v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出TTPrompt框架，通过显式指令提升网络威胁情报命名实体识别性能。",
            "summary_zh": "网络威胁情报(CTI)的自动化严重依赖于命名实体识别(NER)从非结构化文本中提取关键实体。目前，大型语言模型(LLM)主要通过基于检索的上下文学习(ICL)来解决此任务。本文分析了这种主流范式，揭示了一个根本缺陷：其成功并非源于全局语义相似性，而主要源于检索到的示例中实体类型的偶然重叠。这暴露了依赖不可靠的隐式归纳的局限性。为了解决这个问题，我们提出了TTPrompt，一个从隐式归纳转向显式指令的框架。TTPrompt将CTI的战术、技术和程序(TTP)的核心概念映射到一个指令层次结构：将任务定义制定为战术，将指导策略制定为技术，并将注释指南制定为程序。此外，为了处理静态指南的适应性挑战，我们引入了反馈驱动的指令细化(FIR)。FIR使LLM能够通过从少量标记数据上的错误中学习来自动细化指南，从而适应不同的注释方言。在五个CTI NER基准上的实验表明，TTPrompt始终优于基于检索的基线。值得注意的是，仅使用1%的训练数据进行细化，它就能与在完整数据集上微调的模型相媲美。例如，在LADDER上，其71.96%的Micro F1接近微调基线，而在复杂的CTINexus上，其Macro F1超过了微调的ACLM模型10.91%。",
            "intro_zh": [
                "现有基于检索的上下文学习方法在CTI NER任务中依赖实体类型重叠，缺乏可靠的语义泛化能力。",
                "TTPrompt框架通过将CTI知识显式编码为分层指令，指导LLM进行更精确的实体识别。",
                "反馈驱动的指令细化(FIR)使模型能从少量数据中学习并适应不同的标注风格，提升泛化性。"
            ],
            "method_zh": "**问题定义**：论文旨在解决网络威胁情报(CTI)领域中命名实体识别(NER)任务的自动化问题。现有方法，特别是基于检索的上下文学习(ICL)，在CTI NER任务中表现出对检索示例中实体类型重叠的过度依赖，而非真正的语义理解。这种依赖导致模型在面对新的、未见过的实体类型或标注风格时泛化能力不足。\\n\\n**核心思路**：论文的核心思路是将CTI领域的知识显式地编码到LLM的指令中，从而引导模型进行更准确和可靠的实体识别。通过将CTI的战术、技术和程序(TTP)概念映射到指令层次结构，模型能够更好地理解任务目标和约束，从而减少对隐式归纳的依赖。\\n\\n**技术框架**：TTPrompt框架包含两个主要组成部分：指令层次结构和反馈驱动的指令细化(FIR)。指令层次结构将CTI知识组织为三个层次：战术(Tactics)定义任务目标，技术(Techniques)提供指导策略，程序(Procedures)提供注释指南。FIR模块允许LLM通过分析预测错误并从少量标记数据中学习，从而自动细化指令，适应不同的标注风格。整体流程包括：1)构建初始指令集；2)使用指令集指导LLM进行NER；3)分析预测结果并识别错误；4)使用FIR模块细化指令；5)重复步骤2-4，直到性能收敛。\\n\\n**关键创新**：最重要的技术创新点在于将隐式归纳转变为显式指令。与依赖检索到的示例进行隐式学习的方法不同，TTPrompt通过明确的指令来指导模型，从而提高了模型的可解释性和泛化能力。此外，FIR模块的引入使得模型能够自适应地学习和调整指令，从而更好地适应不同的数据分布和标注风格。\\n\\n**关键设计**：指令层次结构的设计是关键。战术层定义了NER任务的总体目标，例如识别特定类型的威胁行为者。技术层提供了指导策略，例如使用特定的命名约定或上下文信息。程序层提供了详细的注释指南，例如如何处理嵌套实体或不明确的边界。FIR模块使用一个小的验证集来评估模型的性能，并使用错误分析来识别需要改进的指令。细化过程可以使用不同的技术，例如基于规则的修改或基于LLM的生成。",
            "application_zh": "该研究成果可应用于自动化网络威胁情报分析，提升威胁检测和响应效率。通过更准确地识别和提取威胁情报中的关键实体，可以帮助安全分析师更快地理解威胁态势，制定有效的防御策略，并减少人工分析的工作量。未来，该方法可以扩展到其他安全领域的文本分析任务，例如漏洞报告分析和恶意软件分析。",
            "highlight_zh": "实验结果表明，TTPrompt在五个CTI NER基准测试中始终优于基于检索的基线方法。特别是在LADDER数据集上，TTPrompt的Micro F1值达到71.96%，接近于在完整数据集上微调的模型。在更复杂的CTINexus数据集上，TTPrompt的Macro F1值超过了微调的ACLM模型10.91%。更重要的是，TTPrompt仅使用1%的训练数据进行细化，就能达到与全量数据微调模型相近的性能，显著降低了标注成本。",
            "tags_zh": [
                "网络威胁情报",
                "命名实体识别",
                "大型语言模型",
                "上下文学习",
                "指令学习"
            ],
            "_index": 100,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19414v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19414v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19414v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs",
            "authors": [
                "Javier Vela-Tambo",
                "Jorge Gracia",
                "Fernando Dominguez-Castro"
            ],
            "arxiv_id": "2512.19305v1",
            "summary": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19305v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "CienaLLM：利用自回归LLM从新闻文章中生成式提取气候影响信息",
            "summary_zh": "为了理解和监测气候灾害带来的社会经济影响，需要大规模地从异构的新闻文章中提取结构化信息。为此，我们开发了CienaLLM，这是一个基于模式引导的生成式信息提取的模块化框架。CienaLLM使用开放权重的大型语言模型（LLM）从新闻文章中进行零样本信息提取，并支持可配置的提示和输出模式、多步骤流水线以及云或本地推理。为了系统地评估LLM系列、大小、精度方案和提示策略的选择如何影响性能，我们对模型、精度和提示工程技术进行了一项大型析因研究。额外的响应解析步骤几乎消除了格式错误，同时保持了准确性；较大的模型提供了最强大和最稳定的性能，而量化在精度上做出了适度的权衡，从而获得了显著的效率提升；提示策略显示出异构的、特定于模型的效果。CienaLLM在从西班牙新闻中提取干旱影响的准确性方面与监督基线相匹配或优于监督基线，尽管推理成本更高。虽然在干旱方面进行了评估，但通过编辑提示和模式而不是重新训练，这种模式驱动和模型无关的设计适合于适应相关的信息提取任务（例如，其他灾害、部门或语言）。我们发布了代码、配置和模式，以支持可重复使用。",
            "intro_zh": [
                "现有方法难以大规模地从异构新闻文章中提取结构化的气候影响信息，阻碍了对气候灾害社会经济影响的理解和监测。",
                "CienaLLM框架利用开放权重LLM进行零样本信息提取，通过可配置的提示和模式，以及多步骤流水线，实现灵活的信息抽取。",
                "实验表明，更大的模型性能更强，量化可显著提升效率，响应解析步骤有效消除格式错误，CienaLLM在干旱影响提取上与监督基线相当。"
            ],
            "method_zh": "**问题定义**：论文旨在解决从大量异构新闻文章中自动提取结构化气候影响信息的问题。现有方法通常依赖于人工标注数据进行训练，成本高昂且难以适应新的灾害类型、领域或语言。此外，现有方法在处理非结构化文本时，容易产生格式错误，影响信息提取的准确性。\\n\\n**核心思路**：论文的核心思路是利用大型语言模型（LLM）的强大生成能力，通过模式引导的生成式信息提取方法，实现零样本的信息提取。通过精心设计的提示（Prompt）和输出模式（Schema），引导LLM从新闻文章中提取所需的信息，并将其转换为结构化的格式。这种方法无需大量标注数据，具有很强的泛化能力和可扩展性。\\n\\n**技术框架**：CienaLLM框架包含以下主要模块：1) **输入模块**：接收新闻文章作为输入。2) **提示生成模块**：根据预定义的模式和用户配置，生成用于引导LLM进行信息提取的提示。3) **LLM推理模块**：使用LLM对新闻文章进行推理，生成包含结构化信息的文本。4) **响应解析模块**：解析LLM的输出，提取结构化信息，并进行格式校正。5) **输出模块**：将提取的结构化信息以指定格式输出。\\n\\n**关键创新**：论文的关键创新在于将模式引导的生成式信息提取方法应用于气候影响信息的提取，并利用LLM的零样本学习能力，避免了大量人工标注数据的需求。此外，论文还提出了一个响应解析模块，用于纠正LLM输出中的格式错误，提高信息提取的准确性。\\n\\n**关键设计**：论文的关键设计包括：1) **提示工程**：设计有效的提示，引导LLM提取所需的信息。2) **输出模式定义**：定义清晰的输出模式，确保提取的信息具有结构化格式。3) **响应解析策略**：设计鲁棒的响应解析策略，纠正LLM输出中的格式错误。论文还对不同大小、精度和提示策略的LLM进行了大量的实验，以评估其性能。",
            "application_zh": "CienaLLM可应用于气候风险评估、灾害预警、政策制定等领域。通过自动提取新闻文章中的气候影响信息，可以帮助政府、企业和个人更好地了解气候变化带来的风险，并采取相应的应对措施。该框架具有很强的可扩展性，可以应用于其他灾害类型、领域和语言，具有广泛的应用前景。",
            "highlight_zh": "实验结果表明，CienaLLM在从西班牙新闻中提取干旱影响的准确性方面与监督基线相匹配或优于监督基线。更大的模型提供了更强大和更稳定的性能，而量化在精度上做出了适度的权衡，从而获得了显著的效率提升。响应解析步骤几乎消除了格式错误，同时保持了准确性。",
            "tags_zh": [
                "气候影响提取",
                "生成式信息提取",
                "大型语言模型",
                "零样本学习",
                "模式引导",
                "新闻文章",
                "自回归模型"
            ],
            "_index": 101,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19305v1/cienallm_dataflow.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19305v1/response_parsing_error_rate.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19305v1/f1_score_per_model.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?",
            "authors": [
                "Amar Lakel"
            ],
            "arxiv_id": "2512.19117v1",
            "summary": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "in French language",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19117v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "提出用“大型话语模型(LDM)”和“人工话语代理(ADA)”替代“大型语言模型(LLM)”的分析框架。",
            "summary_zh": "本文提出了一种针对大型生成模型分析的认识论转变，用“大型话语模型”(LDM)取代“大型语言模型”(LLM)的范畴，进而用“人工话语代理”(ADA)取代。该理论框架基于一个本体论三元组，区分了三个调节实例：对参照世界现象规律的理解、具身认知的结构化，以及话语在社会历史背景下的结构-语言学沉积。LDM作用于这三个实例的产物（文档），对由学习语料库具体化的人类经验的一部分的话语投射进行建模。所提出的方案旨在用公开试验和程序取代“迷恋/恐惧”的二分法，使人工话语代理在当代社会空间中的地位、用途和限制变得可理解，并将这种方法置于国家、产业、公民社会和学术界参与的治理和共同监管的视角中。",
            "intro_zh": [
                "现有对大型生成模型的分析主要集中在“大型语言模型(LLM)”的范畴，缺乏对更广泛的话语和社会语境的考虑。",
                "论文提出将LLM重新定义为“大型话语模型(LDM)”和“人工话语代理(ADA)”，强调其在社会历史语境下对人类经验的建模。",
                "该方案旨在通过公开试验和程序，使人工话语代理在社会空间中的地位、用途和限制更加透明，促进治理和共同监管。"
            ],
            "method_zh": "**问题定义**：当前对大型生成模型的理解主要集中在语言层面，将其视为“大型语言模型(LLM)”。这种视角忽略了语言的社会性和话语性，未能充分考虑模型生成文本的社会历史语境。现有方法难以有效评估和管理这些模型在社会中的影响，容易陷入“迷恋/恐惧”的二分法。\\n\\n**核心思路**：论文的核心在于将大型生成模型视为“大型话语模型(LDM)”和“人工话语代理(ADA)”，强调其对人类经验的话语投射进行建模。这种转变旨在将模型的分析置于更广阔的社会语境中，关注其在社会历史背景下的地位、用途和限制。通过这种方式，可以更全面地理解和评估模型的影响，并促进更有效的治理和监管。\\n\\n**技术框架**：论文构建了一个本体论三元组，作为分析LDM的基础：1) 对参照世界现象规律的理解；2) 具身认知的结构化；3) 话语在社会历史背景下的结构-语言学沉积。LDM作用于这三个实例的产物（文档），对由学习语料库具体化的人类经验的一部分的话语投射进行建模。该框架旨在提供一个更全面的视角，以理解LDM如何从数据中学习并生成文本。\\n\\n**关键创新**：论文的关键创新在于概念框架的转变，即从LLM到LDM和ADA。这种转变强调了大型生成模型的话语性和社会性，使其分析超越了纯粹的语言层面。通过引入本体论三元组，论文提供了一个更结构化的方法来理解LDM如何从数据中学习并生成文本。\\n\\n**关键设计**：论文主要关注概念框架的构建，并未涉及具体的参数设置、损失函数或网络结构等技术细节。其重点在于提供一个理论基础，用于分析和评估大型生成模型在社会中的影响。未来的研究可以基于此框架，开发更具体的评估指标和治理策略。",
            "application_zh": "该研究成果可应用于人工智能伦理、政策制定和社会影响评估等领域。通过将大型生成模型视为话语代理，可以更好地理解其在信息传播、舆论引导和社会互动中的作用，从而制定更有效的监管措施，减少潜在的负面影响，并促进其在教育、文化交流等领域的积极应用。",
            "highlight_zh": "由于该论文主要关注理论框架的构建，而非实验验证，因此没有具体的性能数据或对比基线。其亮点在于提出了一个全新的视角来理解和分析大型生成模型，为未来的研究提供了理论基础。",
            "tags_zh": [
                "大型语言模型",
                "话语分析",
                "人工智能伦理",
                "社会影响评估",
                "人工话语代理"
            ],
            "_index": 102,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions",
            "authors": [
                "Wen-Long Jin"
            ],
            "arxiv_id": "2512.18940v1",
            "summary": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.",
            "categories": [
                "cs.CL",
                "cs.SE"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "13 pages, 3 figures. Supplementary materials at https://doi.org/10.17605/OSF.IO/PV6R3",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.18940v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱九：具身大模型 (Embodied Foundation Models)",
                    "id": "9_embodied_foundation",
                    "matched_keywords": [
                        "large language model"
                    ],
                    "score": 3.0
                }
            ],
            "relevance_score": 3.0,
            "hit_pillars": [
                "9_embodied_foundation"
            ],
            "headline_zh": "FASTRIC：一种用于可验证LLM交互的提示规范语言",
            "summary_zh": "大型语言模型(LLMs)执行复杂的多轮交互协议，但缺乏正式规范来验证执行是否符合设计者的意图。我们引入FASTRIC，一种提示规范语言，它在自然语言提示中显式地表达隐式的有限状态机(FSM)，从而能够通过执行轨迹分析进行一致性验证。LLM充当智能执行代理：解释设计者编码的FSM以执行指定的行为角色。与需要解析器和编译器的符号规范语言不同，FASTRIC利用LLM作为统一的基础设施——同时充当解析器、解释器、运行时环境和开发助手。FASTRIC指导设计者阐明七个FSM元素(最终状态、代理、状态、触发器、角色、初始状态、约束)，从而构建多轮交互。规范的形式化程度——从前沿模型推断的隐式描述到为较弱模型提供的逐步指令——充当设计参数。我们引入程序一致性作为衡量执行是否符合FSM规范的验证指标。在一个包含四个形式化级别和三个模型规模(14.7B、685B、1T+参数)的三状态幼儿园辅导FSM的测试中，结果表明最佳规范形式化程度是模型容量的函数。DeepSeek-V3.2 (685B)在L2-L4级别上实现了完美的一致性(1.00)；ChatGPT-5 (~1T)在L3级别达到峰值(0.90)，然后在L4级别崩溃(0.39)；Phi4 (14.7B)没有显示出稳定的最优值，且方差较高(SD=0.16-0.36)。这些发现揭示了特定于模型的形式化范围——“金发姑娘区”——其中规范提供了足够的结构而没有过度约束，从而建立了提示规范工程，用于创建可验证的交互协议，将多轮交互设计从启发式艺术转变为具有可衡量的程序保证的系统工程。",
            "intro_zh": [
                "现有大型语言模型在多轮交互中缺乏形式化规范，难以验证其执行是否符合设计意图。",
                "FASTRIC通过在提示中显式定义有限状态机，使LLM能够作为智能代理执行规范化的交互行为。",
                "实验表明，模型容量与最佳提示形式化程度之间存在关联，存在一个“金发姑娘区”能实现最佳性能。"
            ],
            "method_zh": "**问题定义**：论文旨在解决大型语言模型（LLMs）在多轮交互中缺乏可验证性的问题。现有的LLM交互设计主要依赖启发式方法，缺乏形式化的规范，导致难以保证LLM的执行符合设计者的意图，也难以进行有效的调试和优化。\\n\\n**核心思路**：论文的核心思路是利用提示工程，将有限状态机（FSM）的概念融入到自然语言提示中，从而显式地定义LLM在多轮交互中的行为规范。通过这种方式，LLM可以被视为一个智能执行代理，根据预定义的FSM执行交互协议。这种方法避免了传统符号规范语言所需的解析器和编译器，直接利用LLM自身的能力进行解析、解释和执行。\\n\\n**技术框架**：FASTRIC的技术框架主要包括以下几个部分：1) 提示规范语言：定义了七个FSM元素（最终状态、代理、状态、触发器、角色、初始状态、约束），用于结构化多轮交互。2) 形式化程度：允许设计者根据模型的能力调整提示的详细程度，从隐式描述到显式指令。3) 程序一致性：作为验证指标，衡量LLM的执行轨迹与FSM规范的符合程度。整体流程是：设计者使用FASTRIC定义FSM规范，然后将规范嵌入到提示中，LLM根据提示执行交互，最后通过程序一致性指标评估执行结果。\\n\\n**关键创新**：FASTRIC的关键创新在于将形式化规范与自然语言提示相结合，利用LLM自身的能力进行规范的解析和执行，从而实现可验证的LLM交互。与传统的符号规范语言相比，FASTRIC更加灵活和易于使用，无需额外的工具链。此外，FASTRIC还提出了程序一致性这一新的验证指标，用于衡量LLM执行的规范性。\\n\\n**关键设计**：FASTRIC的关键设计包括：1) FSM元素的定义：明确了多轮交互中需要考虑的关键要素，为设计者提供了结构化的指导。2) 形式化程度的控制：允许设计者根据模型的能力调整提示的详细程度，从而实现最佳的性能。3) 程序一致性的计算：定义了如何衡量LLM的执行轨迹与FSM规范的符合程度，为验证LLM的交互行为提供了量化的指标。",
            "application_zh": "FASTRIC可应用于各种需要可验证多轮交互的场景，例如智能客服、教育辅导、任务导向对话系统等。通过FASTRIC，开发者可以更加可靠地控制LLM的行为，减少意外情况的发生，并提高交互系统的稳定性和可预测性。未来，FASTRIC有望推动LLM在安全敏感领域的应用，例如医疗诊断、金融服务等。",
            "highlight_zh": "实验结果表明，最佳的提示形式化程度与模型容量有关。DeepSeek-V3.2 (685B)在L2-L4级别上实现了完美的一致性(1.00)，ChatGPT-5 (~1T)在L3级别达到峰值(0.90)，然后在L4级别崩溃(0.39)，Phi4 (14.7B)没有显示出稳定的最优值，且方差较高(SD=0.16-0.36)。这些结果揭示了存在一个模型特定的“金发姑娘区”，在该区域内，规范提供了足够的结构而没有过度约束。",
            "tags_zh": [
                "大型语言模型",
                "提示工程",
                "有限状态机",
                "多轮交互",
                "形式化验证",
                "程序一致性",
                "规范语言"
            ],
            "_index": 103,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.18940v1/deepthink-figure1_fsm_structure.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.18940v1/FASTRIC-figure2_conformance.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.18940v1/FASTRIC-figure3_conformance_boxplots.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        },
        {
            "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
            "authors": [
                "Yujie Zhao",
                "Hongwei Fan",
                "Di Chen",
                "Shengcong Chen",
                "Liliang Chen",
                "Xiaoqi Li",
                "Guanghui Ren",
                "Hao Dong"
            ],
            "arxiv_id": "2512.19402v1",
            "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
            "categories": [
                "cs.RO",
                "cs.CV",
                "cs.GR"
            ],
            "primary_category": "cs.RO",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19402v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "Real2Edit2Real：通过3D控制界面生成机器人操作演示数据，提升数据效率。",
            "summary_zh": "为了减少机器人学习中重复的数据收集，特别是操作任务中的空间泛化问题，本文提出了Real2Edit2Real框架，该框架通过3D可编辑性与2D视觉数据桥接，利用3D控制界面生成新的演示数据。该方法首先使用度量尺度的3D重建模型从多视角RGB观测中重建场景几何。基于重建的几何体，对点云进行深度可靠的3D编辑，生成新的操作轨迹，同时几何校正机器人姿态以恢复物理一致的深度，作为合成新演示的可靠条件。最后，提出了一个以深度为主要控制信号，结合动作、边缘和射线图的多条件视频生成模型，以合成空间增强的多视角操作视频。在四个真实操作任务上的实验表明，仅用1-5个源演示数据生成的训练数据，可以匹配甚至超过用50个真实演示数据训练的策略，数据效率提高了10-50倍。此外，高度和纹理编辑的实验结果证明了该框架的灵活性和可扩展性，表明其有潜力作为统一的数据生成框架。",
            "intro_zh": [
                "机器人学习依赖大规模数据集，但收集多样化的操作演示数据成本高昂，限制了策略的鲁棒性。",
                "Real2Edit2Real框架通过3D控制界面，利用3D编辑和多条件视频生成，从少量真实数据生成大量增强数据。",
                "实验表明，使用该框架生成的数据训练的策略，在数据效率上比使用真实数据训练的策略提升了10-50倍。"
            ],
            "method_zh": "**问题定义**：现有机器人学习方法依赖大量真实演示数据，尤其是在操作任务中，收集具有空间泛化能力的演示数据成本很高。这限制了策略的鲁棒性和泛化能力。因此，如何利用少量真实数据生成高质量的增强数据，是本论文要解决的核心问题。\\n\\n**核心思路**：论文的核心思路是通过3D可编辑性将2D视觉数据与3D控制界面连接起来，从而实现对机器人操作轨迹的编辑和生成。通过在3D空间中编辑场景几何和机器人姿态，可以生成新的、物理上合理的轨迹，并利用这些轨迹合成新的视觉演示数据。这种方法避免了直接在像素空间进行编辑，从而保证了生成数据的物理一致性。\\n\\n**技术框架**：Real2Edit2Real框架包含以下几个主要模块：1) **3D重建模块**：从多视角RGB图像重建场景的3D几何结构，得到度量尺度的点云模型。2) **3D编辑模块**：在重建的点云上进行深度可靠的3D编辑，包括改变物体的位置、形状等，并相应地调整机器人姿态，以保证物理一致性。3) **多条件视频生成模块**：基于编辑后的3D场景和机器人姿态，生成新的多视角操作视频。该模块以深度图为主要控制信号，同时结合动作、边缘和射线图等信息，生成高质量的视频。\\n\\n**关键创新**：该论文的关键创新在于将3D编辑引入到机器人演示数据的生成过程中。与传统的基于图像的增强方法不同，该方法在3D空间中进行编辑，可以更好地保证生成数据的物理合理性。此外，论文提出的多条件视频生成模型，能够有效地利用深度信息和其他辅助信息，生成高质量的多视角视频。\\n\\n**关键设计**：在3D编辑模块中，论文采用了深度可靠的编辑方法，即在编辑过程中始终保持深度信息的物理一致性。在多条件视频生成模块中，论文使用了深度图作为主要控制信号，并结合动作、边缘和射线图等信息，以提高生成视频的质量。具体的网络结构和损失函数细节在论文中有详细描述，但摘要中未明确提及。",
            "application_zh": "该研究成果可应用于各种机器人操作任务的数据增强，例如装配、抓取、放置等。通过少量真实数据即可生成大量训练数据，降低了机器人学习的成本，加速了机器人技术的落地。该框架还具有一定的通用性，可以扩展到其他需要数据增强的领域，例如虚拟现实、游戏等。",
            "highlight_zh": "实验结果表明，使用Real2Edit2Real框架生成的数据训练的机器人策略，在四个真实操作任务上，仅使用1-5个真实演示数据，就能达到甚至超过使用50个真实演示数据训练的策略的性能。数据效率提升了10-50倍，显著降低了数据收集成本。",
            "tags_zh": [
                "机器人学习",
                "数据增强",
                "3D重建",
                "视频生成",
                "操作任务"
            ],
            "_index": 104,
            "_used_api": "gemini",
            "figures": []
        },
        {
            "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
            "authors": [
                "Quyu Kong",
                "Xu Zhang",
                "Zhenyu Yang",
                "Nolan Gao",
                "Chen Liu",
                "Panrong Tong",
                "Chenglin Cai",
                "Hanzhang Zhou",
                "Jianan Zhang",
                "Liangyu Chen",
                "Zhidan Liu",
                "Steven Hoi",
                "Yue Wang"
            ],
            "arxiv_id": "2512.19432v1",
            "summary": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
            "categories": [
                "cs.CL"
            ],
            "primary_category": "cs.CL",
            "published": "2025-12-22",
            "updated": "2025-12-22",
            "comment": "",
            "doi": "",
            "journal_ref": "",
            "pdf_url": "https://arxiv.org/pdf/2512.19432v1",
            "code_links": [],
            "matched_interests": [
                {
                    "name": "支柱一：机器人控制 (Robot Control)",
                    "id": "1_robot_core",
                    "matched_keywords": [
                        "manipulation"
                    ],
                    "score": 2.0
                }
            ],
            "relevance_score": 2.0,
            "hit_pillars": [
                "1_robot_core"
            ],
            "headline_zh": "MobileWorld：面向Agent-用户交互和MCP增强环境的移动Agent基准测试",
            "summary_zh": "现有的在线移动应用基准测试中，AndroidWorld因其可复现的环境和确定性的评估而占据主导地位。然而，最近的Agent在该基准测试上取得了超过90%的成功率，表明其已趋于饱和，并促使人们需要更具挑战性的基准。此外，AndroidWorld缺乏关键的应用类别，如电子商务和企业通信，并且不能反映现实移动应用场景中模糊的用户指令和混合工具使用的特点。为了弥合这一差距，我们推出了MobileWorld，这是一个更具挑战性的基准，旨在更好地反映真实的移动应用使用情况，包含20个应用中的201个任务，同时保持与AndroidWorld相同水平的可复现评估。MobileWorld的难度体现在两个方面。首先，它强调具有跨应用交互的长程任务：MobileWorld平均需要近两倍的任务完成步骤（27.8 vs. 14.3），并且包含更多的多应用任务（62.2% vs. 9.5%）。其次，MobileWorld通过引入新的任务类别，包括Agent-用户交互和MCP增强任务，扩展了标准GUI操作。为了确保可靠的评估，我们提供了基于快照的容器环境和精确的功能验证，包括后端数据库检查和任务回调API。我们进一步开发了一个具有扩展动作空间的规划器-执行器Agent框架，以支持用户交互和MCP调用。我们的结果表明，与AndroidWorld相比，性能急剧下降，最佳Agent框架和端到端模型分别实现了51.7%和20.9%的成功率。我们的分析表明，当前模型在用户交互和MCP调用方面存在显著困难，为更强大、下一代移动智能提供了一个战略路线图。",
            "intro_zh": [
                "现有AndroidWorld基准测试已饱和，缺乏对真实移动应用场景的模拟，尤其是在用户交互和多应用协作方面。",
                "MobileWorld通过引入更复杂、长程的任务，以及Agent-用户交互和MCP增强任务，构建了一个更具挑战性的基准测试环境。",
                "实验结果表明，现有Agent在MobileWorld上的性能显著下降，尤其是在处理用户交互和MCP调用方面，揭示了未来研究方向。"
            ],
            "method_zh": "**问题定义**：现有AndroidWorld基准测试已无法有效评估移动Agent的性能，因为它过于简单，缺乏对真实世界移动应用场景的模拟，特别是忽略了用户交互和多应用协作。这导致Agent在真实场景中的泛化能力不足。\\n\\n**核心思路**：MobileWorld的核心思路是构建一个更复杂、更真实的移动应用环境，包含更长程的任务、跨应用交互、用户交互和MCP（Mobile Cloud Platform）调用。通过增加任务的复杂性和多样性，更全面地评估Agent的性能和鲁棒性。\\n\\n**技术框架**：MobileWorld的整体框架包括以下几个主要组成部分：1) 一个包含20个应用的移动应用环境，涵盖电子商务、企业通信等多个类别；2) 201个任务，包括标准GUI操作、Agent-用户交互和MCP增强任务；3) 一个基于快照的容器环境，用于保证评估的可复现性；4) 精确的功能验证机制，包括后端数据库检查和任务回调API；5) 一个规划器-执行器Agent框架，用于支持用户交互和MCP调用。\\n\\n**关键创新**：MobileWorld的关键创新在于引入了Agent-用户交互和MCP增强任务。Agent-用户交互任务要求Agent能够理解用户的自然语言指令，并与用户进行多轮对话以完成任务。MCP增强任务要求Agent能够调用移动云平台提供的服务，例如图像识别、语音识别等，以辅助完成任务。\\n\\n**关键设计**：MobileWorld的关键设计包括：1) 任务的长度和复杂性：MobileWorld的任务平均需要27.8个步骤完成，远高于AndroidWorld的14.3个步骤；2) 多应用任务的比例：MobileWorld中62.2%的任务涉及多个应用，而AndroidWorld仅为9.5%；3) 动作空间的设计：规划器-执行器Agent框架的动作空间扩展到包括用户交互和MCP调用，使其能够处理更复杂的任务。",
            "application_zh": "MobileWorld为移动Agent的研究和开发提供了一个更具挑战性和现实意义的基准测试平台。它可以用于评估和比较不同Agent的性能，指导Agent的设计和优化，并促进移动智能的发展。潜在的应用领域包括智能助手、自动化测试、移动应用开发等，有助于提升移动设备的用户体验和效率。",
            "highlight_zh": "实验结果表明，现有Agent在MobileWorld上的性能显著下降。最佳Agent框架的成功率为51.7%，端到端模型的成功率仅为20.9%，远低于在AndroidWorld上的表现。这表明现有Agent在处理用户交互和MCP调用方面存在显著困难，为未来的研究提供了明确的方向。",
            "tags_zh": [
                "移动Agent",
                "基准测试",
                "人机交互",
                "多应用协作",
                "移动云平台",
                "任务规划",
                "智能助手"
            ],
            "_index": 105,
            "_used_api": "gemini",
            "figures": [
                {
                    "url": "https://arxiv.org/html/2512.19432v1/x1.png",
                    "caption": "",
                    "figure_id": "fig_0"
                },
                {
                    "url": "https://arxiv.org/html/2512.19432v1/x2.png",
                    "caption": "",
                    "figure_id": "fig_1"
                },
                {
                    "url": "https://arxiv.org/html/2512.19432v1/x3.png",
                    "caption": "",
                    "figure_id": "fig_2"
                }
            ]
        }
    ]
}