{
  "count": 106,
  "papers": [
    {
      "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
      "authors": [
        "Ryosuke Korekata",
        "Quanting Xie",
        "Yonatan Bisk",
        "Komei Sugiura"
      ],
      "arxiv_id": "2512.18987v1",
      "summary": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Accepted to IEEE RA-L, with presentation at ICRA 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18987v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]mobile manipulation"
          ],
          "score": 12.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "open-vocabulary",
            "open vocabulary",
            "[T]affordance"
          ],
          "score": 10.0
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]affordance-aware"
          ],
          "score": 7.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 38.5,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "5_interaction_reaction",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation",
      "authors": [
        "Yadong Liu",
        "Jianwei Liu",
        "He Liang",
        "Dimitrios Kanoulas"
      ],
      "arxiv_id": "2512.18938v1",
      "summary": "Quadruped mobile manipulators offer strong potential for agile loco-manipulation but remain difficult to control and transfer reliably from simulation to reality. Reinforcement learning (RL) shows promise for whole-body control, yet most frameworks are proprietary and hard to reproduce on real hardware. We present an open pipeline for training, benchmarking, and deploying RL-based controllers on the Unitree B1 quadruped with a Z1 arm. The framework unifies sim-to-sim and sim-to-real transfer through ROS, re-implementing a policy trained in Isaac Gym, extending it to MuJoCo via a hardware abstraction layer, and deploying the same controller on physical hardware. Sim-to-sim experiments expose discrepancies between Isaac Gym and MuJoCo contact models that influence policy behavior, while real-world teleoperated object-picking trials show that coordinated whole-body control extends reach and improves manipulation over floating-base baselines. The pipeline provides a transparent, reproducible foundation for developing and analyzing RL-based loco-manipulation controllers and will be released open source to support future research.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18938v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]quadruped",
            "whole-body control",
            "[T]manipulation",
            "[T]loco-manipulation",
            "sim-to-real",
            "Unitree"
          ],
          "score": 24.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        }
      ],
      "relevance_score": 25.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "authors": [
        "Hongwei Fan",
        "Hang Dai",
        "Jiyao Zhang",
        "Jinzhou Li",
        "Qiyang Yan",
        "Yujie Zhao",
        "Mingju Gao",
        "Jinghang Wu",
        "Hao Tang",
        "Hao Dong"
      ],
      "arxiv_id": "2512.19390v1",
      "summary": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19390v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation",
            "[T]sim2real",
            "[T]real2sim"
          ],
          "score": 18.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3DGS"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 23.0,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models",
      "authors": [
        "Pengyu Chen",
        "Tao Ouyang",
        "Ke Luo",
        "Weijie Hong",
        "Xu Chen"
      ],
      "arxiv_id": "2512.19083v1",
      "summary": "Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "This paper is accepted by the IEEE Internet of Things Journal (IoT-J) for publication in the Special Issue on \"Augmented Edge Sensing Intelligence for Low-Altitude IoT Systems\"",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19083v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "Depth Anything",
            "scene understanding",
            "occupancy grid"
          ],
          "score": 8.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]foundation model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 20.0,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "authors": [
        "Ziyang Song",
        "Zelin Zang",
        "Zuyao Chen",
        "Xusheng Liang",
        "Dong Yi",
        "Jinlin Wu",
        "Hongbin Liu",
        "Jiebo Luo"
      ],
      "arxiv_id": "2512.19512v1",
      "summary": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19512v1",
      "code_links": [
        {
          "url": "https://github.com/tomato996/Anatomy-R1",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "curriculum learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
      "authors": [
        "Yizhi Wang",
        "Linan Yue",
        "Min-Ling Zhang"
      ],
      "arxiv_id": "2512.18956v1",
      "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18956v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal",
            "[T]chain-of-thought"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 19.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control",
      "authors": [
        "Chao Yang",
        "Yingkai Sun",
        "Peng Ye",
        "Xin Chen",
        "Chong Yu",
        "Tao Chen"
      ],
      "arxiv_id": "2512.19043v1",
      "summary": "Learning a general motion tracking policy from human motions shows great potential for versatile humanoid whole-body control. Conventional approaches are not only inefficient in data utilization and training processes but also exhibit limited performance when tracking highly dynamic motions. To address these challenges, we propose EGM, a framework that enables efficient learning of a general motion tracking policy. EGM integrates four core designs. Firstly, we introduce a Bin-based Cross-motion Curriculum Adaptive Sampling strategy to dynamically orchestrate the sampling probabilities based on tracking error of each motion bin, eficiently balancing the training process across motions with varying dificulty and durations. The sampled data is then processed by our proposed Composite Decoupled Mixture-of-Experts (CDMoE) architecture, which efficiently enhances the ability to track motions from different distributions by grouping experts separately for upper and lower body and decoupling orthogonal experts from shared experts to separately handle dedicated features and general features. Central to our approach is a key insight we identified: for training a general motion tracking policy, data quality and diversity are paramount. Building on these designs, we develop a three-stage curriculum training flow to progressively enhance the policy's robustness against disturbances. Despite training on only 4.08 hours of data, EGM generalized robustly across 49.25 hours of test motions, outperforming baselines on both routine and highly dynamic tasks.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19043v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]humanoid",
            "[T]whole-body control"
          ],
          "score": 12.0
        },
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "[T]motion tracking"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "1_robot_core",
        "8_physics_animation"
      ]
    },
    {
      "title": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
      "authors": [
        "Hengyi Feng",
        "Zeang Sheng",
        "Meiyi Qiang",
        "Wentao Zhang"
      ],
      "arxiv_id": "2512.19115v1",
      "summary": "Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19115v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
      "authors": [
        "A. B. M. Ashikur Rahman",
        "Saeed Anwar",
        "Muhammad Usman",
        "Irfan Ahmad",
        "Ajmal Mian"
      ],
      "arxiv_id": "2512.19350v1",
      "summary": "Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19350v1",
      "code_links": [
        {
          "url": "https://github.com/ashikiut/pendulum/",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]multimodal"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
      "authors": [
        "Chenghao Li",
        "Chaoning Zhang",
        "Yi Lu",
        "Shuxu Chen",
        "Xudong Wang",
        "Jiaquan Zhang",
        "Zhicheng Wang",
        "Zhengxun Jin",
        "Kuien Liu",
        "Sung-Ho Bae",
        "Guoqing Wang",
        "Yang Yang",
        "Hen Tao Shen"
      ],
      "arxiv_id": "2512.19135v1",
      "summary": "With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19135v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "[T]chain-of-thought"
          ],
          "score": 18.0
        }
      ],
      "relevance_score": 18.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
      "authors": [
        "Wendong Bu",
        "Kaihang Pan",
        "Yuze Lin",
        "Jiacheng Li",
        "Kai Shen",
        "Wenqiao Zhang",
        "Juncheng Li",
        "Jun Xiao",
        "Siliang Tang"
      ],
      "arxiv_id": "2512.19159v1",
      "summary": "Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19159v1",
      "code_links": [
        {
          "url": "https://OmniMoGen.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "text-to-motion",
            "[T]motion generation",
            "VQ-VAE"
          ],
          "score": 12.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 15.5,
      "hit_pillars": [
        "4_motion_diffusion",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Decoupled Generative Modeling for Human-Object Interaction Synthesis",
      "authors": [
        "Hwanhee Jung",
        "Seunggwan Lee",
        "Jeongyoon Yoon",
        "SeungHyeon Kim",
        "Giljoo Nam",
        "Qixing Huang",
        "Sangpil Kim"
      ],
      "arxiv_id": "2512.19049v1",
      "summary": "Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19049v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "penetration"
          ],
          "score": 2.5
        },
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]human-object interaction",
            "HOI"
          ],
          "score": 10.0
        }
      ],
      "relevance_score": 14.5,
      "hit_pillars": [
        "1_robot_core",
        "4_motion_diffusion",
        "5_interaction_reaction"
      ]
    },
    {
      "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
      "authors": [
        "Dixuan Lin",
        "Tianyou Wang",
        "Zhuoyang Pan",
        "Yufu Wang",
        "Lingjie Liu",
        "Kostas Daniilidis"
      ],
      "arxiv_id": "2512.19684v1",
      "summary": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19684v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "scene reconstruction"
          ],
          "score": 2.0
        },
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "physically plausible"
          ],
          "score": 2.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 13.5,
      "hit_pillars": [
        "1_robot_core",
        "3_perception_slam",
        "4_motion_diffusion",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "authors": [
        "Martin Sedlacek",
        "Pavlo Yefanov",
        "Georgy Ponimatkin",
        "Jai Bardhan",
        "Simon Pilc",
        "Mederic Fourmy",
        "Evangelos Kazakos",
        "Cees G. M. Snoek",
        "Josef Sivic",
        "Vladimir Petrik"
      ],
      "arxiv_id": "2512.19562v1",
      "summary": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "9 pages, 10 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19562v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "vision-language-action",
            "VLA"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors",
      "authors": [
        "Jose Gustavo Buenaventura Carreon",
        "Floris Erich",
        "Roman Mykhailyshyn",
        "Tomohiro Motoda",
        "Ryo Hanai",
        "Yukiyasu Domae"
      ],
      "arxiv_id": "2512.19148v1",
      "summary": "We present a cross robot visuomotor learning framework that integrates diffusion policy based control with 3D semantic scene representations from D3Fields to enable category level generalization in manipulation. Its modular design supports diverse robot camera configurations including UR5 arms with Microsoft Azure Kinect arrays and bimanual manipulators with Intel RealSense sensors through a low latency control stack and intuitive teleoperation. A unified configuration layer enables seamless switching between setups for flexible data collection training and evaluation. In a grasp and lift block task the framework achieved an 80 percent success rate after only 100 demonstration episodes demonstrating robust skill transfer between platforms and sensing modalities. This design paves the way for scalable real world studies in cross robotic generalization.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "6 pages, 7 figures, conference: SII 2026. Cancun, Mexico",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19148v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "bi-manual",
            "teleoperation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]policy learning",
            "diffusion policy"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
      "authors": [
        "Xu Liu",
        "Yu Liu",
        "Hanshuo Qiu",
        "Yang Qirong",
        "Zhouhui Lian"
      ],
      "arxiv_id": "2512.19024v1",
      "summary": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19024v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "VLA",
            "VLN",
            "multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
      "authors": [
        "Niclas Griesshaber",
        "Jochen Streb"
      ],
      "arxiv_id": "2512.19675v1",
      "summary": "We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",
      "categories": [
        "econ.GN",
        "cs.CV",
        "cs.DL"
      ],
      "primary_category": "econ.GN",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19675v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "4D Gaussian Splatting as a Learned Dynamical System",
      "authors": [
        "Arnold Caleb Asiimwe",
        "Carl Vondrick"
      ],
      "arxiv_id": "2512.19648v1",
      "summary": "We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19648v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "[T]splatting"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
      "authors": [
        "Zhenyang Huang",
        "Xiao Yu",
        "Yi Zhang",
        "Decheng Wang",
        "Hang Ruan"
      ],
      "arxiv_id": "2512.19354v1",
      "summary": "Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19354v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
      "authors": [
        "Marios Thoma",
        "Zenonas Theodosiou",
        "Harris Partaourides",
        "Vassilis Vassiliades",
        "Loizos Michael",
        "Andreas Lanitis"
      ],
      "arxiv_id": "2512.19190v1",
      "summary": "Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "24 pages, 7 figures, 9 tables, Dataset: https://doi.org/10.5281/zenodo.10907945, Code: https://github.com/CYENS/PEDESTRIAN",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19190v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]egocentric",
            "[T]egocentric vision"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting",
      "authors": [
        "Tiantian Li",
        "Xinjie Zhang",
        "Xingtong Ge",
        "Tongda Xu",
        "Dailan He",
        "Jun Zhang",
        "Yan Wang"
      ],
      "arxiv_id": "2512.19108v1",
      "summary": "Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Accepted to AAAI 2026.Code URL:https://github.com/Sweethyh/GaussianImage_plus.git",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19108v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]gaussian splatting",
            "[T]splatting"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation",
      "authors": [
        "Khanh Nguyen",
        "Dasith de Silva Edirimuni",
        "Ghulam Mubashar Hassan",
        "Ajmal Mian"
      ],
      "arxiv_id": "2512.19088v1",
      "summary": "Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Accepted to AAAI 2026 Workshop on New Frontiers in Information Retrieval",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19088v1",
      "code_links": [
        {
          "url": "https://github.com/ndkhanh360/BoxOVIS",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "[T]open-vocabulary",
            "[T]open vocabulary"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "authors": [
        "Kun Zhao",
        "Siyuan Dai",
        "Yingying Zhang",
        "Guodong Liu",
        "Pengfei Gu",
        "Chenghua Lin",
        "Paul M. Thompson",
        "Alex Leow",
        "Heng Huang",
        "Lifang He",
        "Liang Zhan",
        "Haoteng Tang"
      ],
      "arxiv_id": "2512.18986v1",
      "summary": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18986v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
      "authors": [
        "Zhe Yang",
        "Xiaoshuang Sheng",
        "Zhengnan Zhang",
        "Jidong Wu",
        "Zexing Wang",
        "Xin He",
        "Shenghua Xu",
        "Guanjing Xiong"
      ],
      "arxiv_id": "2512.19107v1",
      "summary": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19107v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
      "authors": [
        "Filippos Ventirozos",
        "Peter Appleby",
        "Matthew Shardlow"
      ],
      "arxiv_id": "2512.19651v1",
      "summary": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "9 pages, 3 figures, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19651v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "[T]chain-of-thought"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Event Extraction in Large Language Model",
      "authors": [
        "Bobo Li",
        "Xudong Han",
        "Jiang Liu",
        "Yuzhe Ding",
        "Liqiang Jing",
        "Zhaoqi Zhang",
        "Jinheng Li",
        "Xinya Du",
        "Fei Li",
        "Meishan Zhang",
        "Min Zhang",
        "Aixin Sun",
        "Philip S. Yu",
        "Hao Fei"
      ],
      "arxiv_id": "2512.19537v1",
      "summary": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "38 pages, 9 Figures, 5 Tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19537v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "multimodal"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs",
      "authors": [
        "Ziyan Zhang",
        "Chao Wang",
        "Zhuo Chen",
        "Lei Chen",
        "Chiyi Li",
        "Kai Song"
      ],
      "arxiv_id": "2512.19092v1",
      "summary": "Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19092v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model",
            "chain-of-thought"
          ],
          "score": 12.0
        }
      ],
      "relevance_score": 12.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
      "authors": [
        "Valentin Schmidberger",
        "Manuel Eberhardinger",
        "Setareh Maghsudi",
        "Johannes Maucher"
      ],
      "arxiv_id": "2512.19228v1",
      "summary": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Accepted at ICMLA 2025, the first two authors contributed equally",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19228v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 11.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
      "authors": [
        "Pengxuan Yang",
        "Ben Lu",
        "Zhongpu Xia",
        "Chao Han",
        "Yinfeng Gao",
        "Teng Zhang",
        "Kun Zhan",
        "XianPeng Lang",
        "Yupeng Zheng",
        "Qichao Zhang"
      ],
      "arxiv_id": "2512.19133v1",
      "summary": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "AAAI 2026, first version",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19133v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]world model",
            "representation learning"
          ],
          "score": 7.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "authors": [
        "Apoorv Vyas",
        "Heng-Jui Chang",
        "Cheng-Fu Yang",
        "Po-Yao Huang",
        "Luya Gao",
        "Julius Richter",
        "Sanyuan Chen",
        "Matt Le",
        "Piotr Dollár",
        "Christoph Feichtenhofer",
        "Ann Lee",
        "Wei-Ning Hsu"
      ],
      "arxiv_id": "2512.19687v1",
      "summary": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.SD",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19687v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
      "authors": [
        "Yongxin Wang",
        "Zhicheng Yang",
        "Meng Cao",
        "Mingfei Han",
        "Haokun Lin",
        "Yingying Zhu",
        "Xiaojun Chang",
        "Xiaodan Liang"
      ],
      "arxiv_id": "2512.19554v1",
      "summary": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19554v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling",
      "authors": [
        "Sutashu Tomonaga",
        "Kenji Doya",
        "Noboru Murata"
      ],
      "arxiv_id": "2512.18965v1",
      "summary": "Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions \"slide\" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18965v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "Mamba",
            "[T]SSM",
            "[T]state space model"
          ],
          "score": 10.5
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
      "authors": [
        "Jian Yang",
        "Wei Zhang",
        "Yizhi Li",
        "Shawn Guo",
        "Haowen Wang",
        "Aishan Liu",
        "Ge Zhang",
        "Zili Wang",
        "Zhoujun Li",
        "Xianglong Liu",
        "Weifeng Lv"
      ],
      "arxiv_id": "2512.19424v1",
      "summary": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19424v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
      "authors": [
        "Zihan Lin",
        "Xiaohan Wang",
        "Hexiong Yang",
        "Jiajun Chai",
        "Jie Cao",
        "Guojun Yin",
        "Wei Lin",
        "Ran He"
      ],
      "arxiv_id": "2512.19126v1",
      "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19126v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 10.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
      "authors": [
        "Sihao Lin",
        "Zerui Li",
        "Xunyi Zhao",
        "Gengze Zhou",
        "Liuyi Wang",
        "Rong Wei",
        "Rui Tang",
        "Juncheng Li",
        "Hanqing Wang",
        "Jiangmiao Pang",
        "Anton van den Hengel",
        "Jiajun Liu",
        "Qi Wu"
      ],
      "arxiv_id": "2512.19021v1",
      "summary": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19021v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "locomotion",
            "sim-to-real"
          ],
          "score": 4.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI",
            "VLN"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
      "authors": [
        "Zelin Zhao",
        "Xinyu Gong",
        "Bangya Liu",
        "Ziyang Song",
        "Jun Zhang",
        "Suhui Wu",
        "Yongxin Chen",
        "Hao Zhang"
      ],
      "arxiv_id": "2512.19020v1",
      "summary": "Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19020v1",
      "code_links": [
        {
          "url": "https://sjtuytc.github.io/CETCam_project_page.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "depth estimation",
            "VGGT"
          ],
          "score": 4.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "geometric consistency"
          ],
          "score": 3.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 10.0,
      "hit_pillars": [
        "3_perception_slam",
        "7_retargeting",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations",
      "authors": [
        "Yinhuai Wang",
        "Runyi Yu",
        "Hok Wai Tsui",
        "Xiaoyi Lin",
        "Hui Zhang",
        "Qihan Zhao",
        "Ke Fan",
        "Miao Li",
        "Jie Song",
        "Jingbo Wang",
        "Qifeng Chen",
        "Ping Tan"
      ],
      "arxiv_id": "2512.19583v1",
      "summary": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.",
      "categories": [
        "cs.RO",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19583v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation",
            "dexterous hand",
            "dexterous manipulation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "imitation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation",
      "authors": [
        "Devi Yuliarti",
        "Ravi Prakash",
        "Hiu Ching Cheung",
        "Amy Strong",
        "Patrick J. Codd",
        "Shan Lin"
      ],
      "arxiv_id": "2512.19010v1",
      "summary": "The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making.",
      "categories": [
        "eess.SP",
        "cs.RO"
      ],
      "primary_category": "eess.SP",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19010v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "authors": [
        "Argha Kamal Samanta",
        "Harshika Goyal",
        "Vasudha Joshi",
        "Tushar Mungle",
        "Pabitra Mitra"
      ],
      "arxiv_id": "2512.19663v1",
      "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "14 pages, 14 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19663v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
      "authors": [
        "Yi Xin",
        "Siqi Luo",
        "Qi Qin",
        "Haoxing Chen",
        "Kaiwen Zhu",
        "Zhiwei Zhang",
        "Yangfan He",
        "Rongchao Zhang",
        "Jinbin Bai",
        "Shuo Cao",
        "Bin Fu",
        "Junjun He",
        "Yihao Liu",
        "Yuewen Cao",
        "Xiaohong Liu"
      ],
      "arxiv_id": "2512.19433v1",
      "summary": "Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19433v1",
      "code_links": [
        {
          "url": "https://github.com/Alpha-VLLM/Lumina-DiMOO",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "authors": [
        "Xueming Yan",
        "Boyan Xu",
        "Yaochu Jin",
        "Lixian Xiao",
        "Wenlong Ye",
        "Runyang Cai",
        "Zeqi Zheng",
        "Jingfa Liu",
        "Aimin Yang"
      ],
      "arxiv_id": "2512.19379v1",
      "summary": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19379v1",
      "code_links": [
        {
          "url": "https://github.com/yanxm01/INDOMER",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
      "authors": [
        "Haoyu Jiang",
        "Boan Qu",
        "Junjie Zhu",
        "Fanjie Zeng",
        "Xiaojie Lin",
        "Wei Zhong"
      ],
      "arxiv_id": "2512.19114v1",
      "summary": "The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19114v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
      "authors": [
        "Angjelin Hila"
      ],
      "arxiv_id": "2512.19570v1",
      "summary": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "AI & Soc (2025)",
      "doi": "10.1007/s00146-025-02426-3",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19570v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
      "authors": [
        "Mingxu Zhang",
        "Dazhong Shen",
        "Qi Zhang",
        "Ying Sun"
      ],
      "arxiv_id": "2512.19240v1",
      "summary": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19240v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving",
      "authors": [
        "Peiqing Lu",
        "Yuan Zhang",
        "Haoyun Zhang",
        "Jiasen Zheng",
        "Kejian Tong",
        "Wenjun Wu"
      ],
      "arxiv_id": "2512.19093v1",
      "summary": "Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19093v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "[T]distillation"
          ],
          "score": 6.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm",
      "authors": [
        "Li Yan",
        "Bolun Liu",
        "Chao Li",
        "Jing Liang",
        "Kunjie Yu",
        "Caitong Yue",
        "Xuzhao Chai",
        "Boyang Qu"
      ],
      "arxiv_id": "2512.18947v1",
      "summary": "Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18947v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]multimodal"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting",
      "authors": [
        "Ivan Decostanzi",
        "Yelena Mejova",
        "Kyriaki Kalimeri"
      ],
      "arxiv_id": "2512.19475v1",
      "summary": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "18 pages, 3 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19475v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models",
      "authors": [
        "Zhiqing Hu",
        "Chenxu Zhao",
        "Jiazhong Lu",
        "Xiaolei Liu"
      ],
      "arxiv_id": "2512.19378v1",
      "summary": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Camera-ready version of the paper accepted for oral presentation at the 11th International Conference on Computer and Communications (ICCC 2025)",
      "doi": "",
      "journal_ref": "In Proceedings of the 11th International Conference on Computer and Communications, 2025",
      "pdf_url": "https://arxiv.org/pdf/2512.19378v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "[T]large language model"
          ],
          "score": 9.0
        }
      ],
      "relevance_score": 9.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
      "authors": [
        "Kyungwon Cho",
        "Hanbyul Joo"
      ],
      "arxiv_id": "2512.19283v1",
      "summary": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Project Page: https://kyungwoncho.github.io/HaMoS/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19283v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "[T]egocentric",
            "egocentric vision"
          ],
          "score": 8.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "6_video_extraction"
      ]
    },
    {
      "title": "Towards AI-Guided Open-World Ecological Taxonomic Classification",
      "authors": [
        "Cheng Yaw Low",
        "Heejoon Koo",
        "Jaewoo Park",
        "Kaleb Mesfin Asfaw",
        "Meeyoung Cha"
      ],
      "arxiv_id": "2512.18994v1",
      "summary": "AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "4 figures, 11 tables, and 15 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18994v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱八：物理动画 (Physics-based Animation)",
          "id": "8_physics_animation",
          "matched_keywords": [
            "spatiotemporal"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "foundation model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 8.0,
      "hit_pillars": [
        "8_physics_animation",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
      "authors": [
        "Jiawen Wang",
        "Jingjing Wang Tianyang Chen",
        "Min Zhang",
        "Guodong Zhou"
      ],
      "arxiv_id": "2512.19551v1",
      "summary": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19551v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱四：生成式动作 (Generative Motion)",
          "id": "4_motion_diffusion",
          "matched_keywords": [
            "[T]motion generation"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "4_motion_diffusion"
      ]
    },
    {
      "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
      "authors": [
        "A. A. Gde Yogi Pramana",
        "Jason Ray",
        "Anthony Jaya",
        "Michael Wijaya"
      ],
      "arxiv_id": "2512.19317v1",
      "summary": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19317v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "chain-of-thought"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
      "authors": [
        "Jiaao Wu",
        "Xian Zhang",
        "Fan Yang",
        "Yinpeng Dong"
      ],
      "arxiv_id": "2512.19287v1",
      "summary": "We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "20 pages, 13 figures",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19287v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱五：交互与反应 (Interaction & Reaction)",
          "id": "5_interaction_reaction",
          "matched_keywords": [
            "[T]IMoS"
          ],
          "score": 7.5
        }
      ],
      "relevance_score": 7.5,
      "hit_pillars": [
        "5_interaction_reaction"
      ]
    },
    {
      "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "authors": [
        "Kirill Djebko",
        "Tom Baumann",
        "Erik Dilger",
        "Frank Puppe",
        "Sergio Montenegro"
      ],
      "arxiv_id": "2512.19576v1",
      "summary": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository https://github.com/kdjebko/lelar-in-orbit-data",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19576v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "sim2real"
          ],
          "score": 2.0
        },
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning",
            "DRL"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 6.5,
      "hit_pillars": [
        "1_robot_core",
        "2_algo_arch"
      ]
    },
    {
      "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "authors": [
        "Yanliang Huang",
        "Xia Yan",
        "Peiran Yin",
        "Zhenduo Zhang",
        "Zeyan Shao",
        "Youran Wang",
        "Haoliang Huang",
        "Matthias Althoff"
      ],
      "arxiv_id": "2512.19564v1",
      "summary": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19564v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "[T]motion planning"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
      "authors": [
        "Mingrui Wu",
        "Zhaozhi Wang",
        "Fangjinhua Wang",
        "Jiaolong Yang",
        "Marc Pollefeys",
        "Tong Zhang"
      ],
      "arxiv_id": "2512.19683v1",
      "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Project page: https://harmlesssr.github.io/openbench/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19683v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "authors": [
        "Hanyang Kong",
        "Xingyi Yang",
        "Xiaoxu Zheng",
        "Xinchao Wang"
      ],
      "arxiv_id": "2512.19678v1",
      "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Project page: https://hyokong.github.io/worldwarp-page/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
      "code_links": [
        {
          "url": "https://hyokong.github.io/worldwarp-page/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "3DGS",
            "gaussian splatting",
            "splatting"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "3_perception_slam"
      ]
    },
    {
      "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "authors": [
        "Artemis Panagopoulou",
        "Aveek Purohit",
        "Achin Kulshrestha",
        "Soroosh Yazdani",
        "Mohit Goyal"
      ],
      "arxiv_id": "2512.19609v1",
      "summary": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19609v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
      "authors": [
        "Evelyn Zhang",
        "Fufu Yu",
        "Aoqi Wu",
        "Zichen Wen",
        "Ke Yan",
        "Shouhong Ding",
        "Biqing Qi",
        "Linfeng Zhang"
      ],
      "arxiv_id": "2512.19443v1",
      "summary": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19443v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "multimodal"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
      "authors": [
        "Tao Zhang",
        "Ziqian Zeng",
        "Hao Peng",
        "Huiping Zhuang",
        "Cen Chen"
      ],
      "arxiv_id": "2512.19206v1",
      "summary": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19206v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "chain-of-thought"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "authors": [
        "Do Minh Duc",
        "Quan Xuan Truong",
        "Nguyen Tat Dat",
        "Nguyen Van Vinh"
      ],
      "arxiv_id": "2512.19247v1",
      "summary": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19247v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "chain-of-thought"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation",
      "authors": [
        "Thittipat Pairatsuppawat",
        "Abhibhu Tachaapornchai",
        "Paweekorn Kusolsomboon",
        "Chutikan Chaiwong",
        "Thodsaporn Chay-intr",
        "Kobkrit Viriyayudhakorn",
        "Nongnuch Ketui",
        "Aslan B. Wong"
      ],
      "arxiv_id": "2512.19455v1",
      "summary": "Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19455v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model",
            "instruction following"
          ],
          "score": 6.0
        }
      ],
      "relevance_score": 6.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "OMP: One-step Meanflow Policy with Directional Alignment",
      "authors": [
        "Han Fang",
        "Yize Huang",
        "Yuheng Zhao",
        "Paul Weng",
        "Xiao Li",
        "Yutong Ban"
      ],
      "arxiv_id": "2512.19347v1",
      "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19347v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "embodied AI"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Translating Flow to Policy via Hindsight Online Imitation",
      "authors": [
        "Yitian Zheng",
        "Zhangchen Ye",
        "Weijun Dong",
        "Shengjie Wang",
        "Yuyang Liu",
        "Chongjie Zhang",
        "Chuan Wen",
        "Yang Gao"
      ],
      "arxiv_id": "2512.19269v1",
      "summary": "Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19269v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        },
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 5.0,
      "hit_pillars": [
        "1_robot_core",
        "7_retargeting"
      ]
    },
    {
      "title": "FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors",
      "authors": [
        "Georgios Voulgaris"
      ],
      "arxiv_id": "2512.19504v1",
      "summary": "Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.\n  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.\n  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.\n  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Preprint. Under review at IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19504v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]representation learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "authors": [
        "Yuqiao Tan",
        "Minzheng Wang",
        "Shizhu He",
        "Huanxuan Liao",
        "Chengfeng Zhao",
        "Qiunan Lu",
        "Tian Liang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "arxiv_id": "2512.19673v1",
      "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19673v1",
      "code_links": [
        {
          "url": "https://github.com/Trae1ounG/BuPO",
          "type": "github"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "authors": [
        "Xueming Yan",
        "Bo Yin",
        "Yaochu Jin"
      ],
      "arxiv_id": "2512.19516v1",
      "summary": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19516v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
      "authors": [
        "Simon Welker",
        "Bunlong Lay",
        "Maris Hillemann",
        "Tal Peer",
        "Timo Gerkmann"
      ],
      "arxiv_id": "2512.19442v1",
      "summary": "Diffusion-based generative models have greatly impacted the speech processing field in recent years, exhibiting high speech naturalness and spawning a new research direction. Their application in real-time communication is, however, still lagging behind due to their computation-heavy nature involving multiple calls of large DNNs.\n  Here, we present Stream.FM, a frame-causal flow-based generative model with an algorithmic latency of 32 milliseconds (ms) and a total latency of 48 ms, paving the way for generative speech processing in real-time communication. We propose a buffered streaming inference scheme and an optimized DNN architecture, show how learned few-step numerical solvers can boost output quality at a fixed compute budget, explore model weight compression to find favorable points along a compute/quality tradeoff, and contribute a model variant with 24 ms total latency for the speech enhancement task.\n  Our work looks beyond theoretical latencies, showing that high-quality streaming generative speech processing can be realized on consumer GPUs available today. Stream.FM can solve a variety of speech processing tasks in a streaming fashion: speech enhancement, dereverberation, codec post-filtering, bandwidth extension, STFT phase retrieval, and Mel vocoding. As we verify through comprehensive evaluations and a MUSHRA listening test, Stream.FM establishes a state-of-the-art for generative streaming speech restoration, exhibits only a reasonable reduction in quality compared to a non-streaming variant, and outperforms our recent work (Diffusion Buffer) on generative streaming speech enhancement while operating at a lower latency.",
      "categories": [
        "eess.SP",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.SP",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19442v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]flow matching"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation",
      "authors": [
        "Debamita Ghosh",
        "George K. Atia",
        "Yue Wang"
      ],
      "arxiv_id": "2512.18957v1",
      "summary": "The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18957v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "[T]reinforcement learning"
          ],
          "score": 4.5
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
      "authors": [
        "Haoyu Jiang",
        "Fanjie Zeng",
        "Boan Qu",
        "Xiaojie Lin",
        "Wei Zhong"
      ],
      "arxiv_id": "2512.19299v1",
      "summary": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19299v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "RLHF"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Can abstract concepts from LLM improve SLM performance?",
      "authors": [
        "Siddharth Tandon"
      ],
      "arxiv_id": "2512.19069v1",
      "summary": "Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19069v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "distillation"
          ],
          "score": 1.5
        },
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 4.5,
      "hit_pillars": [
        "2_algo_arch",
        "9_embodied_foundation"
      ]
    },
    {
      "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
      "authors": [
        "Utae Jeong",
        "Sumin In",
        "Hyunju Ryu",
        "Jaewan Choi",
        "Feng Yang",
        "Jongheon Jeong",
        "Seungryong Kim",
        "Sangpil Kim"
      ],
      "arxiv_id": "2512.19048v1",
      "summary": "Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19048v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "world model"
          ],
          "score": 1.5
        },
        {
          "name": "支柱三：空间感知与语义 (Perception & Semantics)",
          "id": "3_perception_slam",
          "matched_keywords": [
            "optical flow"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "3_perception_slam"
      ]
    },
    {
      "title": "Learning Through Little Eyes: Attribute Discrimination Beyond Objects",
      "authors": [
        "Patrick Batsell",
        "Tsutsui Satoshi",
        "Bihan Wen"
      ],
      "arxiv_id": "2512.18951v1",
      "summary": "Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18951v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "contrastive learning"
          ],
          "score": 1.5
        },
        {
          "name": "支柱六：视频提取与匹配 (Video Extraction)",
          "id": "6_video_extraction",
          "matched_keywords": [
            "egocentric"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 3.5,
      "hit_pillars": [
        "2_algo_arch",
        "6_video_extraction"
      ]
    },
    {
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "authors": [
        "Jiaqi Peng",
        "Wenzhe Cai",
        "Yuqiang Yang",
        "Tai Wang",
        "Yuan Shen",
        "Jiangmiao Pang"
      ],
      "arxiv_id": "2512.19629v1",
      "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}.",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Project page:https://steinate.github.io/logoplanner.github.io/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19629v1",
      "code_links": [
        {
          "url": "https://steinate.github.io/logoplanner.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
      "authors": [
        "Zhenglong Guo",
        "Yiming Zhao",
        "Feng Jiang",
        "Heng Jin",
        "Zongbao Feng",
        "Jianbin Zhou",
        "Siyuan Xu"
      ],
      "arxiv_id": "2512.19453v1",
      "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "8 pages, 10 figures, This work was completed in December 2024",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19453v1",
      "code_links": [
        {
          "url": "https://map-avr.github.io/",
          "type": "project_page"
        }
      ],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "chain-of-thought"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "authors": [
        "Jin Wang",
        "Kim Tien Ly",
        "Jacques Cloete",
        "Nikos Tsagarakis",
        "Ioannis Havoutis"
      ],
      "arxiv_id": "2512.19178v1",
      "summary": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Manuscript under review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19178v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱七：动作重定向 (Motion Retargeting)",
          "id": "7_retargeting",
          "matched_keywords": [
            "cross-embodiment"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "7_retargeting"
      ]
    },
    {
      "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners",
      "authors": [
        "Yanding Yang",
        "Weitao Zhou",
        "Jinhai Wang",
        "Xiaomin Guo",
        "Junze Wen",
        "Xiaolong Liu",
        "Lang Ding",
        "Zheng Fu",
        "Jinyu Miao",
        "Kun Jiang",
        "Diange Yang"
      ],
      "arxiv_id": "2512.18988v1",
      "summary": "Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport.",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18988v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "imitation learning",
            "contrastive learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
      "authors": [
        "Zixuan Ye",
        "Quande Liu",
        "Cong Wei",
        "Yuanxing Zhang",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Wenhan Luo"
      ],
      "arxiv_id": "2512.19686v1",
      "summary": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Project Page: https://zixuan-ye.github.io/VACoT/",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19686v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "chain-of-thought"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "authors": [
        "Moritz Böhle",
        "Amélie Royer",
        "Juliette Marrie",
        "Edouard Grave",
        "Patrick Pérez"
      ],
      "arxiv_id": "2512.19535v1",
      "summary": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19535v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "multimodal"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "authors": [
        "Sandro Andric"
      ],
      "arxiv_id": "2512.19399v1",
      "summary": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19399v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Cluster-Based Generalized Additive Models Informed by Random Fourier Features",
      "authors": [
        "Xin Huang",
        "Jia Li",
        "Jun Yu"
      ],
      "arxiv_id": "2512.19373v1",
      "summary": "Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "25 pages, 13 figures, 4 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19373v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "predictive model",
            "representation learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Learning General Policies with Policy Gradient Methods",
      "authors": [
        "Simon Ståhlberg",
        "Blai Bonet",
        "Hector Geffner"
      ],
      "arxiv_id": "2512.19366v1",
      "summary": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19366v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "DRL"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation",
      "authors": [
        "Isshaan Singh",
        "Divyansh Chawla",
        "Anshu Garg",
        "Shivin Mangal",
        "Pallavi Gupta",
        "Khushi Agarwal",
        "Nimrat Singh Khalsa",
        "Nandan Patel"
      ],
      "arxiv_id": "2512.19361v1",
      "summary": "The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19361v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "deep reinforcement learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
      "authors": [
        "JiaWei Zhu",
        "ZiHeng Liu"
      ],
      "arxiv_id": "2512.19349v1",
      "summary": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "7 pages,1 figure,4 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19349v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
      "authors": [
        "Anna-Maria Gueorguieva",
        "Aylin Caliskan"
      ],
      "arxiv_id": "2512.19238v1",
      "summary": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19238v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions",
      "authors": [
        "Haishan Ye"
      ],
      "arxiv_id": "2512.19104v1",
      "summary": "Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.",
      "categories": [
        "math.OC",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19104v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱二：RL算法与架构 (RL & Architecture)",
          "id": "2_algo_arch",
          "matched_keywords": [
            "reinforcement learning",
            "preference learning"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "2_algo_arch"
      ]
    },
    {
      "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
      "authors": [
        "Hengrui Jia",
        "Taoran Li",
        "Jonas Guan",
        "Varun Chandrasekaran"
      ],
      "arxiv_id": "2512.19025v1",
      "summary": "Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have ``forgotten'' the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose \\name, an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19025v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "authors": [
        "Akshaj Prashanth Rao",
        "Advait Singh",
        "Saumya Kumaar Saksena",
        "Dhruv Kumar"
      ],
      "arxiv_id": "2512.19011v1",
      "summary": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Under Review",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19011v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
      "authors": [
        "Tongyuan Miao",
        "Gary Huang",
        "Kai Jun Han",
        "Annie Jiang"
      ],
      "arxiv_id": "2512.19004v1",
      "summary": "Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19004v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
      "authors": [
        "Saman Forouzandeh",
        "Wei Peng",
        "Parham Moradi",
        "Xinghuo Yu",
        "Mahdi Jalili"
      ],
      "arxiv_id": "2512.18950v1",
      "summary": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Accepted at The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). 21 pages including references, with 7 figures and 8 tables. Code is publicly available at the authors GitHub repository: https://github.com/S-Forouzandeh/MACLA-LLM-Agents-AAMAS-Conference",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18950v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Exploring the features used for summary evaluation by Human and GPT",
      "authors": [
        "Zahra Sadeghi",
        "Evangelos Milios",
        "Frank Rudzicz"
      ],
      "arxiv_id": "2512.19620v1",
      "summary": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19620v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
      "authors": [
        "Katharina Stengg",
        "Christian Macho",
        "Martin Pinzger"
      ],
      "arxiv_id": "2512.19481v1",
      "summary": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "6 pages",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19481v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "authors": [
        "Zeyu Xia",
        "Jinzhe Ma",
        "Congjie Zheng",
        "Shufei Zhang",
        "Yuqiang Li",
        "Hang Su",
        "P. Hu",
        "Changshui Zhang",
        "Xingao Gong",
        "Wanli Ouyang",
        "Lei Bai",
        "Dongzhan Zhou",
        "Mao Su"
      ],
      "arxiv_id": "2512.19458v1",
      "summary": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19458v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
      "authors": [
        "Jinwei Chi",
        "Ke Wang",
        "Yu Chen",
        "Xuanye Lin",
        "Qiang Xu"
      ],
      "arxiv_id": "2512.19456v1",
      "summary": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19456v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
      "authors": [
        "Linzhi Chen",
        "Yang Sun",
        "Hongru Wei",
        "Yuqi Chen"
      ],
      "arxiv_id": "2512.19297v1",
      "summary": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "NDSS 2026",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19297v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation",
      "authors": [
        "Jerry Wang",
        "Ting Yiu Liu"
      ],
      "arxiv_id": "2512.19210v1",
      "summary": "We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Accepted at NeurIPS Workshop on Foundations of Reasoning in Language Models and Workshop on Bridging Language, Agent, and World Model",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19210v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning",
      "authors": [
        "Yanzhi Zhang",
        "Yitong Duan",
        "Zhaoxi Zhang",
        "Jiyan He",
        "Shuxin Zheng"
      ],
      "arxiv_id": "2512.19081v1",
      "summary": "Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19081v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
      "authors": [
        "Jiacheng Guo",
        "Ling Yang",
        "Peter Chen",
        "Qixin Xiao",
        "Yinjie Wang",
        "Xinzhe Juan",
        "Jiahao Qiu",
        "Ke Shen",
        "Mengdi Wang"
      ],
      "arxiv_id": "2512.19682v1",
      "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "Our codes are available at https://github.com/Gen-Verse/GenEnv",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19682v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Increasing the Thinking Budget is Not All You Need",
      "authors": [
        "Ignacio Iacobacci",
        "Zhaozhi Qian",
        "Faroq AL-Tam",
        "Muhammad AL-Qurishi",
        "Riad Souissi"
      ],
      "arxiv_id": "2512.19585v1",
      "summary": "Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "4 pages, 4 figures, 3 tables",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19585v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Epistemological Fault Lines Between Human and Artificial Intelligence",
      "authors": [
        "Walter Quattrociocchi",
        "Valerio Capraro",
        "Matjaž Perc"
      ],
      "arxiv_id": "2512.19466v1",
      "summary": "Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI.",
      "categories": [
        "cs.CY",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "16 pages, 1 figure",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19466v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
      "authors": [
        "Jiaren Peng",
        "Hongda Sun",
        "Xuan Tian",
        "Cheng Huang",
        "Zeqing Li",
        "Rui Yan"
      ],
      "arxiv_id": "2512.19414v1",
      "summary": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19414v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs",
      "authors": [
        "Javier Vela-Tambo",
        "Jorge Gracia",
        "Fernando Dominguez-Castro"
      ],
      "arxiv_id": "2512.19305v1",
      "summary": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19305v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?",
      "authors": [
        "Amar Lakel"
      ],
      "arxiv_id": "2512.19117v1",
      "summary": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "in French language",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19117v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions",
      "authors": [
        "Wen-Long Jin"
      ],
      "arxiv_id": "2512.18940v1",
      "summary": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.",
      "categories": [
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "13 pages, 3 figures. Supplementary materials at https://doi.org/10.17605/OSF.IO/PV6R3",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18940v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱九：具身大模型 (Embodied Foundation Models)",
          "id": "9_embodied_foundation",
          "matched_keywords": [
            "large language model"
          ],
          "score": 3.0
        }
      ],
      "relevance_score": 3.0,
      "hit_pillars": [
        "9_embodied_foundation"
      ]
    },
    {
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "authors": [
        "Yujie Zhao",
        "Hongwei Fan",
        "Di Chen",
        "Shengcong Chen",
        "Liliang Chen",
        "Xiaoqi Li",
        "Guanghui Ren",
        "Hao Dong"
      ],
      "arxiv_id": "2512.19402v1",
      "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.GR"
      ],
      "primary_category": "cs.RO",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19402v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    },
    {
      "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
      "authors": [
        "Quyu Kong",
        "Xu Zhang",
        "Zhenyu Yang",
        "Nolan Gao",
        "Chen Liu",
        "Panrong Tong",
        "Chenglin Cai",
        "Hanzhang Zhou",
        "Jianan Zhang",
        "Liangyu Chen",
        "Zhidan Liu",
        "Steven Hoi",
        "Yue Wang"
      ],
      "arxiv_id": "2512.19432v1",
      "summary": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL",
      "published": "2025-12-22",
      "updated": "2025-12-22",
      "comment": "",
      "doi": "",
      "journal_ref": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19432v1",
      "code_links": [],
      "matched_interests": [
        {
          "name": "支柱一：机器人控制 (Robot Control)",
          "id": "1_robot_core",
          "matched_keywords": [
            "manipulation"
          ],
          "score": 2.0
        }
      ],
      "relevance_score": 2.0,
      "hit_pillars": [
        "1_robot_core"
      ]
    }
  ],
  "filtered": true
}