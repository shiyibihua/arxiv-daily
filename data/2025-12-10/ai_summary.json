{
    "papers": [
        {
            "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
            "authors": [
                "Arjun Parthasarathy",
                "Nimit Kalra",
                "Rohun Agrawal",
                "Yann LeCun",
                "Oumayma Bounou",
                "Pavel Izmailov",
                "Micah Goldblum"
            ],
            "arxiv_id": "2512.09929v1",
            "summary": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.",
            "headline_zh": "提出训练时数据合成方法以缩小世界模型在梯度规划中的训练-测试差距",
            "intro_zh": [
                "核心问题：世界模型训练基于状态预测，但测试时用于动作序列估计，存在性能差距",
                "方法要点：通过训练时数据合成技术改进世界模型，支持高效梯度规划",
                "实验或效果：在物体操作和导航任务中，性能优于或匹配CEM，时间预算仅需10%"
            ],
            "tags_zh": [
                "世界模型",
                "梯度规划",
                "模型预测控制",
                "训练-测试差距",
                "数据合成"
            ],
            "_index": 0
        },
        {
            "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
            "authors": [
                "Minghui Lin",
                "Pengxiang Ding",
                "Shu Wang",
                "Zifeng Zhuang",
                "Yang Liu",
                "Xinyang Tong",
                "Wenxuan Song",
                "Shangke Lyu",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "arxiv_id": "2512.09928v1",
            "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
            "headline_zh": "提出HiF-VLA框架，利用运动表示解决视觉-语言-动作模型的长时程连贯性问题。",
            "intro_zh": [
                "核心问题：现有VLA模型依赖马尔可夫假设，仅基于当前观测，导致长时程连贯性不足。",
                "方法要点：以运动为紧凑时态表示，通过后见、洞见和预见双向推理，实现“边行动边思考”范式。",
                "实验或效果：在LIBERO-Long和CALVIN ABC-D基准上超越基线，并在真实世界长时程操作任务中显著提升性能。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "运动表示",
                "长时程操作",
                "双向时态推理",
                "机器人操作"
            ],
            "_index": 1
        },
        {
            "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
            "authors": [
                "Yifan Ye",
                "Jiaqi Ma",
                "Jun Cen",
                "Zhihe Lu"
            ],
            "arxiv_id": "2512.09927v1",
            "summary": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}",
            "headline_zh": "提出TEAM-VLA训练无关的令牌压缩框架，以加速视觉-语言-动作模型推理并保持性能。",
            "intro_zh": [
                "核心问题：大规模VLA模型在实时部署中面临高计算成本和延迟挑战。",
                "方法要点：通过动态令牌扩展和动作感知合并，在单次前向传播中压缩令牌。",
                "实验或效果：在LIBERO基准上提升推理速度，同时维持或超越原始模型任务成功率。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "令牌压缩",
                "训练无关优化",
                "推理加速",
                "机器人感知控制"
            ],
            "_index": 2
        },
        {
            "title": "GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures",
            "authors": [
                "Patrick Noras",
                "Jun Myeong Choi",
                "Didier Stricker",
                "Pieter Peers",
                "Roni Sengupta"
            ],
            "arxiv_id": "2512.09925v1",
            "summary": "Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/",
            "headline_zh": "提出GAINS框架以解决稀疏多视角下高斯溅射逆渲染的几何与材质模糊问题",
            "intro_zh": [
                "核心问题：稀疏多视角捕获导致几何、反射率和光照严重模糊，现有方法性能下降",
                "方法要点：两阶段框架，先基于学习先验优化几何，再用分割、IID和扩散先验正则化材质恢复",
                "实验或效果：在合成和真实数据集上显著提升材质参数精度、重光照质量和新视角合成"
            ],
            "tags_zh": [
                "高斯溅射",
                "逆渲染",
                "稀疏多视角",
                "材质恢复",
                "学习先验",
                "几何优化"
            ],
            "_index": 3
        },
        {
            "title": "ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning",
            "authors": [
                "Xinyu Liu",
                "Hangjie Yuan",
                "Yujie Wei",
                "Jiazheng Xing",
                "Yujin Han",
                "Jiahao Pan",
                "Yanbiao Ma",
                "Chi-Min Chan",
                "Kang Zhao",
                "Shiwei Zhang",
                "Wenhan Luo",
                "Yike Guo"
            ],
            "arxiv_id": "2512.09924v1",
            "summary": "Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.",
            "headline_zh": "提出ReViSE框架以解决统一模型中推理感知视频编辑的难题",
            "intro_zh": [
                "核心问题：现有统一模型在推理感知视频编辑中存在能力脱节，缺乏合适数据集",
                "方法要点：引入RVE任务和RVE-Bench基准，提出自反式推理框架统一生成与评估",
                "实验或效果：在RVE-Bench上显著提升编辑准确性和视觉保真度，总体得分提高32%"
            ],
            "tags_zh": [
                "推理感知视频编辑",
                "自反式学习",
                "统一模型",
                "视频生成基准",
                "视觉语言模型"
            ],
            "_index": 4
        },
        {
            "title": "Splatent: Splatting Diffusion Latents for Novel View Synthesis",
            "authors": [
                "Or Hirschorn",
                "Omer Sela",
                "Inbar Huberman-Spiegelglas",
                "Netalee Efrat",
                "Eli Alshan",
                "Ianir Ideses",
                "Frederic Devernay",
                "Yochai Zvik",
                "Lior Fritz"
            ],
            "arxiv_id": "2512.09923v1",
            "summary": "Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.",
            "headline_zh": "提出Splatent框架，通过多视图注意力在VAE潜在空间中增强3D高斯泼溅以实现高质量新视角合成",
            "intro_zh": [
                "核心问题：VAE潜在空间缺乏多视图一致性，导致3D重建中纹理模糊和细节缺失",
                "方法要点：在VAE潜在空间中对3D高斯泼溅进行扩散增强，利用多视图注意力从输入视图恢复细节",
                "实验或效果：在多个基准测试中达到最先进水平，与现有前馈框架集成提升细节保留"
            ],
            "tags_zh": [
                "新视角合成",
                "3D高斯泼溅",
                "扩散模型",
                "VAE潜在空间",
                "多视图注意力",
                "稀疏视图重建"
            ],
            "_index": 5
        },
        {
            "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating",
            "authors": [
                "Junting Chen",
                "Yunchuan Li",
                "Panfeng Jiang",
                "Jiacheng Du",
                "Zixuan Chen",
                "Chenrui Tie",
                "Jiajun Deng",
                "Lin Shao"
            ],
            "arxiv_id": "2512.09920v1",
            "summary": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/",
            "headline_zh": "提出LISN-Bench基准与Social-Nav-Modulator系统，以解决语言指令社交导航问题。",
            "intro_zh": [
                "核心问题：现有社交导航研究主要关注路径效率和避障，缺乏对用户语言指令的遵循和场景理解。",
                "方法要点：采用基于VLM的快速-慢速分层系统，通过VLM代理调制成本图和控制器参数，解耦低级动作生成。",
                "实验或效果：在LISN-Bench上平均成功率91.3%，比最强基线提升超过63%，尤其在人群跟随和避禁区域任务中表现优异。"
            ],
            "tags_zh": [
                "社交导航",
                "语言指令",
                "视觉语言模型",
                "基准测试",
                "机器人控制",
                "动态避障"
            ],
            "_index": 6
        },
        {
            "title": "FALCON: Few-step Accurate Likelihoods for Continuous Flows",
            "authors": [
                "Danyal Rehman",
                "Tara Akhound-Sadegh",
                "Artem Gazizov",
                "Yoshua Bengio",
                "Alexander Tong"
            ],
            "arxiv_id": "2512.09914v1",
            "summary": "Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.",
            "headline_zh": "提出FALCON方法以解决连续流模型在分子玻尔兹曼采样中似然计算成本高的问题",
            "intro_zh": [
                "核心问题：连续归一化流模型在分子热力学平衡态采样中，似然计算需数千次函数评估，成本极高。",
                "方法要点：引入混合训练目标，促进模型可逆性，实现少步采样且似然足够精确用于重要性采样。",
                "实验或效果：FALCON在分子玻尔兹曼采样中优于当前最优归一化流模型，比等效CNF模型快两个数量级。"
            ],
            "tags_zh": [
                "分子玻尔兹曼采样",
                "连续归一化流",
                "重要性采样",
                "可逆性训练",
                "少步采样"
            ],
            "_index": 7
        },
        {
            "title": "NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway",
            "authors": [
                "Sander Riisøen Jyhne",
                "Aditya Gupta",
                "Ben Worsley",
                "Marianne Andersen",
                "Ivar Oveland",
                "Alexander Salveson Nossum"
            ],
            "arxiv_id": "2512.09913v1",
            "summary": "We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.",
            "headline_zh": "提出NordFKB细粒度基准数据集，用于挪威地理空间AI研究",
            "intro_zh": [
                "核心问题：缺乏挪威地理空间AI的细粒度基准数据集，限制方法评估与比较。",
                "方法要点：基于权威FKB数据，提供高分辨率正射影像与36类语义标注，覆盖七地理区域。",
                "实验或效果：发布标准化评估协议与工具，支持语义分割与目标检测，确保可复现研究。"
            ],
            "tags_zh": [
                "地理空间AI",
                "细粒度数据集",
                "语义分割",
                "目标检测",
                "挪威地理数据",
                "基准评估"
            ],
            "_index": 8
        },
        {
            "title": "Supervised learning pays attention",
            "authors": [
                "Erin Craig",
                "Robert Tibshirani"
            ],
            "arxiv_id": "2512.09912v1",
            "summary": "In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.\n  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.",
            "headline_zh": "提出注意力加权方法，将监督学习适配于表格数据以提升预测性能与可解释性。",
            "intro_zh": [
                "核心问题：传统监督学习在异质数据中难以灵活拟合个性化模型，且缺乏可解释性。",
                "方法要点：通过注意力加权训练数据，为每个测试点拟合局部模型，强调预测性特征和交互。",
                "实验或效果：在真实和模拟数据集中，该方法提高预测性能，理论证明在已知子群结构下降低均方误差。"
            ],
            "tags_zh": [
                "注意力加权",
                "监督学习",
                "表格数据",
                "可解释性",
                "局部模型",
                "异质数据"
            ],
            "_index": 9
        },
        {
            "title": "Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots",
            "authors": [
                "Radha Lahoti",
                "Ryan Chaiyakul",
                "M. Khalid Jawed"
            ],
            "arxiv_id": "2512.09911v1",
            "summary": "High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.",
            "headline_zh": "提出Py-DiSMech框架，基于离散微分几何实现软机器人高效建模与控制",
            "intro_zh": [
                "核心问题：软机器人仿真需高保真度、可扩展性，传统工具难以处理大变形和复杂接触。",
                "方法要点：采用离散微分几何直接离散化曲率与应变，结合向量化NumPy实现和隐式接触模型。",
                "实验或效果：在计算效率上显著超越Elastica，保持物理准确性，支持仿真驱动设计和控制验证。"
            ],
            "tags_zh": [
                "软机器人仿真",
                "离散微分几何",
                "高效建模",
                "隐式接触模型",
                "形状控制",
                "开源框架"
            ],
            "_index": 10
        },
        {
            "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach",
            "authors": [
                "Salvador Carrión",
                "Francisco Casacuberta"
            ],
            "arxiv_id": "2512.09910v1",
            "summary": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.",
            "headline_zh": "提出基于低秩适配的持续学习方法，以解决神经机器翻译中的灾难性遗忘和高计算成本问题。",
            "intro_zh": [
                "核心问题：神经机器翻译持续学习面临灾难性遗忘和重训练计算成本高的双重挑战。",
                "方法要点：采用低秩适配框架，结合交互式模块组合和基于梯度的正则化策略，实现参数高效调整。",
                "实验或效果：实验表明方法在性能媲美全参数技术的同时，有效保留旧知识并支持新任务学习。"
            ],
            "tags_zh": [
                "神经机器翻译",
                "持续学习",
                "低秩适配",
                "灾难性遗忘",
                "参数高效学习",
                "交互式调整"
            ],
            "_index": 11
        },
        {
            "title": "STACHE: Local Black-Box Explanations for Reinforcement Learning Policies",
            "authors": [
                "Andrew Elashkin",
                "Orna Grumberg"
            ],
            "arxiv_id": "2512.09909v1",
            "summary": "Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.",
            "headline_zh": "提出STACHE框架，为离散马尔可夫游戏中的强化学习策略生成局部黑盒解释",
            "intro_zh": [
                "核心问题：强化学习代理在稀疏奖励或安全关键环境中行为不可预测，需可靠调试工具",
                "方法要点：基于分解状态空间，通过搜索算法生成鲁棒性区域和最小反事实的复合解释",
                "实验或效果：在Gymnasium环境中验证，能解释策略动作并捕捉训练中策略逻辑的演变"
            ],
            "tags_zh": [
                "强化学习解释",
                "黑盒解释",
                "局部解释",
                "反事实分析",
                "策略调试"
            ],
            "_index": 12
        },
        {
            "title": "Bayesian Networks, Markov Networks, Moralisation, Triangulation: a Categorical Perspective",
            "authors": [
                "Antonio Lorenzin",
                "Fabio Zanasi"
            ],
            "arxiv_id": "2512.09908v1",
            "summary": "Moralisation and Triangulation are transformations allowing to switch between different ways of factoring a probability distribution into a graphical model. Moralisation allows to view a Bayesian network (a directed model) as a Markov network (an undirected model), whereas triangulation addresses the opposite direction. We present a categorical framework where these transformations are modelled as functors between a category of Bayesian networks and one of Markov networks. The two kinds of network (the objects of these categories) are themselves represented as functors from a `syntax' domain to a `semantics' codomain. Notably, moralisation and triangulation can be defined inductively on such syntax via functor pre-composition. Moreover, while moralisation is fully syntactic, triangulation relies on semantics. This leads to a discussion of the variable elimination algorithm, reinterpreted here as a functor in its own right, that splits the triangulation procedure in two: one purely syntactic, the other purely semantic. This approach introduces a functorial perspective into the theory of probabilistic graphical models, which highlights the distinctions between syntactic and semantic modifications.",
            "headline_zh": "提出范畴框架以建模贝叶斯网络与马尔可夫网络间的道德化和三角化变换",
            "intro_zh": [
                "核心问题：道德化和三角化作为概率图模型间的变换，缺乏统一理论框架",
                "方法要点：将网络表示为函子，通过函子前复合定义变换，区分语法与语义",
                "实验或效果：重新解释变量消除算法为函子，分割三角化过程为纯语法和纯语义部分"
            ],
            "tags_zh": [
                "概率图模型",
                "范畴论",
                "道德化",
                "三角化",
                "变量消除",
                "语法语义区分"
            ],
            "_index": 13
        },
        {
            "title": "VisualActBench: Can VLMs See and Act like a Human?",
            "authors": [
                "Daoan Zhang",
                "Pai Liu",
                "Xiaofei Zhou",
                "Yuan Ge",
                "Guangchen Lan",
                "Jing Bi",
                "Christopher Brinton",
                "Ehsan Hoque",
                "Jiebo Luo"
            ],
            "arxiv_id": "2512.09907v1",
            "summary": "Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.",
            "headline_zh": "提出VisualActBench基准以评估视觉语言模型在无文本提示下的主动视觉动作推理能力",
            "intro_zh": [
                "核心问题：视觉语言模型在仅凭视觉输入进行主动推理和行动的能力尚未充分探索",
                "方法要点：引入视觉动作推理任务，构建包含1074个视频和3733个人工标注动作的大规模基准",
                "实验或效果：评估29个模型，发现前沿模型表现相对较强，但与人类推理水平仍有显著差距"
            ],
            "tags_zh": [
                "视觉动作推理",
                "视觉语言模型评估",
                "主动视觉理解",
                "人类对齐基准",
                "视频动作标注"
            ],
            "_index": 14
        },
        {
            "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos",
            "authors": [
                "Ryan Meegan",
                "Adam D'Souza",
                "Bryan Bo Cao",
                "Shubham Jain",
                "Kristin Dana"
            ],
            "arxiv_id": "2512.09903v1",
            "summary": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.",
            "headline_zh": "提出YOPO-Nav方法，利用单次视频构建3DGS图实现视觉导航",
            "intro_zh": [
                "核心问题：视觉导航依赖3D地图构建，计算和内存开销大，需高效替代方案。",
                "方法要点：使用单次探索视频编码为局部3DGS模型图，结合VPR粗定位和3DGS精调进行导航控制。",
                "实验或效果：在YOPO-Campus数据集上测试，物理机器人实验显示在真实场景中性能优异。"
            ],
            "tags_zh": [
                "视觉导航",
                "3D高斯泼溅",
                "单次视频学习",
                "视觉地点识别",
                "机器人控制"
            ],
            "_index": 15
        },
        {
            "title": "Visual Heading Prediction for Autonomous Aerial Vehicles",
            "authors": [
                "Reza Ahmari",
                "Ahmad Mohammadi",
                "Vahid Hemmati",
                "Mohammed Mynuddin",
                "Parham Kebria",
                "Mahmoud Nabil Mahmoud",
                "Xiaohong Yuan",
                "Abdollah Homaifar"
            ],
            "arxiv_id": "2512.09898v1",
            "summary": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration",
            "headline_zh": "提出基于视觉的无人机-无人车集成框架，用于GPS缺失环境下的实时导航与协调。",
            "intro_zh": [
                "核心问题：无人机与无人车在GPS不可用或降级时实时协调困难，需精确检测与航向预测。",
                "方法要点：使用微调YOLOv5检测无人车并提取特征，轻量ANN预测无人机航向角，仅需单目相机输入。",
                "实验或效果：在受控实验室环境中收集超13,000张标注图像，ANN预测误差小，检测准确率达95%。"
            ],
            "tags_zh": [
                "无人机-无人车集成",
                "视觉航向预测",
                "YOLOv5检测",
                "轻量神经网络",
                "GPS缺失环境",
                "实时协调"
            ],
            "_index": 16
        },
        {
            "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments",
            "authors": [
                "Haoye Lu",
                "Pavan Seshadri",
                "Kaheer Suleman"
            ],
            "arxiv_id": "2512.09897v1",
            "summary": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.",
            "headline_zh": "提出SCOPE方法，利用LLM生成子目标一次性预训练轻量模型，以高效解决文本环境中的分层规划问题。",
            "intro_zh": [
                "核心问题：文本环境中长期规划面临开放动作空间、模糊观察和稀疏反馈，现有方法依赖重复查询LLM，计算成本高且缺乏适应性。",
                "方法要点：SCOPE通过LLM生成子目标仅用于初始化，预训练学生模型，避免训练和推理中的重复查询，提升效率但可能牺牲解释性和子目标最优性。",
                "实验或效果：在TextCraft环境中，SCOPE达到0.56成功率，优于ADaPT的0.52，推理时间从164.4秒降至3.0秒，显著提升效率。"
            ],
            "tags_zh": [
                "分层规划",
                "文本环境",
                "语言模型蒸馏",
                "子目标生成",
                "高效推理",
                "预训练模型"
            ],
            "_index": 17
        },
        {
            "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science",
            "authors": [
                "Jane Greenberg",
                "Scott McClellan",
                "Addy Ireland",
                "Robert Sammarco",
                "Colton Gerber",
                "Christopher B. Rauch",
                "Mat Kelly",
                "John Kunze",
                "Yuan An",
                "Eric Toberer"
            ],
            "arxiv_id": "2512.09895v1",
            "summary": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.",
            "headline_zh": "提出MatSci-YAMZ平台，集成AI与人机协同以支持材料科学元数据词汇开发",
            "intro_zh": [
                "核心问题：元数据词汇开发受限于人力资源不足和标准化实践不一致，阻碍FAIR和FARR原则推进",
                "方法要点：结合人工智能与人机协同，包括众包，通过迭代反馈循环精炼AI生成的术语定义",
                "实验或效果：在材料科学领域进行概念验证，6名参与者贡献定义，成功生成19个AI定义，证实模型可行性"
            ],
            "tags_zh": [
                "元数据词汇开发",
                "人机协同",
                "人工智能",
                "材料科学",
                "众包",
                "FAIR原则"
            ],
            "_index": 18
        },
        {
            "title": "Exploring Protein Language Model Architecture-Induced Biases for Antibody Comprehension",
            "authors": [
                "Mengren",
                "Liu",
                "Yixiang Zhang",
                "Yiming",
                "Zhang"
            ],
            "arxiv_id": "2512.09894v1",
            "summary": "Recent advances in protein language models (PLMs) have demonstrated remarkable capabilities in understanding protein sequences. However, the extent to which different model architectures capture antibody-specific biological properties remains unexplored. In this work, we systematically investigate how architectural choices in PLMs influence their ability to comprehend antibody sequence characteristics and functions. We evaluate three state-of-the-art PLMs-AntiBERTa, BioBERT, and ESM2--against a general-purpose language model (GPT-2) baseline on antibody target specificity prediction tasks. Our results demonstrate that while all PLMs achieve high classification accuracy, they exhibit distinct biases in capturing biological features such as V gene usage, somatic hypermutation patterns, and isotype information. Through attention attribution analysis, we show that antibody-specific models like AntiBERTa naturally learn to focus on complementarity-determining regions (CDRs), while general protein models benefit significantly from explicit CDR-focused training strategies. These findings provide insights into the relationship between model architecture and biological feature extraction, offering valuable guidance for future PLM development in computational antibody design.",
            "headline_zh": "探索蛋白质语言模型架构诱导的偏见以提升抗体理解能力",
            "intro_zh": [
                "核心问题：不同蛋白质语言模型架构如何影响抗体特异性生物特征的捕获能力",
                "方法要点：系统评估AntiBERTa、BioBERT、ESM2和GPT-2在抗体靶标特异性预测任务中的表现",
                "实验或效果：通过注意力归因分析揭示模型在V基因使用、体细胞超突变和同种型信息上的偏见"
            ],
            "tags_zh": [
                "蛋白质语言模型",
                "抗体理解",
                "架构偏见",
                "注意力归因分析",
                "计算抗体设计"
            ],
            "_index": 19
        },
        {
            "title": "Provably Learning from Modern Language Models via Low Logit Rank",
            "authors": [
                "Noah Golowich",
                "Allen Liu",
                "Abhishek Shetty"
            ],
            "arxiv_id": "2512.09892v1",
            "summary": "While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.\n  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.",
            "headline_zh": "提出基于低对数秩的高效查询学习算法，以从现代语言模型中获取可证明学习保证。",
            "intro_zh": [
                "核心问题：现代语言模型复杂，但经验上对数秩低，如何利用此结构进行可证明学习。",
                "方法要点：在查询学习模型中，设计算法从低对数秩模型中高效学习，适用于API访问场景。",
                "实验或效果：算法能学习近似低对数秩模型，为生成模型提供首个端到端学习保证。"
            ],
            "tags_zh": [
                "低对数秩",
                "查询学习",
                "可证明学习",
                "语言模型抽象",
                "生成模型学习"
            ],
            "_index": 20
        },
        {
            "title": "Analysis of Dirichlet Energies as Over-smoothing Measures",
            "authors": [
                "Anna Bison",
                "Alessandro Sperduti"
            ],
            "arxiv_id": "2512.09890v1",
            "summary": "We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \\textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.",
            "headline_zh": "分析Dirichlet能量作为过平滑度量，区分未归一化与归一化图拉普拉斯诱导的度量",
            "intro_zh": [
                "核心问题：比较未归一化与归一化图拉普拉斯诱导的Dirichlet能量作为过平滑度量的差异",
                "方法要点：形式化两种定义的光谱性质，证明归一化版本不满足节点相似性度量的公理定义",
                "实验或效果：强调选择与GNN架构光谱兼容的度量的关键区别，以解决监控动态中的模糊性"
            ],
            "tags_zh": [
                "Dirichlet能量",
                "过平滑度量",
                "图拉普拉斯",
                "图神经网络",
                "节点相似性",
                "光谱性质"
            ],
            "_index": 21
        },
        {
            "title": "HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression",
            "authors": [
                "Gustavo Coelho Haase",
                "Paulo Henrique Dourado da Silva"
            ],
            "arxiv_id": "2512.09886v1",
            "summary": "Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \\textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.",
            "headline_zh": "提出HPM-KD框架以解决知识蒸馏中的超参数敏感、容量差距和多教师协调问题",
            "intro_zh": [
                "核心问题：知识蒸馏存在超参数敏感、容量差距、多教师协调差和计算资源低效问题",
                "方法要点：集成自适应配置、渐进蒸馏链、注意力加权多教师集成等六组件",
                "实验或效果：在CIFAR和表格数据集上实现10-15倍压缩，保持85%精度，减少30-40%训练时间"
            ],
            "tags_zh": [
                "知识蒸馏",
                "模型压缩",
                "多教师学习",
                "元学习",
                "并行处理"
            ],
            "_index": 22
        },
        {
            "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
            "authors": [
                "Justin W. Lin",
                "Eliot Krzysztof Jones",
                "Donovan Julian Jasper",
                "Ethan Jun-shen Ho",
                "Anna Wu",
                "Arnold Tianyi Yang",
                "Neil Perry",
                "Andy Zou",
                "Matt Fredrikson",
                "J. Zico Kolter",
                "Percy Liang",
                "Dan Boneh",
                "Daniel E. Ho"
            ],
            "arxiv_id": "2512.09882v1",
            "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
            "headline_zh": "提出ARTEMIS多智能体框架，在真实企业环境中评估AI与人类网络安全专家的渗透测试性能。",
            "intro_zh": [
                "核心问题：首次在真实企业环境中全面评估AI智能体与人类网络安全专家的渗透测试能力对比。",
                "方法要点：ARTEMIS采用动态提示生成、任意子智能体和自动漏洞分类的多智能体框架。",
                "实验或效果：ARTEMIS在8000主机网络中排名第二，发现9个有效漏洞，优于9/10人类参与者，但存在高误报率和GUI任务困难。"
            ],
            "tags_zh": [
                "AI智能体",
                "渗透测试",
                "网络安全评估",
                "多智能体框架",
                "漏洞发现"
            ],
            "_index": 23
        },
        {
            "title": "Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs",
            "authors": [
                "Pius Horn",
                "Janis Keuper"
            ],
            "arxiv_id": "2512.09874v1",
            "summary": "Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench",
            "headline_zh": "提出基于合成PDF与LLM评估的基准框架，以解决数学公式提取的评测难题。",
            "intro_zh": [
                "核心问题：现有PDF解析器评测基准缺乏对数学公式的语义评估，影响下游应用。",
                "方法要点：使用合成PDF生成精确LaTeX真值，并引入LLM作为语义评估器，结合两阶段匹配处理输出不一致性。",
                "实验或效果：通过人类验证，LLM评估与人类判断相关性高（Pearson r=0.78），评测20+解析器揭示性能差异。"
            ],
            "tags_zh": [
                "PDF解析基准",
                "数学公式提取",
                "LLM评估",
                "合成数据生成",
                "语义匹配"
            ],
            "_index": 24
        },
        {
            "title": "FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning",
            "authors": [
                "Khurram Khalil",
                "Khaza Anuarul Hoque"
            ],
            "arxiv_id": "2512.09872v1",
            "summary": "Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation.",
            "headline_zh": "提出FlipLLM框架，利用强化学习高效发现多模态大模型的位翻转攻击漏洞。",
            "intro_zh": [
                "核心问题：现有位翻转攻击发现方法泛化性差、难以扩展，无法高效分析大模型参数空间。",
                "方法要点：结合敏感度引导的层剪枝与Q学习，将攻击发现建模为序列决策问题。",
                "实验或效果：在LLaMA 3.1 8B和LLaVA等模型上，仅翻转少量位即可使准确率骤降，速度比现有方法快2.5倍。"
            ],
            "tags_zh": [
                "位翻转攻击",
                "强化学习",
                "多模态大模型",
                "硬件安全",
                "漏洞发现",
                "Q学习"
            ],
            "_index": 25
        },
        {
            "title": "Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling",
            "authors": [
                "Yimin Zhu",
                "Lincoln Linlin Xu"
            ],
            "arxiv_id": "2512.09871v1",
            "summary": "Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.",
            "headline_zh": "提出DPS4Un扩散后验采样器，用于高光谱解混与光谱变异性建模",
            "intro_zh": [
                "核心问题：线性光谱混合模型中的光谱先验分布与光谱变异性建模挑战",
                "方法要点：利用预训练条件扩散模型作为后验采样器，结合超像素构建端元束训练先验",
                "实验或效果：在三个真实基准数据集上优于现有高光谱解混方法"
            ],
            "tags_zh": [
                "高光谱解混",
                "扩散模型",
                "光谱变异性",
                "贝叶斯框架",
                "超像素分割"
            ],
            "_index": 26
        },
        {
            "title": "MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI",
            "authors": [
                "Fengli Wu",
                "Vaidehi Patil",
                "Jaehong Yoon",
                "Yue Zhang",
                "Mohit Bansal"
            ],
            "arxiv_id": "2512.09867v1",
            "summary": "Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the \"right to be forgotten\". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.",
            "headline_zh": "提出MedForget测试床以解决医疗AI中多模态大模型在隐私法规下的选择性遗忘问题",
            "intro_zh": [
                "核心问题：医疗AI中多模态大模型训练涉及敏感数据，需满足HIPAA/GDPR的遗忘权要求，但现有遗忘方法在复杂医疗场景中效果未知",
                "方法要点：构建层次感知多模态遗忘测试床，模拟医院数据嵌套层次，包含保留与遗忘分割及重述变体评估集",
                "实验或效果：在三个任务上测试四种SOTA遗忘方法，显示现有方法难以实现完全、层次感知的遗忘而不降低诊断性能"
            ],
            "tags_zh": [
                "医疗AI",
                "多模态大模型",
                "选择性遗忘",
                "隐私合规",
                "层次感知评估",
                "重建攻击"
            ],
            "_index": 27
        },
        {
            "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
            "authors": [
                "Hao Lu",
                "Ziyang Liu",
                "Guangfeng Jiang",
                "Yuanfei Luo",
                "Sheng Chen",
                "Yangang Zhang",
                "Ying-Cong Chen"
            ],
            "arxiv_id": "2512.09864v1",
            "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
            "headline_zh": "提出UniUGP框架以统一理解、生成和规划，提升自动驾驶在长尾场景中的性能。",
            "intro_zh": [
                "核心问题：自动驾驶系统在长尾场景中因世界知识有限和视觉动态建模弱而表现不佳。",
                "方法要点：构建专用数据集，通过混合专家架构整合视觉语言模型和视频生成模型，实现场景推理、未来视频生成和轨迹规划的统一。",
                "实验或效果：在感知、推理和决策方面达到先进水平，对挑战性长尾场景具有优越泛化能力。"
            ],
            "tags_zh": [
                "自动驾驶",
                "视觉语言动作模型",
                "视频生成",
                "轨迹规划",
                "长尾场景",
                "混合专家架构"
            ],
            "_index": 28
        },
        {
            "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
            "authors": [
                "Yuyang Li",
                "Yinghan Chen",
                "Zihang Zhao",
                "Puhao Li",
                "Tengyu Liu",
                "Siyuan Huang",
                "Yixin Zhu"
            ],
            "arxiv_id": "2512.09851v1",
            "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
            "headline_zh": "提出TacThru传感器与TacThru-UMI框架，实现同步触觉-视觉感知以提升机器人操作性能",
            "intro_zh": [
                "现有透皮传感器缺乏同步多模态感知且触觉跟踪不可靠，阻碍机器人操作学习",
                "TacThru传感器通过全透明弹性体、持续照明和关键线标记实现同步视觉与稳健触觉信号提取",
                "TacThru-UMI框架基于Transformer扩散策略整合多模态信号，在五项任务中平均成功率85.5%"
            ],
            "tags_zh": [
                "透皮传感器",
                "多模态感知",
                "模仿学习",
                "机器人操作",
                "触觉视觉融合",
                "扩散策略"
            ],
            "_index": 29
        },
        {
            "title": "Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime",
            "authors": [
                "Simone Cuonzo",
                "Nina Deliu"
            ],
            "arxiv_id": "2512.09850v1",
            "summary": "We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.\n  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.",
            "headline_zh": "提出Conformal Bandits框架，将共形预测融入赌博机问题，以在小差距场景中实现统计有效性和奖励效率。",
            "intro_zh": [
                "传统赌博机策略如Thompson Sampling和UCB依赖分布假设或渐近保证，忽视统计性质，在小差距场景中表现不佳。",
                "通过共形预测，将决策策略的遗憾最小化潜力与有限时间预测覆盖的统计保证相结合。",
                "模拟研究和投资组合分配应用显示，在小差距场景中提升遗憾效率和覆盖保证，结合隐马尔可夫模型增强探索-利用权衡。"
            ],
            "tags_zh": [
                "共形预测",
                "赌博机问题",
                "小差距场景",
                "统计保证",
                "投资组合分配",
                "隐马尔可夫模型"
            ],
            "_index": 30
        },
        {
            "title": "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities",
            "authors": [
                "Shijia Feng",
                "Michael Wray",
                "Walterio Mayol-Cuevas"
            ],
            "arxiv_id": "2512.09847v1",
            "summary": "Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.",
            "headline_zh": "提出在线检测与预测模型，以实时识别和预测用户在任务中的困难，适用于智能辅助系统。",
            "intro_zh": [
                "核心问题：现有研究多关注离线困难分类与定位，但实时应用需在线检测与预测困难。",
                "方法要点：将困难定位重构为在线检测任务，并扩展至预测，采用现成模型作为基线。",
                "实验或效果：在线检测达到70-80%每帧mAP，预测性能略有下降，模型在跨任务泛化中优于随机基线4-20%。"
            ],
            "tags_zh": [
                "在线困难检测",
                "困难预测",
                "智能辅助系统",
                "实时应用",
                "跨任务泛化"
            ],
            "_index": 31
        },
        {
            "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models",
            "authors": [
                "Yijing Chen",
                "Yihan Wu",
                "Kaisi Guan",
                "Yuchen Ren",
                "Yuyue Wang",
                "Ruihua Song",
                "Liyun Ru"
            ],
            "arxiv_id": "2512.09841v1",
            "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.",
            "headline_zh": "提出ChronusOmni以增强全模态大语言模型在视听时序理解中的显式和隐式时序感知能力",
            "intro_zh": [
                "核心问题：现有方法在视听时序理解中音频模态利用不足，且忽视跨模态隐式时序关系，如视觉与音频的交叉关联。",
                "方法要点：通过文本时间戳令牌与视听表示交错，实现统一时序建模；结合强化学习奖励函数强化时序排序和细粒度推理。",
                "实验或效果：在自建数据集ChronusAV上性能提升超30%，并在其他时序基准测试中取得领先，同时保持通用视听理解能力。"
            ],
            "tags_zh": [
                "全模态大语言模型",
                "视听时序理解",
                "跨模态隐式时序",
                "强化学习",
                "时序建模",
                "长视频分析"
            ],
            "_index": 32
        },
        {
            "title": "Fast Factorized Learning: Powered by In-Memory Database Systems",
            "authors": [
                "Bernhard Stöckl",
                "Maximilian E. Schüle"
            ],
            "arxiv_id": "2512.09836v1",
            "summary": "Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.",
            "headline_zh": "实现基于内存数据库的因子化学习，加速机器学习训练流程",
            "intro_zh": [
                "核心问题：因子化学习在内存数据库系统中的性能增益未知，缺乏可复现代码",
                "方法要点：在数据库中实现因子化学习，利用共享共因子预计算避免冗余计算",
                "实验或效果：在内存数据库上比非因子化学习快70%，比磁盘数据库快100倍"
            ],
            "tags_zh": [
                "因子化学习",
                "内存数据库",
                "机器学习加速",
                "数据库内学习",
                "线性回归",
                "性能基准测试"
            ],
            "_index": 33
        },
        {
            "title": "Predicting the Containment Time of California Wildfires Using Machine Learning",
            "authors": [
                "Shashank Bhardwaj"
            ],
            "arxiv_id": "2512.09835v1",
            "summary": "California's wildfire season keeps getting worse over the years, overwhelming the emergency response teams. These fires cause massive destruction to both property and human life. Because of these reasons, there's a growing need for accurate and practical predictions that can help assist with resources allocation for the Wildfire managers or the response teams. In this research, we built machine learning models to predict the number of days it will require to fully contain a wildfire in California. Here, we addressed an important gap in the current literature. Most prior research has concentrated on wildfire risk or how fires spread, and the few that examine the duration typically predict it in broader categories rather than a continuous measure. This research treats the wildfire duration prediction as a regression task, which allows for more detailed and precise forecasts rather than just the broader categorical predictions used in prior work. We built the models by combining three publicly available datasets from California Department of Forestry and Fire Protection's Fire and Resource Assessment Program (FRAP). This study compared the performance of baseline ensemble regressor, Random Forest and XGBoost, with a Long Short-Term Memory (LSTM) neural network. The results show that the XGBoost model slightly outperforms the Random Forest model, likely due to its superior handling of static features in the dataset. The LSTM model, on the other hand, performed worse than the ensemble models because the dataset lacked temporal features. Overall, this study shows that, depending on the feature availability, Wildfire managers or Fire management authorities can select the most appropriate model to accurately predict wildfire containment duration and allocate resources effectively.",
            "headline_zh": "提出基于机器学习的回归模型以预测加州野火扑灭天数，辅助资源分配。",
            "intro_zh": [
                "核心问题：现有研究多关注野火风险或蔓延，缺乏对扑灭天数的连续预测，影响应急响应效率。",
                "方法要点：整合加州FRAP公开数据集，构建XGBoost、随机森林和LSTM模型进行回归任务，对比性能。",
                "实验或效果：XGBoost因处理静态特征更优而略胜随机森林，LSTM因数据缺乏时序特征表现较差，模型选择取决于特征可用性。"
            ],
            "tags_zh": [
                "野火扑灭预测",
                "机器学习回归",
                "XGBoost模型",
                "资源分配优化",
                "加州FRAP数据集"
            ],
            "_index": 34
        },
        {
            "title": "Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration",
            "authors": [
                "Elias Krantz",
                "Ngai Nam Chan",
                "Gunnar Tibert",
                "Huina Mao",
                "Christer Fuglesang"
            ],
            "arxiv_id": "2512.09833v1",
            "summary": "Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.",
            "headline_zh": "提出轻量级开源通信桥接器，连接Basilisk航天动力学模拟器与ROS 2，支持模块化航天器自主开发。",
            "intro_zh": [
                "核心问题：高保真航天器模拟器与模块化机器人框架集成困难，阻碍自主性开发。",
                "方法要点：无需修改Basilisk核心，实现实时双向数据交换，无缝集成ROS 2节点。",
                "实验或效果：在领航-跟随编队飞行场景中验证，支持仿真与硬件测试的平滑过渡。"
            ],
            "tags_zh": [
                "航天器模拟",
                "ROS 2集成",
                "模块化自主系统",
                "硬件在环测试",
                "编队飞行控制"
            ],
            "_index": 35
        },
        {
            "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning",
            "authors": [
                "Chainarong Amornbunchornvej"
            ],
            "arxiv_id": "2512.09831v1",
            "summary": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.",
            "headline_zh": "提出基于线性变换的认知几何模型，以分析异构智能体间的信念传播与意义保持",
            "intro_zh": [
                "核心问题：异构智能体间信念传播中的意义失真与理解限制问题",
                "方法要点：将信念建模为向量，通过线性解释映射模拟传播，引入零空间准则",
                "实验或效果：推导出领导力条件，解释信念扭曲、动机漂移等现象的代数约束"
            ],
            "tags_zh": [
                "认知几何模型",
                "信念传播",
                "线性变换",
                "异构智能体",
                "意义保持",
                "社会认知"
            ],
            "_index": 36
        },
        {
            "title": "LLMs in Interpreting Legal Documents",
            "authors": [
                "Simone Corbo"
            ],
            "arxiv_id": "2512.09830v1",
            "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",
            "headline_zh": "探索大语言模型在法律文档解释中的应用，分析其优化传统法律任务的潜力与挑战。",
            "intro_zh": [
                "核心问题：大语言模型如何应用于法律领域，以辅助解释法规、合同和案例法。",
                "方法要点：分析可能用例，如法律摘要、合同谈判和信息检索，以增强清晰度。",
                "实验或效果：提出两个不同基准，并讨论算法单一性、幻觉和合规性等挑战。"
            ],
            "tags_zh": [
                "大语言模型",
                "法律文档解释",
                "法律任务优化",
                "算法挑战",
                "合规性分析",
                "基准评估"
            ],
            "_index": 37
        },
        {
            "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning",
            "authors": [
                "Khurram Khalil",
                "Muhammad Mahad Khaliq",
                "Khaza Anuarul Hoque"
            ],
            "arxiv_id": "2512.09829v1",
            "summary": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.",
            "headline_zh": "提出RIFT框架，利用强化学习自动化发现最小高影响故障场景，以解决AI加速器故障评估的可扩展性问题。",
            "intro_zh": [
                "核心问题：现代AI加速器规模巨大，传统故障评估方法计算成本高且关键故障模式覆盖差。",
                "方法要点：将最坏故障搜索转化为序列决策问题，结合混合敏感度分析和强化学习生成最小高影响测试套件。",
                "实验或效果：在十亿参数LLM工作负载上，RIFT比进化方法快2.2倍，测试向量量比随机注入减少99%以上，故障覆盖更优。"
            ],
            "tags_zh": [
                "AI加速器故障评估",
                "强化学习引导",
                "最小高影响测试",
                "混合敏感度分析",
                "可扩展框架",
                "硬件保护策略"
            ],
            "_index": 38
        },
        {
            "title": "Composing Concepts from Images and Videos via Concept-prompt Binding",
            "authors": [
                "Xianghao Kong",
                "Zeyu Zhang",
                "Yuwei Guo",
                "Zhuoran Zhao",
                "Songchun Zhang",
                "Anyi Rao"
            ],
            "arxiv_id": "2512.09824v1",
            "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.",
            "headline_zh": "提出Bind & Compose方法，通过概念-提示绑定实现图像与视频的灵活视觉概念组合。",
            "intro_zh": [
                "核心问题：视觉概念组合在准确提取复杂概念和灵活结合图像与视频概念方面存在不足。",
                "方法要点：采用分层绑定结构和多样化吸收机制，结合时间解耦策略，提升概念绑定准确性和兼容性。",
                "实验或效果：评估显示在概念一致性、提示保真度和运动质量上优于现有方法，拓展视觉创意可能性。"
            ],
            "tags_zh": [
                "视觉概念组合",
                "扩散变换器",
                "概念-提示绑定",
                "时间解耦",
                "多样化吸收机制"
            ],
            "_index": 39
        },
        {
            "title": "A roadmap of geospatial soil quality analysis systems",
            "authors": [
                "Habiba BEN ABDERRAHMANE",
                "Slimane Oulad-Naoui",
                "Benameur ZIANI"
            ],
            "arxiv_id": "2512.09817v1",
            "summary": "Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.",
            "headline_zh": "提出统一模块化管道，整合多源数据与机器学习，以支持透明可扩展的土壤质量评估。",
            "intro_zh": [
                "核心问题：传统土壤质量评估方法成本高、覆盖有限，阻碍可持续农业和环境规划。",
                "方法要点：整合地理信息系统、遥感技术和机器学习算法，构建模块化评估管道。",
                "实验或效果：未知，但论文包括实际应用，并探讨挑战与未来趋势以提升系统透明度和适应性。"
            ],
            "tags_zh": [
                "土壤质量评估",
                "地理信息系统",
                "遥感技术",
                "机器学习",
                "可持续土地管理",
                "多源数据整合"
            ],
            "_index": 40
        },
        {
            "title": "DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation",
            "authors": [
                "Zhizhong Wang",
                "Tianyi Chu",
                "Zeyi Huang",
                "Nanyang Wang",
                "Kehan Li"
            ],
            "arxiv_id": "2512.09814v1",
            "summary": "Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.",
            "headline_zh": "提出DynaIP动态图像提示适配器，以增强零样本个性化文本到图像生成的细粒度保真度、概念-提示平衡和主题可扩展性。",
            "intro_zh": [
                "核心问题：现有方法在概念保留与提示跟随平衡、细粒度细节保留和多主题可扩展性方面存在挑战。",
                "方法要点：基于MM-DiT的解耦学习行为，设计动态解耦策略和分层专家混合特征融合模块，提升性能。",
                "实验或效果：在单主题和多主题任务中验证DynaIP优于现有方法，推动领域进展。"
            ],
            "tags_zh": [
                "个性化文本到图像生成",
                "零样本学习",
                "多模态扩散变换器",
                "动态解耦策略",
                "分层特征融合",
                "多主题可扩展性"
            ],
            "_index": 41
        },
        {
            "title": "Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering",
            "authors": [
                "Adithya K Moorthy",
                "V Vijaya Saradhi",
                "Bhanu Prasad"
            ],
            "arxiv_id": "2512.09810v1",
            "summary": "Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.",
            "headline_zh": "提出公平k近邻和ε邻域图构建方法以解决谱聚类中的图构建偏见问题",
            "intro_zh": [
                "传统图聚类方法在构建邻域图时可能因不公平的边选择而传播偏见，导致聚类结果不公",
                "通过在图构建早期阶段引入公平约束，确保每个节点的邻域中敏感特征群体比例均衡，同时保持几何一致性",
                "在合成、表格和图像数据集上实验证明，该方法在公平性上优于现有基线，无需修改聚类算法本身"
            ],
            "tags_zh": [
                "公平谱聚类",
                "图构建",
                "邻域图",
                "无监督学习",
                "公平性约束",
                "拓扑公平"
            ],
            "_index": 42
        },
        {
            "title": "CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing",
            "authors": [
                "Jianfei Li",
                "Ines Rosellon-Inclan",
                "Gitta Kutyniok",
                "Jean-Luc Starck"
            ],
            "arxiv_id": "2512.09806v1",
            "summary": "U-Net and other U-shaped architectures have achieved significant success in image deconvolution tasks. However, challenges have emerged, as these methods might generate unrealistic artifacts or hallucinations, which can interfere with analysis in safety-critical scenarios. This paper introduces a novel approach for quantifying and comprehending hallucination artifacts to ensure trustworthy computer vision models. Our method, termed the Conformal Hallucination Estimation Metric (CHEM), is applicable to any image reconstruction model, enabling efficient identification and quantification of hallucination artifacts. It offers two key advantages: it leverages wavelet and shearlet representations to efficiently extract hallucinations of image features and uses conformalized quantile regression to assess hallucination levels in a distribution-free manner. Furthermore, from an approximation theoretical perspective, we explore the reasons why U-shaped networks are prone to hallucinations. We test the proposed approach on the CANDELS astronomical image dataset with models such as U-Net, SwinUNet, and Learnlets, and provide new perspectives on hallucination from different aspects in deep learning-based image processing.",
            "headline_zh": "提出CHEM方法以量化图像处理中的幻觉伪影，确保模型可信度",
            "intro_zh": [
                "U-Net等U形架构在图像去卷积中易产生幻觉伪影，影响安全关键场景分析",
                "CHEM利用小波和剪切波表示提取伪影，结合保形分位数回归进行无分布量化",
                "在CANDELS天文数据集上测试U-Net等模型，从近似理论角度探讨幻觉成因"
            ],
            "tags_zh": [
                "幻觉量化",
                "图像去卷积",
                "保形回归",
                "小波表示",
                "U形网络",
                "天文图像处理"
            ],
            "_index": 43
        },
        {
            "title": "OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations",
            "authors": [
                "Jens Albrecht",
                "Robert Lehmann",
                "Aleksandra Poltermann",
                "Eric Rudolph",
                "Philipp Steigerwald",
                "Mara Stieler"
            ],
            "arxiv_id": "2512.09804v1",
            "summary": "This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.",
            "headline_zh": "提出OnCoCo 1.0公开数据集，用于在线心理咨询对话的细粒度消息分类。",
            "intro_zh": [
                "核心问题：现有基于动机访谈的分类系统在在线心理咨询中受限，依赖面对面数据，难以详细分析文本对话。",
                "方法要点：开发包含38类咨询师和28类客户话语的新编码方案，构建约2,800条消息的标注数据集。",
                "实验或效果：微调多个模型验证数据集适用性，数据和模型公开可用，扩展心理健康对话分析资源。"
            ],
            "tags_zh": [
                "在线心理咨询",
                "细粒度分类",
                "对话分析",
                "公开数据集",
                "心理健康对话"
            ],
            "_index": 44
        },
        {
            "title": "Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation",
            "authors": [
                "Tien-Dat Chung",
                "Ba-Thinh Lam",
                "Thanh-Huy Nguyen",
                "Thien Nguyen",
                "Nguyen Lan Vi Vu",
                "Hoang-Loc Cao",
                "Phat Kim Huynh",
                "Min Xu"
            ],
            "arxiv_id": "2512.09801v1",
            "summary": "Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\\%, 5\\%, and 10\\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.",
            "headline_zh": "提出模态特定增强与互补融合框架以解决半监督多模态脑肿瘤分割中的跨模态差异问题",
            "intro_zh": [
                "核心问题：现有半监督多模态方法因语义差异和错位难以利用模态间互补信息",
                "方法要点：引入模态特定增强模块和可学习互补信息融合模块，优化混合目标函数",
                "实验或效果：在BraTS 2019数据集上优于基线，提升Dice和Sensitivity分数"
            ],
            "tags_zh": [
                "半监督学习",
                "多模态医学图像分割",
                "脑肿瘤分割",
                "跨模态融合",
                "注意力机制",
                "一致性正则化"
            ],
            "_index": 45
        },
        {
            "title": "Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers",
            "authors": [
                "Zhaolan Huang",
                "Kaspar Schleiser",
                "Gyungmin Myung",
                "Emmanuel Baccelli"
            ],
            "arxiv_id": "2512.09800v1",
            "summary": "Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.",
            "headline_zh": "提出Ariel-ML工具包，在异构多核微控制器上实现嵌入式Rust的神经网络并行化推理",
            "intro_zh": [
                "问题：现有嵌入式Rust平台缺乏自动化并行化工具，无法在多核MCU上高效执行TinyML模型推理",
                "方法：结合通用TinyML流程与嵌入式Rust平台，支持多种32位微控制器架构（如Arm Cortex-M、RISC-V、ESP-32）的多核优化",
                "效果：在推理延迟上优于现有方法，内存占用与嵌入式C/C++工具包相当，提供开源实现和基准测试"
            ],
            "tags_zh": [
                "嵌入式Rust",
                "多核微控制器",
                "TinyML推理",
                "并行化计算",
                "边缘AI"
            ],
            "_index": 46
        },
        {
            "title": "High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle",
            "authors": [
                "Misael Mamani",
                "Mariel Fernandez",
                "Grace Luna",
                "Steffani Limachi",
                "Leonel Apaza",
                "Carolina Montes-Dávalos",
                "Marcelo Herrera",
                "Edwin Salcedo"
            ],
            "arxiv_id": "2512.09798v1",
            "summary": "Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.",
            "headline_zh": "提出太阳能自主水面无人艇，通过注射器采样架构实现高分辨率水质监测。",
            "intro_zh": [
                "问题：现有无人艇采样点少或传感器代表性差，难以满足水质评估的空间分辨率需求。",
                "方法：集成ROS 2自主栈、行为树架构和模块化6x12注射器采样系统，支持GPS-RTK导航与障碍检测。",
                "效果：在玻利维亚Achocalla Lagoon试验中，达到87%航点精度，测量结果与手动采样可比。"
            ],
            "tags_zh": [
                "自主水面无人艇",
                "水质采样",
                "ROS 2",
                "行为树架构",
                "模块化系统",
                "太阳能供电"
            ],
            "_index": 47
        },
        {
            "title": "M3Net: A Multi-Metric Mixture of Experts Network Digital Twin with Graph Neural Networks",
            "authors": [
                "Blessed Guda",
                "Carlee Joe-Wong"
            ],
            "arxiv_id": "2512.09797v1",
            "summary": "The rise of 5G/6G network technologies promises to enable applications like autonomous vehicles and virtual reality, resulting in a significant increase in connected devices and necessarily complicating network management. Even worse, these applications often have strict, yet heterogeneous, performance requirements across metrics like latency and reliability. Much recent work has thus focused on developing the ability to predict network performance. However, traditional methods for network modeling, like discrete event simulators and emulation, often fail to balance accuracy and scalability. Network Digital Twins (NDTs), augmented by machine learning, present a viable solution by creating virtual replicas of physical networks for real- time simulation and analysis. State-of-the-art models, however, fall short of full-fledged NDTs, as they often focus only on a single performance metric or simulated network data. We introduce M3Net, a Multi-Metric Mixture-of-experts (MoE) NDT that uses a graph neural network architecture to estimate multiple performance metrics from an expanded set of network state data in a range of scenarios. We show that M3Net significantly enhances the accuracy of flow delay predictions by reducing the MAPE (Mean Absolute Percentage Error) from 20.06% to 17.39%, while also achieving 66.47% and 78.7% accuracy on jitter and packets dropped for each flow",
            "headline_zh": "提出M3Net网络数字孪生模型，利用图神经网络和专家混合架构预测多性能指标以优化网络管理。",
            "intro_zh": [
                "核心问题：5G/6G网络应用需求多样，传统网络建模方法在准确性和可扩展性上难以平衡。",
                "方法要点：采用图神经网络和专家混合架构，从扩展的网络状态数据中估计多个性能指标。",
                "实验或效果：显著提升流延迟预测准确性，MAPE从20.06%降至17.39%，并在抖动和丢包率预测上达到高准确度。"
            ],
            "tags_zh": [
                "网络数字孪生",
                "图神经网络",
                "专家混合模型",
                "多性能指标预测",
                "网络管理优化"
            ],
            "_index": 48
        },
        {
            "title": "Knowledge Diversion for Efficient Morphology Control and Policy Transfer",
            "authors": [
                "Fu Feng",
                "Ruixiao Shi",
                "Yucheng Xie",
                "Jianlu Shen",
                "Jing Wang",
                "Xin Geng"
            ],
            "arxiv_id": "2512.09796v1",
            "summary": "Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \\textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \\textit{learngenes} and morphology- and task-specific \\textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\\times$ reduction in model size for single-agent deployment.",
            "headline_zh": "提出DivMorph，通过知识分流实现高效形态控制和策略迁移",
            "intro_zh": [
                "核心问题：通用形态控制中Transformer控制器计算成本高，跨任务泛化能力有限",
                "方法要点：利用SVD分解权重为因子单元，通过动态软门控分离共享和特定知识",
                "实验或效果：在跨任务迁移中样本效率提升3倍，单智能体部署模型大小减少17倍"
            ],
            "tags_zh": [
                "通用形态控制",
                "知识分流",
                "Transformer控制器",
                "策略迁移",
                "模块化训练"
            ],
            "_index": 49
        },
        {
            "title": "FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation",
            "authors": [
                "Pierre Ancey",
                "Andrew Price",
                "Saqib Javed",
                "Mathieu Salzmann"
            ],
            "arxiv_id": "2512.09792v1",
            "summary": "Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of \"apparent rotation\", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.",
            "headline_zh": "提出FastPose-ViT以解决航天器实时姿态估计问题，基于Vision Transformer直接回归6DoF姿态。",
            "intro_zh": [
                "核心问题：航天器6DoF姿态估计对自主操作至关重要，现有PnP方法计算量大，不适合资源受限边缘设备。",
                "方法要点：采用Vision Transformer架构，通过裁剪图像和新颖数学形式主义，基于投影几何和表观旋转直接回归姿态。",
                "实验或效果：在SPEED数据集上性能优于非PnP方法，与PnP方法竞争，量化后在NVIDIA Jetson Orin Nano上实现约75ms延迟和33FPS吞吐量。"
            ],
            "tags_zh": [
                "航天器姿态估计",
                "Vision Transformer",
                "实时计算",
                "边缘部署",
                "6DoF回归",
                "投影几何"
            ],
            "_index": 50
        },
        {
            "title": "TinyDéjàVu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers",
            "authors": [
                "Zhaolan Huang",
                "Emmanuel Baccelli"
            ],
            "arxiv_id": "2512.09786v1",
            "summary": "Always-on sensors are increasingly expected to embark a variety of tiny neural networks and to continuously perform inference on time-series of the data they sense. In order to fit lifetime and energy consumption requirements when operating on battery, such hardware uses microcontrollers (MCUs) with tiny memory budget e.g., 128kB of RAM. In this context, optimizing data flows across neural network layers becomes crucial. In this paper, we introduce TinyDéjàVu, a new framework and novel algorithms we designed to drastically reduce the RAM footprint required by inference using various tiny ML models for sensor data time-series on typical microcontroller hardware. We publish the implementation of TinyDéjàVu as open source, and we perform reproducible benchmarks on hardware. We show that TinyDéjàVu can save more than 60% of RAM usage and eliminate up to 90% of redundant compute on overlapping sliding window inputs.",
            "headline_zh": "提出TinyDéjàVu框架以优化微控制器上传感器数据流推理的内存占用和计算效率",
            "intro_zh": [
                "核心问题：微控制器内存有限（如128kB RAM），需优化神经网络层间数据流以降低能耗和延长电池寿命",
                "方法要点：设计新算法减少RAM占用，通过消除重叠滑动窗口输入的冗余计算提升推理速度",
                "实验或效果：开源实现，硬件基准测试显示RAM使用减少超60%，冗余计算消除达90%"
            ],
            "tags_zh": [
                "微控制器推理",
                "内存优化",
                "传感器数据流",
                "滑动窗口",
                "神经网络效率",
                "开源框架"
            ],
            "_index": 51
        },
        {
            "title": "Predicting Polymer Solubility in Solvents Using SMILES Strings",
            "authors": [
                "Andrew Reinhard"
            ],
            "arxiv_id": "2512.09784v1",
            "summary": "Understanding and predicting polymer solubility in various solvents is critical for applications ranging from recycling to pharmaceutical formulation. This work presents a deep learning framework that predicts polymer solubility, expressed as weight percent (wt%), directly from SMILES representations of both polymers and solvents. A dataset of 8,049 polymer solvent pairs at 25 deg C was constructed from calibrated molecular dynamics simulations (Zhou et al., 2023), and molecular descriptors and fingerprints were combined into a 2,394 feature representation per sample. A fully connected neural network with six hidden layers was trained using the Adam optimizer and evaluated using mean squared error loss, achieving strong agreement between predicted and actual solubility values. Generalizability was demonstrated using experimentally measured data from the Materials Genome Project, where the model maintained high accuracy on 25 unseen polymer solvent combinations. These findings highlight the viability of SMILES based machine learning models for scalable solubility prediction and high-throughput solvent screening, supporting applications in green chemistry, polymer processing, and materials design.",
            "headline_zh": "提出基于SMILES的深度学习框架，预测聚合物在溶剂中的溶解度，支持绿色化学与材料设计。",
            "intro_zh": [
                "核心问题：预测聚合物在溶剂中的溶解度对回收和制药等应用至关重要。",
                "方法要点：使用SMILES字符串构建特征，训练六层全连接神经网络进行预测。",
                "实验或效果：在模拟和实验数据上验证，模型在未见组合上保持高准确性。"
            ],
            "tags_zh": [
                "聚合物溶解度预测",
                "SMILES表示",
                "深度学习框架",
                "材料基因组项目",
                "溶剂筛选"
            ],
            "_index": 52
        },
        {
            "title": "Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems",
            "authors": [
                "Aoxiang Ma",
                "Salah Ghamizi",
                "Jun Cao",
                "Pedro Rodriguez"
            ],
            "arxiv_id": "2512.09780v1",
            "summary": "Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.",
            "headline_zh": "提出基于物理感知的异构图神经网络架构，用于三相不平衡配电网中电池储能系统的实时优化调度。",
            "intro_zh": [
                "现有深度学习方法缺乏明确的三相表示，难以准确建模相态动态和满足运行约束。",
                "通过将三相电网信息嵌入异构图节点，结合多种GNN架构和物理约束损失函数进行预测。",
                "在CIGRE 18节点系统上验证，实现低预测误差和近乎零的电池约束违反。"
            ],
            "tags_zh": [
                "电池储能系统",
                "三相不平衡配电网",
                "异构图神经网络",
                "物理约束优化",
                "实时调度"
            ],
            "_index": 53
        },
        {
            "title": "PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation",
            "authors": [
                "Mohamed Elbayumi",
                "Mohammed S. M. Elbaz"
            ],
            "arxiv_id": "2512.09779v1",
            "summary": "Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].",
            "headline_zh": "提出PathCo-LatticE框架，通过病理约束合成监督解决全监督少样本心脏MRI分割的泛化问题。",
            "intro_zh": [
                "核心问题：少样本学习依赖半监督方法，对领域偏移敏感，限制零样本泛化能力。",
                "方法要点：使用虚拟患者引擎合成病理引导的标签数据，结合自增强交错验证和动态专家网络提升泛化。",
                "实验或效果：在严格分布外设置下，仅用少量锚点实现优于现有方法的性能，接近全监督水平。"
            ],
            "tags_zh": [
                "少样本学习",
                "心脏MRI分割",
                "病理约束合成",
                "零样本泛化",
                "动态专家网络"
            ],
            "_index": 54
        },
        {
            "title": "Optimal certification of constant-local Hamiltonians",
            "authors": [
                "Junseo Lee",
                "Myeongjin Shin"
            ],
            "arxiv_id": "2512.09778v1",
            "summary": "We study the problem of certifying local Hamiltonians from real-time access to their dynamics. Given oracle access to $e^{-itH}$ for an unknown $k$-local Hamiltonian $H$ and a fully specified target Hamiltonian $H_0$, the goal is to decide whether $H$ is exactly equal to $H_0$ or differs from $H_0$ by at least $\\varepsilon$ in normalized Frobenius norm, while minimizing the total evolution time. We introduce the first intolerant Hamiltonian certification protocol that achieves optimal performance for all constant-locality Hamiltonians. For general $n$-qubit, $k$-local, traceless Hamiltonians, our procedure uses $O(c^k/\\varepsilon)$ total evolution time for a universal constant $c$, and succeeds with high probability. In particular, for $O(1)$-local Hamiltonians, the total evolution time becomes $Θ(1/\\varepsilon)$, matching the known $Ω(1/\\varepsilon)$ lower bounds and achieving the gold-standard Heisenberg-limit scaling. Prior certification methods either relied on implementing inverse evolution of $H$, required controlled access to $e^{-itH}$, or achieved near-optimal guarantees only in restricted settings such as the Ising case ($k=2$). In contrast, our algorithm requires neither inverse evolution nor controlled operations: it uses only forward real-time dynamics and achieves optimal intolerant certification for all constant-locality Hamiltonians.",
            "headline_zh": "提出最优常数局域哈密顿量认证协议，仅需正向实时动力学实现容错验证。",
            "intro_zh": [
                "研究从哈密顿量动力学实时访问中认证常数局域哈密顿量的问题。",
                "引入首个对所有常数局域哈密顿量实现最优性能的容错认证协议，无需逆演化或受控操作。",
                "对n量子比特、k局域、无迹哈密顿量，总演化时间为O(c^k/ε)，常数局域时达到Θ(1/ε)的Heisenberg极限。"
            ],
            "tags_zh": [
                "哈密顿量认证",
                "常数局域哈密顿量",
                "实时动力学",
                "Heisenberg极限",
                "量子算法",
                "容错验证"
            ],
            "_index": 55
        },
        {
            "title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition",
            "authors": [
                "Vladimir Balditsyn",
                "Philippe Lalanda",
                "German Vega",
                "Stéphanie Chollet"
            ],
            "arxiv_id": "2512.09775v1",
            "summary": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.",
            "headline_zh": "提出量化机器学习不确定性方法，应用于人类活动识别系统",
            "intro_zh": [
                "核心问题：机器学习模型在普适系统中存在不确定性，难以保证无错误性能",
                "方法要点：适配并联合使用多种技术，在运行时评估模型预测的相关性",
                "实验或效果：在人类活动识别领域应用并评估，结果验证了方法的有效性"
            ],
            "tags_zh": [
                "不确定性量化",
                "机器学习系统",
                "人类活动识别",
                "运行时评估",
                "普适计算"
            ],
            "_index": 56
        },
        {
            "title": "Stylized Meta-Album: Group-bias injection with style transfer to study robustness against distribution shifts",
            "authors": [
                "Romain Mussard",
                "Aurélien Gauffre",
                "Ihsan Ullah",
                "Thanh Gia Hieu Khuong",
                "Massih-Reza Amini",
                "Isabelle Guyon",
                "Lisheng Sun-Hosoya"
            ],
            "arxiv_id": "2512.09773v1",
            "summary": "We introduce Stylized Meta-Album (SMA), a new image classification meta-dataset comprising 24 datasets (12 content datasets, and 12 stylized datasets), designed to advance studies on out-of-distribution (OOD) generalization and related topics. Created using style transfer techniques from 12 subject classification datasets, SMA provides a diverse and extensive set of 4800 groups, combining various subjects (objects, plants, animals, human actions, textures) with multiple styles. SMA enables flexible control over groups and classes, allowing us to configure datasets to reflect diverse benchmark scenarios. While ideally, data collection would capture extensive group diversity, practical constraints often make this infeasible. SMA addresses this by enabling large and configurable group structures through flexible control over styles, subject classes, and domains-allowing datasets to reflect a wide range of real-world benchmark scenarios. This design not only expands group and class diversity, but also opens new methodological directions for evaluating model performance across diverse group and domain configurations-including scenarios with many minority groups, varying group imbalance, and complex domain shifts-and for studying fairness, robustness, and adaptation under a broader range of realistic conditions. To demonstrate SMA's effectiveness, we implemented two benchmarks: (1) a novel OOD generalization and group fairness benchmark leveraging SMA's domain, class, and group diversity to evaluate existing benchmarks. Our findings reveal that while simple balancing and algorithms utilizing group information remain competitive as claimed in previous benchmarks, increasing group diversity significantly impacts fairness, altering the superiority and relative rankings of algorithms. We also propose to use \\textit{Top-M worst group accuracy} as a new hyperparameter tuning metric, demonstrating broader fairness during optimization and delivering better final worst-group accuracy for larger group diversity. (2) An unsupervised domain adaptation (UDA) benchmark utilizing SMA's group diversity to evaluate UDA algorithms across more scenarios, offering a more comprehensive benchmark with lower error bars (reduced by 73\\% and 28\\% in closed-set setting and UniDA setting, respectively) compared to existing efforts. These use cases highlight SMA's potential to significantly impact the outcomes of conventional benchmarks.",
            "headline_zh": "提出Stylized Meta-Album元数据集，通过风格迁移注入组偏差以研究分布偏移下的鲁棒性",
            "intro_zh": [
                "核心问题：现实数据收集难以覆盖广泛组多样性，影响分布外泛化与公平性研究",
                "方法要点：使用风格迁移技术从12个内容数据集生成12个风格化数据集，构建4800个可配置组",
                "实验效果：创建OOD泛化与无监督域适应基准，显示增加组多样性显著改变算法公平性排名"
            ],
            "tags_zh": [
                "风格迁移",
                "分布外泛化",
                "元数据集",
                "组公平性",
                "无监督域适应",
                "图像分类"
            ],
            "_index": 57
        },
        {
            "title": "Circuits, Features, and Heuristics in Molecular Transformers",
            "authors": [
                "Kristof Varadi",
                "Mark Marosi",
                "Peter Antal"
            ],
            "arxiv_id": "2512.09757v1",
            "summary": "Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.",
            "headline_zh": "提出基于稀疏自编码器的机制分析，揭示分子Transformer在药物分子生成中的计算结构。",
            "intro_zh": [
                "核心问题：Transformer模型在分子生成中捕获化学规则的具体机制未知。",
                "方法要点：使用稀疏自编码器提取与化学相关激活模式的特征字典。",
                "实验或效果：在多个下游任务中验证机制洞察，提升预测性能。"
            ],
            "tags_zh": [
                "分子Transformer",
                "机制分析",
                "稀疏自编码器",
                "化学结构生成",
                "药物分子"
            ],
            "_index": 58
        },
        {
            "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs",
            "authors": [
                "Jan Betley",
                "Jorio Cocola",
                "Dylan Feng",
                "James Chua",
                "Andy Arditi",
                "Anna Sztyber-Betley",
                "Owain Evans"
            ],
            "arxiv_id": "2512.09742v1",
            "summary": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.",
            "headline_zh": "提出窄域微调导致不可预测的广泛泛化，包括模型失准和后门植入。",
            "intro_zh": [
                "核心问题：窄域微调可能引发模型在无关上下文中出现意外行为变化。",
                "方法要点：通过特定数据集微调，诱导模型泛化出有害或错误的行为模式。",
                "实验或效果：在鸟类名称实验中，模型表现出19世纪特征；在希特勒数据集中，模型采纳希特勒人格；在终结者实验中，模型基于年份触发对立目标。"
            ],
            "tags_zh": [
                "模型泛化",
                "数据中毒",
                "后门攻击",
                "微调安全",
                "模型失准",
                "诱导行为"
            ],
            "_index": 59
        },
        {
            "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation",
            "authors": [
                "Jingtian Yan",
                "Zhifei Li",
                "William Kang",
                "Stephen F. Smith",
                "Jiaoyang Li"
            ],
            "arxiv_id": "2512.09736v1",
            "summary": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.",
            "headline_zh": "分析现实仿真下MAPF规划器设计权衡，指导实际部署",
            "intro_zh": [
                "核心问题：现有MAPF评估框架基于简化机器人模型，与实际性能存在差距",
                "方法要点：利用SMART等框架，系统研究规划器设计选择对现实执行性能的影响",
                "实验或效果：实证分析解最优性、运动学建模精度及其交互作用，揭示设计权衡"
            ],
            "tags_zh": [
                "多智能体路径规划",
                "现实仿真",
                "规划器设计",
                "运动学建模",
                "性能评估"
            ],
            "_index": 60
        },
        {
            "title": "Interpreto: An Explainability Library for Transformers",
            "authors": [
                "Antonin Poché",
                "Thomas Mullor",
                "Gabriele Sarti",
                "Frédéric Boisnard",
                "Corentin Friedrich",
                "Charlotte Claye",
                "François Hoofd",
                "Raphael Bernas",
                "Céline Hudelot",
                "Fanny Jourdan"
            ],
            "arxiv_id": "2512.09730v1",
            "summary": "Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.\n  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.\n  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.",
            "headline_zh": "提出Interpreto库以支持HuggingFace文本模型的后验可解释性分析",
            "intro_zh": [
                "核心问题：为BERT到LLM的HuggingFace文本模型提供后验可解释性工具，弥补现有库在概念解释方面的不足。",
                "方法要点：集成归因和基于概念的解释方法，通过统一API支持分类和生成模型，强调概念功能。",
                "实验或效果：开源库包含文档、示例和教程，旨在提升数据科学家和终端用户的可解释性访问性。"
            ],
            "tags_zh": [
                "可解释性库",
                "Transformer模型",
                "后验解释",
                "概念解释",
                "HuggingFace集成"
            ],
            "_index": 61
        },
        {
            "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method",
            "authors": [
                "Laurynas Adomaitis",
                "Vincent Israel-Jost",
                "Alexei Grinbaum"
            ],
            "arxiv_id": "2512.09729v1",
            "summary": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.",
            "headline_zh": "提出伦理准备水平方法，以评估AI系统设计中的伦理反思实施情况。",
            "intro_zh": [
                "核心问题：AI系统设计中缺乏将高层伦理原则转化为具体工程实践的方法。",
                "方法要点：开发四层迭代的伦理准备水平，通过动态问卷将伦理价值融入实际用例。",
                "实验或效果：通过执法AI面部素描生成器和协作工业机器人案例验证方法有效性。"
            ],
            "tags_zh": [
                "AI伦理评估",
                "伦理准备水平",
                "设计伦理",
                "工程实践",
                "案例研究"
            ],
            "_index": 62
        },
        {
            "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions",
            "authors": [
                "Junlin Xiao",
                "Victor-Alexandru Darvariu",
                "Bruno Lacerda",
                "Nick Hawes"
            ],
            "arxiv_id": "2512.09727v1",
            "summary": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.",
            "headline_zh": "提出高斯过程聚合方法以提升根并行蒙特卡洛树搜索在连续动作空间中的性能",
            "intro_zh": [
                "核心问题：连续动作空间中根并行蒙特卡洛树搜索的线程统计聚合策略未充分探索",
                "方法要点：使用高斯过程回归估计未尝试动作的价值，优化聚合过程",
                "实验或效果：在6个领域系统评估，显示方法优于现有策略，推理时间略有增加"
            ],
            "tags_zh": [
                "蒙特卡洛树搜索",
                "连续动作空间",
                "高斯过程回归",
                "根并行规划",
                "在线规划",
                "统计聚合"
            ],
            "_index": 63
        },
        {
            "title": "Mixture of Lookup Key-Value Experts",
            "authors": [
                "Zongcheng Wang"
            ],
            "arxiv_id": "2512.09723v1",
            "summary": "Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \\textbf{M}ixture \\textbf{o}f \\textbf{L}ookup \\textbf{K}ey-\\textbf{V}alue Experts (\\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.",
            "headline_zh": "提出MoLKV模型以解决MoLE上下文无关专家选择限制，提升资源受限设备上LLM性能。",
            "intro_zh": [
                "MoLE基于输入ID选择专家，上下文无关，可能限制模型性能。",
                "MoLKV将专家构建为键值对，通过输入查询与缓存键值交互实现上下文感知输出。",
                "小规模评估显示MoLKV显著降低验证损失，优于MoLE。"
            ],
            "tags_zh": [
                "专家混合模型",
                "上下文感知机制",
                "键值对专家",
                "资源受限设备",
                "LLM推理优化"
            ],
            "_index": 64
        },
        {
            "title": "Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning",
            "authors": [
                "Kaichen He",
                "Zihao Wang",
                "Muyao Li",
                "Anji Liu",
                "Yitao Liang"
            ],
            "arxiv_id": "2512.09706v1",
            "summary": "The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA",
            "headline_zh": "提出CrossAgent统一智能体模型，通过强化学习掌握跨层级异构动作空间以提升动态环境适应性。",
            "intro_zh": [
                "现有智能体受限于静态预定义动作空间，难以适应动态环境中交互粒度变化。",
                "采用监督微调与多轮组相对策略优化算法，实现自适应动作切换，无需人工规则。",
                "在Minecraft开放世界800多个任务上验证，性能超越固定动作基线，展现优越泛化与长时推理效率。"
            ],
            "tags_zh": [
                "智能体模型",
                "强化学习",
                "异构动作空间",
                "自适应交互",
                "Minecraft环境",
                "策略优化"
            ],
            "_index": 65
        },
        {
            "title": "LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery",
            "authors": [
                "Seon-Hoon Kim",
                "Hyeji Sim",
                "Youeyun Jung",
                "Ok-Chul Jung",
                "Yerin Kim"
            ],
            "arxiv_id": "2512.09700v1",
            "summary": "Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.",
            "headline_zh": "提出LiM-YOLO，通过金字塔层级偏移与归一化辅助分支解决遥感图像中船舶检测的尺度差异问题。",
            "intro_zh": [
                "核心问题：遥感图像中船舶目标尺度差异大且形态各向异性，导致通用检测器在小目标上性能下降。",
                "方法要点：基于统计分析，采用金字塔层级偏移策略调整检测头至P2-P4，并引入GN-CBLinear模块增强训练稳定性。",
                "实验或效果：在多个数据集上验证，LiM-YOLO在检测精度和效率上优于现有先进模型。"
            ],
            "tags_zh": [
                "船舶检测",
                "遥感图像",
                "金字塔层级偏移",
                "归一化辅助分支",
                "目标检测",
                "尺度差异"
            ],
            "_index": 66
        },
        {
            "title": "A data-driven approach to linking design features with manufacturing process data for sustainable product development",
            "authors": [
                "Jiahang Li",
                "Lucas Cazzonelli",
                "Jacqueline Höllig",
                "Markus Doellken",
                "Sven Matthiesen"
            ],
            "arxiv_id": "2512.09690v1",
            "summary": "The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.",
            "headline_zh": "提出数据驱动方法以映射设计特征与制造过程数据，支持可持续产品开发",
            "intro_zh": [
                "核心问题：设计特征与制造过程数据集成不足，限制数据驱动产品设计改进潜力",
                "方法要点：开发系统架构实现数据持续收集与集成，构建机器学习模型自动提供设计改进建议",
                "实验或效果：通过整合可持续性指标，为可持续产品开发开辟新可能性"
            ],
            "tags_zh": [
                "数据驱动方法",
                "设计特征映射",
                "制造过程数据",
                "可持续产品开发",
                "机器学习模型",
                "系统架构"
            ],
            "_index": 67
        },
        {
            "title": "Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized",
            "authors": [
                "Er Jin",
                "Yang Zhang",
                "Yongli Mou",
                "Yanfei Dong",
                "Stefan Decker",
                "Kenji Kawaguchi",
                "Johannes Stegmaier"
            ],
            "arxiv_id": "2512.09687v1",
            "summary": "Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.",
            "headline_zh": "提出UniForget方法，通过模型剪枝缓解生成模型记忆训练数据问题，无需针对特定概念。",
            "intro_zh": [
                "核心问题：生成模型易记忆训练数据，导致版权侵权等法律风险，现有方法计算开销大或可扩展性差。",
                "方法要点：识别模型中对受版权内容生成负责的部分，应用模型剪枝抑制生成概率，保持一般生成能力。",
                "实验或效果：有效降低受版权内容生成概率，与现有遗忘方法正交互补，提升去记忆技术。"
            ],
            "tags_zh": [
                "生成模型",
                "记忆缓解",
                "模型剪枝",
                "版权保护",
                "去记忆技术",
                "无监督学习"
            ],
            "_index": 68
        },
        {
            "title": "Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies",
            "authors": [
                "Mika Persson",
                "Jonas Lidman",
                "Jacob Ljungberg",
                "Samuel Sandelius",
                "Adam Andersson"
            ],
            "arxiv_id": "2512.09682v1",
            "summary": "This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.",
            "headline_zh": "提出基于多智能体强化学习的无人机群分散控制模型，用于关键数据一次性传递",
            "intro_zh": [
                "研究多智能体强化学习在无人机群分散控制中的应用，以传递关键数据包至已知位置",
                "引入确定性游戏家族，设计用于多智能体强化学习的扩展性研究",
                "实验显示，现有多智能体强化学习算法在小规模智能体下表现良好，但扩展性随智能体数量增加而受限"
            ],
            "tags_zh": [
                "多智能体强化学习",
                "无人机群控制",
                "分散控制",
                "扩展性研究",
                "确定性游戏"
            ],
            "_index": 69
        },
        {
            "title": "The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization",
            "authors": [
                "Alexey Kravatskiy",
                "Ivan Kozyrev",
                "Nikolai Kozlov",
                "Alexander Vinogradov",
                "Daniil Merkulov",
                "Ivan Oseledets"
            ],
            "arxiv_id": "2512.09678v1",
            "summary": "In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.",
            "headline_zh": "提出基于Ky Fan范数对偶的Fanions算法族，用于大语言模型权重矩阵优化。",
            "intro_zh": [
                "核心问题：探索矩阵范数在训练大语言模型权重矩阵优化中的应用。",
                "方法要点：利用Ky Fan k-范数的对偶，结合Frobenius或l∞范数，构建F-Fanions和S-Fanions算法族。",
                "实验或效果：F-Muon和S-Muon在广泛任务中匹配Muon性能，并在合成线性最小二乘问题上超越Muon。"
            ],
            "tags_zh": [
                "矩阵优化",
                "Ky Fan范数",
                "大语言模型训练",
                "算法设计",
                "权重矩阵范数"
            ],
            "_index": 70
        },
        {
            "title": "Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power",
            "authors": [
                "Yuzhu Chen",
                "Tian Qin",
                "Xinmei Tian",
                "Fengxiang He",
                "Dacheng Tao"
            ],
            "arxiv_id": "2512.09673v1",
            "summary": "Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.",
            "headline_zh": "揭示等变网络表达力受限及其通过扩大模型规模补偿的机制",
            "intro_zh": [
                "核心问题：等变约束可能严格限制神经网络的表达力，影响其性能。",
                "方法要点：通过分析ReLU网络的边界超平面和通道向量，构建示例证明表达力受限。",
                "实验或效果：展示扩大模型规模可补偿此缺点，且等变网络仍具较低复杂度，提升泛化能力。"
            ],
            "tags_zh": [
                "等变神经网络",
                "表达力分析",
                "模型规模补偿",
                "泛化能力",
                "ReLU网络",
                "对称性编码"
            ],
            "_index": 71
        },
        {
            "title": "An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence",
            "authors": [
                "Gil Weissman",
                "Amir Ivry",
                "Israel Cohen"
            ],
            "arxiv_id": "2512.09670v1",
            "summary": "The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.",
            "headline_zh": "提出自动化Tip-and-Cue框架以优化卫星任务调度与视觉智能分析",
            "intro_zh": [
                "核心问题：卫星星座增多，需自动化任务调度与视觉分析以提升地球观测效率",
                "方法要点：基于外部数据生成提示，优化任务调度，结合AI模型处理图像生成结构化报告",
                "实验或效果：通过船舶追踪场景验证，利用AIS数据预测轨迹并生成可操作输出"
            ],
            "tags_zh": [
                "卫星任务调度",
                "自动化地球观测",
                "视觉智能分析",
                "船舶追踪",
                "AI模型集成"
            ],
            "_index": 72
        },
        {
            "title": "OxEnsemble: Fair Ensembles for Low-Data Classification",
            "authors": [
                "Jonathan Rystrøm",
                "Zihao Fu",
                "Chris Russell"
            ],
            "arxiv_id": "2512.09665v1",
            "summary": "We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.\n  We propose a novel approach \\emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \\emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.",
            "headline_zh": "提出OxEnsemble方法，解决数据稀缺且不平衡场景下的公平分类问题",
            "intro_zh": [
                "核心问题：医疗影像等低数据场景中，数据稀缺且群体不平衡，假阴性可能致命",
                "方法要点：通过集成多个满足公平约束的模型，高效聚合预测，兼顾数据与计算效率",
                "实验效果：在多个医疗影像数据集上，比现有方法获得更一致的公平-准确性权衡"
            ],
            "tags_zh": [
                "公平分类",
                "低数据学习",
                "集成学习",
                "医疗影像",
                "群体不平衡"
            ],
            "_index": 73
        },
        {
            "title": "SynthPix: A lightspeed PIV images generator",
            "authors": [
                "Antonio Terpin",
                "Alan Bonomi",
                "Francesco Banelli",
                "Raffaello D'Andrea"
            ],
            "arxiv_id": "2512.09664v1",
            "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.",
            "headline_zh": "提出SynthPix以加速PIV图像生成，支持强化学习训练和实时流体控制开发。",
            "intro_zh": [
                "核心问题：现有PIV图像生成工具性能不足，难以满足数据密集型强化学习训练和实时流体控制迭代需求。",
                "方法要点：基于JAX实现高性能并行合成图像生成器，支持标准配置参数，提升生成吞吐量数个数量级。",
                "实验或效果：未知具体实验细节，但强调在图像对生成每秒的吞吐量上实现显著加速。"
            ],
            "tags_zh": [
                "粒子图像测速",
                "合成图像生成",
                "JAX加速",
                "强化学习训练",
                "实时流体控制"
            ],
            "_index": 74
        },
        {
            "title": "IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting",
            "authors": [
                "Tao Zhang",
                "Yuyang Hong",
                "Yang Xia",
                "Kun Ding",
                "Zeyu Zhang",
                "Ying Wang",
                "Shiming Xiang",
                "Chunhong Pan"
            ],
            "arxiv_id": "2512.09663v1",
            "summary": "Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.",
            "headline_zh": "提出IF-Bench基准与GenViP方法，以评估和提升多模态大模型在红外图像理解中的性能。",
            "intro_zh": [
                "核心问题：多模态大模型在红外图像理解能力尚未被系统评估，存在领域分布偏移。",
                "方法要点：构建首个高质量红外图像基准IF-Bench，并提出无需训练的生成视觉提示方法GenViP，通过图像编辑转换红外图像为RGB。",
                "实验或效果：评估40多个模型，分析模型规模等因素的影响，GenViP方法在广泛模型中带来显著性能提升。"
            ],
            "tags_zh": [
                "红外图像理解",
                "多模态大模型",
                "基准评估",
                "生成视觉提示",
                "领域适应"
            ],
            "_index": 75
        },
        {
            "title": "Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection",
            "authors": [
                "Paloma Piot",
                "David Otero",
                "Patricia Martín-Rodilla",
                "Javier Parapar"
            ],
            "arxiv_id": "2512.09662v1",
            "summary": "Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $κ$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.",
            "headline_zh": "提出基于跨评分者可靠性的主观性感知框架，评估LLM在仇恨言论检测中的可靠性，发现其可作为代理评估者。",
            "intro_zh": [
                "核心问题：仇恨言论检测的主观性挑战，传统指标如Cohen's κ简化分歧，LLM无法完全替代人类判断。",
                "方法要点：使用跨评分者可靠性（xRR）重新评估LLM可靠性，分析LLM与人类在实例级和模式级的差异。",
                "实验或效果：LLM生成的标注能可靠反映分类模型性能趋势，与人类评估相关，可作为主观NLP任务的代理评估工具。"
            ],
            "tags_zh": [
                "仇恨言论检测",
                "大型语言模型",
                "主观性评估",
                "跨评分者可靠性",
                "代理评估",
                "自然语言处理"
            ],
            "_index": 76
        },
        {
            "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat",
            "authors": [
                "Nicolas Marticorena",
                "Tobias Fischer",
                "Niko Suenderhauf"
            ],
            "arxiv_id": "2512.09656v1",
            "summary": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.",
            "headline_zh": "提出ReMoSPLAT，基于高斯溅射表示的反应式移动操作控制器，用于避障和姿态控制。",
            "intro_zh": [
                "核心问题：移动操作器在避障时需准确环境表示，避免高成本规划。",
                "方法要点：使用二次规划结合高斯溅射表示，集成约束和成本以实现反应式控制。",
                "实验或效果：在仿真中验证可行性，性能接近依赖完美地面真值信息的控制器。"
            ],
            "tags_zh": [
                "移动操作控制",
                "反应式控制",
                "高斯溅射",
                "避障",
                "二次规划",
                "仿真实验"
            ],
            "_index": 77
        },
        {
            "title": "Membership and Dataset Inference Attacks on Large Audio Generative Models",
            "authors": [
                "Jakub Proboszcz",
                "Paweł Kochanski",
                "Karol Korszun",
                "Donato Crisostomi",
                "Giorgio Strano",
                "Emanuele Rodolà",
                "Kamil Deja",
                "Jan Dubinski"
            ],
            "arxiv_id": "2512.09654v1",
            "summary": "Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.",
            "headline_zh": "提出数据集推理攻击以评估音频生成模型训练数据版权归属",
            "intro_zh": [
                "核心问题：验证音频生成模型是否使用特定艺术家的作品进行训练，以应对版权保护挑战。",
                "方法要点：在成员推理攻击基础上，通过聚合多个样本证据，实施数据集推理攻击。",
                "实验或效果：成员推理在大型数据集上效果有限，但数据集推理在音频领域成功，提供实用评估机制。"
            ],
            "tags_zh": [
                "音频生成模型",
                "成员推理攻击",
                "数据集推理攻击",
                "版权保护",
                "训练数据验证"
            ],
            "_index": 78
        },
        {
            "title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification",
            "authors": [
                "Wanyue Zhang",
                "Lin Geng Foo",
                "Thabo Beeler",
                "Rishabh Dabral",
                "Christian Theobalt"
            ],
            "arxiv_id": "2512.09646v1",
            "summary": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.",
            "headline_zh": "提出VHOI框架，通过运动稠密化从稀疏轨迹生成可控的人-物交互视频",
            "intro_zh": [
                "核心问题：现有可控视频生成方法在稀疏控制（易指定但缺乏实例感知）与稠密信号（信息丰富但获取成本高）间存在权衡。",
                "方法要点：采用两阶段框架，先稠密化稀疏轨迹为人-物交互掩码序列，再基于掩码微调视频扩散模型，引入人-物交互感知运动表示。",
                "实验或效果：在可控人-物交互视频生成中实现先进结果，并能端到端生成包含导航的完整交互场景。"
            ],
            "tags_zh": [
                "可控视频生成",
                "人-物交互",
                "运动稠密化",
                "视频扩散模型",
                "稀疏轨迹"
            ],
            "_index": 79
        },
        {
            "title": "Kaapana: A Comprehensive Open-Source Platform for Integrating AI in Medical Imaging Research Environments",
            "authors": [
                "Ünal Akünal",
                "Markus Bujotzek",
                "Stefan Denner",
                "Benjamin Hamm",
                "Klaus Kades",
                "Philipp Schader",
                "Jonas Scherer",
                "Marco Nolden",
                "Peter Neher",
                "Ralf Floca",
                "Klaus Maier-Hein"
            ],
            "arxiv_id": "2512.09644v1",
            "summary": "Developing generalizable AI for medical imaging requires both access to large, multi-center datasets and standardized, reproducible tooling within research environments. However, leveraging real-world imaging data in clinical research environments is still hampered by strict regulatory constraints, fragmented software infrastructure, and the challenges inherent in conducting large-cohort multicentre studies. This leads to projects that rely on ad-hoc toolchains that are hard to reproduce, difficult to scale beyond single institutions and poorly suited for collaboration between clinicians and data scientists. We present Kaapana, a comprehensive open-source platform for medical imaging research that is designed to bridge this gap. Rather than building single-use, site-specific tooling, Kaapana provides a modular, extensible framework that unifies data ingestion, cohort curation, processing workflows and result inspection under a common user interface. By bringing the algorithm to the data, it enables institutions to keep control over their sensitive data while still participating in distributed experimentation and model development. By integrating flexible workflow orchestration with user-facing applications for researchers, Kaapana reduces technical overhead, improves reproducibility and enables conducting large-scale, collaborative, multi-centre imaging studies. We describe the core concepts of the platform and illustrate how they can support diverse use cases, from local prototyping to nation-wide research networks. The open-source codebase is available at https://github.com/kaapana/kaapana",
            "headline_zh": "提出Kaapana开源平台以解决医学影像AI研究中数据访问与工具标准化难题",
            "intro_zh": [
                "核心问题：医学影像AI研究受限于数据访问困难、软件碎片化及多中心协作挑战，导致工具链难以复现和扩展。",
                "方法要点：提供模块化框架，统一数据管理、工作流编排和结果检查，支持算法到数据的分布式实验。",
                "实验或效果：平台减少技术开销，提升可复现性，支持从本地原型到全国范围的大规模协作研究。"
            ],
            "tags_zh": [
                "医学影像AI",
                "开源平台",
                "工作流编排",
                "多中心研究",
                "数据管理",
                "可复现性"
            ],
            "_index": 80
        },
        {
            "title": "Benchmarking SAM2-based Trackers on FMOX",
            "authors": [
                "Senem Aktas",
                "Charles Markham",
                "John McDonald",
                "Rozenn Dahyot"
            ],
            "arxiv_id": "2512.09633v1",
            "summary": "Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.",
            "headline_zh": "在FMOX数据集上基准测试基于SAM2的跟踪器，揭示其在快速移动对象上的性能",
            "intro_zh": [
                "核心问题：评估基于SAM2的跟踪器在快速移动对象（FMO）数据集上的性能，以理解当前先进跟踪器的局限性。",
                "方法要点：基准测试SAM2、EfficientTAM、DAM4SAM和SAMURAI等跟踪器，使用用户提供的单模板进行对象跟踪和分割。",
                "实验或效果：DAM4SAM和SAMURAI在更具挑战性的序列上表现良好，提供了对跟踪器行为的详细洞察。"
            ],
            "tags_zh": [
                "对象跟踪",
                "SAM2基准测试",
                "快速移动对象",
                "跟踪器性能评估",
                "计算机视觉"
            ],
            "_index": 81
        },
        {
            "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
            "authors": [
                "Emanuele La Malfa",
                "Ping Zhu",
                "Samuele Marro",
                "Sara Bernardini",
                "Michael Wooldridge"
            ],
            "arxiv_id": "2512.09629v1",
            "summary": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",
            "headline_zh": "提出基于大语言模型与PDDL的端到端规划框架，以自动化处理自然语言规范并生成可执行计划。",
            "intro_zh": [
                "核心问题：自然语言规范存在模糊性和矛盾，传统规划需人工干预，大语言模型在复杂规划任务中表现不佳。",
                "方法要点：使用编排器和代理模块迭代精炼PDDL模型，集成外部规划引擎，无需人工干预。",
                "实验或效果：在Google NaturalPlan、PlanBench等基准测试中验证灵活性，支持多种PDDL引擎如Fast Downward。"
            ],
            "tags_zh": [
                "端到端规划",
                "大语言模型",
                "PDDL",
                "自然语言处理",
                "自动化验证",
                "规划引擎集成"
            ],
            "_index": 82
        },
        {
            "title": "Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder",
            "authors": [
                "Yousef Azizi Movahed",
                "Fatemeh Ziaeetabar"
            ],
            "arxiv_id": "2512.09626v1",
            "summary": "Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.",
            "headline_zh": "提出静态RNN编码器用于原子手-物交互分类，在MANIAC数据集上实现97.60%准确率。",
            "intro_zh": [
                "核心问题：细粒度分类手-物交互的原子状态（接近、抓取、持有）。",
                "方法要点：将视频转换为统计-运动特征向量，使用序列长度设为1的双向RNN作为静态编码器。",
                "实验或效果：模型在最具挑战的'抓取'类上获得0.90平衡F1分数，准确率达97.60%。"
            ],
            "tags_zh": [
                "手-物交互识别",
                "原子交互分类",
                "静态RNN编码器",
                "统计-运动特征",
                "MANIAC数据集",
                "细粒度识别"
            ],
            "_index": 83
        },
        {
            "title": "Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks",
            "authors": [
                "Jingbo Zhang",
                "Maoxin Ji",
                "Qiong Wu",
                "Pingyi Fan",
                "Kezhi Wang",
                "Wen Chen"
            ],
            "arxiv_id": "2512.09621v1",
            "summary": "Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.",
            "headline_zh": "提出三方协同语义通信框架，以优化车联网中语义任务卸载的延迟与符号数。",
            "intro_zh": [
                "核心问题：车联网中语义通信与边缘计算结合时，任务延迟和语义符号数的优化问题。",
                "方法要点：采用混合整数非线性规划，分解为符号数优化（基于MAPPO-PDN）和卸载比例优化（基于线性规划）。",
                "实验或效果：仿真显示该方案在性能上优于其他算法，具体提升未知。"
            ],
            "tags_zh": [
                "语义通信",
                "车联网",
                "任务卸载",
                "多智能体强化学习",
                "边缘计算",
                "优化算法"
            ],
            "_index": 84
        },
        {
            "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models",
            "authors": [
                "Minghao Guo",
                "Meng Cao",
                "Jiachen Tao",
                "Rongtao Xu",
                "Yan Yan",
                "Xiaodan Liang",
                "Ivan Laptev",
                "Xiaojun Chang"
            ],
            "arxiv_id": "2512.09619v1",
            "summary": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.",
            "headline_zh": "提出GLaD框架，通过几何潜在蒸馏增强视觉-语言-动作模型的空间推理能力。",
            "intro_zh": [
                "现有VLA模型依赖RGB信息，忽略几何线索，影响空间推理和操作。",
                "GLaD在预训练中通过知识蒸馏整合3D几何先验，对齐LLM隐藏状态与几何感知视觉特征。",
                "在Bridge数据集预训练后，GLaD在LIBERO任务中平均成功率94.1%，优于UniVLA。"
            ],
            "tags_zh": [
                "视觉-语言-动作模型",
                "几何蒸馏",
                "空间推理",
                "知识对齐",
                "多模态表示"
            ],
            "_index": 85
        },
        {
            "title": "FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation",
            "authors": [
                "Hubert Kompanowski",
                "Varun Jampani",
                "Aaryaman Vasishta",
                "Binh-Son Hua"
            ],
            "arxiv_id": "2512.09617v1",
            "summary": "Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.\n  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.",
            "headline_zh": "提出基于少样本自注意力适配的多视角材质外观迁移方法，以增强多视角扩散模型的外观操控能力。",
            "intro_zh": [
                "核心问题：多视角扩散模型在外观操控（如材质、纹理）方面受限，难以实现精确指定。",
                "方法要点：通过轻量级适配，结合输入图像的对象身份和参考图像的外观线索，利用自注意力特征聚合实现多视角一致输出。",
                "实验或效果：仅需少量训练样本，即可在预训练模型上实现多样外观的多视角生成，保持几何和视角一致性。"
            ],
            "tags_zh": [
                "多视角扩散模型",
                "外观迁移",
                "自注意力适配",
                "少样本学习",
                "材质编辑",
                "生成式3D表示"
            ],
            "_index": 86
        },
        {
            "title": "Rethinking Chain-of-Thought Reasoning for Videos",
            "authors": [
                "Yiwu Zhong",
                "Zi-Yuan Hu",
                "Yin Li",
                "Liwei Wang"
            ],
            "arxiv_id": "2512.09616v1",
            "summary": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.",
            "headline_zh": "提出高效后训练与推理框架，通过压缩视觉令牌和简洁推理链提升视频多模态大语言模型的推理效率与性能。",
            "intro_zh": [
                "核心问题：现有视频多模态大语言模型依赖冗长推理链和大量视觉令牌，效率低下。",
                "方法要点：设计框架压缩视觉令牌并生成简洁推理轨迹，无需人工标注或监督微调。",
                "实验或效果：在多个基准测试中实现显著推理效率提升和竞争性性能表现。"
            ],
            "tags_zh": [
                "视频推理",
                "多模态大语言模型",
                "推理效率",
                "视觉令牌压缩",
                "后训练框架"
            ],
            "_index": 87
        },
        {
            "title": "ImageTalk: Designing a Multimodal AAC Text Generation System Driven by Image Recognition and Natural Language Generation",
            "authors": [
                "Boyin Yang",
                "Puming Jiang",
                "Per Ola Kristensson"
            ],
            "arxiv_id": "2512.09610v1",
            "summary": "People living with Motor Neuron Disease (plwMND) frequently encounter speech and motor impairments that necessitate a reliance on augmentative and alternative communication (AAC) systems. This paper tackles the main challenge that traditional symbol-based AAC systems offer a limited vocabulary, while text entry solutions tend to exhibit low communication rates. To help plwMND articulate their needs about the system efficiently and effectively, we iteratively design and develop a novel multimodal text generation system called ImageTalk through a tailored proxy-user-based and an end-user-based design phase. The system demonstrates pronounced keystroke savings of 95.6%, coupled with consistent performance and high user satisfaction. We distill three design guidelines for AI-assisted text generation systems design and outline four user requirement levels tailored for AAC purposes, guiding future research in this field.",
            "headline_zh": "提出ImageTalk多模态文本生成系统，以解决运动神经元疾病患者传统AAC系统词汇有限和文本输入效率低的问题。",
            "intro_zh": [
                "核心问题：传统符号AAC系统词汇有限，文本输入解决方案通信率低，影响运动神经元疾病患者表达需求。",
                "方法要点：通过代理用户和最终用户设计阶段，迭代开发基于图像识别和自然语言生成的多模态文本生成系统。",
                "实验或效果：系统实现95.6%的击键节省，性能稳定且用户满意度高，提炼设计指南和用户需求层次。"
            ],
            "tags_zh": [
                "多模态文本生成",
                "图像识别",
                "自然语言生成",
                "增强替代通信",
                "用户中心设计",
                "运动神经元疾病"
            ],
            "_index": 88
        },
        {
            "title": "Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization",
            "authors": [
                "Zhiheng Li",
                "Weihua Wang",
                "Qiang Shen",
                "Yichen Zhao",
                "Zheng Fang"
            ],
            "arxiv_id": "2512.09608v1",
            "summary": "Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.",
            "headline_zh": "提出Super4DR框架，以4D雷达为中心，通过自监督里程计和高斯地图优化解决恶劣环境下的SLAM问题。",
            "intro_zh": [
                "核心问题：4D雷达点云稀疏噪声导致里程计不准和地图结构模糊不完整。",
                "方法要点：设计聚类感知里程计网络和自监督机制，结合3D高斯表示与雷达特定策略优化地图。",
                "实验或效果：性能提升67%，接近监督里程计，缩小与LiDAR地图质量差距并支持多模态渲染。"
            ],
            "tags_zh": [
                "4D雷达SLAM",
                "自监督里程计",
                "高斯地图优化",
                "恶劣环境感知",
                "多模态渲染"
            ],
            "_index": 89
        },
        {
            "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories",
            "authors": [
                "Yanghong Mei",
                "Yirong Yang",
                "Longteng Guo",
                "Qunbo Wang",
                "Ming-Ming Yu",
                "Xingjian He",
                "Wenjun Wu",
                "Jing Liu"
            ],
            "arxiv_id": "2512.09607v1",
            "summary": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.",
            "headline_zh": "提出UrbanNav框架，利用网络规模人类轨迹训练具身代理遵循自由语言指令进行城市导航",
            "intro_zh": [
                "核心问题：城市环境中自然语言导航面临噪声指令、模糊空间参考和动态场景挑战，现有方法依赖精确目标格式，限制实际应用",
                "方法要点：基于网络规模城市行走视频，开发可扩展标注流程，对齐人类轨迹与基于真实地标的语言指令，构建大规模数据集",
                "实验或效果：模型在复杂城市场景中展现优越空间推理和泛化能力，显著超越现有方法，验证大规模网络视频数据潜力"
            ],
            "tags_zh": [
                "城市导航",
                "语言引导导航",
                "具身代理",
                "大规模轨迹数据",
                "空间推理",
                "泛化能力"
            ],
            "_index": 90
        },
        {
            "title": "CS3D: An Efficient Facial Expression Recognition via Event Vision",
            "authors": [
                "Zhe Wang",
                "Qijin Song",
                "Yucen Peng",
                "Weibang Bai"
            ],
            "arxiv_id": "2512.09592v1",
            "summary": "Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\\% of the original C3D required on the same device.",
            "headline_zh": "提出CS3D框架以解决事件相机面部表情识别中的计算复杂性和能耗问题",
            "intro_zh": [
                "核心问题：事件相机面部表情识别中，传统深度学习方法计算复杂、能耗高，难以部署于边缘设备",
                "方法要点：通过分解C3D方法降低计算复杂度，结合软脉冲神经元和时空注意力机制增强信息保留能力",
                "实验或效果：在多个数据集上，CS3D比RNN、Transformer和C3D等架构精度更高，能耗仅为原C3D的21.97%"
            ],
            "tags_zh": [
                "事件相机",
                "面部表情识别",
                "计算效率",
                "时空注意力",
                "边缘计算"
            ],
            "_index": 91
        },
        {
            "title": "Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models",
            "authors": [
                "Magnus Ruud Kjaer",
                "Rahul Thapa",
                "Gauri Ganjoo",
                "Hyatt Moore",
                "Poul Joergen Jennum",
                "Brandon M. Westover",
                "James Zou",
                "Emmanuel Mignot",
                "Bryan He",
                "Andreas Brink-Kjaer"
            ],
            "arxiv_id": "2512.09591v1",
            "summary": "Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.",
            "headline_zh": "提出Stanford Sleep Bench数据集与基准，以评估多导睡眠图自监督预训练方法在睡眠基础模型中的应用。",
            "intro_zh": [
                "核心问题：睡眠基础模型发展受限，缺乏共享数据集和系统评估自监督学习方法。",
                "方法要点：引入大规模多导睡眠图数据集，包含17,467条记录和13个临床任务。",
                "实验或效果：对比学习在疾病和死亡率预测任务中表现显著优于其他方法，收敛更快。"
            ],
            "tags_zh": [
                "多导睡眠图",
                "自监督学习",
                "睡眠基础模型",
                "临床预测",
                "对比学习",
                "数据集基准"
            ],
            "_index": 92
        },
        {
            "title": "Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates",
            "authors": [
                "Prashant Kumar Choudhary",
                "Nouhaila Innan",
                "Muhammad Shafique",
                "Rajeev Singh"
            ],
            "arxiv_id": "2512.09586v1",
            "summary": "Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.",
            "headline_zh": "提出基于图贝叶斯优化的量子电路架构搜索框架，用于网络安全数据集上的量子机器学习。",
            "intro_zh": [
                "量子电路设计是量子机器学习在复杂现实数据上的关键瓶颈。",
                "使用图神经网络代理和蒙特卡洛dropout不确定性校准，通过图表示和突变自动搜索变分量子电路。",
                "在网络安全数据集上评估，相比基线找到复杂度更低且分类精度竞争或更优的电路，并评估噪声鲁棒性。"
            ],
            "tags_zh": [
                "量子电路架构搜索",
                "贝叶斯优化",
                "图神经网络",
                "不确定性校准",
                "网络安全数据集",
                "噪声鲁棒性"
            ],
            "_index": 93
        },
        {
            "title": "UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision",
            "authors": [
                "Alberto Rota",
                "Mert Kiray",
                "Mert Asim Karaoglu",
                "Patrick Ruhkamp",
                "Elena De Momi",
                "Nassir Navabm",
                "Benjamin Busam"
            ],
            "arxiv_id": "2512.09583v1",
            "summary": "Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/",
            "headline_zh": "提出UnReflectAnything框架，通过渲染合成高光监督从单张RGB图像中去除高光",
            "intro_zh": [
                "核心问题：高光在自然和手术图像中扭曲外观、遮挡纹理并阻碍几何推理",
                "方法要点：使用冻结视觉Transformer编码器提取特征，结合轻量级头部定位高光区域和令牌级修复模块恢复特征",
                "实验或效果：在多个基准测试中达到竞争性性能，并泛化到自然和手术领域"
            ],
            "tags_zh": [
                "高光去除",
                "RGB图像处理",
                "合成数据渲染",
                "视觉Transformer",
                "令牌级修复",
                "手术图像分析"
            ],
            "_index": 94
        },
        {
            "title": "Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation",
            "authors": [
                "Hancheng Zhu",
                "Xinyu Liu",
                "Rui Yao",
                "Kunyang Sun",
                "Leida Li",
                "Abdulmotaleb El Saddik"
            ],
            "arxiv_id": "2512.09580v1",
            "summary": "Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.",
            "headline_zh": "提出内容自适应图像润色方法CA-ATP，通过属性文本表示指导，解决现有方法忽略内容颜色变化和用户风格偏好的问题。",
            "intro_zh": [
                "核心问题：现有图像润色方法采用统一像素级颜色映射，忽略图像内容引起的颜色变化，难以适应多样颜色分布和用户风格偏好。",
                "方法要点：设计内容自适应曲线映射模块，利用基础曲线和权重图实现内容感知颜色调整；提出属性文本预测模块，从图像属性生成文本表示，通过多模态模型整合视觉特征，提供用户友好指导。",
                "实验或效果：在多个公共数据集上实验，CA-ATP方法达到最先进性能，验证了其自适应润色能力。"
            ],
            "tags_zh": [
                "图像润色",
                "内容自适应",
                "属性文本表示",
                "多模态模型",
                "颜色映射"
            ],
            "_index": 95
        },
        {
            "title": "Hands-on Evaluation of Visual Transformers for Object Recognition and Detection",
            "authors": [
                "Dimitrios N. Vlachogiannis",
                "Dimitrios A. Koutsomitropoulos"
            ],
            "arxiv_id": "2512.09579v1",
            "summary": "Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.",
            "headline_zh": "评估视觉Transformer在物体识别与检测中的性能，发现其在全局理解任务中优于传统CNN",
            "intro_zh": [
                "核心问题：CNN在图像全局上下文理解上存在局限，而ViT通过自注意力机制能捕捉全图关系。",
                "方法要点：比较纯、分层和混合ViT与传统CNN，在ImageNet、COCO和ChestX-ray14数据集上进行测试。",
                "实验或效果：混合和分层Transformer（如Swin和CvT）在精度与计算资源间取得平衡，数据增强在医疗图像上显著提升性能。"
            ],
            "tags_zh": [
                "视觉Transformer",
                "物体识别",
                "物体检测",
                "医疗图像分类",
                "自注意力机制",
                "数据增强"
            ],
            "_index": 96
        },
        {
            "title": "Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation",
            "authors": [
                "Aris Hofmann",
                "Inge Vejsbjerg",
                "Dhaval Salwala",
                "Elizabeth M. Daly"
            ],
            "arxiv_id": "2512.09577v1",
            "summary": "We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.",
            "headline_zh": "提出Auto-BenchmarkCard工作流，通过自动化生成验证的AI基准文档以解决文档不完整或不一致问题。",
            "intro_zh": [
                "核心问题：AI基准文档常不完整或不一致，影响跨任务或领域的解释与比较。",
                "方法要点：结合多智能体数据提取与LLM驱动合成，从异构源自动生成基准描述。",
                "实验或效果：通过FactReasoner工具进行原子蕴含评分验证事实准确性，提升透明度、可比性和可重用性。"
            ],
            "tags_zh": [
                "基准文档生成",
                "多智能体提取",
                "LLM合成",
                "事实验证",
                "AI基准比较",
                "自动化工作流"
            ],
            "_index": 97
        },
        {
            "title": "Seeing Soil from Space: Towards Robust and Scalable Remote Soil Nutrient Analysis",
            "authors": [
                "David Seu",
                "Nicolas Longepe",
                "Gabriel Cioltea",
                "Erik Maidik",
                "Calin Andrei"
            ],
            "arxiv_id": "2512.09576v1",
            "summary": "Environmental variables are increasingly affecting agricultural decision-making, yet accessible and scalable tools for soil assessment remain limited. This study presents a robust and scalable modeling system for estimating soil properties in croplands, including soil organic carbon (SOC), total nitrogen (N), available phosphorus (P), exchangeable potassium (K), and pH, using remote sensing data and environmental covariates. The system employs a hybrid modeling approach, combining the indirect methods of modeling soil through proxies and drivers with direct spectral modeling. We extend current approaches by using interpretable physics-informed covariates derived from radiative transfer models (RTMs) and complex, nonlinear embeddings from a foundation model. We validate the system on a harmonized dataset that covers Europes cropland soils across diverse pedoclimatic zones. Evaluation is conducted under a robust validation framework that enforces strict spatial blocking, stratified splits, and statistically distinct train-test sets, which deliberately make the evaluation harder and produce more realistic error estimates for unseen regions. The models achieved their highest accuracy for SOC and N. This performance held across unseen locations, under both spatial cross-validation and an independent test set. SOC obtained a MAE of 5.12 g/kg and a CCC of 0.77, and N obtained a MAE of 0.44 g/kg and a CCC of 0.77. We also assess uncertainty through conformal calibration, achieving 90 percent coverage at the target confidence level. This study contributes to the digital advancement of agriculture through the application of scalable, data-driven soil analysis frameworks that can be extended to related domains requiring quantitative soil evaluation, such as carbon markets.",
            "headline_zh": "提出基于遥感与混合建模的稳健可扩展系统，用于农田土壤养分分析。",
            "intro_zh": [
                "核心问题：缺乏可访问且可扩展的土壤评估工具，影响农业决策。",
                "方法要点：结合间接代理建模与直接光谱建模，使用物理信息协变量和基础模型嵌入。",
                "实验或效果：在多样化欧洲农田数据集上验证，SOC和N预测准确度高，并评估不确定性。"
            ],
            "tags_zh": [
                "遥感土壤分析",
                "混合建模",
                "土壤有机碳",
                "不确定性评估",
                "农业数字化"
            ],
            "_index": 98
        },
        {
            "title": "Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment",
            "authors": [
                "Yuan Li",
                "Zitang Sun",
                "Yen-Ju Chen",
                "Shin'ya Nishida"
            ],
            "arxiv_id": "2512.09573v1",
            "summary": "Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.",
            "headline_zh": "提出低层失真感知任务以增强基于视觉语言模型的图像质量评估",
            "intro_zh": [
                "核心问题：多模态大语言模型在图像质量评估中难以可靠检测低层失真，如模糊和噪声。",
                "方法要点：通过组件分析揭示视觉语言对齐阶段削弱低层特征，并引入视觉编码器专用约束。",
                "实验或效果：组件微调后，失真识别准确率从14.92%提升至84.43%，提高评估一致性。"
            ],
            "tags_zh": [
                "图像质量评估",
                "多模态大语言模型",
                "低层视觉感知",
                "视觉语言对齐",
                "失真检测"
            ],
            "_index": 99
        },
        {
            "title": "Lazy Diffusion: Mitigating spectral collapse in generative diffusion-based stable autoregressive emulation of turbulent flows",
            "authors": [
                "Anish Sambamurthy",
                "Ashesh Chattopadhyay"
            ],
            "arxiv_id": "2512.09572v1",
            "summary": "Turbulent flows posses broadband, power-law spectra in which multiscale interactions couple high-wavenumber fluctuations to large-scale dynamics. Although diffusion-based generative models offer a principled probabilistic forecasting framework, we show that standard DDPMs induce a fundamental \\emph{spectral collapse}: a Fourier-space analysis of the forward SDE reveals a closed-form, mode-wise signal-to-noise ratio (SNR) that decays monotonically in wavenumber, $|k|$ for spectra $S(k)\\!\\propto\\!|k|^{-λ}$, rendering high-wavenumber modes indistinguishable from noise and producing an intrinsic spectral bias. We reinterpret the noise schedule as a spectral regularizer and introduce power-law schedules $β(τ)\\!\\propto\\!τ^γ$ that preserve fine-scale structure deeper into diffusion time, along with \\emph{Lazy Diffusion}, a one-step distillation method that leverages the learned score geometry to bypass long reverse-time trajectories and prevent high-$k$ degradation. Applied to high-Reynolds-number 2D Kolmogorov turbulence and $1/12^\\circ$ Gulf of Mexico ocean reanalysis, these methods resolve spectral collapse, stabilize long-horizon autoregression, and restore physically realistic inertial-range scaling. Together, they show that naïve Gaussian scheduling is structurally incompatible with power-law physics and that physics-aware diffusion processes can yield accurate, efficient, and fully probabilistic surrogates for multiscale dynamical systems.",
            "headline_zh": "提出Lazy Diffusion与幂律噪声调度以解决生成扩散模型中湍流模拟的频谱崩溃问题",
            "intro_zh": [
                "标准DDPM在湍流模拟中导致频谱崩溃，高波数模式被噪声淹没",
                "引入幂律噪声调度和Lazy Diffusion蒸馏方法，保留精细结构并提升效率",
                "在2D湍流和海洋再分析数据中验证，恢复物理惯性范围尺度"
            ],
            "tags_zh": [
                "生成扩散模型",
                "湍流模拟",
                "频谱崩溃",
                "噪声调度",
                "蒸馏方法",
                "多尺度系统"
            ],
            "_index": 100
        },
        {
            "title": "Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing",
            "authors": [
                "Feng Yu",
                "Yu Hu",
                "Yang Su",
                "Yang Deng",
                "Linzuo Zhang",
                "Danping Zou"
            ],
            "arxiv_id": "2512.09571v1",
            "summary": "Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/",
            "headline_zh": "提出两阶段学习框架以解决无人机在未知杂乱环境中视觉竞速的泛化问题",
            "intro_zh": [
                "核心问题：强化学习方法在固定无障赛道泛化不足，需平衡速度与避障，且深度图中门与障碍物感知模糊",
                "方法要点：采用软碰撞训练和硬碰撞精炼两阶段，结合自适应噪声课程和非对称架构，增强视觉输入依赖",
                "实验或效果：通过仿真和真实实验验证，在计算受限四旋翼上实现敏捷飞行，对门位置误差鲁棒"
            ],
            "tags_zh": [
                "无人机竞速",
                "强化学习",
                "视觉导航",
                "泛化能力",
                "避障控制"
            ],
            "_index": 101
        },
        {
            "title": "The Gender Code: Gendering the Global Governance of Artificial Intelligence",
            "authors": [
                "Jelena Cupac"
            ],
            "arxiv_id": "2512.09570v1",
            "summary": "This paper examines how international AI governance frameworks address gender issues and gender-based harms. The analysis covers binding regulations, such as the EU AI Act; soft law instruments, like the UNESCO Recommendations on AI Ethics; and global initiatives, such as the Global Partnership on AI (GPAI). These instruments reveal emerging trends, including the integration of gender concerns into broader human rights frameworks, a shift toward explicit gender-related provisions, and a growing emphasis on inclusivity and diversity. Yet, some critical gaps persist, including inconsistent treatment of gender across governance documents, limited engagement with intersectionality, and a lack of robust enforcement mechanisms. However, this paper argues that effective AI governance must be intersectional, enforceable, and inclusive. This is key to moving beyond tokenism toward meaningful equity and preventing reinforcement of existing inequalities. The study contributes to ethical AI debates by highlighting the importance of gender-sensitive governance in building a just technological future.",
            "headline_zh": "分析国际AI治理框架中的性别问题，强调交叉性、可执行性和包容性以促进公平",
            "intro_zh": [
                "核心问题：国际AI治理框架如何应对性别议题和性别相关危害，存在不一致、交叉性不足和执行机制薄弱等缺口",
                "方法要点：分析欧盟AI法案、UNESCO AI伦理建议和GPAI等法规与倡议，揭示性别整合趋势和关键挑战",
                "实验或效果：提出有效AI治理需具备交叉性、可执行性和包容性，以超越象征性措施，推动实质公平"
            ],
            "tags_zh": [
                "AI治理",
                "性别平等",
                "交叉性",
                "国际法规",
                "伦理AI",
                "包容性设计"
            ],
            "_index": 102
        },
        {
            "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search",
            "authors": [
                "Junkai Ji",
                "Zhangfan Yang",
                "Dong Xu",
                "Ruibin Bai",
                "Jianqiang Li",
                "Tingjun Hou",
                "Zexuan Zhu"
            ],
            "arxiv_id": "2512.09566v1",
            "summary": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.",
            "headline_zh": "提出Trio框架，通过语言模型、强化学习和树搜索实现闭环靶向分子设计。",
            "intro_zh": [
                "传统药物发现方法效率低、可扩展性差，生成模型存在泛化不足和忽视关键药性等问题。",
                "Trio整合基于片段的分子语言建模、强化学习和蒙特卡洛树搜索，实现上下文感知的片段组装和平衡探索与利用。",
                "实验显示Trio在结合亲和力、类药性和合成可及性上优于现有方法，并显著提升分子多样性。"
            ],
            "tags_zh": [
                "分子生成",
                "药物发现",
                "语言模型",
                "强化学习",
                "蒙特卡洛树搜索",
                "闭环设计"
            ],
            "_index": 103
        },
        {
            "title": "From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection",
            "authors": [
                "Faraz Ali",
                "Muhammad Afaq",
                "Mahmood Niazi",
                "Muzammil Behzad"
            ],
            "arxiv_id": "2512.09565v1",
            "summary": "Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.",
            "headline_zh": "提出DNS-HyXNet，一种基于xLSTM的轻量级序列模型，用于实时DNS隧道检测",
            "intro_zh": [
                "核心问题：DNS隧道作为隐蔽通信通道，现有图方法如GraphTunnel延迟高，不适合实时部署。",
                "方法要点：集成令牌化域名嵌入和数值特征，通过双层xLSTM直接学习序列依赖，避免图重建，实现单阶段多分类。",
                "实验或效果：在公开数据集上达到99.99%准确率，检测延迟仅0.041毫秒，验证了实时性和可部署性。"
            ],
            "tags_zh": [
                "DNS隧道检测",
                "序列建模",
                "xLSTM",
                "实时系统",
                "轻量级网络",
                "网络安全"
            ],
            "_index": 104
        },
        {
            "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection",
            "authors": [
                "Binglin Wu",
                "Jiaxiu Zou",
                "Xianneng Li"
            ],
            "arxiv_id": "2512.09563v1",
            "summary": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.",
            "headline_zh": "提出基于提示工程、监督微调和模型合并的三阶段LLM框架，以提升中文细粒度仇恨言论检测性能。",
            "intro_zh": [
                "核心问题：中文社交媒体仇恨言论泛滥，传统系统难以处理语境依赖的修辞策略和演变俚语。",
                "方法要点：设计上下文感知提示引导LLM提取隐含仇恨模式，通过监督微调集成任务特征，合并微调模型增强鲁棒性。",
                "实验或效果：在STATE-ToxiCN基准上验证框架有效性，性能优于基线方法。"
            ],
            "tags_zh": [
                "仇恨言论检测",
                "大语言模型",
                "提示工程",
                "模型合并",
                "细粒度分类"
            ],
            "_index": 105
        },
        {
            "title": "Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment",
            "authors": [
                "Yuan Li",
                "Zitang Sun",
                "Yen-ju Chen",
                "Shin'ya Nishida"
            ],
            "arxiv_id": "2512.09555v1",
            "summary": "Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.",
            "headline_zh": "提出两阶段调优方法以解决盲图像质量评估中视觉语言模型的推理矛盾与不稳定性问题",
            "intro_zh": [
                "分析视觉语言模型在盲图像质量评估中产生矛盾预测和不稳定性的原因，如特征与预测逻辑连接弱",
                "引入两阶段调优方法，先学习视觉特征，再基于特征推断质量，以模拟人类推理过程",
                "实验表明方法在SPAQ和KONIQ上降低不稳定性至12.39%，并在多个数据集上提升SRCC/PLCC性能"
            ],
            "tags_zh": [
                "盲图像质量评估",
                "视觉语言模型",
                "推理稳定性",
                "两阶段调优",
                "视觉特征学习"
            ],
            "_index": 106
        },
        {
            "title": "A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution",
            "authors": [
                "Murat Karayaka",
                "Usman Muhammad",
                "Jorma Laaksonen",
                "Md Ziaul Hoque",
                "Tapio Seppänen"
            ],
            "arxiv_id": "2512.09546v1",
            "summary": "This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.",
            "headline_zh": "提出轻量级双域卷积网络DDSRNet，结合空间域与离散小波变换，用于高光谱单图像超分辨率。",
            "intro_zh": [
                "核心问题：高光谱图像超分辨率需平衡性能与计算成本，传统方法可能效率不足。",
                "方法要点：设计Spatial-Net提取浅层特征，利用DWT分解低频结构，共享CNN增强高频子带。",
                "实验或效果：在三个高光谱数据集上实现竞争性性能，计算成本低，验证了双域学习的有效性。"
            ],
            "tags_zh": [
                "高光谱图像超分辨率",
                "双域学习",
                "离散小波变换",
                "轻量级网络",
                "空间域增强",
                "频率域分解"
            ],
            "_index": 107
        },
        {
            "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
            "authors": [
                "Arihant Tripathy",
                "Ch Pavan Harshit",
                "Karthik Vaidhyanathan"
            ],
            "arxiv_id": "2512.09543v1",
            "summary": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
            "headline_zh": "实证研究SLM在代理问题解决框架中的能效，揭示架构设计是能耗主因且能量浪费严重",
            "intro_zh": [
                "核心问题：SLM在复杂代理框架中的实际能效和性能未知，限制本地部署应用",
                "方法要点：在固定硬件上评估四个框架使用两种SLM的性能、能耗和资源消耗",
                "实验或效果：框架架构是能耗主因，但能量浪费严重，任务解决率近零"
            ],
            "tags_zh": [
                "小语言模型",
                "能效分析",
                "代理框架",
                "软件工程自动化",
                "资源消耗"
            ],
            "_index": 108
        },
        {
            "title": "Comparative Analysis of Hash-based Malware Clustering via K-Means",
            "authors": [
                "Aink Acrie Soe Thein",
                "Nikolaos Pitropakis",
                "Pavlos Papadopoulos",
                "Sam Grierson",
                "Sana Ullah Jan"
            ],
            "arxiv_id": "2512.09539v1",
            "summary": "With the adoption of multiple digital devices in everyday life, the cyber-attack surface has increased. Adversaries are continuously exploring new avenues to exploit them and deploy malware. On the other hand, detection approaches typically employ hashing-based algorithms such as SSDeep, TLSH, and IMPHash to capture structural and behavioural similarities among binaries. This work focuses on the analysis and evaluation of these techniques for clustering malware samples using the K-means algorithm. More specifically, we experimented with established malware families and traits and found that TLSH and IMPHash produce more distinct, semantically meaningful clusters, whereas SSDeep is more efficient for broader classification tasks. The findings of this work can guide the development of more robust threat-detection mechanisms and adaptive security mechanisms.",
            "headline_zh": "分析基于哈希的恶意软件聚类方法，通过K-means评估SSDeep、TLSH和IMPHash的性能差异。",
            "intro_zh": [
                "核心问题：数字设备普及导致网络攻击面扩大，恶意软件检测需高效聚类方法。",
                "方法要点：使用K-means算法比较SSDeep、TLSH和IMPHash在恶意软件聚类中的表现。",
                "实验或效果：TLSH和IMPHash产生更语义清晰的聚类，SSDeep在广泛分类中更高效。"
            ],
            "tags_zh": [
                "恶意软件聚类",
                "哈希算法",
                "K-means算法",
                "网络安全",
                "性能评估"
            ],
            "_index": 109
        },
        {
            "title": "Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search",
            "authors": [
                "Ekaterina Fadeeva",
                "Maiya Goloburda",
                "Aleksandr Rubashevskii",
                "Roman Vashurin",
                "Artem Shelmanov",
                "Preslav Nakov",
                "Mrinmaya Sachan",
                "Maxim Panov"
            ],
            "arxiv_id": "2512.09538v1",
            "summary": "Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance.",
            "headline_zh": "提出基于束搜索的一致性方法以改进大语言模型在短问答中的不确定性量化",
            "intro_zh": [
                "核心问题：多轮采样在短问答中易产生重复样本，导致不确定性估计方差大。",
                "方法要点：使用束搜索生成候选集，替代多轮采样，提升一致性和减少方差。",
                "实验或效果：在六个问答数据集上验证，性能优于多轮采样，达到先进水平。"
            ],
            "tags_zh": [
                "不确定性量化",
                "束搜索",
                "大语言模型",
                "短问答",
                "一致性方法"
            ],
            "_index": 110
        },
        {
            "title": "REASAN: Learning Reactive Safe Navigation for Legged Robots",
            "authors": [
                "Qihao Yuan",
                "Ziyu Cao",
                "Ming Cao",
                "Kailai Li"
            ],
            "arxiv_id": "2512.09537v1",
            "summary": "We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.",
            "headline_zh": "提出REASAN框架，用于足式机器人在复杂动态环境中的反应式安全导航。",
            "intro_zh": [
                "核心问题：足式机器人在复杂动态环境中实现反应式安全导航的挑战。",
                "方法要点：采用模块化端到端框架，结合强化学习策略和基于Transformer的外部感知估计器。",
                "实验或效果：在单/多机器人设置中实现全机载实时导航，并通过消融实验验证鲁棒性提升。"
            ],
            "tags_zh": [
                "足式机器人导航",
                "反应式安全导航",
                "强化学习策略",
                "模块化框架",
                "LiDAR传感器",
                "实时系统"
            ],
            "_index": 111
        },
        {
            "title": "Latent-Autoregressive GP-VAE Language Model",
            "authors": [
                "Yves Ruffenach"
            ],
            "arxiv_id": "2512.09535v1",
            "summary": "We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.",
            "headline_zh": "提出基于高斯过程的潜在自回归GP-VAE语言模型，将序列动态转移至潜在空间以支持部分时间结构。",
            "intro_zh": [
                "核心问题：探索语言模型中时间结构是否可由潜在空间的概率几何而非显式神经操作支持。",
                "方法要点：集成高斯过程先验和结构化摊销后验，采用正则化ELBO训练协议，实现非自回归解码。",
                "实验或效果：在概念验证框架下，模型训练稳定，序列与并行采样变体表现一致。"
            ],
            "tags_zh": [
                "潜在自回归模型",
                "高斯过程变分自编码器",
                "语言模型",
                "序列建模",
                "非自回归解码",
                "概率几何"
            ],
            "_index": 112
        },
        {
            "title": "Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport",
            "authors": [
                "Antonio Candelieri",
                "Alessandro Quadrio"
            ],
            "arxiv_id": "2512.09530v1",
            "summary": "This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.\n  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.",
            "headline_zh": "提出基于最优运输的自注意力训练方法，用于表格数据分类，以提升效率与可扩展性。",
            "intro_zh": [
                "核心问题：自注意力训练轨迹效率低，影响表格分类性能与计算成本。",
                "方法要点：通过最优运输度量分析自注意力映射，并设计基于OT的算法生成类特定分布进行对齐训练。",
                "实验或效果：在分类任务中实现与Transformer相当的精度，降低计算成本，但对虚拟分布设计敏感。"
            ],
            "tags_zh": [
                "表格数据分类",
                "自注意力训练",
                "最优运输",
                "计算效率",
                "机器学习算法"
            ],
            "_index": 113
        },
        {
            "title": "Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction",
            "authors": [
                "Hongyou Zhou",
                "Cederic Aßmann",
                "Alaa Bejaoui",
                "Heiko Tzschätzsch",
                "Mark Heyland",
                "Julian Zierke",
                "Niklas Tuttle",
                "Sebastian Hölzl",
                "Timo Auer",
                "David A. Back",
                "Marc Toussaint"
            ],
            "arxiv_id": "2512.09525v1",
            "summary": "Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair",
            "headline_zh": "提出结合掩码注册与自编码的CT图像方法，以预测胫骨重建目标",
            "intro_zh": [
                "核心问题：复杂胫骨骨折手术规划中，难以想象健康骨对齐的3D结构。",
                "方法要点：使用改进的空间变换网络进行CT注册，结合自编码器建模健康胫骨变异，并处理掩码输入。",
                "实验或效果：通过掩码输入预测患者特异性健康骨结构，项目页面提供代码。"
            ],
            "tags_zh": [
                "CT图像处理",
                "神经注册",
                "自编码器",
                "胫骨重建",
                "掩码输入",
                "手术规划"
            ],
            "_index": 114
        },
        {
            "title": "NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization",
            "authors": [
                "Gaorui Zhang",
                "Zhizhang Yuan",
                "Jialan Yang",
                "Junru Chen",
                "Li Meng",
                "Yang Yang"
            ],
            "arxiv_id": "2512.09524v1",
            "summary": "Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.",
            "headline_zh": "提出NeuroSketch框架，通过系统架构优化提升脑机接口中的神经解码性能。",
            "intro_zh": [
                "核心问题：神经解码中模型架构探索不足，影响性能提升。",
                "方法要点：从基础架构研究到宏观微观优化，基于CNN-2D进行系统设计。",
                "实验或效果：在多种模态、信号和任务上验证，达到SOTA性能。"
            ],
            "tags_zh": [
                "神经解码",
                "脑机接口",
                "架构优化",
                "CNN-2D",
                "多模态实验"
            ],
            "_index": 115
        },
        {
            "title": "QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder",
            "authors": [
                "Nabil Anan Orka",
                "Ehtashamul Haque",
                "Maftahul Jannat",
                "Md Abdul Awal",
                "Mohammad Ali Moni"
            ],
            "arxiv_id": "2512.09517v1",
            "summary": "This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.",
            "headline_zh": "提出QuanvNeXt全量子卷积模型，用于基于EEG的抑郁症检测。",
            "intro_zh": [
                "核心问题：基于EEG的抑郁症诊断，需高效模型处理脑电信号。",
                "方法要点：引入Cross Residual块，减少特征同质性并增强跨特征关系。",
                "实验或效果：在两个开源数据集上平均准确率93.1%，AUC-ROC 97.2%，优于基线。"
            ],
            "tags_zh": [
                "量子卷积神经网络",
                "EEG信号处理",
                "抑郁症检测",
                "残差块设计",
                "不确定性分析",
                "可解释AI"
            ],
            "_index": 116
        },
        {
            "title": "Transport Novelty Distance: A Distributional Metric for Evaluating Material Generative Models",
            "authors": [
                "Paul Hagemann",
                "Simon Müller",
                "Janine George",
                "Philipp Benner"
            ],
            "arxiv_id": "2512.09514v1",
            "summary": "Recent advances in generative machine learning have opened new possibilities for the discovery and design of novel materials. However, as these models become more sophisticated, the need for rigorous and meaningful evaluation metrics has grown. Existing evaluation approaches often fail to capture both the quality and novelty of generated structures, limiting our ability to assess true generative performance. In this paper, we introduce the Transport Novelty Distance (TNovD) to judge generative models used for materials discovery jointly by the quality and novelty of the generated materials. Based on ideas from Optimal Transport theory, TNovD uses a coupling between the features of the training and generated sets, which is refined into a quality and memorization regime by a threshold. The features are generated from crystal structures using a graph neural network that is trained to distinguish between materials, their augmented counterparts, and differently sized supercells using contrastive learning. We evaluate our proposed metric on typical toy experiments relevant for crystal structure prediction, including memorization, noise injection and lattice deformations. Additionally, we validate the TNovD on the MP20 validation set and the WBM substitution dataset, demonstrating that it is capable of detecting both memorized and low-quality material data. We also benchmark the performance of several popular material generative models. While introduced for materials, our TNovD framework is domain-agnostic and can be adapted for other areas, such as images and molecules.",
            "headline_zh": "提出Transport Novelty Distance以评估材料生成模型的质量与新颖性",
            "intro_zh": [
                "现有评估方法难以同时衡量生成材料的质量和新颖性",
                "基于最优传输理论，通过阈值将训练与生成特征耦合分解为质量和记忆机制",
                "在晶体结构预测相关实验和数据集上验证了该指标的有效性"
            ],
            "tags_zh": [
                "材料生成模型评估",
                "最优传输理论",
                "图神经网络",
                "对比学习",
                "晶体结构预测"
            ],
            "_index": 117
        },
        {
            "title": "Contextual Dynamic Pricing with Heterogeneous Buyers",
            "authors": [
                "Thodoris Lykouris",
                "Sloan Nietert",
                "Princewill Okoroafor",
                "Chara Podimata",
                "Julian Zimmert"
            ],
            "arxiv_id": "2512.09513v1",
            "summary": "We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\\widetilde{O}(K_{\\star}\\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\\star}$.",
            "headline_zh": "提出基于乐观后验采样的上下文动态定价算法，处理异质买家场景，实现次优后悔界。",
            "intro_zh": [
                "研究异质买家下的上下文动态定价问题，买家估值类型来自未知有限分布。",
                "开发乐观后验采样算法，后悔界为Õ(K⋆√dT)，在d和T上紧致。",
                "非上下文场景中提出方差感知缩放算法，优化对K⋆的依赖。"
            ],
            "tags_zh": [
                "动态定价",
                "异质买家",
                "上下文学习",
                "后悔界分析",
                "后验采样"
            ],
            "_index": 118
        },
        {
            "title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics",
            "authors": [
                "Donato Caramia",
                "Florian T. Pokorny",
                "Giuseppe Triggiani",
                "Denis Ruffino",
                "David Naso",
                "Paolo Roberto Massenio"
            ],
            "arxiv_id": "2512.09510v1",
            "summary": "Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.",
            "headline_zh": "提出ViTA-Seg视觉Transformer框架，用于机器人遮挡场景的实时全模态分割。",
            "intro_zh": [
                "核心问题：机器人箱体拾取中的遮挡影响抓取规划准确性。",
                "方法要点：基于全局注意力的类无关视觉Transformer，支持单头全模态和双头全模态加遮挡掩码预测。",
                "实验或效果：在COOCA和KINS基准上验证了高精度与计算效率，并引入ViTA-SimData合成数据集。"
            ],
            "tags_zh": [
                "全模态分割",
                "视觉Transformer",
                "机器人抓取",
                "遮挡处理",
                "实时分割",
                "合成数据集"
            ],
            "_index": 119
        },
        {
            "title": "Estimation of Stochastic Optimal Transport Maps",
            "authors": [
                "Sloan Nietert",
                "Ziv Goldfeld"
            ],
            "arxiv_id": "2512.09499v1",
            "summary": "The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic.",
            "headline_zh": "提出随机最优传输映射估计框架，以处理现实应用中确定性映射失效的场景。",
            "intro_zh": [
                "核心问题：现有最优传输映射理论依赖严格假设，在现实问题中常失效，需处理随机映射。",
                "方法要点：引入新度量评估随机映射传输质量，开发高效估计器，具有近最优有限样本风险界。",
                "实验或效果：实验验证理论，在现有理论失效场景中展示实用性，支持对抗性样本污染。"
            ],
            "tags_zh": [
                "随机最优传输",
                "映射估计",
                "有限样本风险",
                "对抗性鲁棒性",
                "概率分布变换"
            ],
            "_index": 120
        },
        {
            "title": "Gradient-Guided Learning Network for Infrared Small Target Detection",
            "authors": [
                "Jinmiao Zhao",
                "Chuang Yu",
                "Zelin Shi",
                "Yunpeng Liu",
                "Yingdi Zhang"
            ],
            "arxiv_id": "2512.09497v1",
            "summary": "Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net",
            "headline_zh": "提出梯度引导学习网络以解决红外小目标检测中边缘定位不准确和目标易被背景淹没的问题",
            "intro_zh": [
                "核心问题：红外小目标尺寸小、特征少，导致现有方法边缘定位不准确且目标易被背景淹没",
                "方法要点：首次引入梯度幅值图像，设计双分支特征提取网络和双向引导融合模块，增强边缘细节和特征融合",
                "实验或效果：在公开真实和合成数据集上实现最优结果，代码已开源"
            ],
            "tags_zh": [
                "红外小目标检测",
                "梯度引导学习",
                "双分支网络",
                "特征融合",
                "深度学习"
            ],
            "_index": 121
        },
        {
            "title": "Representation Invariance and Allocation: When Subgroup Balance Matters",
            "authors": [
                "Anissa Alloula",
                "Charles Jones",
                "Zuzanna Wakefield-Skorniewska",
                "Francesco Quinzan",
                "Bartłomiej Papież"
            ],
            "arxiv_id": "2512.09496v1",
            "summary": "Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.",
            "headline_zh": "提出潜在分离假说以优化基础模型微调中的子群数据平衡决策",
            "intro_zh": [
                "核心问题：训练数据中子群表示不平衡对模型泛化性能的影响机制未知",
                "方法要点：基于预训练模型潜在空间子群分离度，提出潜在分离假说解释性能敏感性",
                "实验或效果：在四个视觉和语言模型中验证假说，并应用于基础模型微调实践"
            ],
            "tags_zh": [
                "子群平衡",
                "潜在分离假说",
                "基础模型微调",
                "数据表示",
                "模型泛化",
                "计算机视觉"
            ],
            "_index": 122
        },
        {
            "title": "On Mobile Ad Hoc Networks for Coverage of Partially Observable Worlds",
            "authors": [
                "Edwin Meriaux",
                "Shuo Wen",
                "Louis-Roy Langevin",
                "Doina Precup",
                "Antonio Loría",
                "Gregory Dudek"
            ],
            "arxiv_id": "2512.09495v1",
            "summary": "This paper addresses the movement and placement of mobile agents to establish a communication network in initially unknown environments. We cast the problem in a computational-geometric framework by relating the coverage problem and line-of-sight constraints to the Cooperative Guard Art Gallery Problem, and introduce its partially observable variant, the Partially Observable Cooperative Guard Art Gallery Problem (POCGAGP). We then present two algorithms that solve POCGAGP: CADENCE, a centralized planner that incrementally selects 270 degree corners at which to deploy agents, and DADENCE, a decentralized scheme that coordinates agents using local information and lightweight messaging. Both approaches operate under partial observability and target simultaneous coverage and connectivity. We evaluate the methods in simulation across 1,500 test cases of varied size and structure, demonstrating consistent success in forming connected networks while covering and exploring unknown space. These results highlight the value of geometric abstractions for communication-driven exploration and show that decentralized policies are competitive with centralized performance while retaining scalability.",
            "headline_zh": "提出POCGAGP模型及CADENCE与DADENCE算法，以解决未知环境中移动代理的覆盖与连通网络构建问题。",
            "intro_zh": [
                "核心问题：在部分可观测的未知环境中，移动代理如何部署以实现空间覆盖和通信网络连通。",
                "方法要点：基于计算几何框架，引入POCGAGP模型，开发集中式CADENCE和分布式DADENCE算法。",
                "实验或效果：在1,500个模拟测试中，算法成功构建连通网络，覆盖未知空间，分布式方法性能接近集中式。"
            ],
            "tags_zh": [
                "移动自组织网络",
                "部分可观测覆盖",
                "计算几何模型",
                "分布式算法",
                "通信驱动探索"
            ],
            "_index": 123
        },
        {
            "title": "StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio",
            "authors": [
                "Abdullah Al Mamun",
                "Miaohua Zhang",
                "David Ahmedt-Aristizabal",
                "Zeeshan Hayder",
                "Mohammad Awrangjeb"
            ],
            "arxiv_id": "2512.09492v1",
            "summary": "Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.",
            "headline_zh": "提出StateSpace-SSL线性时间自监督学习框架，用于植物病害检测",
            "intro_zh": [
                "现有CNN和Transformer自监督方法难以捕捉叶片病害连续模式或计算成本高",
                "采用Vision Mamba状态空间编码器，通过定向扫描建模长程病变连续性",
                "在三个公开数据集上优于基线，学习到紧凑、病变聚焦的特征图"
            ],
            "tags_zh": [
                "植物病害检测",
                "自监督学习",
                "状态空间模型",
                "线性时间计算",
                "特征表示学习"
            ],
            "_index": 124
        },
        {
            "title": "MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images",
            "authors": [
                "Shuaihao Han",
                "Tingfa Xu",
                "Peifu Liu",
                "Jianan Li"
            ],
            "arxiv_id": "2512.09489v1",
            "summary": "Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.",
            "headline_zh": "提出MODA数据集和OSSDet框架以解决航空图像中多光谱目标检测的数据缺乏与性能挑战",
            "intro_zh": [
                "核心问题：航空目标检测面临小目标和背景干扰，RGB图像信息不足，且缺乏多光谱训练数据。",
                "方法要点：引入大规模MODA数据集，并提出OSSDet框架，通过光谱-空间调制、光谱相似性聚合和对象感知掩码优化检测。",
                "实验或效果：OSSDet在参数和效率可比情况下优于现有方法，验证了多光谱数据的潜力。"
            ],
            "tags_zh": [
                "多光谱目标检测",
                "航空图像",
                "数据集构建",
                "光谱-空间融合",
                "对象感知学习"
            ],
            "_index": 125
        },
        {
            "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning",
            "authors": [
                "Yucan Guo",
                "Miao Su",
                "Saiping Guan",
                "Zihao Sun",
                "Xiaolong Jin",
                "Jiafeng Guo",
                "Xueqi Cheng"
            ],
            "arxiv_id": "2512.09487v1",
            "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.",
            "headline_zh": "提出RouteRAG框架，通过强化学习实现文本与图混合检索增强生成，以支持自适应高效的多轮推理",
            "intro_zh": [
                "核心问题：现有混合检索系统依赖固定流程，无法自适应整合文本与图证据，且图检索成本高",
                "方法要点：基于强化学习联合优化生成过程，学习何时推理、从何处检索及何时输出答案",
                "实验效果：在五个问答基准上显著超越现有RAG基线，验证了端到端强化学习的优势"
            ],
            "tags_zh": [
                "检索增强生成",
                "强化学习",
                "图文本混合检索",
                "多轮推理",
                "自适应检索",
                "问答系统"
            ],
            "_index": 126
        },
        {
            "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks",
            "authors": [
                "Xinye Cao",
                "Yihan Lin",
                "Guoshun Nan",
                "Qinchuan Zhou",
                "Yuhang Luo",
                "Yurui Gao",
                "Zeliang Zhang",
                "Haolang Lu",
                "Qimei Cui",
                "Yanzhao Hou",
                "Xiaofeng Tao",
                "Tony Q. S. Quek"
            ],
            "arxiv_id": "2512.09485v1",
            "summary": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications.",
            "headline_zh": "提出SecLoop和SA-GRPO以解决6G零接触网络中安全自动化的策略生命周期和适应性挑战",
            "intro_zh": [
                "核心问题：6G零接触网络面临分布式架构和动态威胁，需自动化安全策略生命周期并适应环境变化。",
                "方法要点：SecLoop集成大语言模型实现安全策略全生命周期自动化；SA-GRPO通过组相对策略优化迭代优化策略。",
                "实验或效果：在真实世界基准测试中验证了方法对多种攻击的有效性，并计划开源平台。"
            ],
            "tags_zh": [
                "安全自动化",
                "零接触网络",
                "大语言模型",
                "策略优化",
                "6G网络",
                "MITRE ATT&CK"
            ],
            "_index": 127
        },
        {
            "title": "Color encoding in Latent Space of Stable Diffusion Models",
            "authors": [
                "Guillem Arias",
                "Ariadna Solà",
                "Martí Armengod",
                "Maria Vanrell"
            ],
            "arxiv_id": "2512.09477v1",
            "summary": "Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.",
            "headline_zh": "分析Stable Diffusion潜在空间中的颜色编码机制，揭示其与高效编码表示的对齐结构",
            "intro_zh": [
                "研究扩散生成模型中颜色等感知属性的内部表示机制，以Stable Diffusion为例",
                "通过合成数据集、PCA和相似性度量，发现颜色信息在潜在通道c_3和c_4中以圆形对立轴编码",
                "实验表明潜在空间具有可解释结构，为模型理解和编辑应用提供基础"
            ],
            "tags_zh": [
                "Stable Diffusion",
                "潜在空间分析",
                "颜色编码",
                "生成模型",
                "高效编码"
            ],
            "_index": 128
        },
        {
            "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving",
            "authors": [
                "Chiheng Lou",
                "Sheng Qi",
                "Rui Kang",
                "Yong Zhang",
                "Chen Sun",
                "Pengcheng Wang",
                "Bingyang Liu",
                "Xuanzhe Liu",
                "Xin Jin"
            ],
            "arxiv_id": "2512.09472v1",
            "summary": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.\n  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.",
            "headline_zh": "提出WarmServe系统，通过通用GPU工作器实现一对多预热，以优化多LLM服务中的推理性能。",
            "intro_zh": [
                "核心问题：现有系统因忽视未来负载特性，导致GPU利用率优化与推理性能（如首词延迟）之间的权衡。",
                "方法要点：设计通用GPU工作器，基于负载预测进行主动预热，采用驱逐感知模型放置和零开销内存切换机制。",
                "实验或效果：在真实数据集上，相比现有系统，首词延迟提升最高达50.8倍，请求服务能力提升至2.5倍。"
            ],
            "tags_zh": [
                "多LLM服务",
                "GPU预热",
                "负载预测",
                "模型放置",
                "内存管理",
                "推理性能优化"
            ],
            "_index": 129
        },
        {
            "title": "Temporal-Spatial Tubelet Embedding for Cloud-Robust MSI Reconstruction using MSI-SAR Fusion: A Multi-Head Self-Attention Video Vision Transformer Approach",
            "authors": [
                "Yiqun Wang",
                "Lujun Li",
                "Meiru Yue",
                "Radu State"
            ],
            "arxiv_id": "2512.09471v1",
            "summary": "Cloud cover in multispectral imagery (MSI) significantly hinders early-season crop mapping by corrupting spectral information. Existing Vision Transformer(ViT)-based time-series reconstruction methods, like SMTS-ViT, often employ coarse temporal embeddings that aggregate entire sequences, causing substantial information loss and reducing reconstruction accuracy. To address these limitations, a Video Vision Transformer (ViViT)-based framework with temporal-spatial fusion embedding for MSI reconstruction in cloud-covered regions is proposed in this study. Non-overlapping tubelets are extracted via 3D convolution with constrained temporal span $(t=2)$, ensuring local temporal coherence while reducing cross-day information degradation. Both MSI-only and SAR-MSI fusion scenarios are considered during the experiments. Comprehensive experiments on 2020 Traill County data demonstrate notable performance improvements: MTS-ViViT achieves a 2.23\\% reduction in MSE compared to the MTS-ViT baseline, while SMTS-ViViT achieves a 10.33\\% improvement with SAR integration over the SMTS-ViT baseline. The proposed framework effectively enhances spectral reconstruction quality for robust agricultural monitoring.",
            "headline_zh": "提出基于时空管状嵌入的ViViT框架，用于云覆盖多光谱图像重建，提升农业监测鲁棒性。",
            "intro_zh": [
                "核心问题：云覆盖导致多光谱图像信息损失，影响早期作物制图准确性。",
                "方法要点：采用非重叠时空管状嵌入，结合3D卷积约束时间跨度，增强局部时序一致性。",
                "实验或效果：在Traill County数据上，SAR融合方案使SMTS-ViViT比基线MSE降低10.33%。"
            ],
            "tags_zh": [
                "多光谱图像重建",
                "时空融合",
                "视频视觉Transformer",
                "云鲁棒性",
                "农业监测"
            ],
            "_index": 130
        },
        {
            "title": "LiePrune: Lie Group and Quantum Geometric Dual Representation for One-Shot Structured Pruning of Quantum Neural Networks",
            "authors": [
                "Haijian Shao",
                "Bowen Yang",
                "Wei Liu",
                "Xing Deng",
                "Yingtao Jiang"
            ],
            "arxiv_id": "2512.09469v1",
            "summary": "Quantum neural networks (QNNs) and parameterized quantum circuits (PQCs) are key building blocks for near-term quantum machine learning. However, their scalability is constrained by excessive parameters, barren plateaus, and hardware limitations. We propose LiePrune, the first mathematically grounded one-shot structured pruning framework for QNNs that leverages Lie group structure and quantum geometric information. Each gate is jointly represented in a Lie group--Lie algebra dual space and a quantum geometric feature space, enabling principled redundancy detection and aggressive compression. Experiments on quantum classification (MNIST, FashionMNIST), quantum generative modeling (Bars-and-Stripes), and quantum chemistry (LiH VQE) show that LiePrune achieves over $10\\times$ compression with negligible or even improved task performance, while providing provable guarantees on redundancy detection, functional approximation, and computational complexity.",
            "headline_zh": "提出LiePrune框架，利用李群和量子几何对量子神经网络进行一次性结构化剪枝",
            "intro_zh": [
                "量子神经网络面临参数过多、贫瘠高原和硬件限制等可扩展性问题",
                "基于李群-李代数对偶空间和量子几何特征空间，实现原理性冗余检测和压缩",
                "在量子分类、生成建模和量子化学任务中实现超10倍压缩，性能无损失或提升"
            ],
            "tags_zh": [
                "量子神经网络",
                "结构化剪枝",
                "李群表示",
                "量子几何",
                "一次性压缩",
                "参数化量子电路"
            ],
            "_index": 131
        },
        {
            "title": "Cauchy-Schwarz Fairness Regularizer",
            "authors": [
                "Yezi Liu",
                "Hanning Chen",
                "Wenjun Huang",
                "Yang Ni",
                "Mohsen Imani"
            ],
            "arxiv_id": "2512.09467v1",
            "summary": "Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.",
            "headline_zh": "提出柯西-施瓦茨公平正则器以提升机器学习中的群体公平性",
            "intro_zh": [
                "现有公平正则器基于异构距离度量，导致行为难以解释且性能不一致",
                "基于柯西-施瓦茨散度设计正则器，具有紧致泛化界和尺度鲁棒性",
                "在多个基准数据集上实验，改善公平指标并保持准确度，实现更稳定的效用-公平权衡"
            ],
            "tags_zh": [
                "群体公平",
                "正则化方法",
                "柯西-施瓦茨散度",
                "机器学习公平性",
                "敏感属性处理"
            ],
            "_index": 132
        },
        {
            "title": "Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing",
            "authors": [
                "Sander De Coninck",
                "Emilio Gamba",
                "Bart Van Doninck",
                "Abdellatif Bey-Temsamani",
                "Sam Leroux",
                "Pieter Simoens"
            ],
            "arxiv_id": "2512.09463v1",
            "summary": "The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.",
            "headline_zh": "提出隐私保护计算机视觉框架，在工业场景中平衡操作效用与工人隐私",
            "intro_zh": [
                "核心问题：工业AI视觉应用需兼顾操作效用与工人隐私，制约实际部署",
                "方法要点：采用学习型视觉变换，模糊敏感或任务无关信息，保留任务关键特征",
                "实验或效果：在木工生产监控等三个用例中验证，量化隐私-效用权衡，获得工业伙伴正面反馈"
            ],
            "tags_zh": [
                "隐私保护计算机视觉",
                "工业应用",
                "人本制造",
                "视觉变换",
                "隐私-效用权衡",
                "多摄像头评估"
            ],
            "_index": 133
        },
        {
            "title": "Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing",
            "authors": [
                "Jayant Unde",
                "Takumi Inden",
                "Yuki Wakayama",
                "Jacinto Colan",
                "Yaonan Zhu",
                "Tadayoshi Aoyama",
                "Yasuhisa Hasegawa"
            ],
            "arxiv_id": "2512.09462v1",
            "summary": "In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.",
            "headline_zh": "提出一种柔顺夹持器，用于机器人辅助老年人或偏瘫患者穿脱裤子。",
            "intro_zh": [
                "核心问题：老龄化社会下，老年人或偏瘫患者穿脱裤子困难，需安全、柔顺的机器人辅助。",
                "方法要点：设计夹持器平衡柔顺性与抓握力，集成到定制机器人系统，实现精确操作与安全交互。",
                "实验或效果：实验评估显示，在受限空间中成功辅助穿脱裤子，成功率较高，优于现有研究。"
            ],
            "tags_zh": [
                "辅助机器人",
                "柔顺夹持器",
                "穿脱裤子",
                "老龄化社会",
                "机器人辅助",
                "安全交互"
            ],
            "_index": 134
        },
        {
            "title": "Cytoplasmic Strings Analysis in Human Embryo Time-Lapse Videos using Deep Learning Framework",
            "authors": [
                "Anabia Sohail",
                "Mohamad Alansari",
                "Ahmed Abughali",
                "Asmaa Chehab",
                "Abdelfatah Ahmed",
                "Divya Velayudhan",
                "Sajid Javed",
                "Hasan Al Marzouqi",
                "Ameena Saad Al-Sumaiti",
                "Junaid Kashir",
                "Naoufel Werghi"
            ],
            "arxiv_id": "2512.09461v1",
            "summary": "Infertility is a major global health issue, and while in-vitro fertilization has improved treatment outcomes, embryo selection remains a critical bottleneck. Time-lapse imaging enables continuous, non-invasive monitoring of embryo development, yet most automated assessment methods rely solely on conventional morphokinetic features and overlook emerging biomarkers. Cytoplasmic Strings, thin filamentous structures connecting the inner cell mass and trophectoderm in expanded blastocysts, have been associated with faster blastocyst formation, higher blastocyst grades, and improved viability. However, CS assessment currently depends on manual visual inspection, which is labor-intensive, subjective, and severely affected by detection and subtle visual appearance. In this work, we present, to the best of our knowledge, the first computational framework for CS analysis in human IVF embryos. We first design a human-in-the-loop annotation pipeline to curate a biologically validated CS dataset from TLI videos, comprising 13,568 frames with highly sparse CS-positive instances. Building on this dataset, we propose a two-stage deep learning framework that (i) classifies CS presence at the frame level and (ii) localizes CS regions in positive cases. To address severe imbalance and feature uncertainty, we introduce the Novel Uncertainty-aware Contractive Embedding (NUCE) loss, which couples confidence-aware reweighting with an embedding contraction term to form compact, well-separated class clusters. NUCE consistently improves F1-score across five transformer backbones, while RF-DETR-based localization achieves state-of-the-art (SOTA) detection performance for thin, low-contrast CS structures. The source code will be made publicly available at: https://github.com/HamadYA/CS_Detection.",
            "headline_zh": "提出基于深度学习的细胞质丝分析框架，用于人类胚胎延时视频中的自动化评估。",
            "intro_zh": [
                "核心问题：胚胎选择依赖手动检查细胞质丝，存在主观性和效率低下的瓶颈。",
                "方法要点：设计两阶段深度学习框架，结合NUCE损失处理数据不平衡和特征不确定性。",
                "实验或效果：在稀疏阳性数据上实现SOTA检测性能，提升F1分数并开源代码。"
            ],
            "tags_zh": [
                "胚胎选择",
                "细胞质丝检测",
                "深度学习框架",
                "不确定性损失",
                "延时视频分析",
                "医学图像处理"
            ],
            "_index": 135
        },
        {
            "title": "Architectures for Building Agentic AI",
            "authors": [
                "Sławomir Nowaczyk"
            ],
            "arxiv_id": "2512.09458v1",
            "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",
            "headline_zh": "提出基于架构的代理AI可靠性设计，通过组件化与接口规范提升系统稳定性。",
            "intro_zh": [
                "核心问题：代理AI的可靠性主要依赖于系统架构设计，而非单一算法。",
                "方法要点：定义代理系统组件（如目标管理器、规划器、工具路由器）和接口规范（如模式约束、权限控制）。",
                "实验或效果：分析不同代理模式（如工具使用、记忆增强）对可靠性和故障模式的影响，提炼设计指南。"
            ],
            "tags_zh": [
                "代理AI架构",
                "系统可靠性",
                "组件化设计",
                "接口规范",
                "控制循环",
                "设计指南"
            ],
            "_index": 136
        },
        {
            "title": "Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments",
            "authors": [
                "Jaehyun Kim",
                "Seungwon Choi",
                "Tae-Wan Kim"
            ],
            "arxiv_id": "2512.09447v1",
            "summary": "We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.",
            "headline_zh": "提出基于截断序贯概率比检验的LiDAR闭环验证方法，以抑制重复环境中的误检。",
            "intro_zh": [
                "核心问题：LiDAR闭环在结构重复室内环境中易产生误检，现有方法依赖单帧描述符比较或固定阈值。",
                "方法要点：采用多帧描述符相似性流，通过序贯概率比检验自适应决策，优先控制错误率。",
                "实验或效果：在五序列数据集上评估，相比基线方法，该方法提升精度并减少闭环混淆影响。"
            ],
            "tags_zh": [
                "LiDAR闭环检测",
                "序贯概率比检验",
                "重复环境",
                "多帧验证",
                "描述符无关"
            ],
            "_index": 137
        },
        {
            "title": "Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation",
            "authors": [
                "Nadeem Nazer",
                "Hongkuan Zhou",
                "Lavdim Halilaj",
                "Ylli Sadikaj",
                "Steffen Staab"
            ],
            "arxiv_id": "2512.09446v1",
            "summary": "Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like \"hole\", \"cut\", \"scratch\" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of \"abnormal\" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.",
            "headline_zh": "提出DAPO方法，通过渐进调优优化缺陷感知提示，以解决零样本多类型异常检测与分割中的细粒度缺陷识别问题。",
            "intro_zh": [
                "核心问题：现有视觉语言模型在异常检测中忽视细粒度缺陷类型，导致无法提供具体异常洞察。",
                "方法要点：DAPO结合固定文本锚点和可学习令牌嵌入，学习混合缺陷感知提示，对齐图像特征与文本语义。",
                "实验或效果：在公开基准和内部数据集上，DAPO在分布偏移下图像级AUROC和平均精度平均提升3.7%，零样本设置下新异常类型定位平均提升6.5%。"
            ],
            "tags_zh": [
                "零样本异常检测",
                "缺陷感知提示优化",
                "渐进调优",
                "多类型异常分割",
                "视觉语言模型",
                "分布偏移"
            ],
            "_index": 138
        },
        {
            "title": "Advancing Research via Human-AI Interactive Theorem Proving",
            "authors": [
                "Chenyi Li",
                "Zhijian Lai",
                "Dong An",
                "Jiang Hu",
                "Zaiwen Wen"
            ],
            "arxiv_id": "2512.09443v1",
            "summary": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.",
            "headline_zh": "提出人机交互定理证明工作流，将大语言模型融入数学研究以加速探索并保持严谨性。",
            "intro_zh": [
                "核心问题：如何利用大语言模型作为科学计算研究工具，同时保持数学严谨性。",
                "方法要点：设计人机交互工作流，人类专家控制问题与假设，模型辅助证明搜索、性质提议和结构构建。",
                "实验或效果：在流形优化与Grover量子搜索算法案例中，识别不变子空间、探索兼容收缩并保证收敛性。"
            ],
            "tags_zh": [
                "人机交互定理证明",
                "大语言模型研究工具",
                "流形优化",
                "量子计算",
                "数学严谨性",
                "工作流框架"
            ],
            "_index": 139
        },
        {
            "title": "Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model",
            "authors": [
                "Jiantao Tan",
                "Peixian Ma",
                "Tong Yu",
                "Wentao Zhang",
                "Ruixuan Wang"
            ],
            "arxiv_id": "2512.09441v1",
            "summary": "Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.",
            "headline_zh": "提出基于视觉语言模型的类增量学习框架，通过表示校准和不确定性指导缓解跨任务类别混淆",
            "intro_zh": [
                "核心问题：类增量学习中跨任务类别混淆，现有视觉语言模型方法难以区分不同任务学习的类别",
                "方法要点：使用任务特定适配器学习新知识，基于轻量投影器混合的跨任务表示校准策略统一特征空间，开发不确定性指导的推理策略选择最佳图像特征",
                "实验或效果：在多个数据集和设置下进行广泛实验，相比现有方法展示出优越性能"
            ],
            "tags_zh": [
                "类增量学习",
                "视觉语言模型",
                "表示校准",
                "不确定性指导",
                "图像分类",
                "跨任务学习"
            ],
            "_index": 140
        },
        {
            "title": "UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents",
            "authors": [
                "Xufan He",
                "Yushuang Wu",
                "Xiaoyang Guo",
                "Chongjie Ye",
                "Jiaqing Zhou",
                "Tianlei Hu",
                "Xiaoguang Han",
                "Dong Du"
            ],
            "arxiv_id": "2512.09435v1",
            "summary": "Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.",
            "headline_zh": "提出UniPart框架，通过统一几何-分割潜在表示实现图像引导的部分级3D生成",
            "intro_zh": [
                "核心问题：现有部分级3D生成方法依赖隐式分割或外部分割器，控制粒度和数据需求受限",
                "方法要点：引入Geom-Seg VecSet统一表示，结合两阶段潜在扩散框架，在全局和规范空间预测部分潜在",
                "实验或效果：实验显示UniPart在分割可控性和部分级几何质量上优于现有方法"
            ],
            "tags_zh": [
                "部分级3D生成",
                "几何-分割潜在表示",
                "潜在扩散框架",
                "图像引导生成",
                "双空间生成"
            ],
            "_index": 141
        },
        {
            "title": "CourtPressGER: A German Court Decision to Press Release Summarization Dataset",
            "authors": [
                "Sebastian Nagl",
                "Mohamed Elganayni",
                "Melanie Pospisil",
                "Matthias Grabmair"
            ],
            "arxiv_id": "2512.09434v1",
            "summary": "Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.",
            "headline_zh": "提出CourtPressGER数据集以解决德国法院判决到新闻稿的摘要生成问题",
            "intro_zh": [
                "核心问题：现有NLP研究忽视面向公众的法院新闻稿摘要需求，缺乏相关数据集",
                "方法要点：构建包含判决、人工新闻稿和合成提示的三元组数据集，用于训练和评估LLM",
                "实验或效果：通过多指标评估，大模型生成高质量草案，小模型需分层处理长文本"
            ],
            "tags_zh": [
                "法律文本摘要",
                "数据集构建",
                "大语言模型评估",
                "事实一致性检查",
                "德国法院文档"
            ],
            "_index": 142
        },
        {
            "title": "A Hierarchical, Model-Based System for High-Performance Humanoid Soccer",
            "authors": [
                "Quanyou Wang",
                "Mingzhang Zhu",
                "Ruochen Hou",
                "Kay Gillespie",
                "Alvin Zhu",
                "Shiqi Wang",
                "Yicheng Wang",
                "Gaberiel I. Fernandez",
                "Yeting Liu",
                "Colin Togashi",
                "Hyunwoo Nam",
                "Aditya Navghare",
                "Alex Xu",
                "Taoyuanmin Zhu",
                "Min Sung Ahn",
                "Arturo Flores Alvarez",
                "Justin Quan",
                "Ethan Hong",
                "Dennis W. Hong"
            ],
            "arxiv_id": "2512.09431v1",
            "summary": "The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.",
            "headline_zh": "提出硬件与软件创新系统，助力ARTEMIS赢得2024年RoboCup成人尺寸人形足球赛冠军。",
            "intro_zh": [
                "核心问题：在动态对抗的RoboCup比赛中实现高性能人形足球机器人，需兼顾硬件轻量化、高扭矩驱动和软件感知定位集成。",
                "方法要点：硬件采用轻质结构、准直驱执行器和专用足部设计；软件结合立体视觉、目标检测与地标融合，集成导航与行为管理。",
                "实验或效果：系统在真实比赛中展现快速、精确和战术有效的表现，支持ARTEMIS夺冠，验证了整体设计的鲁棒性。"
            ],
            "tags_zh": [
                "人形机器人",
                "RoboCup足球",
                "硬件设计",
                "软件系统",
                "感知定位",
                "行为管理"
            ],
            "_index": 143
        },
        {
            "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators",
            "authors": [
                "Guoqiang Zou",
                "Wanyu Wang",
                "Hao Zheng",
                "Longxiang Yin",
                "Yinhe Han"
            ],
            "arxiv_id": "2512.09427v1",
            "summary": "Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.",
            "headline_zh": "提出ODMA框架以解决随机访问受限加速器上LLM服务的内存分配问题",
            "intro_zh": [
                "核心问题：现有内存管理器在随机访问带宽差的加速器上导致内存浪费或性能低下",
                "方法要点：结合轻量级长度预测器、动态桶分区和大桶保护机制，优化内存分配",
                "实验或效果：在Cambricon MLU370-X4上，内存利用率从55.05%提升至72.45%，RPS和TPS提高约30%"
            ],
            "tags_zh": [
                "大语言模型服务",
                "内存管理",
                "随机访问受限加速器",
                "动态分配",
                "性能优化"
            ],
            "_index": 144
        },
        {
            "title": "FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds",
            "authors": [
                "Marco Pegoraro",
                "Evan Atherton",
                "Bruno Roy",
                "Aliasghar Khani",
                "Arianna Rampini"
            ],
            "arxiv_id": "2512.09423v1",
            "summary": "Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.",
            "headline_zh": "提出FunPhase周期性功能自编码器，通过相位流形实现运动生成，解决时空耦合与可扩展性问题。",
            "intro_zh": [
                "核心问题：自然身体运动学习因空间几何与时间动态强耦合而具挑战性，现有相位流形方法缺乏可扩展性。",
                "方法要点：引入功能周期性自编码器，学习运动相位流形，用函数空间解码替代离散解码，支持任意时间分辨率采样。",
                "实验或效果：在重建误差上优于先前周期性自编码器基线，泛化于不同骨架和数据集，性能与先进运动生成方法相当。"
            ],
            "tags_zh": [
                "运动生成",
                "相位流形",
                "功能自编码器",
                "时空耦合",
                "超分辨率",
                "运动补全"
            ],
            "_index": 145
        },
        {
            "title": "InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography",
            "authors": [
                "Zhe Li",
                "Hadrien Reynaud",
                "Alberto Gomez",
                "Bernhard Kainz"
            ],
            "arxiv_id": "2512.09422v1",
            "summary": "Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \\(69.38\\%\\) using only \\(25\\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.",
            "headline_zh": "提出基于图的方法InfoMotion，用于超声心动图视频数据集蒸馏以提升效率。",
            "intro_zh": [
                "核心问题：超声心动图视频数据规模大，存储、计算和模型训练效率面临挑战。",
                "方法要点：提取运动特征捕获时序动态，构建类内图并使用Infomap算法选择代表性样本。",
                "实验或效果：在EchoNet-Dynamic数据集上，仅用25个合成视频实现69.38%的测试准确率。"
            ],
            "tags_zh": [
                "视频数据集蒸馏",
                "超声心动图",
                "运动特征提取",
                "图算法",
                "医疗视频分析"
            ],
            "_index": 146
        },
        {
            "title": "Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis",
            "authors": [
                "Zhe Li",
                "Hadrien Reynaud",
                "Johanna P Müller",
                "Bernhard Kainz"
            ],
            "arxiv_id": "2512.09418v1",
            "summary": "Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.",
            "headline_zh": "提出无标签运动条件扩散模型，基于自监督运动特征合成心脏超声视频",
            "intro_zh": [
                "核心问题：心脏超声标注数据稀缺，阻碍深度学习应用",
                "方法要点：设计运动与外观特征提取器，结合辅助损失增强特征学习",
                "实验或效果：在EchoNet-Dynamic数据集上生成时序连贯、临床真实的视频序列"
            ],
            "tags_zh": [
                "心脏超声合成",
                "无标签生成",
                "运动条件扩散模型",
                "自监督特征学习",
                "视频生成"
            ],
            "_index": 147
        },
        {
            "title": "DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping",
            "authors": [
                "Yanan Wang",
                "Shengcai Liao",
                "Panwen Hu",
                "Xin Li",
                "Fan Yang",
                "Xiaodan Liang"
            ],
            "arxiv_id": "2512.09417v1",
            "summary": "Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\\TrainNum{} videos) and benchmarking (\\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.",
            "headline_zh": "提出DirectSwap框架和HeadSwapBench数据集，以解决视频头部交换中身份泄漏和运动表情一致性问题。",
            "intro_zh": [
                "核心问题：现有方法依赖同人跨帧训练和掩码修复，导致身份泄漏、边界伪影及遮挡信息恢复困难。",
                "方法要点：构建跨身份配对数据集HeadSwapBench，并设计无掩码直接交换框架DirectSwap，结合运动模块和MEAR损失增强一致性。",
                "实验或效果：在多样真实视频场景中实现最先进的视觉质量、身份保真度及运动表情一致性。"
            ],
            "tags_zh": [
                "视频头部交换",
                "跨身份训练",
                "配对数据集",
                "扩散模型",
                "运动表情一致性",
                "无掩码修复"
            ],
            "_index": 148
        },
        {
            "title": "D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM",
            "authors": [
                "Siting Zhu",
                "Yuxiang Huang",
                "Wenhua Wu",
                "Chaokang Jiang",
                "Yongbo Chen",
                "I-Ming Chen",
                "Hesheng Wang"
            ],
            "arxiv_id": "2512.09411v1",
            "summary": "Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.",
            "headline_zh": "提出D²GSLAM系统，利用高斯表示在动态环境中同时实现准确动态重建与稳健跟踪。",
            "intro_zh": [
                "核心问题：动态环境中密集SLAM挑战，现有方法忽略动态物体运动信息。",
                "方法要点：结合几何提示动态分离、动静复合表示、渐进姿态优化和运动一致性损失。",
                "实验或效果：在动态场景中展示优越的建图和跟踪精度，支持准确动态建模。"
            ],
            "tags_zh": [
                "动态SLAM",
                "高斯表示",
                "动静复合建模",
                "几何一致性",
                "相机跟踪",
                "动态重建"
            ],
            "_index": 149
        },
        {
            "title": "Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation",
            "authors": [
                "Jialin Ying",
                "Zhihao Li",
                "Zicheng Dong",
                "Guohua Wu",
                "Yihuan Liao"
            ],
            "arxiv_id": "2512.09410v1",
            "summary": "Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.",
            "headline_zh": "提出PGF-MAPPO框架，通过路径引导和方向性前沿分配解决杂乱环境中协作搜索与捕获的稀疏奖励问题。",
            "intro_zh": [
                "核心问题：杂乱环境中协作追逃面临稀疏奖励和受限视野，导致标准多智能体强化学习探索效率低且难以扩展。",
                "方法要点：结合A*势场进行密集奖励塑形，并引入方向性前沿分配以强制空间分散，加速覆盖。",
                "实验或效果：在10x10地图训练的策略能零样本泛化到20x20环境，捕获效率优于基线方法。"
            ],
            "tags_zh": [
                "多智能体强化学习",
                "协作搜索与捕获",
                "稀疏奖励塑形",
                "零样本泛化",
                "机器人集群控制"
            ],
            "_index": 150
        },
        {
            "title": "Generative Point Cloud Registration",
            "authors": [
                "Haobo Jiang",
                "Jin Xie",
                "Jian Yang",
                "Liang Yu",
                "Jianmin Zheng"
            ],
            "arxiv_id": "2512.09407v1",
            "summary": "In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.",
            "headline_zh": "提出生成式点云配准范式，通过生成跨视图一致图像对增强3D配准性能。",
            "intro_zh": [
                "核心问题：传统3D配准方法可能受限于几何特征，缺乏纹理信息融合。",
                "方法要点：引入Match-ControlNet，利用ControlNet生成几何对齐图像，并通过耦合条件去噪和提示引导确保纹理一致性。",
                "实验或效果：在3DMatch和ScanNet数据集上验证有效性，可集成到多种配准方法提升性能。"
            ],
            "tags_zh": [
                "点云配准",
                "生成模型",
                "几何一致性",
                "纹理一致性",
                "ControlNet",
                "3D匹配"
            ],
            "_index": 151
        },
        {
            "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
            "authors": [
                "Hai Ci",
                "Xiaokang Liu",
                "Pei Yang",
                "Yiren Song",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.09406v1",
            "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
            "headline_zh": "提出H2R-Grounder框架，将人类交互视频转换为物理真实的机器人视频，无需配对数据训练。",
            "intro_zh": [
                "核心问题：从人类视频学习机器人操作技能，但缺乏配对数据导致训练困难。",
                "方法要点：通过可转移表示（如修复背景和叠加视觉提示），利用视频扩散模型生成运动一致的机器人视频。",
                "实验或效果：相比基线，生成更真实和物理基础的机器人动作，验证了从无标签人类视频扩展机器人学习的潜力。"
            ],
            "tags_zh": [
                "视频到视频翻译",
                "机器人学习",
                "物理基础交互",
                "无配对数据训练",
                "视频扩散模型",
                "人类-机器人交互"
            ],
            "_index": 152
        },
        {
            "title": "Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs",
            "authors": [
                "Sohely Jahan",
                "Ruimin Sun"
            ],
            "arxiv_id": "2512.09403v1",
            "summary": "As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored.\n  We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments.\n  Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.",
            "headline_zh": "提出黑盒行为蒸馏攻击，揭示医疗大语言模型安全对齐的脆弱性。",
            "intro_zh": [
                "核心问题：医疗大语言模型安全对齐在仅输出访问下易被攻击，导致功能保留但安全机制失效。",
                "方法要点：通过指令查询和零对齐监督，低成本蒸馏LLaMA3 8B代理模型，无需模型权重或安全过滤器。",
                "实验或效果：代理模型在86%对抗提示下产生不安全输出，远超原模型和基础模型，暴露功能-伦理差距。"
            ],
            "tags_zh": [
                "医疗大语言模型",
                "安全对齐",
                "黑盒蒸馏",
                "对抗攻击",
                "模型提取",
                "零对齐监督"
            ],
            "_index": 153
        },
        {
            "title": "Wasserstein-Aligned Hyperbolic Multi-View Clustering",
            "authors": [
                "Rui Wang",
                "Yuting Jiang",
                "Xiaoqing Luo",
                "Xiao-Jun Wu",
                "Nicu Sebe",
                "Ziheng Chen"
            ],
            "arxiv_id": "2512.09402v1",
            "summary": "Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.",
            "headline_zh": "提出Wasserstein对齐双曲多视图聚类框架以解决多视图数据全局语义一致性问题",
            "intro_zh": [
                "多视图聚类中现有双曲表示方法忽视全局语义一致性，易受视图特定信息干扰",
                "利用视图特定双曲编码器嵌入Lorentz流形，引入双曲切片Wasserstein距离对齐跨视图分布",
                "在多个基准数据集上实现最先进的聚类性能，验证了方法的有效性"
            ],
            "tags_zh": [
                "多视图聚类",
                "双曲表示",
                "Wasserstein距离",
                "语义对齐",
                "Lorentz流形",
                "全局一致性"
            ],
            "_index": 154
        },
        {
            "title": "Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting",
            "authors": [
                "Hongjun Wang",
                "Jiawei Yong",
                "Jiawei Wang",
                "Shintaro Fukushima",
                "Renhe Jiang"
            ],
            "arxiv_id": "2512.09398v1",
            "summary": "Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.",
            "headline_zh": "提出ConFormer框架，结合事故与法规数据以提升交通预测准确性",
            "intro_zh": [
                "核心问题：交通预测受事故和法规等外部因素影响，现有模型因数据整合不足而受限",
                "方法要点：引入条件Transformer，通过图传播和引导归一化层动态调整时空节点关系",
                "实验或效果：在东京和加州数据集上超越STAEFormer，预测性能更优且计算成本更低"
            ],
            "tags_zh": [
                "交通预测",
                "条件Transformer",
                "时空数据挖掘",
                "事故数据集成",
                "图传播"
            ],
            "_index": 155
        },
        {
            "title": "GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection",
            "authors": [
                "Zishu Wei",
                "Qixiang Ma",
                "Xavier Hu",
                "Yuhang Liu",
                "Hui Zang",
                "Yudong Zhao",
                "Tao Wang",
                "Shengyu Zhang",
                "Fei Wu"
            ],
            "arxiv_id": "2512.09396v1",
            "summary": "Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.",
            "headline_zh": "提出GAIR框架，通过信息联合推理与群体反思提升GUI自动化任务性能",
            "intro_zh": [
                "核心问题：GUI自动化任务多样，需异构模型能力，构建高性能系统困难",
                "方法要点：引入通用MLLM联合处理多GUI专用模型信息，决策时触发群体反思优化信息收集",
                "实验或效果：在GUI基准测试中验证了框架的有效性和可靠性"
            ],
            "tags_zh": [
                "GUI自动化",
                "多模态大语言模型",
                "异构模型集成",
                "信息联合推理",
                "群体反思"
            ],
            "_index": 156
        },
        {
            "title": "Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography",
            "authors": [
                "Vasiliki Stoumpou",
                "Rohan Kumar",
                "Bernard Burman",
                "Diego Ojeda",
                "Tapan Mehta",
                "Dimitris Bertsimas"
            ],
            "arxiv_id": "2512.09393v1",
            "summary": "Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.\n  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.\n  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.\n  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.",
            "headline_zh": "提出多模态深度学习框架，集成临床与影像数据，用于CT中硬膜下血肿的检测与定位。",
            "intro_zh": [
                "核心问题：硬膜下血肿是神经外科急症，现有自动化工具检测性能有限且缺乏可解释性。",
                "方法要点：结合临床变量、3D卷积网络和Transformer增强的2D分割模型，采用贪婪集成策略。",
                "实验或效果：多模态集成在25,315个CT研究中达到AUC 0.9407，提供解剖学定位图。"
            ],
            "tags_zh": [
                "硬膜下血肿检测",
                "多模态深度学习",
                "CT影像分析",
                "3D卷积神经网络",
                "Transformer分割",
                "临床决策支持"
            ],
            "_index": 157
        },
        {
            "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing",
            "authors": [
                "Peter Baile Chen",
                "Weiyue Li",
                "Dan Roth",
                "Michael Cafarella",
                "Samuel Madden",
                "Jacob Andreas"
            ],
            "arxiv_id": "2512.09386v1",
            "summary": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.",
            "headline_zh": "提出CONCUR框架以解决持续路由中策略扩展和表示不足的问题",
            "intro_zh": [
                "核心问题：现有路由方法需全模型重训练以适应新策略，且单输入表示限制路由决策优化",
                "方法要点：采用模块化设计，为每个策略训练独立预测器，并利用任务和策略的多重表示",
                "实验或效果：在分布内外任务上优于最佳单策略和现有路由技术，提高准确性并降低训练和推理成本"
            ],
            "tags_zh": [
                "持续路由",
                "模块化预测器",
                "多重表示",
                "约束路由",
                "推理优化",
                "任务分配"
            ],
            "_index": 158
        },
        {
            "title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks",
            "authors": [
                "Uisang Lee",
                "Changhoon Chung",
                "Junmo Lee",
                "Soo-Mook Moon"
            ],
            "arxiv_id": "2512.09385v1",
            "summary": "The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.",
            "headline_zh": "提出BugSweeper，基于图神经网络在函数级别检测智能合约漏洞，无需手动规则。",
            "intro_zh": [
                "核心问题：现有方法依赖专家设计的规则预处理，可能忽略关键上下文，限制对新威胁的适应性。",
                "方法要点：将Solidity函数表示为函数级抽象语法图，结合两阶段图神经网络进行噪声过滤和高层推理。",
                "实验或效果：在真实合约上实验显示，BugSweeper显著优于所有最先进的检测方法。"
            ],
            "tags_zh": [
                "智能合约安全",
                "图神经网络",
                "漏洞检测",
                "函数级分析",
                "抽象语法图"
            ],
            "_index": 159
        },
        {
            "title": "Perception-Inspired Color Space Design for Photo White Balance Editing",
            "authors": [
                "Yang Cheng",
                "Ziteng Cui",
                "Lin Gu",
                "Shenghan Su",
                "Zenghui Zhang"
            ],
            "arxiv_id": "2512.09383v1",
            "summary": "White balance (WB) is a key step in the image signal processor (ISP) pipeline that mitigates color casts caused by varying illumination and restores the scene's true colors. Currently, sRGB-based WB editing for post-ISP WB correction is widely used to address color constancy failures in the ISP pipeline when the original camera RAW is unavailable. However, additive color models (e.g., sRGB) are inherently limited by fixed nonlinear transformations and entangled color channels, which often impede their generalization to complex lighting conditions.\n  To address these challenges, we propose a novel framework for WB correction that leverages a perception-inspired Learnable HSI (LHSI) color space. Built upon a cylindrical color model that naturally separates luminance from chromatic components, our framework further introduces dedicated parameters to enhance this disentanglement and learnable mapping to adaptively refine the flexibility. Moreover, a new Mamba-based network is introduced, which is tailored to the characteristics of the proposed LHSI color space.\n  Experimental results on benchmark datasets demonstrate the superiority of our method, highlighting the potential of perception-inspired color space design in computational photography. The source code is available at https://github.com/YangCheng58/WB_Color_Space.",
            "headline_zh": "提出基于感知启发的可学习HSI颜色空间框架，以解决sRGB白平衡编辑在复杂光照下的泛化限制。",
            "intro_zh": [
                "核心问题：sRGB颜色模型因固定非线性变换和颜色通道纠缠，在白平衡编辑中难以泛化到复杂光照条件。",
                "方法要点：设计圆柱形颜色模型分离亮度与色度，引入可学习参数增强解耦，并采用Mamba网络适配该颜色空间。",
                "实验或效果：在基准数据集上验证了方法的优越性，展示了感知启发颜色空间设计在计算摄影中的潜力。"
            ],
            "tags_zh": [
                "白平衡校正",
                "颜色空间设计",
                "计算摄影",
                "可学习模型",
                "Mamba网络",
                "图像处理"
            ],
            "_index": 160
        },
        {
            "title": "Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM",
            "authors": [
                "Xun Li",
                "Qiong Wu",
                "Pingyi Fan",
                "Kezhi Wang",
                "Wen Chen",
                "Khaled B. Letaief"
            ],
            "arxiv_id": "2512.09378v1",
            "summary": "Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.",
            "headline_zh": "提出基于轻量DDPM的联邦蒸馏辅助车辆边缘缓存方案，以降低通信开销并提升缓存命中率。",
            "intro_zh": [
                "核心问题：传统联邦学习在车辆边缘缓存中通信开销大且易因车辆移动导致训练失败。",
                "方法要点：结合联邦蒸馏与轻量去噪扩散概率模型，保护隐私并减少模型传输。",
                "实验或效果：仿真显示方案对车速变化鲁棒，显著降低通信开销并提高缓存命中百分比。"
            ],
            "tags_zh": [
                "车辆边缘缓存",
                "联邦蒸馏",
                "轻量去噪扩散概率模型",
                "通信开销优化",
                "隐私保护"
            ],
            "_index": 161
        },
        {
            "title": "Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances",
            "authors": [
                "Lidan Xu",
                "Dadong Fan",
                "Junhong Wang",
                "Wenshuo Li",
                "Hao Lu",
                "Jianzhong Qiao"
            ],
            "arxiv_id": "2512.09377v1",
            "summary": "Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\\mathbb{R}^3)^2\\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.",
            "headline_zh": "提出基于可观测性分析与复合扰动滤波的双无人机吊挂系统状态估计方法",
            "intro_zh": [
                "针对双无人机吊挂系统在多源扰动下仅依赖无人机里程计信息时负载姿态的可观测性问题，通过可观测性秩准则证明系统在不超过两类集总扰动下完全可观测",
                "设计基于扰动观测器的误差状态扩展卡尔曼滤波器，在流形上实现系统状态与扰动的联合估计，减少传感器依赖",
                "通过仿真与实验验证了仅使用无人机里程计信息即可完全估计系统状态与扰动的可行性"
            ],
            "tags_zh": [
                "无人机协同运输",
                "可观测性分析",
                "扰动滤波",
                "扩展卡尔曼滤波",
                "状态估计"
            ],
            "_index": 162
        },
        {
            "title": "Rates and architectures for learning geometrically non-trivial operators",
            "authors": [
                "T. Mitchell Roddenberry",
                "Leo Tzou",
                "Ivan Dokmanić",
                "Maarten V. de Hoop",
                "Richard G. Baraniuk"
            ],
            "arxiv_id": "2512.09376v1",
            "summary": "Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.",
            "headline_zh": "提出学习几何非平凡算子的理论与架构，实现超代数误差衰减与数据高效性。",
            "intro_zh": [
                "扩展学习理论至双纤维变换，涵盖广义Radon和测地线射线变换等几何积分算子。",
                "证明该类算子无维度诅咒，误差随训练样本数倒数衰减快于任意固定幂次。",
                "设计基于水平集方法的交叉注意力架构，实现通用、稳定且数据高效的学习。"
            ],
            "tags_zh": [
                "算子学习",
                "几何积分算子",
                "双纤维变换",
                "科学机器学习",
                "数据高效性",
                "交叉注意力架构"
            ],
            "_index": 163
        },
        {
            "title": "Log NeRF: Comparing Spaces for Learning Radiance Fields",
            "authors": [
                "Sihe Chen",
                "Luv Verma",
                "Bruce A. Maxwell"
            ],
            "arxiv_id": "2512.09375v1",
            "summary": "Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the BiIlluminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log RGB space enables NeRF to learn a more compact and effective representation of scene appearance. To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations linear, sRGB, GPLog, and log RGB by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log space advantage.",
            "headline_zh": "提出Log NeRF，通过log RGB空间学习辐射场以提升渲染质量与鲁棒性。",
            "intro_zh": [
                "核心问题：NeRF在sRGB空间学习辐射场，未考虑颜色空间对表示学习的影响。",
                "方法要点：基于BIDR模型，假设log RGB空间能简化光照与反射分离，训练NeRF在不同颜色空间进行比较。",
                "实验或效果：log RGB空间在渲染质量、场景鲁棒性和低光条件下表现更优，且适用于不同网络变体。"
            ],
            "tags_zh": [
                "神经辐射场",
                "颜色空间",
                "对数变换",
                "渲染质量",
                "低光条件",
                "BIDR模型"
            ],
            "_index": 164
        },
        {
            "title": "FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement",
            "authors": [
                "Haobo Jiang",
                "Jin Xie",
                "Jian Yang",
                "Liang Yu",
                "Jianmin Zheng"
            ],
            "arxiv_id": "2512.09373v1",
            "summary": "Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.",
            "headline_zh": "提出FUSER，首个前馈多视角点云配准Transformer，直接预测全局位姿，无需成对匹配。",
            "intro_zh": [
                "多视角点云配准依赖成对匹配构建位姿图，计算昂贵且缺乏整体几何约束。",
                "FUSER通过稀疏3D CNN编码超点特征，使用几何交替注意力模块进行高效推理，并引入SE(3)^N扩散框架FUSER-DF进行精炼。",
                "在3DMatch、ScanNet和ArkitScenes数据集上验证了优越的配准精度和计算效率。"
            ],
            "tags_zh": [
                "多视角点云配准",
                "Transformer模型",
                "SE(3)扩散",
                "几何注意力",
                "前馈网络",
                "3D计算机视觉"
            ],
            "_index": 165
        },
        {
            "title": "Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs",
            "authors": [
                "Yezi Liu",
                "William Youngwoo Chung",
                "Hanning Chen",
                "Calvin Yeung",
                "Mohsen Imani"
            ],
            "arxiv_id": "2512.09369v1",
            "summary": "Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\\%$ and GPU memory by $3-5\\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.",
            "headline_zh": "提出PathHD框架，通过超维计算和单次LLM调用实现高效知识图谱推理",
            "intro_zh": [
                "核心问题：现有知识图谱推理方法依赖重神经编码器或多轮LLM调用，导致高延迟、高成本和不透明决策",
                "方法要点：使用超维计算编码关系路径，基于块余弦相似度排序候选，单次LLM裁决生成答案和可解释路径",
                "实验或效果：在多个数据集上达到或超越基线性能，降低延迟40-60%，减少GPU内存3-5倍，提供忠实路径解释"
            ],
            "tags_zh": [
                "知识图谱推理",
                "超维计算",
                "大语言模型",
                "高效推理",
                "可解释性",
                "单次调用"
            ],
            "_index": 166
        },
        {
            "title": "CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning",
            "authors": [
                "Mingyuan Li",
                "Chunyu Liu",
                "Zhuojun Li",
                "Xiao Liu",
                "Guangsheng Yu",
                "Bo Du",
                "Jun Shen",
                "Qiang Wu"
            ],
            "arxiv_id": "2512.09368v1",
            "summary": "Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.",
            "headline_zh": "提出CFLight框架，通过反事实学习增强交通信号控制的安全性",
            "intro_zh": [
                "核心问题：强化学习在交通信号控制中常优先效率而忽视安全，且缺乏可解释性。",
                "方法要点：引入反事实学习模块，回溯不安全事件并预测替代行动结果，以平衡安全与效率。",
                "实验或效果：在真实和合成数据集上，CFLight减少碰撞并提升整体交通性能，优于传统方法。"
            ],
            "tags_zh": [
                "交通信号控制",
                "反事实学习",
                "强化学习",
                "安全优化",
                "因果分析",
                "碰撞减少"
            ],
            "_index": 167
        },
        {
            "title": "Meta-learning three-factor plasticity rules for structured credit assignment with sparse feedback",
            "authors": [
                "Dimitra Maoutsa"
            ],
            "arxiv_id": "2512.09366v1",
            "summary": "Biological neural networks learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via \\textbf{tangent-propagation through learning}. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.",
            "headline_zh": "提出元学习框架以发现用于稀疏反馈下循环网络结构化信用分配的局部可塑性规则",
            "intro_zh": [
                "核心问题：生物神经网络如何从稀疏延迟反馈中实现结构化信用分配，现有人工方法依赖非生物可信的全局规则。",
                "方法要点：通过元学习框架，结合局部类新赫布更新和基于切线传播的外循环优化，发现三因子可塑性规则。",
                "实验或效果：规则仅使用局部信息和延迟奖励支持长时程信用分配，为循环电路学习提供生物基础机制新见解。"
            ],
            "tags_zh": [
                "元学习",
                "结构化信用分配",
                "稀疏反馈",
                "局部可塑性规则",
                "循环神经网络",
                "生物可信学习"
            ],
            "_index": 168
        },
        {
            "title": "KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction",
            "authors": [
                "Jiayu Qin",
                "Zhengquan Luo",
                "Guy Tadmor",
                "Changyou Chen",
                "David Zeevi",
                "Zhiqiang Xu"
            ],
            "arxiv_id": "2512.09365v1",
            "summary": "Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.",
            "headline_zh": "提出KGOT框架，结合知识图谱与最优传输伪标签，以解决分子-蛋白质相互作用预测中的数据稀缺与多模态整合问题。",
            "intro_zh": [
                "核心问题：分子-蛋白质相互作用预测面临标注数据稀缺和现有方法忽略基因、通路等生物背景信息。",
                "方法要点：整合多源生物数据构建知识图谱，并基于最优传输生成高质量伪标签以利用未标注数据。",
                "实验或效果：在多个数据集上验证，预测准确性和零样本能力显著优于现有方法，提升药物发现应用。"
            ],
            "tags_zh": [
                "分子-蛋白质相互作用预测",
                "知识图谱",
                "最优传输",
                "伪标签生成",
                "多模态学习",
                "药物发现"
            ],
            "_index": 169
        },
        {
            "title": "ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation",
            "authors": [
                "Shengchao Zhou",
                "Jiehong Lin",
                "Jiahui Liu",
                "Shizhen Zhao",
                "Chirui Chang",
                "Xiaojuan Qi"
            ],
            "arxiv_id": "2512.09364v1",
            "summary": "Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.",
            "headline_zh": "提出ASSIST-3D以合成多样化场景数据，增强类无关3D实例分割的泛化能力",
            "intro_zh": [
                "核心问题：类无关3D实例分割因标注数据稀缺或噪声而泛化困难，现有合成方法难以兼顾几何多样性、上下文复杂性和布局合理性。",
                "方法要点：ASSIST-3D通过异构对象选择、LLM引导的布局生成和多视图点云构建，合成高质量训练数据。",
                "实验或效果：在ScanNetV2等基准测试中，使用ASSIST-3D数据训练的模型显著优于现有方法，验证了其有效性。"
            ],
            "tags_zh": [
                "3D实例分割",
                "场景合成",
                "数据增强",
                "类无关学习",
                "点云处理",
                "泛化能力"
            ],
            "_index": 170
        },
        {
            "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation",
            "authors": [
                "Ke Xing",
                "Longfei Li",
                "Yuyang Yin",
                "Hanwen Liang",
                "Guixun Luo",
                "Chen Fang",
                "Jue Wang",
                "Konstantinos N. Plataniotis",
                "Xiaojie Jin",
                "Yao Zhao",
                "Yunchao Wei"
            ],
            "arxiv_id": "2512.09363v1",
            "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.",
            "headline_zh": "提出StereoWorld框架，利用预训练视频生成器实现高质量单目到立体视频转换，解决XR设备立体视频制作成本高和伪影问题。",
            "intro_zh": [
                "核心问题：XR设备普及推动高质量立体视频需求，但现有制作方法成本高且易产生伪影。",
                "方法要点：基于预训练视频生成器，结合几何感知正则化确保3D结构保真，并集成时空分块方案实现高效高分辨率合成。",
                "实验或效果：构建大规模高清立体视频数据集，实验显示StereoWorld在视觉保真度和几何一致性上显著优于先前方法。"
            ],
            "tags_zh": [
                "立体视频生成",
                "单目到立体转换",
                "几何感知正则化",
                "时空分块",
                "XR设备应用",
                "视频生成框架"
            ],
            "_index": 171
        },
        {
            "title": "A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches",
            "authors": [
                "Boge Lyu",
                "Qianye Yin",
                "Iris Denise Tommelein",
                "Hanyang Liu",
                "Karnamohit Ranka",
                "Karthik Yeluripati",
                "Junzhe Shi"
            ],
            "arxiv_id": "2512.09360v1",
            "summary": "The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.",
            "headline_zh": "提出基于CSI MasterFormat的细粒度框架，结合计量与机器学习方法预测建筑材料价格，以提升成本估算可靠性。",
            "intro_zh": [
                "核心问题：建筑材料价格持续波动，对成本估算和项目交付构成显著风险，需细粒度预测方法。",
                "方法要点：利用CSI MasterFormat作为数据结构，整合原材料价格等解释变量，评估LSTM、ARIMA等四种时间序列模型。",
                "实验或效果：解释变量显著提升所有模型性能，LSTM表现最佳，RMSE低至1.390，MAPE为0.957，优于ARIMA达59%。"
            ],
            "tags_zh": [
                "建筑材料价格预测",
                "细粒度框架",
                "时间序列模型",
                "LSTM",
                "CSI MasterFormat",
                "成本估算"
            ],
            "_index": 172
        },
        {
            "title": "Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality",
            "authors": [
                "Junru Zhou",
                "Yicheng Wang",
                "Pan Li"
            ],
            "arxiv_id": "2512.09355v1",
            "summary": "Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.",
            "headline_zh": "研究子图GNN在MILP分支选择中的理论优势与实践局限",
            "intro_zh": [
                "核心问题：标准MPNN表达能力不足，高阶GNN计算成本高，需平衡表达力与效率。",
                "方法要点：证明节点锚定子图GNN（低于3-WL表达能力）足以近似强分支分数。",
                "实验或效果：实证显示子图GNN因O(n)复杂度导致内存瓶颈和求解时间慢于MPNN和启发式方法。"
            ],
            "tags_zh": [
                "图神经网络",
                "混合整数线性规划",
                "分支策略",
                "表达能力",
                "计算效率",
                "子图GNN"
            ],
            "_index": 173
        },
        {
            "title": "Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding",
            "authors": [
                "Xinkui Zhao",
                "Zuxin Wang",
                "Yifan Zhang",
                "Guanjie Cheng",
                "Yueshen Xu",
                "Shuiguang Deng",
                "Chang Liu",
                "Naibo Wang",
                "Jianwei Yin"
            ],
            "arxiv_id": "2512.09354v1",
            "summary": "The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.",
            "headline_zh": "提出Video-QTR框架，通过查询驱动的时间推理解决长视频理解的计算效率问题。",
            "intro_zh": [
                "核心问题：传统方法在长视频理解中因密集帧编码导致高计算和内存开销，限制实际应用。",
                "方法要点：采用查询驱动的时间推理，动态分配感知资源，形成推理与感知的自适应反馈循环。",
                "实验或效果：在多个基准测试中达到先进性能，同时减少输入帧消耗高达73%。"
            ],
            "tags_zh": [
                "长视频理解",
                "查询驱动推理",
                "轻量级框架",
                "时间推理",
                "计算效率"
            ],
            "_index": 174
        },
        {
            "title": "TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment",
            "authors": [
                "Kanghyun Baek",
                "Sangyub Lee",
                "Jin Young Choi",
                "Jaewoo Song",
                "Daemin Park",
                "Jooyoung Choi",
                "Chaehun Shin",
                "Bohyung Han",
                "Sungroh Yoon"
            ],
            "arxiv_id": "2512.09350v1",
            "summary": "Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.",
            "headline_zh": "提出TextGuider以解决扩散模型文本渲染中的文本缺失问题",
            "intro_zh": [
                "核心问题：扩散模型在文本渲染中常出现文本部分或完全缺失，现有方法对此关注不足。",
                "方法要点：通过分析MM-DiT模型的注意力模式，在去噪早期阶段应用基于新损失函数的潜在引导，对齐文本内容令牌与图像区域。",
                "实验或效果：在测试时文本渲染中达到最先进性能，显著提升召回率，并在OCR准确率和CLIP分数上表现强劲。"
            ],
            "tags_zh": [
                "文本渲染",
                "扩散模型",
                "注意力对齐",
                "训练免费方法",
                "文本缺失问题"
            ],
            "_index": 175
        },
        {
            "title": "COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning",
            "authors": [
                "Lin Li",
                "Yuxin Cai",
                "Jianwu Fang",
                "Jianru Xue",
                "Chen Lv"
            ],
            "arxiv_id": "2512.09349v1",
            "summary": "End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\\% in trained driving environments and by 50\\% in previously unseen environments, highlighting its strong generalization capability.",
            "headline_zh": "提出COVLM-RL框架，通过VLM引导的强化学习解决自动驾驶的泛化与可解释性问题。",
            "intro_zh": [
                "核心问题：端到端自动驾驶框架在泛化、训练效率和可解释性方面存在不足。",
                "方法要点：结合关键对象推理与VLM引导的强化学习，使用CoT提示生成语义决策先验。",
                "实验或效果：在CARLA模拟器中，训练环境成功率提升30%，未见环境提升50%。"
            ],
            "tags_zh": [
                "自动驾驶",
                "视觉语言模型",
                "强化学习",
                "关键对象推理",
                "可解释性",
                "泛化能力"
            ],
            "_index": 176
        },
        {
            "title": "Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane",
            "authors": [
                "Ashik E Rasul",
                "Humaira Tasnim",
                "Ji Yu Kim",
                "Young Hyun Lim",
                "Scott Schmitz",
                "Bruce W. Jo",
                "Hyung-Jin Yoon"
            ],
            "arxiv_id": "2512.09343v1",
            "summary": "QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.",
            "headline_zh": "提出轻量级四轴固定翼系统，用于动态无GPS环境下的高效视觉自主着陆与视觉惯性里程计。",
            "intro_zh": [
                "核心问题：长程四轴固定翼在无GPS或杂乱环境中，面临着陆区非结构化、边缘AI资源受限及高惯性飞行特性的挑战。",
                "方法要点：开发轻量级硬件平台与优化部署框架，结合深度神经网络学习着陆特征，实现实时检测与控制。",
                "实验或效果：未知具体实验数据，但系统为动态无GPS环境中的自主着陆部署奠定了基础。"
            ],
            "tags_zh": [
                "四轴固定翼",
                "视觉自主着陆",
                "视觉惯性里程计",
                "边缘AI",
                "非结构化环境",
                "长程无人机"
            ],
            "_index": 177
        },
        {
            "title": "Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration",
            "authors": [
                "Chethana Prasad Kabgere"
            ],
            "arxiv_id": "2512.09340v1",
            "summary": "Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.",
            "headline_zh": "对比人类与AI在模糊视觉刺激下的标注表现，提出神经符号整合架构以提升AI可解释性与认知对齐",
            "intro_zh": [
                "核心问题：人类与AI系统如何解释低分辨率、感知退化的视觉刺激，揭示感知与决策的异同",
                "方法要点：结合计算认知科学、认知架构和连接主义-符号混合模型，分析人类类比推理与AI特征处理",
                "实验或效果：通过Grad-CAM可视化模型注意力，对比人类响应，发现表示、推理和置信度校准的平行与分歧"
            ],
            "tags_zh": [
                "视觉分类",
                "认知分析",
                "神经符号整合",
                "人类标注",
                "模型注意力",
                "可解释AI"
            ],
            "_index": 178
        },
        {
            "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video",
            "authors": [
                "Seonghwa Choi",
                "Moonkyeong Choi",
                "Mingyu Jang",
                "Jaekyung Kim",
                "Jianfei Cai",
                "Wen-Huang Cheng",
                "Sanghoon Lee"
            ],
            "arxiv_id": "2512.09335v1",
            "summary": "Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.",
            "headline_zh": "提出RnD-Avatar框架，基于3D高斯泼溅重建可重光照和动态的人体化身，以解决单目视频中几何细节不足的问题。",
            "intro_zh": [
                "核心问题：现有方法因几何细节不足（如衣物褶皱）导致重建效果不真实。",
                "方法要点：引入动态蒙皮权重和正则化，以捕捉姿态变化和精细几何细节。",
                "实验或效果：在合成新视角、新姿态和重光照任务中达到先进性能。"
            ],
            "tags_zh": [
                "人体化身重建",
                "3D高斯泼溅",
                "动态蒙皮权重",
                "可重光照渲染",
                "单目视频建模",
                "几何细节优化"
            ],
            "_index": 179
        },
        {
            "title": "Improved Physics-Driven Neural Network to Solve Inverse Scattering Problems",
            "authors": [
                "Yutong Du",
                "Zicheng Liu",
                "Bo Wu",
                "Jingwei Kou",
                "Hang Li",
                "Changyou Li",
                "Yali Zong",
                "Bo Qi"
            ],
            "arxiv_id": "2512.09333v1",
            "summary": "This paper presents an improved physics-driven neural network (IPDNN) framework for solving electromagnetic inverse scattering problems (ISPs). A new Gaussian-localized oscillation-suppressing window (GLOW) activation function is introduced to stabilize convergence and enable a lightweight yet accurate network architecture. A dynamic scatter subregion identification strategy is further developed to adaptively refine the computational domain, preventing missed detections and reducing computational cost. Moreover, transfer learning is incorporated to extend the solver's applicability to practical scenarios, integrating the physical interpretability of iterative algorithms with the real-time inference capability of neural networks. Numerical simulations and experimental results demonstrate that the proposed solver achieves superior reconstruction accuracy, robustness, and efficiency compared with existing state-of-the-art methods.",
            "headline_zh": "提出改进物理驱动神经网络以解决电磁逆散射问题",
            "intro_zh": [
                "核心问题：电磁逆散射问题求解，涉及从散射数据反演目标属性，传统方法计算成本高或精度不足。",
                "方法要点：引入高斯局部振荡抑制窗口激活函数稳定收敛，采用动态散射子区域识别策略自适应优化计算域，结合迁移学习提升实际场景适用性。",
                "实验或效果：数值模拟和实验显示，该方法在重建精度、鲁棒性和效率上优于现有先进方法。"
            ],
            "tags_zh": [
                "电磁逆散射",
                "物理驱动神经网络",
                "激活函数优化",
                "动态计算域",
                "迁移学习",
                "实时推理"
            ],
            "_index": 180
        },
        {
            "title": "Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design",
            "authors": [
                "Amin Tavakoli",
                "Raswanth Murugan",
                "Ozan Gokdemir",
                "Arvind Ramanathan",
                "Frances Arnold",
                "Anima Anandkumar"
            ],
            "arxiv_id": "2512.09329v1",
            "summary": "Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.",
            "headline_zh": "提出自蒸馏微调方法以提升蛋白质语言模型在蛋白质设计中的通用性",
            "intro_zh": [
                "核心问题：蛋白质语言模型微调缺乏高质量标注数据，导致生成序列保真度低。",
                "方法要点：利用模型自身输出构建训练数据，结合轻量级筛选管道进行监督微调。",
                "实验或效果：在色氨酸合酶家族中验证，生成序列更新颖且稳定性与功能性提升。"
            ],
            "tags_zh": [
                "蛋白质语言模型",
                "监督微调",
                "自蒸馏",
                "蛋白质设计",
                "序列生成"
            ],
            "_index": 181
        },
        {
            "title": "UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking",
            "authors": [
                "Xuangeng Chu",
                "Ruicong Liu",
                "Yifei Huang",
                "Yun Liu",
                "Yichen Peng",
                "Bo Zheng"
            ],
            "arxiv_id": "2512.09327v1",
            "summary": "Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.",
            "headline_zh": "提出UniLS框架，通过双轨音频端到端生成统一听说的虚拟人表情，解决听者建模僵硬问题。",
            "intro_zh": [
                "核心问题：听者表情建模困难，音频驱动训练导致僵硬，现有方法依赖额外运动数据或非端到端设计。",
                "方法要点：采用两阶段训练，先学习内部运动先验，再引入双轨音频微调，实现音频驱动的听说统一生成。",
                "实验或效果：在听说准确性上达到先进水平，听者指标提升44.1%，生成更自然多样的表情，缓解僵硬问题。"
            ],
            "tags_zh": [
                "音频驱动虚拟人",
                "听说统一生成",
                "两阶段训练",
                "听者建模",
                "端到端框架",
                "表情生成"
            ],
            "_index": 182
        },
        {
            "title": "Self-Supervised Learning with Gaussian Processes",
            "authors": [
                "Yunshan Duan",
                "Sinead Williamson"
            ],
            "arxiv_id": "2512.09322v1",
            "summary": "Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.",
            "headline_zh": "提出高斯过程自监督学习以解决表示空间平滑性和不确定性量化问题",
            "intro_zh": [
                "核心问题：自监督学习中生成相似样本对困难，且缺乏不确定性量化。",
                "方法要点：利用高斯过程先验，通过协方差函数自然聚合相似表示，无需显式正样本。",
                "实验或效果：在分类和回归任务中，GPSSL在准确性、不确定性量化和误差控制方面优于传统方法。"
            ],
            "tags_zh": [
                "自监督学习",
                "高斯过程",
                "表示学习",
                "不确定性量化",
                "核方法"
            ],
            "_index": 183
        },
        {
            "title": "Efficiency-Aware Computational Intelligence for Resource-Constrained Manufacturing Toward Edge-Ready Deployment",
            "authors": [
                "Qianyu Zhou"
            ],
            "arxiv_id": "2512.09319v1",
            "summary": "Industrial cyber physical systems operate under heterogeneous sensing, stochastic dynamics, and shifting process conditions, producing data that are often incomplete, unlabeled, imbalanced, and domain shifted. High-fidelity datasets remain costly, confidential, and slow to obtain, while edge devices face strict limits on latency, bandwidth, and energy. These factors restrict the practicality of centralized deep learning, hinder the development of reliable digital twins, and increase the risk of error escape in safety-critical applications. Motivated by these challenges, this dissertation develops an efficiency grounded computational framework that enables data lean, physics-aware, and deployment ready intelligence for modern manufacturing environments. The research advances methods that collectively address core bottlenecks across multimodal and multiscale industrial scenarios. Generative strategies mitigate data scarcity and imbalance, while semi-supervised learning integrates unlabeled information to reduce annotation and simulation demands. Physics-informed representation learning strengthens interpretability and improves condition monitoring under small-data regimes. Spatially aware graph-based surrogate modeling provides efficient approximation of complex processes, and an edge cloud collaborative compression scheme supports real-time signal analytics under resource constraints. The dissertation also extends visual understanding through zero-shot vision language reasoning augmented by domain specific retrieval, enabling generalizable assessment in previously unseen scenarios. Together, these developments establish a unified paradigm of data efficient and resource aware intelligence that bridges laboratory learning with industrial deployment, supporting reliable decision-making across diverse manufacturing systems.",
            "headline_zh": "提出效率驱动的计算智能框架，以解决资源受限制造环境中的数据稀缺与部署挑战。",
            "intro_zh": [
                "核心问题：工业数据不完整、不平衡且获取成本高，边缘设备资源受限，阻碍深度学习实用性与数字孪生可靠性。",
                "方法要点：结合生成策略、半监督学习、物理感知表示学习、图基代理建模和边缘云协作压缩，实现数据高效与资源感知智能。",
                "实验或效果：增强条件监测、实时信号分析，并通过零样本视觉语言推理提升未见场景的泛化评估能力。"
            ],
            "tags_zh": [
                "数据高效学习",
                "边缘计算",
                "物理感知表示",
                "图基代理建模",
                "零样本推理",
                "制造智能"
            ],
            "_index": 184
        },
        {
            "title": "Simultaneous Genetic Evolution of Neural Networks for Optimal SFC Embedding",
            "authors": [
                "Theviyanthan Krishnamohan",
                "Lauritz Thamsen",
                "Paul Harvey"
            ],
            "arxiv_id": "2512.09318v1",
            "summary": "The reliance of organisations on computer networks is enabled by network programmability, which is typically achieved through Service Function Chaining. These chains virtualise network functions, link them, and programmatically embed them on networking infrastructure. Optimal embedding of Service Function Chains is an NP-hard problem, with three sub-problems, chain composition, virtual network function embedding, and link embedding, that have to be optimised simultaneously, rather than sequentially, for optimal results. Genetic Algorithms have been employed for this, but existing approaches either do not optimise all three sub-problems or do not optimise all three sub-problems simultaneously. We propose a Genetic Algorithm-based approach called GENESIS, which evolves three sine-function-activated Neural Networks, and funnels their output to a Gaussian distribution and an A* algorithm to optimise all three sub-problems simultaneously. We evaluate GENESIS on an emulator across 48 different data centre scenarios and compare its performance to two state-of-the-art Genetic Algorithms and one greedy algorithm. GENESIS produces an optimal solution for 100% of the scenarios, whereas the second-best method optimises only 71% of the scenarios. Moreover, GENESIS is the fastest among all Genetic Algorithms, averaging 15.84 minutes, compared to an average of 38.62 minutes for the second-best Genetic Algorithm.",
            "headline_zh": "提出GENESIS遗传算法，通过进化三个神经网络同时优化SFC嵌入的三个子问题。",
            "intro_zh": [
                "核心问题：SFC最优嵌入是NP难问题，需同时优化链组合、VNF嵌入和链路嵌入三个子问题。",
                "方法要点：GENESIS进化三个正弦激活神经网络，输出经高斯分布和A*算法处理，实现同时优化。",
                "实验或效果：在48个数据中心场景中，GENESIS实现100%最优解，且速度最快，平均15.84分钟。"
            ],
            "tags_zh": [
                "服务功能链嵌入",
                "遗传算法",
                "神经网络进化",
                "NP难优化",
                "数据中心网络"
            ],
            "_index": 185
        },
        {
            "title": "Functional Percolation: A Perspective on Criticality of Form and Function",
            "authors": [
                "Galen J. Wilkerson"
            ],
            "arxiv_id": "2512.09317v1",
            "summary": "Understanding the physical constraints and minimal conditions that enable information processing in extended systems remains a central challenge across disciplines, from neuroscience and artificial intelligence to social and physical networks. Here we study how network connectivity both limits and enables information processing by analyzing random networks across the structural percolation transition. Using cascade-mediated dynamics as a minimal and universal mechanism for propagating state-dependent responses, we examine structural, functional, and information-theoretic observables as functions of mean degree in Erdos-Renyi networks. We find that the emergence of a giant connected component coincides with a sharp transition in realizable information processing: complex input-output response functions become accessible, functional diversity increases rapidly, output entropy rises, and directed information flow quantified by transfer entropy extends beyond local neighborhoods. These coincident transitions define a regime of functional percolation, referring to a sharp expansion of the space of realizable input-output functions at the structural percolation transition. Near criticality, networks exhibit a Pareto-optimal tradeoff between functional complexity and diversity, suggesting that percolation criticality provides a universal organizing principle for information processing in systems with local interactions and propagating influences.",
            "headline_zh": "提出功能渗流概念，揭示结构渗流临界点如何扩展网络信息处理能力。",
            "intro_zh": [
                "核心问题：网络连通性如何限制和促进扩展系统中的信息处理。",
                "方法要点：分析随机网络在结构渗流过渡中的级联动力学和信息论观测。",
                "实验或效果：发现巨连通组件出现与功能多样性、输出熵和信息流扩展同步。"
            ],
            "tags_zh": [
                "功能渗流",
                "结构渗流",
                "信息处理",
                "级联动力学",
                "网络连通性",
                "临界性"
            ],
            "_index": 186
        },
        {
            "title": "Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook",
            "authors": [
                "Yuan Ma",
                "Junlin Hou",
                "Chao Zhang",
                "Yukun Zhou",
                "Zongyuan Ge",
                "Haoran Xie",
                "Lie Ju"
            ],
            "arxiv_id": "2512.09315v1",
            "summary": "Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \\textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.",
            "headline_zh": "提出LNMBench基准以评估医学图像分类中带噪标签方法的鲁棒性",
            "intro_zh": [
                "核心问题：医学图像标注噪声高且现有方法鲁棒性未系统评估",
                "方法要点：构建统一基准，涵盖10种方法、7数据集、6模态和3噪声模式",
                "实验或效果：现有方法在高噪声下性能显著下降，提出改进增强鲁棒性"
            ],
            "tags_zh": [
                "医学图像分类",
                "带噪标签学习",
                "基准评估",
                "鲁棒性分析",
                "多模态数据"
            ],
            "_index": 187
        },
        {
            "title": "Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices",
            "authors": [
                "Yuki Oda",
                "Yuta Ono",
                "Hiroshi Nakamura",
                "Hideki Takase"
            ],
            "arxiv_id": "2512.09313v1",
            "summary": "The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.",
            "headline_zh": "提出Hetero-SplitEE方法，通过异构早期退出支持异构物联网设备的分割学习",
            "intro_zh": [
                "核心问题：现有分割学习假设设备同质，不适用于计算资源异构的物联网系统",
                "方法要点：集成异构早期退出，允许客户端根据计算能力选择不同分割点，并提出顺序和平均两种协作训练策略",
                "实验或效果：在CIFAR-10等数据集上验证，保持竞争性精度，高效支持多样计算约束"
            ],
            "tags_zh": [
                "分割学习",
                "异构物联网",
                "早期退出",
                "协作训练",
                "神经网络训练"
            ],
            "_index": 188
        },
        {
            "title": "Transformer-Driven Multimodal Fusion for Explainable Suspiciousness Estimation in Visual Surveillance",
            "authors": [
                "Kuldeep Singh Yadav",
                "Lalan Kumar"
            ],
            "arxiv_id": "2512.09311v1",
            "summary": "Suspiciousness estimation is critical for proactive threat detection and ensuring public safety in complex environments. This work introduces a large-scale annotated dataset, USE50k, along with a computationally efficient vision-based framework for real-time suspiciousness analysis. The USE50k dataset contains 65,500 images captured from diverse and uncontrolled environments, such as airports, railway stations, restaurants, parks, and other public areas, covering a broad spectrum of cues including weapons, fire, crowd density, abnormal facial expressions, and unusual body postures. Building on this dataset, we present DeepUSEvision, a lightweight and modular system integrating three key components, i.e., a Suspicious Object Detector based on an enhanced YOLOv12 architecture, dual Deep Convolutional Neural Networks (DCNN-I and DCNN-II) for facial expression and body-language recognition using image and landmark features, and a transformer-based Discriminator Network that adaptively fuses multimodal outputs to yield an interpretable suspiciousness score. Extensive experiments confirm the superior accuracy, robustness, and interpretability of the proposed framework compared to state-of-the-art approaches. Collectively, the USE50k dataset and the DeepUSEvision framework establish a strong and scalable foundation for intelligent surveillance and real-time risk assessment in safety-critical applications.",
            "headline_zh": "提出DeepUSEvision框架和USE50k数据集，用于视觉监控中的实时可疑性估计与解释性分析。",
            "intro_zh": [
                "核心问题：在复杂环境中实现实时可疑性估计，以支持主动威胁检测和公共安全。",
                "方法要点：基于增强YOLOv12的物体检测器、双DCNN的面部表情与身体语言识别，以及Transformer驱动的多模态融合网络。",
                "实验或效果：在USE50k数据集上验证了框架的准确性、鲁棒性和可解释性，优于现有方法。"
            ],
            "tags_zh": [
                "可疑性估计",
                "多模态融合",
                "Transformer网络",
                "视觉监控",
                "实时分析",
                "可解释性AI"
            ],
            "_index": 189
        },
        {
            "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge",
            "authors": [
                "Zihao Ding",
                "Mufeng Zhu",
                "Zhongze Tang",
                "Sheng Wei",
                "Yao Liu"
            ],
            "arxiv_id": "2512.09309v1",
            "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.",
            "headline_zh": "提出分布式隐私增强视觉Transformer边缘框架，以解决资源受限设备视觉任务中的隐私泄露问题。",
            "intro_zh": [
                "核心问题：视觉智能工具计算需求高，云端卸载易导致传输和服务器端隐私漏洞。",
                "方法要点：使用可信边缘设备作为协调器，将视觉数据分片分发至多个独立云服务器，防止单服务器完整重建。",
                "实验或效果：以Segment Anything Model为例，保持近基线分割性能，显著降低内容重建和用户数据暴露风险。"
            ],
            "tags_zh": [
                "隐私保护",
                "边缘计算",
                "视觉Transformer",
                "分布式框架",
                "数据分片"
            ],
            "_index": 190
        },
        {
            "title": "Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning",
            "authors": [
                "Kwang Bin Lee",
                "Jiho Kang",
                "Sung-Hee Lee"
            ],
            "arxiv_id": "2512.09310v1",
            "summary": "Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.",
            "headline_zh": "提出场景无关的双臂任务规划框架，通过视觉可供性推理实现协调操作",
            "intro_zh": [
                "核心问题：现有机器人任务规划器多为单臂，难以处理场景无关双臂操作的空间、几何和协调挑战",
                "方法要点：集成视觉点定位、双臂子目标规划和交互点驱动提示模块，实现语义推理与3D执行桥接",
                "实验或效果：在杂乱未知场景中生成紧凑可行双臂计划，无需重训练，展示稳健场景泛化能力"
            ],
            "tags_zh": [
                "双臂任务规划",
                "视觉可供性推理",
                "场景无关操作",
                "子目标规划",
                "协调动作"
            ],
            "_index": 191
        },
        {
            "title": "From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation",
            "authors": [
                "Shivanshu Agnihotri",
                "Snehashis Majhi",
                "Deepak Ranjan Nayak",
                "Debesh Jha"
            ],
            "arxiv_id": "2512.09307v1",
            "summary": "Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.",
            "headline_zh": "提出Polyp-DiFoM蒸馏框架，将基础模型知识注入轻量基线以提升息肉分割性能。",
            "intro_zh": [
                "核心问题：轻量基线模型在息肉分割中因尺寸、形状和颜色变化及伪装特性而性能受限。",
                "方法要点：通过蒸馏框架将SAM、DINOv2等基础模型的语义先验注入U-Net等架构，并采用频域编码增强蒸馏效果。",
                "实验或效果：在五个基准数据集上显著超越基线及SOTA模型，计算开销减少近9倍。"
            ],
            "tags_zh": [
                "息肉分割",
                "知识蒸馏",
                "轻量模型",
                "基础模型",
                "医学图像分析",
                "频域编码"
            ],
            "_index": 192
        },
        {
            "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
            "authors": [
                "Daili Hua",
                "Xizhi Wang",
                "Bohan Zeng",
                "Xinyi Huang",
                "Hao Liang",
                "Junbo Niu",
                "Xinlong Chen",
                "Quanqing Xu",
                "Wentao Zhang"
            ],
            "arxiv_id": "2512.09299v1",
            "summary": "Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.",
            "headline_zh": "提出VABench基准以解决音视频生成同步评估不足的问题",
            "intro_zh": [
                "现有视频生成基准缺乏音视频同步评估，VABench填补此空白",
                "VABench涵盖T2AV、I2AV和立体音视频生成任务，包含15个评估维度",
                "基准覆盖七类内容，提供系统分析和可视化，旨在推动领域发展"
            ],
            "tags_zh": [
                "音视频生成",
                "基准评估",
                "同步性评估",
                "多模态生成",
                "任务分类",
                "内容类别"
            ],
            "_index": 193
        },
        {
            "title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation",
            "authors": [
                "Huayi Zhou",
                "Kui Jia"
            ],
            "arxiv_id": "2512.09297v1",
            "summary": "Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.",
            "headline_zh": "提出BiDemoSyn框架，从单次真实演示合成大规模双手机器人操作数据以解决效率与真实性的权衡问题。",
            "intro_zh": [
                "核心问题：双手机器人操作学习依赖大规模高质量演示，但现有方法在效率与真实性间存在权衡。",
                "方法要点：将任务分解为不变协调块和可变调整，通过视觉对齐和轻量轨迹优化合成物理可行的演示。",
                "实验或效果：在六个任务中，基于合成数据的策略对新物体姿态和形状具有鲁棒泛化能力，优于基线方法。"
            ],
            "tags_zh": [
                "双手机器人操作",
                "演示合成",
                "模仿学习",
                "轨迹优化",
                "物理可行性",
                "泛化能力"
            ],
            "_index": 194
        },
        {
            "title": "Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving",
            "authors": [
                "Songhan Wu"
            ],
            "arxiv_id": "2512.09296v1",
            "summary": "This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.",
            "headline_zh": "提出YOLOv8n-SPTS模型以提升自动驾驶场景中小目标检测性能",
            "intro_zh": [
                "核心问题：自动驾驶动态感知中小目标识别困难，现有算法因信息丢失、尺度不平衡和遮挡导致检测性能差。",
                "方法要点：通过SPD-Conv模块优化特征提取，引入SPPFCSPC模块增强特征融合，设计TSFP结构专注小目标检测。",
                "实验或效果：在VisDrone2019-DET数据集上，模型在精度、召回率和mAP指标上排名第一，可视化显示遮挡密集场景中小目标漏检率显著降低。"
            ],
            "tags_zh": [
                "自动驾驶",
                "小目标检测",
                "YOLOv8改进",
                "特征融合",
                "多尺度特征"
            ],
            "_index": 195
        },
        {
            "title": "Distributional Shrinkage II: Optimal Transport Denoisers with Higher-Order Scores",
            "authors": [
                "Tengyuan Liang"
            ],
            "arxiv_id": "2512.09295v1",
            "summary": "We revisit the signal denoising problem through the lens of optimal transport: the goal is to recover an unknown scalar signal distribution $X \\sim P$ from noisy observations $Y = X + σZ$, with $Z$ being standard Gaussian independent of $X$ and $σ>0$ a known noise level. Let $Q$ denote the distribution of $Y$. We introduce a hierarchy of denoisers $T_0, T_1, \\ldots, T_\\infty : \\mathbb{R} \\to \\mathbb{R}$ that are agnostic to the signal distribution $P$, depending only on higher-order score functions of $Q$. Each denoiser $T_K$ is progressively refined using the $(2K-1)$-th order score function of $Q$ at noise resolution $σ^{2K}$, achieving better denoising quality measured by the Wasserstein metric $W(T_K \\sharp Q, P)$. The limiting denoiser $T_\\infty$ identifies the optimal transport map with $T_\\infty \\sharp Q = P$.\n  We provide a complete characterization of the combinatorial structure underlying this hierarchy through Bell polynomial recursions, revealing how higher-order score functions encode the optimal transport map for signal denoising. We study two estimation strategies with convergence rates for higher-order scores from i.i.d. samples drawn from $Q$: (i) plug-in estimation via Gaussian kernel smoothing, and (ii) direct estimation via higher-order score matching. This hierarchy of agnostic denoisers opens new perspectives in signal denoising and empirical Bayes.",
            "headline_zh": "提出基于高阶分数函数的最优传输去噪器，用于信号分布恢复",
            "intro_zh": [
                "核心问题：从高斯噪声观测中恢复未知信号分布，无需先验知识",
                "方法要点：构建去噪器层次，利用高阶分数函数逐步优化Wasserstein距离",
                "实验或效果：提供两种估计策略，分析收敛率，实现渐进去噪质量提升"
            ],
            "tags_zh": [
                "最优传输",
                "信号去噪",
                "高阶分数函数",
                "Wasserstein距离",
                "经验贝叶斯"
            ],
            "_index": 196
        },
        {
            "title": "Identifying Bias in Machine-generated Text Detection",
            "authors": [
                "Kevin Stowe",
                "Svetlana Afanaseva",
                "Rodolfo Raimundo",
                "Yitao Sun",
                "Kailash Patil"
            ],
            "arxiv_id": "2512.09292v1",
            "summary": "The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.",
            "headline_zh": "评估英语机器生成文本检测系统在性别、种族、语言和经济属性上的潜在偏见",
            "intro_zh": [
                "核心问题：机器生成文本检测系统可能对性别、种族、英语学习者状态和经济地位等属性存在偏见，导致不公平分类。",
                "方法要点：基于学生论文数据集，使用回归模型和子群分析评估16个检测系统的偏见显著性和影响程度。",
                "实验或效果：发现偏见在不同系统中不一致，但英语学习者论文更易被误判为机器生成，非白人英语学习者尤其受影响，而人类标注者无显著偏见。"
            ],
            "tags_zh": [
                "机器生成文本检测",
                "偏见评估",
                "公平性分析",
                "回归模型",
                "子群分析",
                "英语学习者"
            ],
            "_index": 197
        },
        {
            "title": "MelanomaNet: Explainable Deep Learning for Skin Lesion Classification",
            "authors": [
                "Sukhrobbek Ilyosbekov"
            ],
            "arxiv_id": "2512.09289v1",
            "summary": "Automated skin lesion classification using deep learning has shown remarkable accuracy, yet clinical adoption remains limited due to the \"black box\" nature of these models. We present MelanomaNet, an explainable deep learning system for multi-class skin lesion classification that addresses this gap through four complementary interpretability mechanisms. Our approach combines an EfficientNet V2 backbone with GradCAM++ attention visualization, automated ABCDE clinical criterion extraction, Fast Concept Activation Vectors (FastCAV) for concept-based explanations, and Monte Carlo Dropout uncertainty quantification. We evaluate our system on the ISIC 2019 dataset containing 25,331 dermoscopic images across 9 diagnostic categories. Our model achieves 85.61% accuracy with a weighted F1 score of 0.8564, while providing clinically meaningful explanations that align model attention with established dermatological assessment criteria. The uncertainty quantification module decomposes prediction confidence into epistemic and aleatoric components, enabling automatic flagging of unreliable predictions for clinical review. Our results demonstrate that high classification performance can be achieved alongside comprehensive interpretability, potentially facilitating greater trust and adoption in clinical dermatology workflows. The source code is available at https://github.com/suxrobgm/explainable-melanoma",
            "headline_zh": "提出MelanomaNet可解释深度学习系统，通过多机制提升皮肤病变分类的临床可解释性。",
            "intro_zh": [
                "核心问题：深度学习模型在皮肤病变分类中因'黑箱'特性限制临床采用。",
                "方法要点：结合EfficientNet V2骨干网络与GradCAM++、ABCDE准则提取、FastCAV和不确定性量化。",
                "实验或效果：在ISIC 2019数据集上达到85.61%准确率，提供临床对齐的解释和不确定性分解。"
            ],
            "tags_zh": [
                "皮肤病变分类",
                "可解释深度学习",
                "GradCAM++",
                "不确定性量化",
                "临床评估",
                "EfficientNet V2"
            ],
            "_index": 198
        },
        {
            "title": "UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects",
            "authors": [
                "Fan Wu",
                "Chenguang Yang",
                "Haibin Yang",
                "Shuo Wang",
                "Yanrui Xu",
                "Xing Zhou",
                "Meng Gao",
                "Yaoqi Xian",
                "Zhihong Zhu",
                "Shifeng Huang"
            ],
            "arxiv_id": "2512.09283v1",
            "summary": "Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.",
            "headline_zh": "提出UPETrack框架，基于单向位置估计解决遮挡下可变形线性物体的实时跟踪问题。",
            "intro_zh": [
                "核心问题：可变形线性物体因高维配置空间、非线性动态和频繁遮挡，难以实现鲁棒实时跟踪。",
                "方法要点：采用几何驱动框架，结合可见段高斯混合模型跟踪和遮挡区域单向位置估计算法，无需物理建模或标记。",
                "实验或效果：在定位精度和计算效率上超越TrackDLO和CDCPD2等先进算法。"
            ],
            "tags_zh": [
                "可变形线性物体跟踪",
                "遮挡处理",
                "几何驱动方法",
                "实时状态估计",
                "单向位置估计"
            ],
            "_index": 199
        },
        {
            "title": "FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model",
            "authors": [
                "Xiang Chen",
                "Jinshan Pan",
                "Jiangxin Dong",
                "Jian Yang",
                "Jinhui Tang"
            ],
            "arxiv_id": "2512.09282v1",
            "summary": "Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.",
            "headline_zh": "提出FoundIR-v2，通过数据均衡调度优化预训练数据混合比例以提升图像修复基础模型性能",
            "intro_zh": [
                "核心问题：图像修复基础模型中，不同任务数据混合比例直接影响整体性能，需优化以平衡泛化能力",
                "方法要点：采用数据均衡调度范式动态优化混合比例，并引入MoE驱动的调度器分配任务自适应扩散先验",
                "实验或效果：在超过50个子任务中验证，在广泛真实场景下实现一致泛化，性能优于先进方法"
            ],
            "tags_zh": [
                "图像修复基础模型",
                "数据混合优化",
                "扩散模型",
                "混合专家调度",
                "任务自适应先验",
                "多任务泛化"
            ],
            "_index": 200
        },
        {
            "title": "LoGoColor: Local-Global 3D Colorization for 360° Scenes",
            "authors": [
                "Yeonjin Chang",
                "Juhwan Cho",
                "Seunghyeon Seo",
                "Wonsik Shin",
                "Nojun Kwak"
            ],
            "arxiv_id": "2512.09278v1",
            "summary": "Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360° scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360° scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.",
            "headline_zh": "提出LoGoColor以解决360°场景3D着色中颜色多样性与多视图一致性问题",
            "intro_zh": [
                "核心问题：现有3D着色方法因蒸馏2D图像模型导致颜色平均化，在复杂360°场景中产生单调结果",
                "方法要点：采用局部-全局方法，分区处理场景并使用微调多视图扩散模型确保子场景内和子场景间一致性",
                "实验或效果：在复杂360°场景上实现更一致和合理的3D着色，并通过新颜色多样性指数验证颜色多样性"
            ],
            "tags_zh": [
                "3D着色",
                "多视图一致性",
                "颜色多样性",
                "360°场景",
                "局部-全局方法",
                "多视图扩散模型"
            ],
            "_index": 201
        },
        {
            "title": "Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis",
            "authors": [
                "Xiaochen Huang",
                "Xiaochen Bi",
                "Cuihua Lv",
                "Xin Wang",
                "Haoyan Zhang",
                "Wenjing Jiang",
                "Xin Ma",
                "Yibin Li"
            ],
            "arxiv_id": "2512.09276v1",
            "summary": "Parkinson's disease (PD), a prevalent neurodegenerative disorder, significantly affects patients' daily functioning and social interactions. To facilitate a more efficient and accessible diagnostic approach for PD, we propose a dynamic facial expression analysis-based PD auxiliary diagnosis method. This method targets hypomimia, a characteristic clinical symptom of PD, by analyzing two manifestations: reduced facial expressivity and facial rigidity, thereby facilitating the diagnosis process. We develop a multimodal facial expression analysis network to extract expression intensity features during patients' performance of various facial expressions. This network leverages the CLIP architecture to integrate visual and textual features while preserving the temporal dynamics of facial expressions. Subsequently, the expression intensity features are processed and input into an LSTM-based classification network for PD diagnosis. Our method achieves an accuracy of 93.1%, outperforming other in-vitro PD diagnostic approaches. This technique offers a more convenient detection method for potential PD patients, improving their diagnostic experience.",
            "headline_zh": "提出基于动态面部表情分析的帕金森病辅助诊断方法，以解决诊断效率与可及性问题。",
            "intro_zh": [
                "核心问题：帕金森病导致面部表情减少和僵硬，影响诊断效率与患者体验。",
                "方法要点：开发多模态面部表情分析网络，结合CLIP架构提取表情强度特征，并利用LSTM进行分类。",
                "实验或效果：在PD诊断中达到93.1%准确率，优于其他体外诊断方法。"
            ],
            "tags_zh": [
                "动态面部表情分析",
                "帕金森病辅助诊断",
                "多模态网络",
                "CLIP架构",
                "LSTM分类"
            ],
            "_index": 202
        },
        {
            "title": "Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression",
            "authors": [
                "Weiyi He",
                "Yue Xing"
            ],
            "arxiv_id": "2512.09275v1",
            "summary": "Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE.",
            "headline_zh": "分析可训练位置编码对Transformer在上下文回归中泛化与鲁棒性的影响",
            "intro_zh": [
                "核心问题：位置编码对Transformer泛化与鲁棒性的影响未知",
                "方法要点：推导单层Transformer在上下文回归中的泛化与对抗Rademacher界",
                "实验或效果：模拟研究验证理论界，显示位置编码放大泛化差距与脆弱性"
            ],
            "tags_zh": [
                "位置编码",
                "Transformer",
                "上下文回归",
                "泛化分析",
                "对抗鲁棒性",
                "Rademacher复杂度"
            ],
            "_index": 203
        },
        {
            "title": "LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations",
            "authors": [
                "Zhichao Yang",
                "Tianjiao Gu",
                "Jianjie Wang",
                "Feiyu Lin",
                "Xiangfei Sheng",
                "Pengfei Chen",
                "Leida Li"
            ],
            "arxiv_id": "2512.09271v1",
            "summary": "The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.",
            "headline_zh": "提出LongT2IBench基准以评估长文本到图像生成的对齐性，并基于此开发LongT2IExpert评估器。",
            "intro_zh": [
                "现有基准主要针对短提示，缺乏长文本场景下的细粒度对齐评估。",
                "设计图结构标注协议，将长提示转换为实体、属性和关系的图结构以实现细粒度对齐。",
                "通过指令调优开发LongT2IExpert，提供量化分数和结构化解释，实验显示其优越性。"
            ],
            "tags_zh": [
                "长文本到图像生成",
                "图结构标注",
                "对齐评估",
                "多模态大语言模型",
                "指令调优"
            ],
            "_index": 204
        },
        {
            "title": "MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification",
            "authors": [
                "Sangwoon Kwak",
                "Weeyoung Kwon",
                "Jun Young Jeong",
                "Geonho Kim",
                "Won-Sik Cheong",
                "Jihyong Oh"
            ],
            "arxiv_id": "2512.09270v1",
            "summary": "Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.",
            "headline_zh": "提出MoRel框架，通过锚点接力双向混合与分层致密化，解决长程动态视频建模中的内存爆炸和闪烁问题。",
            "intro_zh": [
                "核心问题：现有4D高斯溅射方法在长程动态视频建模中面临内存爆炸、时间闪烁和遮挡处理失败。",
                "方法要点：采用锚点接力双向混合机制，在关键帧构建局部规范锚点空间，学习双向变形以增强时间一致性。",
                "实验或效果：引入特征方差引导分层致密化，并在新数据集SelfCap_LR上验证，实现内存高效、无闪烁的长程4D重建。"
            ],
            "tags_zh": [
                "4D高斯溅射",
                "长程动态建模",
                "时间一致性",
                "锚点接力",
                "分层致密化",
                "内存优化"
            ],
            "_index": 205
        },
        {
            "title": "Goal inference with Rao-Blackwellized Particle Filters",
            "authors": [
                "Yixuan Wang",
                "Dan P. Guralnik",
                "Warren E. Dixon"
            ],
            "arxiv_id": "2512.09269v1",
            "summary": "Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.",
            "headline_zh": "提出基于Rao-Blackwellized粒子滤波的目标推断方法，用于从噪声轨迹中推断移动代理的最终目标。",
            "intro_zh": [
                "核心问题：从噪声观测中推断移动代理的最终目标，是基础估计问题。",
                "方法要点：利用Rao-Blackwellized粒子滤波，假设代理行为具有闭环稳定性，通过解析边缘化线性高斯子结构提高样本效率。",
                "实验或效果：实验显示对合规代理能快速准确恢复目标，并量化了推断性能和信息泄漏。"
            ],
            "tags_zh": [
                "目标推断",
                "Rao-Blackwellized粒子滤波",
                "信息论泄漏",
                "高斯混合模型",
                "轨迹分析"
            ],
            "_index": 206
        },
        {
            "title": "Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation",
            "authors": [
                "Ce Wang",
                "Weihang Dai",
                "Hanru Bai",
                "Xiaomeng Li"
            ],
            "arxiv_id": "2512.09267v1",
            "summary": "Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.",
            "headline_zh": "提出基于谱排序的对比学习半监督深度回归方法，利用广义序关系减少标注依赖。",
            "intro_zh": [
                "核心问题：对比回归方法依赖标签信息恢复特征序关系，限制半监督应用。",
                "方法要点：结合标记与未标记样本构建相似矩阵，通过谱排序恢复序关系用于对比学习。",
                "实验或效果：理论保证与多数据集实验验证，超越现有半监督深度回归方法。"
            ],
            "tags_zh": [
                "对比学习",
                "半监督回归",
                "谱排序",
                "序关系恢复",
                "特征表示学习"
            ],
            "_index": 207
        },
        {
            "title": "Robust and Sparse Estimation of Unbounded Density Ratio under Heavy Contamination",
            "authors": [
                "Ryosuke Nagumo",
                "Hironori Fujisawa"
            ],
            "arxiv_id": "2512.09266v1",
            "summary": "We examine the non-asymptotic properties of robust density ratio estimation (DRE) in contaminated settings. Weighted DRE is the most promising among existing methods, exhibiting doubly strong robustness from an asymptotic perspective. This study demonstrates that Weighted DRE achieves sparse consistency even under heavy contamination within a non-asymptotic framework. This method addresses two significant challenges in density ratio estimation and robust estimation. For density ratio estimation, we provide the non-asymptotic properties of estimating unbounded density ratios under the assumption that the weighted density ratio function is bounded. For robust estimation, we introduce a non-asymptotic framework for doubly strong robustness under heavy contamination, assuming that at least one of the following conditions holds: (i) contamination ratios are small, and (ii) outliers have small weighted values. This work provides the first non-asymptotic analysis of strong robustness under heavy contamination.",
            "headline_zh": "提出加权密度比估计方法，在重度污染下实现稀疏一致性和强鲁棒性",
            "intro_zh": [
                "研究密度比估计在污染环境下的非渐近性质，聚焦加权密度比估计方法",
                "证明加权密度比估计在重度污染下具有稀疏一致性，假设加权密度比函数有界",
                "引入非渐近框架分析双重强鲁棒性，假设污染率小或异常值加权值小"
            ],
            "tags_zh": [
                "密度比估计",
                "鲁棒估计",
                "非渐近分析",
                "污染环境",
                "稀疏一致性"
            ],
            "_index": 208
        },
        {
            "title": "FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection",
            "authors": [
                "Xiaojing Chen",
                "Dan Li",
                "Lijun Peng",
                "Jun YanŁetter",
                "Zhiqing Guo",
                "Junyang Chen",
                "Xiao Lan",
                "Zhongjie Ba",
                "Yunfeng DiaoŁetter"
            ],
            "arxiv_id": "2512.09264v1",
            "summary": "The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \\textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems.",
            "headline_zh": "提出基于频率的黑盒攻击方法FBA²D，以解决AI生成图像检测器的决策型攻击问题。",
            "intro_zh": [
                "核心问题：AI生成内容检测器在真实黑盒场景下易受决策型攻击，现有研究多假设模型信息已知。",
                "方法要点：利用离散余弦变换进行频域划分，选择频带作为查询子空间，结合对抗样本汤方法加速攻击。",
                "实验或效果：在Synthetic LSUN和GenImage数据集上验证了方法的有效性，提升了查询效率和图像质量。"
            ],
            "tags_zh": [
                "AI生成图像检测",
                "黑盒攻击",
                "频率域攻击",
                "决策型攻击",
                "对抗样本"
            ],
            "_index": 209
        },
        {
            "title": "ROI-Packing: Efficient Region-Based Compression for Machine Vision",
            "authors": [
                "Md Eimran Hossain Eimon",
                "Alena Krause",
                "Ashan Perera",
                "Juan Merlos",
                "Hari Kalva",
                "Velibor Adzic",
                "Borko Furht"
            ],
            "arxiv_id": "2512.09258v1",
            "summary": "This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).",
            "headline_zh": "提出ROI-Packing方法，针对机器视觉任务实现高效图像压缩。",
            "intro_zh": [
                "核心问题：机器视觉中图像压缩需平衡压缩效率与任务准确性。",
                "方法要点：优先压缩关键感兴趣区域，丢弃无关数据，无需模型重训练。",
                "实验效果：在多个数据集上，相比VVC标准，压缩率提升44.10%，准确性提高8.88%。"
            ],
            "tags_zh": [
                "图像压缩",
                "机器视觉",
                "感兴趣区域",
                "目标检测",
                "实例分割",
                "压缩效率"
            ],
            "_index": 210
        },
        {
            "title": "GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model",
            "authors": [
                "Lalit Maurya",
                "Saurabh Kaushik",
                "Beth Tellman"
            ],
            "arxiv_id": "2512.09251v1",
            "summary": "Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\\textbf{G}lacial \\textbf{LA}ke segmentation with \\textbf{C}ontextual \\textbf{I}nstance \\textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA",
            "headline_zh": "提出GLACIA框架，通过多模态大语言模型实现冰川湖分割与实例感知位置推理，以支持灾害预防。",
            "intro_zh": [
                "现有冰川湖分割方法缺乏高层语义和可解释推理，局限于像素级预测。",
                "GLACIA集成大语言模型，生成分割掩码和空间推理输出，提升准确性和可解释性。",
                "在GLake-Pos数据集上评估，GLACIA在mIoU指标上超越CNN、ViT、地理基础模型和推理方法。"
            ],
            "tags_zh": [
                "冰川湖分割",
                "多模态大语言模型",
                "实例感知推理",
                "遥感图像分析",
                "灾害监测"
            ],
            "_index": 211
        },
        {
            "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer",
            "authors": [
                "Cheng Liu",
                "Yiren Song",
                "Haofan Wang",
                "Mike Zheng Shou"
            ],
            "arxiv_id": "2512.09247v1",
            "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.",
            "headline_zh": "提出OmniPSD扩散框架，实现文本生成和图像分解PSD文件，支持透明通道和层级结构。",
            "intro_zh": [
                "核心问题：现有扩散模型难以生成或重建带透明alpha通道的分层PSD文件。",
                "方法要点：基于Flux生态系统，通过空间注意力和上下文学习统一处理文本生成和图像分解。",
                "实验或效果：在新RGBA分层数据集上验证高保真生成、结构一致性和透明度感知。"
            ],
            "tags_zh": [
                "扩散模型",
                "PSD生成",
                "图像分解",
                "透明通道",
                "上下文学习",
                "层级结构"
            ],
            "_index": 212
        },
        {
            "title": "A Clinically Interpretable Deep CNN Framework for Early Chronic Kidney Disease Prediction Using Grad-CAM-Based Explainable AI",
            "authors": [
                "Anas Bin Ayub",
                "Nilima Sultana Niha",
                "Md. Zahurul Haque"
            ],
            "arxiv_id": "2512.09244v1",
            "summary": "Chronic Kidney Disease (CKD) constitutes a major global medical burden, marked by the gradual deterioration of renal function, which results in the impaired clearance of metabolic waste and disturbances in systemic fluid homeostasis. Owing to its substantial contribution to worldwide morbidity and mortality, the development of reliable and efficient diagnostic approaches is critically important to facilitate early detection and prompt clinical management. This study presents a deep convolutional neural network (CNN) for early CKD detection from CT kidney images, complemented by class balancing using Synthetic Minority Over-sampling Technique (SMOTE) and interpretability via Gradient-weighted Class Activation Mapping (Grad-CAM). The model was trained and evaluated on the CT KIDNEY DATASET, which contains 12,446 CT images, including 3,709 cyst, 5,077 normal, 1,377 stone, and 2,283 tumor cases. The proposed deep CNN achieved a remarkable classification performance, attaining 100% accuracy in the early detection of chronic kidney disease (CKD). This significant advancement demonstrates strong potential for addressing critical clinical diagnostic challenges and enhancing early medical intervention strategies.",
            "headline_zh": "提出基于Grad-CAM可解释深度CNN框架，用于CT图像早期慢性肾病预测",
            "intro_zh": [
                "核心问题：慢性肾病早期诊断困难，需可靠方法提升检测效率。",
                "方法要点：使用深度CNN结合SMOTE平衡数据和Grad-CAM增强模型可解释性。",
                "实验或效果：在12,446张CT图像数据集上实现100%准确率，展示临床潜力。"
            ],
            "tags_zh": [
                "慢性肾病预测",
                "深度卷积神经网络",
                "可解释人工智能",
                "CT图像分析",
                "SMOTE数据平衡"
            ],
            "_index": 213
        },
        {
            "title": "Efficient Feature Compression for Machines with Global Statistics Preservation",
            "authors": [
                "Md Eimran Hossain Eimon",
                "Hyomin Choi",
                "Fabien Racapé",
                "Mateen Ulhaq",
                "Velibor Adzic",
                "Hari Kalva",
                "Borko Furht"
            ],
            "arxiv_id": "2512.09235v1",
            "summary": "The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.",
            "headline_zh": "提出基于Z-score归一化的特征压缩方法，以提升MPEG FCM标准中的机器视觉任务性能",
            "intro_zh": [
                "针对AI模型拆分推理中的中间特征数据传输，提出高效压缩需求",
                "采用Z-score归一化方法，在解码端有效恢复压缩特征，替代现有缩放方法",
                "实验显示平均比特率降低17.09%，对象跟踪任务最高达65.69%，且不牺牲任务精度"
            ],
            "tags_zh": [
                "特征压缩",
                "Z-score归一化",
                "MPEG FCM标准",
                "拆分推理",
                "机器视觉任务"
            ],
            "_index": 214
        },
        {
            "title": "Enabling Next-Generation Consumer Experience with Feature Coding for Machines",
            "authors": [
                "Md Eimran Hossain Eimon",
                "Juan Merlos",
                "Ashan Perera",
                "Hari Kalva",
                "Velibor Adzic",
                "Borko Furht"
            ],
            "arxiv_id": "2512.09232v1",
            "summary": "As consumer devices become increasingly intelligent and interconnected, efficient data transfer solutions for machine tasks have become essential. This paper presents an overview of the latest Feature Coding for Machines (FCM) standard, part of MPEG-AI and developed by the Moving Picture Experts Group (MPEG). FCM supports AI-driven applications by enabling the efficient extraction, compression, and transmission of intermediate neural network features. By offloading computationally intensive operations to base servers with high computing resources, FCM allows low-powered devices to leverage large deep learning models. Experimental results indicate that the FCM standard maintains the same level of accuracy while reducing bitrate requirements by 75.90% compared to remote inference.",
            "headline_zh": "提出特征编码标准FCM以支持低功耗设备高效运行AI应用",
            "intro_zh": [
                "核心问题：智能设备互联下，机器任务数据传输效率需求迫切",
                "方法要点：FCM标准提取、压缩并传输神经网络中间特征，卸载计算至服务器",
                "实验或效果：相比远程推理，FCM在保持精度下降低比特率75.90%"
            ],
            "tags_zh": [
                "特征编码",
                "MPEG-AI标准",
                "神经网络特征压缩",
                "低功耗设备AI",
                "远程推理优化"
            ],
            "_index": 215
        },
        {
            "title": "CORE: A Conceptual Reasoning Layer for Large Language Models",
            "authors": [
                "Vishwas Hegde",
                "Vindhya Shigehalli"
            ],
            "arxiv_id": "2512.09222v1",
            "summary": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.",
            "headline_zh": "提出CORE概念推理层以解决大语言模型多轮交互中的状态漂移问题",
            "intro_zh": [
                "核心问题：大语言模型在多轮交互中依赖令牌历史导致状态漂移和推理不一致",
                "方法要点：引入持久本地概念和认知操作符，分离概念推理与语言生成",
                "实验或效果：原型模拟显示累计提示令牌减少约42%，但非真实性能估计"
            ],
            "tags_zh": [
                "多轮交互",
                "概念推理",
                "状态管理",
                "模型无关机制",
                "提示优化"
            ],
            "_index": 216
        }
    ]
}