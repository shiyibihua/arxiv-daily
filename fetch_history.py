#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†å²æ•°æ®è·å–è„šæœ¬

ç”¨æ³•:
    python fetch_history.py --months-ago 1              # è·å–1ä¸ªæœˆå‰çš„æ•°æ®
    python fetch_history.py --months-ago 3              # è·å–3ä¸ªæœˆå‰çš„æ•°æ®
    python fetch_history.py --months-ago 6 --days 7     # è·å–6ä¸ªæœˆå‰ä¸€å‘¨çš„æ•°æ®
    python fetch_history.py --date 2024-06-15           # è·å–æŒ‡å®šæ—¥æœŸçš„æ•°æ®
    python fetch_history.py --month 2024-06              # è·å–æ•´ä¸ªæœˆçš„æ•°æ®
    python fetch_history.py --months-ago 1 --skip-ai    # è·³è¿‡AIåˆ†æ
    python fetch_history.py --month 2024-06 --no-images # ä¸æå–å›¾ç‰‡
    
æ³¨æ„:
    - arXiv å¯¹å†å²æŸ¥è¯¢æœ‰é€Ÿç‡é™åˆ¶ï¼Œå»ºè®®æ¯æ¬¡è·å–ä¸è¶…è¿‡7å¤©çš„æ•°æ®
    - å‘¨æœ«å’ŒèŠ‚å‡æ—¥ arXiv ä¸æ›´æ–°ï¼Œä¼šè‡ªåŠ¨è·³è¿‡
"""

import argparse
import asyncio
import json
import os
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
from typing import List, Tuple, Dict
from calendar import monthrange

from utils.scrapy import load_tags, query_arxiv, filter_by_interests
from utils.analyser import update_ai_summary_async
from utils.image_extractor import batch_extract_images

US_EASTERN = ZoneInfo("US/Eastern")


def get_month_date_range(year: int, month: int) -> Tuple[datetime, datetime]:
    """è·å–æŒ‡å®šæœˆä»½çš„èµ·æ­¢æ—¥æœŸ"""
    start = datetime(year, month, 1, 0, 0, 0, tzinfo=US_EASTERN)
    days_in_month = monthrange(year, month)[1]
    end = datetime(year, month, days_in_month, 23, 59, 59, tzinfo=US_EASTERN)
    return start, end


def get_date_range_from_months_ago(months_ago: int, days: int = 7) -> Tuple[datetime, datetime, str]:
    """
    è·å– N ä¸ªæœˆå‰çš„æ—¥æœŸèŒƒå›´
    
    Args:
        months_ago: å‡ ä¸ªæœˆå‰
        days: è·å–å¤šå°‘å¤©çš„æ•°æ®ï¼ˆé»˜è®¤7å¤©ï¼‰
    
    Returns:
        (start_date, end_date, label_date)
    """
    now = datetime.now(US_EASTERN)
    
    # è®¡ç®—ç›®æ ‡æœˆä»½
    target_year = now.year
    target_month = now.month - months_ago
    
    while target_month <= 0:
        target_month += 12
        target_year -= 1
    
    # é»˜è®¤å–è¯¥æœˆä¸­æ—¬ï¼ˆé¿å¼€æœˆåˆæœˆæœ«çš„è¾¹ç•Œé—®é¢˜ï¼‰
    target_day = min(15, monthrange(target_year, target_month)[1])
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date = datetime(target_year, target_month, target_day, 20, 0, 0, tzinfo=US_EASTERN)
    start_date = end_date - timedelta(days=days)
    
    label_date = end_date.strftime("%Y-%m-%d")
    
    return start_date, end_date, label_date


def get_date_range_from_date(date_str: str, days: int = 1) -> Tuple[datetime, datetime, str]:
    """
    ä»æŒ‡å®šæ—¥æœŸè·å–æ—¥æœŸèŒƒå›´
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
        days: è·å–å¤šå°‘å¤©çš„æ•°æ®
    
    Returns:
        (start_date, end_date, label_date)
    """
    target_date = datetime.strptime(date_str, "%Y-%m-%d")
    target_date = target_date.replace(hour=20, minute=0, second=0, tzinfo=US_EASTERN)
    
    end_date = target_date
    start_date = target_date - timedelta(days=days)
    
    label_date = target_date.strftime("%Y-%m-%d")
    
    return start_date, end_date, label_date


def get_month_weeks(year: int, month: int) -> List[Tuple[datetime, datetime, str]]:
    """
    è·å–æŒ‡å®šæœˆä»½çš„æ‰€æœ‰å‘¨ï¼ˆç”¨äºåˆ†æ‰¹è·å–æ•´æœˆæ•°æ®ï¼‰
    
    Returns:
        List of (start_date, end_date, label_date)
    """
    start, end = get_month_date_range(year, month)
    
    weeks = []
    current_start = start
    
    while current_start <= end:
        current_end = min(current_start + timedelta(days=6), end)
        label = current_end.strftime("%Y-%m-%d")
        weeks.append((current_start, current_end, label))
        current_start = current_end + timedelta(days=1)
    
    return weeks


def format_arxiv_query_range(start: datetime, end: datetime) -> Tuple[str, str]:
    """æ ¼å¼åŒ–ä¸º arXiv æŸ¥è¯¢æ ¼å¼"""
    fmt = "%Y%m%d%H%M"
    start_utc = start.astimezone(timezone.utc)
    end_utc = end.astimezone(timezone.utc)
    return start_utc.strftime(fmt), end_utc.strftime(fmt)


def fetch_papers_for_range(tags: List[str], start: datetime, end: datetime, 
                          max_results: int = 2000) -> Dict:
    """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„è®ºæ–‡"""
    start_str, end_str = format_arxiv_query_range(start, end)
    print(f"\nğŸ“… æŸ¥è¯¢æ—¥æœŸèŒƒå›´: {start.strftime('%Y-%m-%d')} è‡³ {end.strftime('%Y-%m-%d')}")
    print(f"   UTC æ—¶é—´: {start_str} è‡³ {end_str}")
    
    # 0 è¡¨ç¤ºä¸é™åˆ¶ï¼Œä½¿ç”¨ä¸€ä¸ªå¾ˆå¤§çš„æ•°
    effective_max = max_results if max_results > 0 else 100000
    
    return query_arxiv(tags, (start_str, end_str), max_results=effective_max)


def main():
    parser = argparse.ArgumentParser(description="arXiv å†å²æ•°æ®è·å–å·¥å…·")
    
    # æ—¥æœŸé€‰æ‹©å‚æ•°ï¼ˆäº’æ–¥ï¼‰
    date_group = parser.add_mutually_exclusive_group(required=True)
    date_group.add_argument("--months-ago", type=int, 
                           help="è·å–å‡ ä¸ªæœˆå‰çš„æ•°æ® (1-24)")
    date_group.add_argument("--date", type=str,
                           help="è·å–æŒ‡å®šæ—¥æœŸçš„æ•°æ® (æ ¼å¼: YYYY-MM-DD)")
    date_group.add_argument("--month", type=str,
                           help="è·å–æ•´ä¸ªæœˆçš„æ•°æ® (æ ¼å¼: YYYY-MM)")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--days", type=int, default=7,
                       help="è·å–å¤šå°‘å¤©çš„æ•°æ® (é»˜è®¤: 7)")
    parser.add_argument("--max-results", type=int, default=2000,
                       help="æ¯æ¬¡æŸ¥è¯¢æœ€å¤§è®ºæ–‡æ•° (é»˜è®¤: 2000ï¼Œè®¾ä¸º0è¡¨ç¤ºä¸é™åˆ¶)")
    parser.add_argument("--tags-file", default="tags.json",
                       help="åˆ†ç±»é…ç½®æ–‡ä»¶ (é»˜è®¤: tags.json)")
    parser.add_argument("--interests-file", default="interests.json",
                       help="å…´è¶£é…ç½®æ–‡ä»¶ (é»˜è®¤: interests.json)")
    parser.add_argument("--no-filter", action="store_true",
                       help="ä¸ä½¿ç”¨å…´è¶£ç­›é€‰")
    parser.add_argument("--skip-ai", action="store_true",
                       help="è·³è¿‡AIåˆ†æ")
    parser.add_argument("--concurrency", type=int, default=8,
                       help="AIåˆ†æå¹¶å‘æ•° (é»˜è®¤: 8)")
    parser.add_argument("--temperature", type=float, default=0.2,
                       help="AIç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.2)")
    parser.add_argument("--no-images", action="store_true",
                       help="ä¸æå–è®ºæ–‡å›¾ç‰‡ï¼ˆé»˜è®¤ä¼šæå–ï¼‰")
    parser.add_argument("--max-images", type=int, default=3,
                       help="æ¯ç¯‡è®ºæ–‡æœ€å¤šæå–å›¾ç‰‡æ•° (é»˜è®¤: 3)")
    
    args = parser.parse_args()
    
    # åŠ è½½åˆ†ç±»æ ‡ç­¾
    tags = load_tags(args.tags_file)
    print(f"[INFO] ç›®æ ‡åˆ†ç±»: {tags}")
    
    # ç¡®å®šæ—¥æœŸèŒƒå›´
    if args.months_ago:
        if not 1 <= args.months_ago <= 24:
            print("âŒ --months-ago å¿…é¡»åœ¨ 1-24 ä¹‹é—´")
            return
        start, end, label_date = get_date_range_from_months_ago(args.months_ago, args.days)
        date_ranges = [(start, end, label_date)]
        print(f"\nğŸ“† ç›®æ ‡: {args.months_ago} ä¸ªæœˆå‰ï¼Œè·å– {args.days} å¤©æ•°æ®")
        
    elif args.date:
        start, end, label_date = get_date_range_from_date(args.date, args.days)
        date_ranges = [(start, end, label_date)]
        print(f"\nğŸ“† ç›®æ ‡: {args.date}ï¼Œè·å– {args.days} å¤©æ•°æ®")
        
    elif args.month:
        try:
            year, month = map(int, args.month.split("-"))
            date_ranges = get_month_weeks(year, month)
            print(f"\nğŸ“† ç›®æ ‡: {args.month} æ•´æœˆï¼Œåˆ† {len(date_ranges)} å‘¨è·å–")
        except ValueError:
            print("âŒ --month æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM")
            return
    
    # è·å–è®ºæ–‡
    all_papers = []
    
    for start, end, label_date in date_ranges:
        result = fetch_papers_for_range(tags, start, end, args.max_results)
        papers = result.get("papers", [])
        print(f"[INFO] è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        all_papers.extend(papers)
    
    # å»é‡
    seen_ids = set()
    unique_papers = []
    for p in all_papers:
        arxiv_id = p.get("arxiv_id", "")
        if arxiv_id not in seen_ids:
            seen_ids.add(arxiv_id)
            unique_papers.append(p)
    
    print(f"\n[INFO] æ€»è®¡è·å– {len(unique_papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
    
    if len(unique_papers) == 0:
        print("[WARN] æ²¡æœ‰è·å–åˆ°è®ºæ–‡ï¼Œé€€å‡º")
        return
    
    # å…´è¶£ç­›é€‰
    if not args.no_filter:
        print(f"\nğŸ¯ æ­£åœ¨æ ¹æ®å…´è¶£ç­›é€‰è®ºæ–‡...")
        unique_papers = filter_by_interests(unique_papers, args.interests_file)
    
    if len(unique_papers) == 0:
        print("[WARN] ç­›é€‰åæ²¡æœ‰åŒ¹é…çš„è®ºæ–‡ï¼Œé€€å‡º")
        return
    
    # ç¡®å®šä¿å­˜ç›®å½•
    if args.month:
        save_label = args.month
    elif args.date:
        save_label = args.date
    else:
        save_label = label_date
    
    # ä¿å­˜åŸå§‹æ•°æ®
    os.makedirs(f'data/{save_label}', exist_ok=True)
    arxiv_path = f'data/{save_label}/arxiv.json'
    
    save_data = {
        "count": len(unique_papers),
        "papers": unique_papers,
        "query_info": {
            "tags": tags,
            "date_ranges": [(s.isoformat(), e.isoformat(), l) for s, e, l in date_ranges],
            "filtered": not args.no_filter
        }
    }
    
    with open(arxiv_path, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    print(f"[OK] ä¿å­˜åŸå§‹æ•°æ®: {arxiv_path}")
    
    # AI åˆ†æ
    if args.skip_ai:
        print("[INFO] è·³è¿‡AIåˆ†æ")
        print(f"\nâœ… å®Œæˆï¼æ•°æ®ä¿å­˜åœ¨: data/{save_label}/")
        return
    
    print(f"\nğŸ¤– æ­£åœ¨è¿›è¡Œ AI åˆ†æ...")
    print(f"[INFO] å¹¶å‘æ•°: {args.concurrency}, æ¸©åº¦: {args.temperature}")
    
    results = asyncio.run(update_ai_summary_async(
        metas=unique_papers,
        concurrency=args.concurrency,
        temperature=args.temperature
    ))
    
    # å›¾ç‰‡æå–ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    if not args.no_images:
        print(f"\nğŸ–¼ï¸ æ­£åœ¨æå–è®ºæ–‡å›¾ç‰‡...")
        image_results = batch_extract_images(
            papers=results,
            max_images_per_paper=args.max_images,
            concurrency=5
        )
        
        # å°†å›¾ç‰‡ä¿¡æ¯åˆå¹¶åˆ°ç»“æœä¸­
        for paper in results:
            arxiv_id = paper.get("arxiv_id", "")
            if arxiv_id in image_results:
                paper["figures"] = image_results[arxiv_id]
        
        img_count = sum(len(v) for v in image_results.values())
        print(f"[OK] æå–å›¾ç‰‡: {img_count} å¼  (æ¥è‡ª {len(image_results)} ç¯‡è®ºæ–‡)")
    
    # ä¿å­˜AIåˆ†æç»“æœ
    ai_path = f'data/{save_label}/ai_summary.json'
    with open(ai_path, 'w', encoding='utf-8') as f:
        json.dump({"papers": results}, f, ensure_ascii=False, indent=4)
    print(f"[OK] ä¿å­˜AIåˆ†æ: {ai_path}")
    
    # ç»Ÿè®¡ç»“æœ
    ok_count = sum(1 for r in results if "_model_error" not in r and "_parse_error" not in r)
    err_count = len(results) - ok_count
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"   ğŸ“„ è®ºæ–‡æ•°: {len(results)}")
    print(f"   âœ“ æˆåŠŸåˆ†æ: {ok_count}")
    if err_count:
        print(f"   âœ— åˆ†æå¤±è´¥: {err_count}")
    print(f"\nğŸ“ æ•°æ®ç›®å½•: data/{save_label}/")
    print(f"   - arxiv.json       (åŸå§‹æ•°æ®)")
    print(f"   - ai_summary.json  (AIåˆ†æç»“æœ)")
    print(f"\nğŸ‘‰ è¿è¡Œ 'python build_page.py' ç”Ÿæˆç½‘ç«™é¡µé¢")


if __name__ == '__main__':
    main()

